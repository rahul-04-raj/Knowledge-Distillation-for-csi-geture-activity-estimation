{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31790f6d-8873-4c20-9f52-583b46ac13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cf8e1f-be9a-45d0-95e6-b8ff825c236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdb459",
   "metadata": {},
   "source": [
    "# data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "161e6a00-1b4c-4549-934d-3c875ae51d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture = joblib.load(\"csi_up_data.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3722be49-2f82-4ae4-889e-79734109215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = joblib.load(\"labels_data.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "996b3e97-08c0-41e2-9063-1a24019819e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 30, 3, 2760)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gesture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "545d9800-2876-40de-bc75-6dab0e8ad770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.248446951029349+9.530279344393342j)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gesture[0][0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c1b59c9-bb1e-47a0-8bec-14c40a55a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude = np.zeros(shape = (200, 30, 3, 2760))\n",
    "for timestep in range(200):\n",
    "    for antenna in range(30):\n",
    "        for subcar in range(3):\n",
    "            for sample in range(2760):\n",
    "                amplitude[timestep][antenna][subcar][sample] = abs(gesture[timestep][antenna][subcar][sample])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0af9f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = np.zeros(shape = (200, 30, 3, 2760))\n",
    "for timestep in range(200):\n",
    "    for antenna in range(30):\n",
    "        for subcar in range(3):\n",
    "            for sample in range(2760):\n",
    "                phase[timestep][antenna][subcar][sample] = np.angle(gesture[timestep][antenna][subcar][sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b2626d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 30, 3, 2760), (200, 30, 3, 2760))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(amplitude), np.shape(phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "273b872a-e0dc-40fb-8bbd-1296bce6f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data = np.zeros(shape = (200, 3, 30, 2760))\n",
    "csi_amp = np.zeros(shape = (2760, 200, 90))\n",
    "\n",
    "for sample in range(2760):\n",
    "  for timestep in range(200):\n",
    "    for antenna in range(30):\n",
    "      for subcar in range(3):\n",
    "        csi_amp[sample][timestep][3*antenna+subcar] = amplitude[timestep][antenna][subcar][sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b3281c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.zeros(shape = (200, 3, 30, 5520))\n",
    "csi_phase = np.zeros(shape = (2760, 200, 90))\n",
    "\n",
    "for sample in range(2760):\n",
    "  for timestep in range(200):\n",
    "    for antenna in range(30):\n",
    "      for subcar in range(3):\n",
    "        csi_phase[sample][timestep][3*antenna+subcar] = phase[timestep][antenna][subcar][sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ba39ca3a-24fd-4245-bccc-238bbf2af1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2760, 200, 90), (2760, 200, 90))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(csi_amp), np.shape(csi_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "03b362ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of top 10 subcarriers with highest amplitude variance: [ 1 37 85 88 28 40 34 82 43 79 25 31 55 76 46  4 22 58  7 73 49 74 77 26\n",
      "  0 52 23 70 71  2 19 29 38 68 62 14 41 89 67 11 10 65 80 86 16 20 83 35\n",
      " 17 13 32 59 44 61 64  8 56  5 47 53 75 72 42 60 39 24 27 78 36 45 81 63\n",
      " 12 84  9 57 21 48 50 33 18 69 66 30 51  3 15 54 87  6]\n",
      "Indices of top 10 subcarriers with highest phase variance: [ 0  1  2 53 50 56 80 77  5 20 59 23 71 21 22 34 74 33 47 17 10 29 26 35\n",
      " 61 11 62 64 70 89 67  8 68 46 65 44 49 85 88 58 83 32 40 41 86 69 73 25\n",
      "  4 19 52 16 13 87 37 75 72 36 43 48 76 38 82 14  9 18 31 78 81 84 51 79\n",
      " 55 28 57 66 15  7 45 54 39 24 12 60 63 42  6 30  3 27]\n"
     ]
    }
   ],
   "source": [
    "amplitude_variances = np.var(csi_amp, axis=(0, 1))\n",
    "phase_variances = np.var(csi_phase, axis=(0, 1))\n",
    "\n",
    "# Find the indices of the 10 subcarriers with the highest variance for amplitude and phase\n",
    "top_10_amplitude_indices = np.argsort(amplitude_variances)\n",
    "top_10_phase_indices = np.argsort(phase_variances)\n",
    "\n",
    "# Print the indices of the top 10 subcarriers with the highest variance\n",
    "print(\"Indices of top 10 subcarriers with highest amplitude variance:\", top_10_amplitude_indices)\n",
    "print(\"Indices of top 10 subcarriers with highest phase variance:\", top_10_phase_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "6a967343",
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_amp_3 = np.array(csi_amp)[:, :,[30,3,27]]\n",
    "csi_phase_3 = np.array(csi_phase)[:,:,[30,3,27]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "abd1ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.concatenate((csi_amp_3, csi_phase_3), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "4fb1d1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2760, 200, 6)"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ad3a0b8d-6943-4443-8b79-1eec7b0a7c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1],\n",
       "       [  2],\n",
       "       [  3],\n",
       "       ...,\n",
       "       [274],\n",
       "       [275],\n",
       "       [276]], dtype=uint16)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df94cbfd-8e1d-46d8-b967-cdead23bcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "b48ce29c-f3d1-402c-8b91-3e168bed3a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "79eb67e7-d67e-4d9f-b2e0-8356e4f154a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(zip(combined, labels))\n",
    "random.shuffle(temp)\n",
    "combined, labels = zip(*temp)\n",
    "combined, labels = list(combined), list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "c09025ac-e676-490e-8f6e-f8a77bd08966",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.constant(labels[:1500])\n",
    "y_val = tf.constant(labels[1500:2000])\n",
    "y_test = tf.constant(labels[2000:2760])\n",
    "x_train = (combined[:1500])\n",
    "x_val = (combined[1500:2000])\n",
    "x_test = (combined[2000:2760])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "432ee75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "x_train shape: (1500, 200, 6)\n",
      "x_val shape: (500, 200, 6)\n",
      "x_test shape: (760, 200, 6)\n",
      "y_train shape: (1500, 1)\n",
      "y_val shape: (500, 1)\n",
      "y_test shape: (760, 1)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes (optional)\n",
    "print(\"Shapes:\")\n",
    "print(\"x_train shape:\", np.shape(x_train))\n",
    "print(\"x_val shape:\", np.shape(x_val))\n",
    "print(\"x_test shape:\", np.shape(x_test))\n",
    "print(\"y_train shape:\", np.shape(y_train))\n",
    "print(\"y_val shape:\", np.shape(y_val))\n",
    "print(\"y_test shape:\", np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "391adf0f-04a7-4a0f-a2e4-d623aef5289f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0241417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 200, 6)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "369f68e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 200, 6)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b9e12",
   "metadata": {},
   "source": [
    "# trying models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7e15f1f9-d502-49e9-b93f-47c181fa21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalMaxPooling1D, Masking, Input\n",
    "model_1 = models.Sequential([\n",
    "    # layers.Masking(mask_value = 0.0, input_shape = (None,90), ragged = True),\n",
    "    # layers.Input([None, 90], dtype=tf.float16, ragged=True),\n",
    "    layers.Conv1D(filters=128, kernel_size=(1), activation='relu', input_shape = (200, 90)),\n",
    "    layers.MaxPooling1D((2)),\n",
    "    \n",
    "    layers.Conv1D(filters=64, kernel_size=(1), activation='relu'),\n",
    "    layers.MaxPooling1D((2)),\n",
    "    # layers.BatchNormalization(),\n",
    "    \n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(276, activation='softmax')\n",
    "])\n",
    "\n",
    "model_1.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1fed3971-f01f-499b-9a6e-ceece590a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_55 (Conv1D)          (None, 200, 128)          11648     \n",
      "                                                                 \n",
      " max_pooling1d_33 (MaxPooli  (None, 100, 128)          0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_56 (Conv1D)          (None, 100, 64)           8256      \n",
      "                                                                 \n",
      " max_pooling1d_34 (MaxPooli  (None, 50, 64)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 64)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 276)               17940     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42004 (164.08 KB)\n",
      "Trainable params: 42004 (164.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9f2f1a7c-21a3-4d25-9dd8-1f369f208c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "54c374d6-832c-4593-a1c4-07bea5e7a872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 200, 90), found shape=(None, 200, 6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_1\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(x_train), np\u001b[39m.\u001b[39;49marray(y_train), epochs \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/pv/wvqp_ty90s5dnpzrbzh6mkt00000gn/T/__autograph_generated_file219_5gd_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 200, 90), found shape=(None, 200, 6)\n"
     ]
    }
   ],
   "source": [
    "model_1.fit(np.array(x_train), np.array(y_train), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0ef70-7ff5-419a-b58b-1afffdc90637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 3.8359 - accuracy: 0.2895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.8358521461486816, 0.28947368264198303]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f7ad0-d1bd-4dc0-afa5-567181303a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalMaxPooling1D, Masking, Input\n",
    "model_2 = models.Sequential([\n",
    "    # layers.Masking(mask_value = 0.0, input_shape = (None,90), ragged = True),\n",
    "    # layers.Input([None, 90], dtype=tf.float16, ragged=True),\n",
    "    layers.Conv1D(filters=32, kernel_size=(3), activation='relu', input_shape = (200, 90)),\n",
    "    layers.MaxPooling1D((2)),\n",
    "    \n",
    "    layers.Conv1D(filters=64, kernel_size=(3), activation='relu'),\n",
    "    # layers.MaxPooling1D((2)),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(276, activation='softmax')\n",
    "])\n",
    "# sgdm_optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model_2.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=0.1, momentum=0.9),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1d305-6820-461f-bff9-91788fdc3e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2.fit(np.array(x_train), np.array(y_train), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb11f69-1f17-4968-a337-2422751bf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a622e2d9-695d-425e-a688-8db1afa95700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 90)]            0         []                            \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDi  (None, 200, 32)              2912      ['input_1[0][0]']             \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 200, 32)              2912      ['input_1[0][0]']             \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDi  (None, 200, 32)              0         ['time_distributed_2[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 200, 32)              0         ['time_distributed[0][0]']    \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " query (TimeDistributed)     (None, 200, 32)              1056      ['time_distributed_3[0][0]']  \n",
      "                                                                                                  \n",
      " key (TimeDistributed)       (None, 200, 32)              1056      ['time_distributed_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.linalg.matmul (TFOpLamb  (None, 200, 200)             0         ['query[0][0]',               \n",
      " da)                                                                 'key[0][0]']                 \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDi  (None, 200, 32)              2912      ['input_1[0][0]']             \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 200, 200)             0         ['tf.linalg.matmul[0][0]']    \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDi  (None, 200, 32)              0         ['time_distributed_4[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " tf.nn.softmax (TFOpLambda)  (None, 200, 200)             0         ['tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " value (TimeDistributed)     (None, 200, 32)              1056      ['time_distributed_5[0][0]']  \n",
      "                                                                                                  \n",
      " tf.linalg.matmul_1 (TFOpLa  (None, 200, 32)              0         ['tf.nn.softmax[0][0]',       \n",
      " mbda)                                                               'value[0][0]']               \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDi  (None, 200, 276)             9108      ['tf.linalg.matmul_1[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21012 (82.08 KB)\n",
      "Trainable params: 21012 (82.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM, TimeDistributed\n",
    "\n",
    "def transformer_mid():\n",
    "  inp = Input(shape = (200, 90))\n",
    "\n",
    "  #   Q, K, V (non-linear)\n",
    "  #inp = Reshape((620, 1))(inp)\n",
    "  x_k = layers.TimeDistributed(Dense(32, activation = 'relu'))(inp)\n",
    "  x_k = TimeDistributed(Dropout(0.4))(x_k)\n",
    "  x_k = layers.TimeDistributed(Dense(32, activation = 'relu'), name='key')(x_k)\n",
    "\n",
    "  x_q = layers.TimeDistributed(Dense(32, activation = 'relu'))(inp)\n",
    "  x_q = TimeDistributed(Dropout(0.4))(x_q)\n",
    "  x_q = layers.TimeDistributed(Dense(32, activation = 'relu'), name='query')(x_q)\n",
    "\n",
    "  x_v = layers.TimeDistributed(Dense(32, activation = 'relu'))(inp)\n",
    "  x_v = TimeDistributed(Dropout(0.4))(x_v)\n",
    "  x_v = layers.TimeDistributed(Dense(32, activation = 'relu'), name='value')(x_v)\n",
    "\n",
    "  #   QKT/sqrt(d_model)\n",
    "  kqT = tf.linalg.matmul(x_q, x_k, transpose_b=True, name='QKT')*tf.constant(0.176)\n",
    "\n",
    "  #   ROW-WISE SOFTMAX\n",
    "  sfmx = tf.nn.softmax(kqT, axis=1)\n",
    "  attn = tf.linalg.matmul(sfmx, x_v, name='penultimate')\n",
    "\n",
    "  #   FINAL OUTPUT\n",
    "  out = layers.TimeDistributed(Dense(276))(attn)\n",
    "  model = Model(inputs = inp, outputs = out)\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "m = transformer_mid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a4c941-6ec4-4965-9ae6-62cae494e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00315cf5-b6b7-4afd-9604-8f9b7e26aee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 200, 90)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d13f7c-9dfe-4065-adb7-647e3ea515e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(np.array(x_train), np.array(y_train), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a90bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_gd = load_model('model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58935be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 6ms/step - loss: 90.5095 - accuracy: 0.0080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[90.5095443725586, 0.00800000037997961]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gd.evaluate(np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2a5ba",
   "metadata": {},
   "source": [
    "# model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ecfbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalMaxPooling1D, Masking, Input\n",
    "\n",
    "model_2 = models.Sequential([\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv1D(filters=32, kernel_size=(7), activation='relu', input_shape = (200, 90)),\n",
    "    layers.MaxPooling1D((2)),\n",
    "\n",
    "    layers.Conv1D(filters=64, kernel_size=(4), activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=(4), activation='relu'),\n",
    "    layers.MaxPooling1D((2)),\n",
    "    \n",
    "    layers.Conv1D(filters=128, kernel_size=(4), activation='relu'),\n",
    "    layers.Conv1D(filters=128, kernel_size=(4), activation='relu'),\n",
    "    layers.MaxPooling1D((2)),\n",
    "    \n",
    "    layers.GlobalMaxPooling1D(),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(276, activation='softmax')\n",
    "])\n",
    "\n",
    "model_2.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ffb2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 2s 19ms/step - loss: 5.3743 - accuracy: 0.0150\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 4.0580 - accuracy: 0.0620\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 3.1907 - accuracy: 0.1625\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 2.7618 - accuracy: 0.2235\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 2.4239 - accuracy: 0.3050\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 1.9727 - accuracy: 0.4290\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.6213 - accuracy: 0.5100\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.3050 - accuracy: 0.6160\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 1.1238 - accuracy: 0.6675\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.9121 - accuracy: 0.7200\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.7972 - accuracy: 0.7510\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.7909 - accuracy: 0.7665\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.9237 - accuracy: 0.7270\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.6683 - accuracy: 0.8020\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.6822 - accuracy: 0.8010\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.5777 - accuracy: 0.8270\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.5065 - accuracy: 0.8445\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.5145 - accuracy: 0.8515\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.4845 - accuracy: 0.8555\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.4281 - accuracy: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29a4d3b50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(np.array(x_train), np.array(y_train), epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b16dbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 7ms/step - loss: 0.5856 - accuracy: 0.8158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5856219530105591, 0.8157894611358643]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8df83ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (Batch  (None, 200, 90)           360       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 194, 32)           20192     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 97, 32)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 94, 64)            8256      \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 91, 64)            16448     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 45, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 42, 128)           32896     \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 39, 128)           65664     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 19, 128)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 276)               35604     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 195932 (765.36 KB)\n",
      "Trainable params: 195752 (764.66 KB)\n",
      "Non-trainable params: 180 (720.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee2d5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5,4,3,2)\n",
    "b = np.random.rand(2,2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39fb3ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.31897836, 0.11064455, 0.87486736],\n",
       "         [0.1601562 , 0.53903031, 0.43403518]],\n",
       "\n",
       "        [[0.55491494, 0.87925592, 0.77658073],\n",
       "         [0.0409227 , 0.26965137, 0.29632147]]],\n",
       "\n",
       "\n",
       "       [[[0.46670649, 0.99980276, 0.25899502],\n",
       "         [0.87074491, 0.68126889, 0.50476343]],\n",
       "\n",
       "        [[0.43342808, 0.5364656 , 0.73412034],\n",
       "         [0.43642995, 0.3495513 , 0.71503663]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8aaa1b7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd328ae",
   "metadata": {},
   "source": [
    "# running model_2 on only on subcarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccb59264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 200, 90)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caa820f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d0df179",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_1_train = x_train[:, :, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10ee78a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 200, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5dc409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_1_test = np.array(x_test)[:, :, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc0ab2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 200, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_1_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8f15a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be3ce613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalMaxPooling1D, Masking, Input\n",
    "\n",
    "model_2_sub = models.Sequential([\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv1D(filters=32, kernel_size=(7), activation='relu', input_shape = (200, 1)),\n",
    "    layers.MaxPooling1D((2)),\n",
    "\n",
    "    layers.Conv1D(filters=64, kernel_size=(4), activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=(4), activation='relu'),\n",
    "    layers.MaxPooling1D((2)),\n",
    "    \n",
    "    layers.Conv1D(filters=128, kernel_size=(4), activation='relu'),\n",
    "    layers.Conv1D(filters=128, kernel_size=(4), activation='relu'),\n",
    "    layers.MaxPooling1D((2)),\n",
    "    \n",
    "    layers.GlobalMaxPooling1D(),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(276, activation='softmax')\n",
    "])\n",
    "\n",
    "model_2_sub.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0408c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 5.5088 - accuracy: 0.0050\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 4.8423 - accuracy: 0.0170\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 4.4619 - accuracy: 0.0380\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 4.1162 - accuracy: 0.0765\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 3.7090 - accuracy: 0.1375\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 3.2600 - accuracy: 0.2260\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.9632 - accuracy: 0.2775\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.5826 - accuracy: 0.3560\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.3219 - accuracy: 0.3970\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.1450 - accuracy: 0.4275\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.2073 - accuracy: 0.4060\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.8132 - accuracy: 0.5105\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.6181 - accuracy: 0.5630\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.5999 - accuracy: 0.5515\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.4361 - accuracy: 0.5895\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.3025 - accuracy: 0.6325\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.2533 - accuracy: 0.6410\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.9961 - accuracy: 0.7145\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.9516 - accuracy: 0.7345\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.7917 - accuracy: 0.7705\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.9075 - accuracy: 0.7435\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.7287 - accuracy: 0.7905\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.5908 - accuracy: 0.8240\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.5497 - accuracy: 0.8335\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.8103 - accuracy: 0.7660\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4854 - accuracy: 0.8590\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4681 - accuracy: 0.8605\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4608 - accuracy: 0.8580\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4591 - accuracy: 0.8665\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4496 - accuracy: 0.8640\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.3295 - accuracy: 0.8960\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2911 - accuracy: 0.9085\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.2696 - accuracy: 0.9150\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.3293 - accuracy: 0.9005\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.3942 - accuracy: 0.8755\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.3101 - accuracy: 0.9040\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4559 - accuracy: 0.8620\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.4023 - accuracy: 0.8745\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.2422 - accuracy: 0.9200\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2176 - accuracy: 0.9290\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.2060 - accuracy: 0.9295\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.1937 - accuracy: 0.9350\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2266 - accuracy: 0.9245\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2336 - accuracy: 0.9235\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2153 - accuracy: 0.9330\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.2419 - accuracy: 0.9175\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2021 - accuracy: 0.9310\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.1726 - accuracy: 0.9375\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2675 - accuracy: 0.9100\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2968 - accuracy: 0.9055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3243a59d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_sub.fit(np.array(sub_1_train), np.array(y_train), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4179b919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step - loss: 1.3610 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3609789609909058, 0.75]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_sub.evaluate(np.array(sub_1_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d4cb5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 200, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_1_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "84a5fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalAveragePooling1D, Masking, Input, MaxPooling1D, GlobalMaxPooling1D, Add, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a624b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d79f2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 1))\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu')(dataInp) # 194, 32\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid')(d_T)\n",
    "\n",
    "model = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "acf72526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)       [(None, 200, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_66 (Conv1D)          (None, 194, 32)              256       ['input_11[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_39 (MaxPooli  (None, 97, 32)               0         ['conv1d_66[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " conv1d_67 (Conv1D)          (None, 93, 64)               10304     ['max_pooling1d_39[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d_40 (MaxPooli  (None, 46, 64)               0         ['conv1d_67[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " conv1d_68 (Conv1D)          (None, 44, 256)              49408     ['max_pooling1d_40[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_69 (Conv1D)          (None, 42, 32)               24608     ['conv1d_68[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_41 (MaxPooli  (None, 21, 32)               0         ['conv1d_69[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " conv1d_70 (Conv1D)          (None, 19, 512)              49664     ['max_pooling1d_41[0][0]']    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_6  (None, 512)                  0         ['conv1d_70[0][0]']           \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " global_max_pooling1d_10 (G  (None, 512)                  0         ['conv1d_70[0][0]']           \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 512)                  0         ['global_average_pooling1d_6[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'global_max_pooling1d_10[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 300)                  153900    ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 300)                  0         ['dense_17[0][0]']            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 276)                  83076     ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 371216 (1.42 MB)\n",
      "Trainable params: 371216 (1.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e269d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 5.6534 - accuracy: 0.0030\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 5.3977 - accuracy: 0.0085\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 5.0793 - accuracy: 0.0090\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 4.8573 - accuracy: 0.0175\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 4.5975 - accuracy: 0.0425\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 4.3219 - accuracy: 0.0740\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 4.0639 - accuracy: 0.1035\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 3.7955 - accuracy: 0.1585\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 3.6189 - accuracy: 0.1880\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 3.4206 - accuracy: 0.2140\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 3.3371 - accuracy: 0.2430\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 3.1648 - accuracy: 0.2660\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 3.0140 - accuracy: 0.3055\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.8531 - accuracy: 0.3330\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.7730 - accuracy: 0.3485\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.6764 - accuracy: 0.3615\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.7064 - accuracy: 0.3630\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.5363 - accuracy: 0.3845\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.4061 - accuracy: 0.4130\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.3539 - accuracy: 0.4255\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.2708 - accuracy: 0.4430\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.1584 - accuracy: 0.4595\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.0521 - accuracy: 0.4870\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.0743 - accuracy: 0.4735\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.9987 - accuracy: 0.4775\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.8804 - accuracy: 0.5050\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.7815 - accuracy: 0.5390\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.7523 - accuracy: 0.5295\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.6803 - accuracy: 0.5610\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.5998 - accuracy: 0.5660\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.5330 - accuracy: 0.5820\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.6187 - accuracy: 0.5460\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.4726 - accuracy: 0.6025\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.5098 - accuracy: 0.5860\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.3762 - accuracy: 0.6130\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.3251 - accuracy: 0.6225\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.3179 - accuracy: 0.6260\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.2607 - accuracy: 0.6440\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.2063 - accuracy: 0.6570\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.1394 - accuracy: 0.6730\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.0845 - accuracy: 0.6850\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.0901 - accuracy: 0.6680\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.0991 - accuracy: 0.6760\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 1.1181 - accuracy: 0.6885\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.9855 - accuracy: 0.7185\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.9514 - accuracy: 0.7175\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.9388 - accuracy: 0.7075\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.9059 - accuracy: 0.7305\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.8617 - accuracy: 0.7470\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.8408 - accuracy: 0.7460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x32be34490>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(sub_1_train), np.array(y_train), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ec7c9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step - loss: 1.9151 - accuracy: 0.6303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9150515794754028, 0.6302631497383118]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(sub_1_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f082993",
   "metadata": {},
   "source": [
    "# testing model on 3 sub carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59ab81da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1500, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c22e0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 200, 90)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff6d48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_3_train = np.array(x_train)[:, :, [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfd0750d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 200, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_3_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f49412bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_3_test = np.array(x_test)[:, :, [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "502ff462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 200, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_3_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "18550c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalAveragePooling1D, Masking, Input, MaxPooling1D, GlobalMaxPooling1D, Add, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "901ac06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 3))\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "# avg_T = GlobalAveragePooling1D()(x_T)\n",
    "# max_T = GlobalMaxPooling1D()(x_T)\n",
    "# flat_T = Add()([avg_T, max_T])\n",
    "flat_T = Flatten()(x_T)\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid')(d_T)\n",
    "\n",
    "model_sub3 = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model_sub3.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7e1d7721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 5.4901 - accuracy: 0.0120\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 4.5577 - accuracy: 0.0387\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 3.7452 - accuracy: 0.1193\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 3.2963 - accuracy: 0.1673\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 2.8895 - accuracy: 0.2353\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 2.5547 - accuracy: 0.3020\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 2.2940 - accuracy: 0.3573\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 2.1183 - accuracy: 0.4093\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 1.8598 - accuracy: 0.4680\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 1.6704 - accuracy: 0.5080\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 1.4978 - accuracy: 0.5560\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 1.3641 - accuracy: 0.5893\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 1.1658 - accuracy: 0.6447\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 1.1098 - accuracy: 0.6713\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 1.0216 - accuracy: 0.6807\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.8511 - accuracy: 0.7320\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.7478 - accuracy: 0.7727\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6949 - accuracy: 0.7907\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.6803 - accuracy: 0.7953\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.6238 - accuracy: 0.8153\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5470 - accuracy: 0.8260\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5048 - accuracy: 0.8473\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.4962 - accuracy: 0.8467\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.4831 - accuracy: 0.8540\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.4772 - accuracy: 0.8607\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.5400 - accuracy: 0.8393\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.4645 - accuracy: 0.8653\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.4642 - accuracy: 0.8613\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.4800 - accuracy: 0.8487\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.4462 - accuracy: 0.8667\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.3523 - accuracy: 0.8987\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.3391 - accuracy: 0.8927\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.3421 - accuracy: 0.8960\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.2989 - accuracy: 0.9033\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.3598 - accuracy: 0.8887\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.3925 - accuracy: 0.8847\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.3576 - accuracy: 0.8907\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.3455 - accuracy: 0.8960\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.3229 - accuracy: 0.9033\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.2736 - accuracy: 0.9140\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.2682 - accuracy: 0.9187\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.2241 - accuracy: 0.9253\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.2800 - accuracy: 0.9127\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.2299 - accuracy: 0.9260\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.2386 - accuracy: 0.9267\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.2123 - accuracy: 0.9320\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.2522 - accuracy: 0.9213\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.1769 - accuracy: 0.9440\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.1778 - accuracy: 0.9420\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.2356 - accuracy: 0.9307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x381faf810>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sub3.fit(np.array(x_train), np.array(y_train), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "189f7b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44081810116767883, 0.8999999761581421]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sub3.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c505146a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "results  = model_sub3.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b3cf038c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 276)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fbc45190",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = []\n",
    "for i in range(500):\n",
    "    y_pred_1.append(np.argmax(results[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "54ea491d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5d955e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([500, 1])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "de6b9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3936c1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_val, y_pred_1)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sub3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c89c6",
   "metadata": {},
   "source": [
    "# trying softmax on same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f1f16b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 3))\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'softmax')(d_T)\n",
    "\n",
    "model_sub3_soft = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model_sub3_soft.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fcb9f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 5.5110 - accuracy: 0.0075\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 4.7225 - accuracy: 0.0255\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 4.0080 - accuracy: 0.0690\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 3.5339 - accuracy: 0.0995\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 3.1796 - accuracy: 0.1565\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 2.9026 - accuracy: 0.2165\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 2.6411 - accuracy: 0.2625\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 2.4260 - accuracy: 0.3045\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 2.2385 - accuracy: 0.3535\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 2.1219 - accuracy: 0.3680\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 1.9092 - accuracy: 0.4260\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.7384 - accuracy: 0.4760\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.6473 - accuracy: 0.5180\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.5309 - accuracy: 0.5380\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.3237 - accuracy: 0.5860\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 1.2350 - accuracy: 0.6210\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.0853 - accuracy: 0.6590\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 1.0950 - accuracy: 0.6715\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.9700 - accuracy: 0.6995\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.9113 - accuracy: 0.7145\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.8386 - accuracy: 0.7525\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.7580 - accuracy: 0.7710\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.8280 - accuracy: 0.7480\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.7756 - accuracy: 0.7565\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.5785 - accuracy: 0.8260\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.6117 - accuracy: 0.8135\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.5521 - accuracy: 0.8340\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.5933 - accuracy: 0.8190\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.5487 - accuracy: 0.8370\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.5517 - accuracy: 0.8285\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.5463 - accuracy: 0.8390\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.4388 - accuracy: 0.8695\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.6200 - accuracy: 0.8195\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.5358 - accuracy: 0.8335\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.3833 - accuracy: 0.8845\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.4701 - accuracy: 0.8630\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.4494 - accuracy: 0.8650\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.3826 - accuracy: 0.8830\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.3278 - accuracy: 0.9055\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.3162 - accuracy: 0.9045\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.4002 - accuracy: 0.8780\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.3730 - accuracy: 0.8915\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.3584 - accuracy: 0.8840\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.3468 - accuracy: 0.8995\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.3337 - accuracy: 0.9005\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.2927 - accuracy: 0.9130\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 1s 14ms/step - loss: 0.2899 - accuracy: 0.9110\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 1s 13ms/step - loss: 0.2699 - accuracy: 0.9195\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.2911 - accuracy: 0.9080\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.3337 - accuracy: 0.8930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x32c4f89d0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sub3_soft.fit(np.array(sub_3_train), np.array(y_train), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8457e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step - loss: 0.5469 - accuracy: 0.8513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5469476580619812, 0.8513157963752747]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sub3_soft.evaluate(np.array(sub_3_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "528794d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)       [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_101 (Conv1D)         (None, 194, 32)              704       ['input_17[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 194, 32)              128       ['conv1d_101[0][0]']          \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_60 (MaxPooli  (None, 97, 32)               0         ['batch_normalization_8[0][0]'\n",
      " ng1D)                                                              ]                             \n",
      "                                                                                                  \n",
      " conv1d_102 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_60[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d_61 (MaxPooli  (None, 46, 64)               0         ['conv1d_102[0][0]']          \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " conv1d_103 (Conv1D)         (None, 44, 256)              49408     ['max_pooling1d_61[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_104 (Conv1D)         (None, 42, 32)               24608     ['conv1d_103[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_62 (MaxPooli  (None, 21, 32)               0         ['conv1d_104[0][0]']          \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " conv1d_105 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_62[0][0]']    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 512)                  0         ['conv1d_105[0][0]']          \n",
      " 2 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_17 (G  (None, 512)                  0         ['conv1d_105[0][0]']          \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 512)                  0         ['global_average_pooling1d_12[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_17[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_31 (Dense)            (None, 300)                  153900    ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 300)                  0         ['dense_31[0][0]']            \n",
      "                                                                                                  \n",
      " dense_32 (Dense)            (None, 276)                  83076     ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 371792 (1.42 MB)\n",
      "Trainable params: 371728 (1.42 MB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_sub3_soft.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc7c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2625a385",
   "metadata": {},
   "source": [
    "# concatenating globalmax and globalaverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bafe9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 200, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99e7ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "385d44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1ff80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalAveragePooling1D, Masking, Input, MaxPooling1D, GlobalMaxPooling1D, Concatenate, Dropout, BatchNormalization, Add,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ccd7ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "# avg_T = GlobalAveragePooling1D()(x_T)\n",
    "# max_T = GlobalMaxPooling1D()(x_T)\n",
    "# flat_T = Concatenate()([avg_T, max_T])\n",
    "flat_T = Flatten()(x_T)\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "d_T = Dense(300, activation = 'relu')(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid')(d_T)\n",
    "\n",
    "model_sub3_1 = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model_sub3_1.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c4fd6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 200, 6)]          0         \n",
      "                                                                 \n",
      " conv1d_77 (Conv1D)          (None, 194, 32)           1376      \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 194, 32)           128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling1d_47 (MaxPooli  (None, 97, 32)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_78 (Conv1D)          (None, 93, 64)            10304     \n",
      "                                                                 \n",
      " max_pooling1d_48 (MaxPooli  (None, 46, 64)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_31 (Ba  (None, 46, 64)            256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv1d_79 (Conv1D)          (None, 44, 256)           49408     \n",
      "                                                                 \n",
      " conv1d_80 (Conv1D)          (None, 42, 32)            24608     \n",
      "                                                                 \n",
      " max_pooling1d_49 (MaxPooli  (None, 21, 32)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_81 (Conv1D)          (None, 19, 512)           49664     \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 9728)              0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 300)               2918700   \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 276)               83076     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3227820 (12.31 MB)\n",
      "Trainable params: 3227628 (12.31 MB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_sub3_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5936753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 5.5475 - accuracy: 0.0120 - val_loss: 5.6675 - val_accuracy: 0.0160\n",
      "Epoch 2/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 4.4468 - accuracy: 0.1227 - val_loss: 7.1915 - val_accuracy: 0.0780\n",
      "Epoch 3/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 2.9882 - accuracy: 0.3293 - val_loss: 8.3388 - val_accuracy: 0.2000\n",
      "Epoch 4/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 2.0835 - accuracy: 0.4813 - val_loss: 4.9530 - val_accuracy: 0.2680\n",
      "Epoch 5/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 1.6208 - accuracy: 0.5767 - val_loss: 4.1054 - val_accuracy: 0.3900\n",
      "Epoch 6/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 1.2530 - accuracy: 0.6660 - val_loss: 3.3398 - val_accuracy: 0.4920\n",
      "Epoch 7/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.8488 - accuracy: 0.7687 - val_loss: 2.0925 - val_accuracy: 0.5480\n",
      "Epoch 8/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.6595 - accuracy: 0.8193 - val_loss: 1.2519 - val_accuracy: 0.6880\n",
      "Epoch 9/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.6747 - accuracy: 0.8180 - val_loss: 0.9469 - val_accuracy: 0.7260\n",
      "Epoch 10/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.5542 - accuracy: 0.8500 - val_loss: 1.0227 - val_accuracy: 0.7040\n",
      "Epoch 11/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.4611 - accuracy: 0.8640 - val_loss: 0.6595 - val_accuracy: 0.7960\n",
      "Epoch 12/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.4418 - accuracy: 0.8687 - val_loss: 0.4740 - val_accuracy: 0.8620\n",
      "Epoch 13/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.4057 - accuracy: 0.8873 - val_loss: 0.5146 - val_accuracy: 0.8420\n",
      "Epoch 14/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.3935 - accuracy: 0.8887 - val_loss: 0.4808 - val_accuracy: 0.8620\n",
      "Epoch 15/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 0.3533 - accuracy: 0.8927 - val_loss: 0.5035 - val_accuracy: 0.8560\n",
      "Epoch 16/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.3707 - accuracy: 0.9007 - val_loss: 0.4451 - val_accuracy: 0.8700\n",
      "Epoch 17/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.3517 - accuracy: 0.8973 - val_loss: 0.3902 - val_accuracy: 0.8820\n",
      "Epoch 18/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.3238 - accuracy: 0.8993 - val_loss: 0.4076 - val_accuracy: 0.8900\n",
      "Epoch 19/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.3193 - accuracy: 0.9020 - val_loss: 0.3523 - val_accuracy: 0.9000\n",
      "Epoch 20/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.2980 - accuracy: 0.9127 - val_loss: 0.3812 - val_accuracy: 0.8820\n",
      "Epoch 21/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.2681 - accuracy: 0.9207 - val_loss: 0.4083 - val_accuracy: 0.8840\n",
      "Epoch 22/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.2242 - accuracy: 0.9360 - val_loss: 0.3179 - val_accuracy: 0.9080\n",
      "Epoch 23/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.1802 - accuracy: 0.9513 - val_loss: 0.2620 - val_accuracy: 0.9300\n",
      "Epoch 24/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.1863 - accuracy: 0.9447 - val_loss: 0.3138 - val_accuracy: 0.9100\n",
      "Epoch 25/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.1365 - accuracy: 0.9580 - val_loss: 0.1738 - val_accuracy: 0.9460\n",
      "Epoch 26/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.1723 - accuracy: 0.9487 - val_loss: 0.3459 - val_accuracy: 0.9000\n",
      "Epoch 27/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.1970 - accuracy: 0.9393 - val_loss: 0.2659 - val_accuracy: 0.9160\n",
      "Epoch 28/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.1509 - accuracy: 0.9553 - val_loss: 0.1876 - val_accuracy: 0.9420\n",
      "Epoch 29/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.1464 - accuracy: 0.9487 - val_loss: 0.2439 - val_accuracy: 0.9300\n",
      "Epoch 30/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.0924 - accuracy: 0.9720 - val_loss: 0.1929 - val_accuracy: 0.9440\n",
      "Epoch 31/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 0.0747 - accuracy: 0.9760 - val_loss: 0.1296 - val_accuracy: 0.9660\n",
      "Epoch 32/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.0705 - accuracy: 0.9787 - val_loss: 0.1429 - val_accuracy: 0.9620\n",
      "Epoch 33/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.0896 - accuracy: 0.9707 - val_loss: 0.1485 - val_accuracy: 0.9580\n",
      "Epoch 34/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.1017 - accuracy: 0.9733 - val_loss: 0.1815 - val_accuracy: 0.9480\n",
      "Epoch 35/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.0667 - accuracy: 0.9813 - val_loss: 0.1491 - val_accuracy: 0.9600\n",
      "Epoch 36/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.0864 - accuracy: 0.9747 - val_loss: 0.2056 - val_accuracy: 0.9400\n",
      "Epoch 37/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.0614 - accuracy: 0.9800 - val_loss: 0.1109 - val_accuracy: 0.9580\n",
      "Epoch 38/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.0777 - accuracy: 0.9760 - val_loss: 0.1314 - val_accuracy: 0.9560\n",
      "Epoch 39/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.1173 - accuracy: 0.9647 - val_loss: 0.1740 - val_accuracy: 0.9560\n",
      "Epoch 40/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.1143 - accuracy: 0.9653 - val_loss: 0.1324 - val_accuracy: 0.9620\n",
      "Epoch 41/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.0683 - accuracy: 0.9840 - val_loss: 0.1067 - val_accuracy: 0.9740\n",
      "Epoch 42/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 0.0644 - accuracy: 0.9833 - val_loss: 0.1389 - val_accuracy: 0.9660\n",
      "Epoch 43/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.0365 - accuracy: 0.9893 - val_loss: 0.1344 - val_accuracy: 0.9660\n",
      "Epoch 44/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 0.0624 - accuracy: 0.9820 - val_loss: 0.1166 - val_accuracy: 0.9760\n",
      "Epoch 45/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0309 - accuracy: 0.9913 - val_loss: 0.1347 - val_accuracy: 0.9660\n",
      "Epoch 46/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0323 - accuracy: 0.9887 - val_loss: 0.0985 - val_accuracy: 0.9740\n",
      "Epoch 47/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0284 - accuracy: 0.9907 - val_loss: 0.0837 - val_accuracy: 0.9800\n",
      "Epoch 48/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0217 - accuracy: 0.9940 - val_loss: 0.0766 - val_accuracy: 0.9800\n",
      "Epoch 49/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0262 - accuracy: 0.9920 - val_loss: 0.0928 - val_accuracy: 0.9760\n",
      "Epoch 50/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0370 - accuracy: 0.9920 - val_loss: 0.0726 - val_accuracy: 0.9780\n",
      "Epoch 51/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0260 - accuracy: 0.9920 - val_loss: 0.0866 - val_accuracy: 0.9780\n",
      "Epoch 52/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0265 - accuracy: 0.9927 - val_loss: 0.0958 - val_accuracy: 0.9820\n",
      "Epoch 53/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 0.0331 - accuracy: 0.9900 - val_loss: 0.0892 - val_accuracy: 0.9780\n",
      "Epoch 54/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 0.0209 - accuracy: 0.9920 - val_loss: 0.1047 - val_accuracy: 0.9740\n",
      "Epoch 55/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 0.0248 - accuracy: 0.9920 - val_loss: 0.0806 - val_accuracy: 0.9760\n",
      "Epoch 56/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 0.0281 - accuracy: 0.9900 - val_loss: 0.0866 - val_accuracy: 0.9760\n",
      "Epoch 57/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.0182 - accuracy: 0.9953 - val_loss: 0.0644 - val_accuracy: 0.9860\n",
      "Epoch 58/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0618 - val_accuracy: 0.9860\n",
      "Epoch 59/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 0.0099 - accuracy: 0.9967 - val_loss: 0.0731 - val_accuracy: 0.9820\n",
      "Epoch 60/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 0.0180 - accuracy: 0.9953 - val_loss: 0.0769 - val_accuracy: 0.9800\n",
      "Epoch 61/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.0234 - accuracy: 0.9893 - val_loss: 0.0584 - val_accuracy: 0.9860\n",
      "Epoch 62/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.0247 - accuracy: 0.9907 - val_loss: 0.0634 - val_accuracy: 0.9800\n",
      "Epoch 63/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.0623 - accuracy: 0.9840 - val_loss: 0.2311 - val_accuracy: 0.9480\n",
      "Epoch 64/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.0667 - accuracy: 0.9827 - val_loss: 0.0453 - val_accuracy: 0.9900\n",
      "Epoch 65/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.0405 - accuracy: 0.9867 - val_loss: 0.0527 - val_accuracy: 0.9840\n",
      "Epoch 66/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.0554 - accuracy: 0.9860 - val_loss: 0.0915 - val_accuracy: 0.9760\n",
      "Epoch 67/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 0.0780 - accuracy: 0.9793 - val_loss: 0.1325 - val_accuracy: 0.9560\n",
      "Epoch 68/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 0.0522 - accuracy: 0.9847 - val_loss: 0.1400 - val_accuracy: 0.9600\n",
      "Epoch 69/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 0.0712 - accuracy: 0.9787 - val_loss: 0.1157 - val_accuracy: 0.9640\n",
      "Epoch 70/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.0355 - accuracy: 0.9887 - val_loss: 0.0393 - val_accuracy: 0.9880\n",
      "Epoch 71/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 0.0497 - accuracy: 0.9860 - val_loss: 0.0567 - val_accuracy: 0.9800\n",
      "Epoch 72/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.0436 - accuracy: 0.9873 - val_loss: 0.0674 - val_accuracy: 0.9760\n",
      "Epoch 73/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.0359 - accuracy: 0.9893 - val_loss: 0.0792 - val_accuracy: 0.9680\n",
      "Epoch 74/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 0.0490 - accuracy: 0.9893 - val_loss: 0.0577 - val_accuracy: 0.9800\n",
      "Epoch 75/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 0.0224 - accuracy: 0.9947 - val_loss: 0.0390 - val_accuracy: 0.9920\n",
      "Epoch 76/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 0.0329 - accuracy: 0.9913 - val_loss: 0.0371 - val_accuracy: 0.9840\n",
      "Epoch 77/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 0.0360 - accuracy: 0.9920 - val_loss: 0.1582 - val_accuracy: 0.9480\n",
      "Epoch 78/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 0.0501 - accuracy: 0.9873 - val_loss: 0.1136 - val_accuracy: 0.9620\n",
      "Epoch 79/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 0.0538 - accuracy: 0.9867 - val_loss: 0.1608 - val_accuracy: 0.9540\n",
      "Epoch 80/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 0.0522 - accuracy: 0.9840 - val_loss: 0.0897 - val_accuracy: 0.9720\n",
      "Epoch 81/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.1349 - val_accuracy: 0.9880\n",
      "Epoch 82/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.0328 - accuracy: 0.9927 - val_loss: 0.0477 - val_accuracy: 0.9860\n",
      "Epoch 83/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 0.0219 - accuracy: 0.9947 - val_loss: 0.0591 - val_accuracy: 0.9820\n",
      "Epoch 84/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.0185 - accuracy: 0.9947 - val_loss: 0.0438 - val_accuracy: 0.9840\n",
      "Epoch 85/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 0.0323 - accuracy: 0.9907 - val_loss: 0.0560 - val_accuracy: 0.9820\n",
      "Epoch 86/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 0.0276 - accuracy: 0.9953 - val_loss: 0.0887 - val_accuracy: 0.9800\n",
      "Epoch 87/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.0171 - accuracy: 0.9967 - val_loss: 0.0579 - val_accuracy: 0.9880\n",
      "Epoch 88/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.0279 - accuracy: 0.9980 - val_loss: 0.0353 - val_accuracy: 0.9900\n",
      "Epoch 89/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.0165 - accuracy: 0.9953 - val_loss: 0.0292 - val_accuracy: 0.9920\n",
      "Epoch 90/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 0.0075 - accuracy: 0.9967 - val_loss: 0.0404 - val_accuracy: 0.9880\n",
      "Epoch 91/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 0.0196 - accuracy: 0.9947 - val_loss: 0.0519 - val_accuracy: 0.9820\n",
      "Epoch 92/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 0.0121 - accuracy: 0.9967 - val_loss: 0.0545 - val_accuracy: 0.9860\n",
      "Epoch 93/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 0.0187 - accuracy: 0.9953 - val_loss: 0.0682 - val_accuracy: 0.9760\n",
      "Epoch 94/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 0.0166 - accuracy: 0.9940 - val_loss: 0.0800 - val_accuracy: 0.9760\n",
      "Epoch 95/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 0.0408 - accuracy: 0.9920 - val_loss: 0.0371 - val_accuracy: 0.9920\n",
      "Epoch 96/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 0.0438 - accuracy: 0.9893 - val_loss: 0.0935 - val_accuracy: 0.9720\n",
      "Epoch 97/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 0.0571 - accuracy: 0.9893 - val_loss: 0.0889 - val_accuracy: 0.9780\n",
      "Epoch 98/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 0.0801 - accuracy: 0.9793 - val_loss: 0.1616 - val_accuracy: 0.9640\n",
      "Epoch 99/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 0.0533 - accuracy: 0.9867 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 100/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 0.0314 - accuracy: 0.9913 - val_loss: 0.0277 - val_accuracy: 0.9900\n",
      "Epoch 101/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 0.0473 - accuracy: 0.9900 - val_loss: 0.0439 - val_accuracy: 0.9860\n",
      "Epoch 102/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 0.0506 - accuracy: 0.9827 - val_loss: 0.0840 - val_accuracy: 0.9800\n",
      "Epoch 103/2000\n",
      "24/24 [==============================] - 1s 48ms/step - loss: 0.0696 - accuracy: 0.9820 - val_loss: 0.0556 - val_accuracy: 0.9840\n",
      "Epoch 104/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 0.0507 - accuracy: 0.9887 - val_loss: 0.0566 - val_accuracy: 0.9820\n",
      "Epoch 105/2000\n",
      "24/24 [==============================] - 1s 48ms/step - loss: 0.0773 - accuracy: 0.9867 - val_loss: 0.1362 - val_accuracy: 0.9660\n",
      "Epoch 106/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 0.0492 - accuracy: 0.9893 - val_loss: 0.0349 - val_accuracy: 0.9900\n",
      "Epoch 107/2000\n",
      "24/24 [==============================] - 1s 49ms/step - loss: 0.0511 - accuracy: 0.9840 - val_loss: 0.0909 - val_accuracy: 0.9700\n",
      "Epoch 108/2000\n",
      "24/24 [==============================] - 1s 49ms/step - loss: 0.0327 - accuracy: 0.9907 - val_loss: 0.1018 - val_accuracy: 0.9660\n",
      "Epoch 109/2000\n",
      "24/24 [==============================] - 1s 50ms/step - loss: 0.0349 - accuracy: 0.9900 - val_loss: 0.0669 - val_accuracy: 0.9740\n",
      "Epoch 110/2000\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0512 - accuracy: 0.9907 - val_loss: 0.0547 - val_accuracy: 0.9840\n",
      "Epoch 111/2000\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0426 - accuracy: 0.9893 - val_loss: 0.0365 - val_accuracy: 0.9900\n",
      "Epoch 112/2000\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.0396 - val_accuracy: 0.9800\n",
      "Epoch 113/2000\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0154 - accuracy: 0.9967 - val_loss: 0.0650 - val_accuracy: 0.9780\n",
      "Epoch 114/2000\n",
      "24/24 [==============================] - 1s 51ms/step - loss: 0.0362 - accuracy: 0.9907 - val_loss: 0.0684 - val_accuracy: 0.9800\n",
      "Epoch 115/2000\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 0.0361 - accuracy: 0.9920 - val_loss: 0.0409 - val_accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_sub3_1.fit(\n",
    "    np.array(x_train), np.array(y_train),  #pairTrain[:, 1]\n",
    "  validation_data = ( np.array(x_val), np.array(y_val)),  #pairVal[:, 1]\n",
    "    epochs=2000\n",
    "    ,batch_size = 64    #8192\n",
    "  , callbacks=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "27dd00dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0304 - accuracy: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03038148395717144, 0.99210524559021]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sub3_1.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5757e95",
   "metadata": {},
   "source": [
    "# New Teacher Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d95304",
   "metadata": {},
   "source": [
    "# extracting probablity outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0eb54edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 4ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 5ms/step\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "24/24 [==============================] - 0s 6ms/step\n",
      "Bottleneck Train Shape: (1500, 42, 32)\n",
      "Bottleneck Val Shape: (500, 42, 32)\n",
      "Bottleneck Test Shape: (760, 42, 32)\n",
      "Final Output Train Shape: (1500, 276)\n",
      "Final Output Val Shape: (500, 276)\n",
      "Final Output Test Shape: (760, 276)\n"
     ]
    }
   ],
   "source": [
    "model_bottleneck = Model(inputs=model_sub3_1.input, outputs=x_b_T)\n",
    "\n",
    "# Create a model to extract the final output\n",
    "model_final_output = Model(inputs=model_sub3_1.input, outputs=out_T)\n",
    "\n",
    "# Assuming train_data, val_data, and test_data are your datasets\n",
    "# For demonstration, using random data with correct shape\n",
    "# train_data = np.random.rand(1000, 200, 6)\n",
    "# val_data = np.random.rand(200, 200, 6)\n",
    "# test_data = np.random.rand(200, 200, 6)\n",
    "\n",
    "# Predict the bottleneck outputs\n",
    "bottleneck_train = model_bottleneck.predict(np.array(x_train))\n",
    "bottleneck_val = model_bottleneck.predict(np.array(x_val))\n",
    "bottleneck_test = model_bottleneck.predict(np.array(x_test))\n",
    "\n",
    "# Predict the final outputs (probability scores)\n",
    "output_train = model_final_output.predict(np.array(x_train))\n",
    "output_val = model_final_output.predict(np.array(x_val))\n",
    "output_test = model_final_output.predict(np.array(x_test))\n",
    "\n",
    "# Print shapes of the outputs\n",
    "print(f\"Bottleneck Train Shape: {bottleneck_train.shape}\")\n",
    "print(f\"Bottleneck Val Shape: {bottleneck_val.shape}\")\n",
    "print(f\"Bottleneck Test Shape: {bottleneck_test.shape}\")\n",
    "print(f\"Final Output Train Shape: {output_train.shape}\")\n",
    "print(f\"Final Output Val Shape: {output_val.shape}\")\n",
    "print(f\"Final Output Test Shape: {output_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "33c15334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck Train Shape: (1500, 42, 32)\n",
      "Bottleneck Val Shape: (500, 42, 32)\n",
      "Bottleneck Test Shape: (760, 42, 32)\n",
      "Final Output Train Shape: (1500, 276)\n",
      "Final Output Val Shape: (500, 276)\n",
      "Final Output Test Shape: (760, 276)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bottleneck Train Shape: {bottleneck_train.shape}\")\n",
    "print(f\"Bottleneck Val Shape: {bottleneck_val.shape}\")\n",
    "print(f\"Bottleneck Test Shape: {bottleneck_test.shape}\")\n",
    "print(f\"Final Output Train Shape: {output_train.shape}\")\n",
    "print(f\"Final Output Val Shape: {output_val.shape}\")\n",
    "print(f\"Final Output Test Shape: {output_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87ae05a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 276)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d654373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prob = np.concatenate((output_train, output_val), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02e8bd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 276)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54ba62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_output = np.concatenate((bottleneck_train, bottleneck_val), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b8b4aa45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 42, 32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a1c12",
   "metadata": {},
   "source": [
    "# student and Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "5b7531ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalAveragePooling1D, Masking, Input, MaxPooling1D, GlobalMaxPooling1D, Add, Dropout, BatchNormalization, UpSampling1D, Lambda, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "184be300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using subcarriers = 1, 2, 3\n",
    "x_train = np.array(x_train)[:, :, [0, 1, 2]]\n",
    "x_test = np.array(x_test)[:, :, [0, 1, 2]]\n",
    "x_val = np.array(x_val)[:, :, [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "132da9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 200, 3)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "id": "cb736e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_430 (B  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_496 (MaxPool  (None, 97, 32)               0         ['batch_normalization_430[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " conv1d_556 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_496[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_497 (MaxPool  (None, 46, 64)               0         ['conv1d_556[0][0]']          \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_431 (B  (None, 46, 64)               256       ['max_pooling1d_497[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_557 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_431[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['conv1d_557[0][0]']          \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_498 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_432 (B  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_558 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_498[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_499 (MaxPool  (None, 48, 8)                0         ['batch_normalization_432[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 512)                  0         ['conv1d_558[0][0]']          \n",
      " 02 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_208 (  (None, 512)                  0         ['conv1d_558[0][0]']          \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_499[0][0]']   \n",
      "                                                                                                  \n",
      " add_261 (Add)               (None, 512)                  0         ['global_average_pooling1d_202\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_208[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 03 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_209 (  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_251 (Dense)           (None, 300)                  153900    ['add_261[0][0]']             \n",
      "                                                                                                  \n",
      " add_262 (Add)               (None, 32)                   0         ['global_average_pooling1d_203\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_209[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_245 (Dropout)       (None, 300)                  0         ['dense_251[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_246 (Dropout)       (None, 32)                   0         ['add_262[0][0]']             \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_245[0][0]']         \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_246[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 383188 (1.46 MB)\n",
      "Trainable params: 382980 (1.46 MB)\n",
      "Non-trainable params: 208 (832.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dataInp = Input(shape = (200, 3), name='in')\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_T')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T) #old\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T) #old\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu', name='bottle_T')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid', name='out_T')(d_T)\n",
    "\n",
    "# STUDENT MODEL\n",
    "x_S = Conv1D(kernel_size = 7, filters=8, activation='relu', name='l1_S')(dataInp) # 194, 8\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(4)(x_S) # 48, 32\n",
    "x_b_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "avg_S = GlobalAveragePooling1D()(x_b_S)\n",
    "max_S = GlobalMaxPooling1D()(x_b_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "\n",
    "#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\n",
    "# abs_sub = tf.abs(x_b_T - x_b_S)\n",
    "# l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\n",
    "# abs_sub = tf.abs(tf.keras.layers.Subtract()([x_b_T, x_b_S]))  \n",
    "# l2_output = tf.abs(tf.keras.layers.Subtract()([out_T, out_S]))\n",
    "\n",
    "model = Model( inputs=dataInp, outputs=[out_T, out_S ], name='distillationFramework' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "6ce5f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_212 (B  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_299 (MaxPool  (None, 97, 32)               0         ['batch_normalization_212[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " conv1d_345 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_299[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_300 (MaxPool  (None, 46, 64)               0         ['conv1d_345[0][0]']          \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_213 (B  (None, 46, 64)               256       ['max_pooling1d_300[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_346 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_213[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['conv1d_346[0][0]']          \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_347 (Conv1D)         (None, 42, 32)               1056      ['bottle_T[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_214 (B  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_348 (Conv1D)         (None, 42, 32)               1056      ['conv1d_347[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_301 (MaxPool  (None, 48, 8)                0         ['batch_normalization_214[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " add_129 (Add)               (None, 42, 32)               0         ['bottle_T[0][0]',            \n",
      "                                                                     'conv1d_348[0][0]']          \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_301[0][0]']   \n",
      "                                                                                                  \n",
      " global_max_pooling1d_125 (  (None, 32)                   0         ['add_129[0][0]']             \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 20 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_126 (  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_112 (Dense)           (None, 300)                  9900      ['global_max_pooling1d_125[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " add_130 (Add)               (None, 32)                   0         ['global_average_pooling1d_120\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_126[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_118 (Dropout)       (None, 300)                  0         ['dense_112[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_119 (Dropout)       (None, 32)                   0         ['add_130[0][0]']             \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_118[0][0]']         \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_119[0][0]']         \n",
      "                                                                                                  \n",
      " subtract_80 (Subtract)      (None, 42, 32)               0         ['bottle_T[0][0]',            \n",
      "                                                                     'bottle_S[0][0]']            \n",
      "                                                                                                  \n",
      " subtract_81 (Subtract)      (None, 276)                  0         ['out_T[0][0]',               \n",
      "                                                                     'out_S[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.abs_85 (TFOpLambda  (None, 42, 32)               0         ['subtract_80[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.abs_86 (TFOpLambda  (None, 276)                  0         ['subtract_81[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 191636 (748.58 KB)\n",
      "Trainable params: 191428 (747.77 KB)\n",
      "Non-trainable params: 208 (832.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using residual connection\n",
    "\n",
    "dataInp = Input(shape = (200, 3), name='in')\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_T')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T) #old\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T) #old\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu', name='bottle_T')(x_T) # 42, 32\n",
    "# x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = 1, filters=32, activation='relu', padding = 'same')(x_b_T) # 42, 32\n",
    "x_T = Conv1D(kernel_size = 1, filters=32, activation='relu', padding = 'same')(x_T) # 42, 32\n",
    "# x_T = UpSampling1D(size=2)(x_T)  # 42, 32\n",
    "x_r = Add()([x_b_T, x_T]) #residual\n",
    "# x_T = BatchNormalization()(x_r)\n",
    "avg_T = GlobalAveragePooling1D()(x_r)\n",
    "max_T = GlobalMaxPooling1D()(x_r)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid', name='out_T')(d_T)\n",
    "\n",
    "# STUDENT MODEL\n",
    "x_S = Conv1D(kernel_size = 7, filters=8, activation='relu', name='l1_S')(dataInp) # 194, 8\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(4)(x_S) # 48, 32\n",
    "x_b_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "avg_S = GlobalAveragePooling1D()(x_b_S)\n",
    "max_S = GlobalMaxPooling1D()(x_b_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "\n",
    "#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\n",
    "# abs_sub = tf.abs(x_b_T - x_b_S)\n",
    "# l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\n",
    "abs_sub = tf.abs(tf.keras.layers.Subtract()([x_b_T, x_b_S]))  \n",
    "l2_output = tf.abs(tf.keras.layers.Subtract()([out_T, out_S]))\n",
    "\n",
    "model = Model( inputs=dataInp, outputs=[out_T, out_S, abs_sub, l2_output], name='distillationFramework' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "id": "396b6811",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 200, 3), dtype=tf.float32, name='in'), name='in', description=\"created by layer 'in'\") at layer \"l1_T\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1080], line 48\u001b[0m\n\u001b[1;32m     40\u001b[0m out_S \u001b[39m=\u001b[39m Dense(\u001b[39m276\u001b[39m, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mout_S\u001b[39m\u001b[39m'\u001b[39m)(d_S)\n\u001b[1;32m     42\u001b[0m \u001b[39m#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# abs_sub = tf.abs(x_b_T - x_b_S)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# abs_sub = tf.abs(tf.keras.layers.Subtract()([x_b_T, x_b_S]))  \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# l2_output = tf.abs(tf.keras.layers.Subtract()([out_T, out_S]))\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m model \u001b[39m=\u001b[39m Model( inputs\u001b[39m=\u001b[39;49mdataInp, outputs\u001b[39m=\u001b[39;49m[out_T, out_S, abs_sub, l2_output], name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdistillationFramework\u001b[39;49m\u001b[39m'\u001b[39;49m )\n\u001b[1;32m     49\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/functional.py:166\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    158\u001b[0m         [\n\u001b[1;32m    159\u001b[0m             functional_utils\u001b[39m.\u001b[39mis_input_keras_tensor(t)\n\u001b[1;32m    160\u001b[0m             \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[1;32m    161\u001b[0m         ]\n\u001b[1;32m    162\u001b[0m     ):\n\u001b[1;32m    163\u001b[0m         inputs, outputs \u001b[39m=\u001b[39m functional_utils\u001b[39m.\u001b[39mclone_graph_nodes(\n\u001b[1;32m    164\u001b[0m             inputs, outputs\n\u001b[1;32m    165\u001b[0m         )\n\u001b[0;32m--> 166\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_graph_network(inputs, outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/functional.py:265\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_coordinates\u001b[39m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[1;32m    264\u001b[0m \u001b[39m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[39m=\u001b[39m _map_graph_network(\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutputs\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    268\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_nodes \u001b[39m=\u001b[39m nodes\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nodes_by_depth \u001b[39m=\u001b[39m nodes_by_depth\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/functional.py:1145\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(node\u001b[39m.\u001b[39mkeras_inputs):\n\u001b[1;32m   1144\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(x) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m computable_tensors:\n\u001b[0;32m-> 1145\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1146\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGraph disconnected: cannot obtain value for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1147\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensor \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m at layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1148\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe following previous layers were accessed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwithout issue: \u001b[39m\u001b[39m{\u001b[39;00mlayers_with_complete_input\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1150\u001b[0m         )\n\u001b[1;32m   1151\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(node\u001b[39m.\u001b[39moutputs):\n\u001b[1;32m   1152\u001b[0m     computable_tensors\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(x))\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 200, 3), dtype=tf.float32, name='in'), name='in', description=\"created by layer 'in'\") at layer \"l1_T\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "# using attention layer mechanism\n",
    "dataInp = Input(shape = (200, 3), name='in')\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_T')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T) #old\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T) #old\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu', name='bottle_T')(x_T) # 42, 32\n",
    "# x_b_T = Conv1D(kernel_size = 1, filters=1, activation='relu', name='bottle_T')(x_T) # 42,1\n",
    "\n",
    "# attention layer\n",
    "attention_weights = (Dense(1, activation = 'softmax'))(x_b_T) \n",
    "attention_T = tf.keras.layers.Multiply()([x_b_T, attention_weights])\n",
    "# x_b_T = Conv1D(kernel_size = 1, filters=1, activation='relu', name='bottle_T')(x_b_T) # 42,1\n",
    "# attention_T_1 = Lambda(lambda x: tf.squeeze(tf.matmul(x[0], x[1], transpose_b = True), axis = -1))([attention_weights, x_b_T])\n",
    "# attention_T_1 = Flatten()(attention_T_1)\n",
    "# attention_T_2 = tf.keras.layers.Reshape((42, 1))(attention_T_1)\n",
    "# attention_T = tf.keras.layers.Multiply()([x_b_T, attention_T_1])\n",
    "\n",
    "out_T = Dense(276, activation = 'sigmoid', name='out_T')(d_T)\n",
    "\n",
    "# STUDENT MODEL\n",
    "x_S = Conv1D(kernel_size = 7, filters=8, activation='relu', name='l1_S')(dataInp) # 194, 8\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(4)(x_S) # 48, 32\n",
    "x_b_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "\n",
    "attention_weights_S = (Dense(1, activation = 'softmax'))(x_b_S) \n",
    "attention_T = tf.keras.layers.Multiply()([x_b_S, attention_weights_S])\n",
    "\n",
    "avg_S = GlobalAveragePooling1D()(attention_T)\n",
    "max_S = GlobalMaxPooling1D()(attention_T)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "\n",
    "#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\n",
    "# abs_sub = tf.abs(x_b_T - x_b_S)\n",
    "# l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\n",
    "abs_sub = tf.abs(tf.keras.layers.Subtract()([x_b_T, x_b_S]))  \n",
    "l2_output = tf.abs(tf.keras.layers.Subtract()([out_T, out_S]))\n",
    "\n",
    "model = Model( inputs=dataInp, outputs=[out_T, out_S, abs_sub, l2_output], name='distillationFramework' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "id": "85ec1a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 93, 64)]          0         \n",
      "                                                                 \n",
      " tf.reshape_87 (TFOpLambda)  (None, 64)                0         \n",
      "                                                                 \n",
      " sequential_49 (Sequential)  (None, 32)                2080      \n",
      "                                                                 \n",
      " tf.reshape_88 (TFOpLambda)  (None, 93, 1)             0         \n",
      "                                                                 \n",
      " conv1d_540 (Conv1D)         (None, 93, 1)             6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2086 (8.15 KB)\n",
      "Trainable params: 2086 (8.15 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 44, 256)]         0         \n",
      "                                                                 \n",
      " tf.reshape_89 (TFOpLambda)  (None, 256)               0         \n",
      "                                                                 \n",
      " sequential_50 (Sequential)  (None, 32)                8224      \n",
      "                                                                 \n",
      " tf.reshape_90 (TFOpLambda)  (None, 44, 1)             0         \n",
      "                                                                 \n",
      " conv1d_542 (Conv1D)         (None, 44, 1)             4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8228 (32.14 KB)\n",
      "Trainable params: 8228 (32.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_413 (B  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_476 (MaxPool  (None, 97, 32)               0         ['batch_normalization_413[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " conv1d_539 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_476[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_414 (B  (None, 93, 64)               256       ['conv1d_539[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " model_50 (Functional)       (None, 93, 1)                2086      ['batch_normalization_414[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multiply_59 (Multiply)      (None, 93, 64)               0         ['model_50[0][0]',            \n",
      "                                                                     'batch_normalization_414[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_249 (Add)               (None, 93, 64)               0         ['multiply_59[0][0]',         \n",
      "                                                                     'conv1d_539[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_477 (MaxPool  (None, 46, 64)               0         ['add_249[0][0]']             \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_415 (B  (None, 46, 64)               256       ['max_pooling1d_477[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_541 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_415[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_416 (B  (None, 44, 256)              1024      ['conv1d_541[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " model_51 (Functional)       (None, 44, 1)                8228      ['batch_normalization_416[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multiply_60 (Multiply)      (None, 44, 256)              0         ['model_51[0][0]',            \n",
      "                                                                     'batch_normalization_416[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_250 (Add)               (None, 44, 256)              0         ['multiply_60[0][0]',         \n",
      "                                                                     'conv1d_541[0][0]']          \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['add_250[0][0]']             \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_478 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_417 (B  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_543 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_478[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_479 (MaxPool  (None, 48, 8)                0         ['batch_normalization_417[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 512)                  0         ['conv1d_543[0][0]']          \n",
      " 92 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_198 (  (None, 512)                  0         ['conv1d_543[0][0]']          \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_479[0][0]']   \n",
      "                                                                                                  \n",
      " add_251 (Add)               (None, 512)                  0         ['global_average_pooling1d_192\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_198[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 93 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_199 (  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_246 (Dense)           (None, 300)                  153900    ['add_251[0][0]']             \n",
      "                                                                                                  \n",
      " add_252 (Add)               (None, 32)                   0         ['global_average_pooling1d_193\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_199[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_235 (Dropout)       (None, 300)                  0         ['dense_246[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_236 (Dropout)       (None, 32)                   0         ['add_252[0][0]']             \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_235[0][0]']         \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_236[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 394782 (1.51 MB)\n",
      "Trainable params: 393934 (1.50 MB)\n",
      "Non-trainable params: 848 (3.31 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# def spatial_mean_attention(shape, kernel):\n",
    "#   inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "#   # CHANNEL SUMMARIZATION\n",
    "#   mean_across_channels = tf.keras.backend.mean(inp, axis = -1)\n",
    "#   max_across_channels = tf.keras.backend.max(inp, axis = -1)\n",
    "#   cat = tf.keras.layers.concatenate([ tf.expand_dims(mean_across_channels, axis=-1), tf.expand_dims(max_across_channels, axis=-1)])\n",
    "\n",
    "#   # CONV & ACTIVATION\n",
    "#   out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( tf.expand_dims(mean_across_channels, axis=-1) )\n",
    "\n",
    "#   model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "#   model.summary()\n",
    "#   return model\n",
    "\n",
    "def spatial_mlp_attention(shape, kernel):\n",
    "  inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "  # CHANNEL SUMMARIZATION\n",
    "  w, C = shape[0], shape[1]\n",
    "  # Reshape the feature map to (w * h, C)\n",
    "  reshaped_feature_map = tf.reshape(inp, [-1, C])\n",
    "  # Define the MLP architecture\n",
    "  hidden_units = 32\n",
    "  output_units = 1\n",
    "  mlp = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      Dropout(0.4),\n",
    "      #BatchNormalization(),      tf.keras.layers.Dense(output_units, activation='sigmoid')\n",
    "  ])\n",
    "  # Apply the MLP on the reshaped feature map\n",
    "  output_mlp = mlp(reshaped_feature_map)\n",
    "  reshaped_output = tf.reshape(output_mlp, ( -1, w, 1 ))\n",
    "  # CONV & ACTIVATION\n",
    "  out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( reshaped_output )\n",
    "\n",
    "  model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "dataInp = Input(shape = (200, 3), name='in')\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_c1 = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_T')(dataInp) # 194, 32\n",
    "x_b1 = BatchNormalization()(x_c1)\n",
    "x_m1 = MaxPooling1D(2)(x_b1) # 97, 32\n",
    "x_c2 = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_m1) # 93, 64\n",
    "x_b2 = BatchNormalization()(x_c2)\n",
    "\n",
    "at1 = spatial_mlp_attention(shape = (93, 64), kernel = 5)\n",
    "x_a1 = at1(x_b2) \n",
    "x_T = tf.keras.layers.Multiply()([x_a1, x_b2])\n",
    "# x_T = tf.multiply([x_a1, x_b2])\n",
    "x_T = Add()([x_T, x_c2])\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_c3 = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b3 = BatchNormalization()(x_c3)\n",
    "\n",
    "at2 = spatial_mlp_attention(shape = (44, 256), kernel = kernel_size)\n",
    "x_a2 = at2(x_b3)\n",
    "x_T = tf.keras.layers.Multiply()([x_a2, x_b3])\n",
    "# x_T = tf.multiply([x_a2, x_b3])\n",
    "x_T = Add()([x_T, x_c3])\n",
    "\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu', name='bottle_T')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid', name='out_T')(d_T)\n",
    "\n",
    "# STUDENT MODEL\n",
    "x_S = Conv1D(kernel_size = 7, filters=8, activation='relu', name='l1_S')(dataInp) # 194, 8\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(4)(x_S) # 48, 32\n",
    "x_b_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "avg_S = GlobalAveragePooling1D()(x_b_S)\n",
    "max_S = GlobalMaxPooling1D()(x_b_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "\n",
    "#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\n",
    "# abs_sub = tf.abs(x_b_T - x_b_S)\n",
    "# l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\n",
    "abs_sub = tf.abs(tf.keras.layers.Subtract()([x_b_T, x_b_S]))  \n",
    "l2_output = tf.abs(tf.keras.layers.Subtract()([out_T, out_S]))\n",
    "\n",
    "model = Model( inputs=dataInp, outputs=[out_T, out_S, abs_sub, l2_output], name='distillationFramework' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "id": "1b7dfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "id": "511e4973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] building and compilation complete!\n",
      "\n",
      " Model summary:\n",
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_430 (B  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_496 (MaxPool  (None, 97, 32)               0         ['batch_normalization_430[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " conv1d_556 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_496[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_497 (MaxPool  (None, 46, 64)               0         ['conv1d_556[0][0]']          \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_431 (B  (None, 46, 64)               256       ['max_pooling1d_497[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_557 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_431[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['conv1d_557[0][0]']          \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_498 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_432 (B  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_558 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_498[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_499 (MaxPool  (None, 48, 8)                0         ['batch_normalization_432[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 512)                  0         ['conv1d_558[0][0]']          \n",
      " 02 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_208 (  (None, 512)                  0         ['conv1d_558[0][0]']          \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_499[0][0]']   \n",
      "                                                                                                  \n",
      " add_261 (Add)               (None, 512)                  0         ['global_average_pooling1d_202\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_208[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 03 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_209 (  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_251 (Dense)           (None, 300)                  153900    ['add_261[0][0]']             \n",
      "                                                                                                  \n",
      " add_262 (Add)               (None, 32)                   0         ['global_average_pooling1d_203\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_209[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_245 (Dropout)       (None, 300)                  0         ['dense_251[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_246 (Dropout)       (None, 32)                   0         ['add_262[0][0]']             \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_245[0][0]']         \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_246[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 383188 (1.46 MB)\n",
      "Trainable params: 382980 (1.46 MB)\n",
      "Non-trainable params: 208 (832.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate = 0.0001) # 0.00001\n",
    "print(\"[INFO] compiling model...\")\n",
    "model.compile(loss = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"], loss_weights = [1., 10], optimizer= opt)\n",
    "print('[INFO] building and compilation complete!')\n",
    "print('\\n Model summary:') \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "id": "9f69ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 42, 32)\n",
      "(500, 42, 32)\n",
      "(1500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "layer_zero_train = np.zeros(shape = (y_train.shape[0], 42, 32))\n",
    "layer_zero_val = np.zeros(shape = (y_val.shape[0], 42, 32))\n",
    "\n",
    "print(layer_zero_train.shape)\n",
    "print(layer_zero_val.shape)\n",
    "\n",
    "out_zero_train = np.zeros(shape = (y_train.shape[0], ))\n",
    "out_zero_val = np.zeros(shape = (y_val.shape[0], ))\n",
    "\n",
    "print(out_zero_train.shape)\n",
    "print(out_zero_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "id": "9888a470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/2000\n",
      "24/24 [==============================] - 3s 30ms/step - loss: 69.0554 - out_T_loss: 5.6349 - out_S_loss: 6.3421 - val_loss: 91.4828 - val_out_T_loss: 5.6217 - val_out_S_loss: 8.5861\n",
      "Epoch 2/2000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 67.4652 - out_T_loss: 5.5500 - out_S_loss: 6.1915 - val_loss: 75.5172 - val_out_T_loss: 5.5980 - val_out_S_loss: 6.9919\n",
      "Epoch 3/2000\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 66.0058 - out_T_loss: 5.4760 - out_S_loss: 6.0530 - val_loss: 69.7287 - val_out_T_loss: 5.5554 - val_out_S_loss: 6.4173\n",
      "Epoch 4/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 64.7632 - out_T_loss: 5.3486 - out_S_loss: 5.9415 - val_loss: 66.7614 - val_out_T_loss: 5.4557 - val_out_S_loss: 6.1306\n",
      "Epoch 5/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 63.9188 - out_T_loss: 5.1521 - out_S_loss: 5.8767 - val_loss: 64.9155 - val_out_T_loss: 5.2783 - val_out_S_loss: 5.9637\n",
      "Epoch 6/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 63.0880 - out_T_loss: 4.9357 - out_S_loss: 5.8152 - val_loss: 63.5831 - val_out_T_loss: 5.0577 - val_out_S_loss: 5.8525\n",
      "Epoch 7/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 62.3312 - out_T_loss: 4.7439 - out_S_loss: 5.7587 - val_loss: 62.5767 - val_out_T_loss: 4.8328 - val_out_S_loss: 5.7744\n",
      "Epoch 8/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 61.6474 - out_T_loss: 4.5344 - out_S_loss: 5.7113 - val_loss: 61.7716 - val_out_T_loss: 4.5997 - val_out_S_loss: 5.7172\n",
      "Epoch 9/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 61.0332 - out_T_loss: 4.2955 - out_S_loss: 5.6738 - val_loss: 61.0918 - val_out_T_loss: 4.3763 - val_out_S_loss: 5.6715\n",
      "Epoch 10/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 60.2218 - out_T_loss: 4.0913 - out_S_loss: 5.6131 - val_loss: 60.5291 - val_out_T_loss: 4.1564 - val_out_S_loss: 5.6373\n",
      "Epoch 11/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 59.8999 - out_T_loss: 3.9205 - out_S_loss: 5.5979 - val_loss: 60.0700 - val_out_T_loss: 3.9724 - val_out_S_loss: 5.6098\n",
      "Epoch 12/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 59.5960 - out_T_loss: 3.7639 - out_S_loss: 5.5832 - val_loss: 59.6440 - val_out_T_loss: 3.7977 - val_out_S_loss: 5.5846\n",
      "Epoch 13/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 59.0580 - out_T_loss: 3.5965 - out_S_loss: 5.5461 - val_loss: 59.2537 - val_out_T_loss: 3.6182 - val_out_S_loss: 5.5636\n",
      "Epoch 14/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 59.0137 - out_T_loss: 3.4996 - out_S_loss: 5.5514 - val_loss: 58.9382 - val_out_T_loss: 3.4906 - val_out_S_loss: 5.5448\n",
      "Epoch 15/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 58.4402 - out_T_loss: 3.3631 - out_S_loss: 5.5077 - val_loss: 58.6258 - val_out_T_loss: 3.3571 - val_out_S_loss: 5.5269\n",
      "Epoch 16/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 58.1100 - out_T_loss: 3.1958 - out_S_loss: 5.4914 - val_loss: 58.3268 - val_out_T_loss: 3.2211 - val_out_S_loss: 5.5106\n",
      "Epoch 17/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 58.0137 - out_T_loss: 3.1212 - out_S_loss: 5.4893 - val_loss: 58.0688 - val_out_T_loss: 3.1269 - val_out_S_loss: 5.4942\n",
      "Epoch 18/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 57.6525 - out_T_loss: 2.9921 - out_S_loss: 5.4660 - val_loss: 57.7694 - val_out_T_loss: 2.9845 - val_out_S_loss: 5.4785\n",
      "Epoch 19/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 57.1398 - out_T_loss: 2.8640 - out_S_loss: 5.4276 - val_loss: 57.5104 - val_out_T_loss: 2.8988 - val_out_S_loss: 5.4612\n",
      "Epoch 20/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 57.0266 - out_T_loss: 2.8099 - out_S_loss: 5.4217 - val_loss: 57.2219 - val_out_T_loss: 2.7675 - val_out_S_loss: 5.4454\n",
      "Epoch 21/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 56.6400 - out_T_loss: 2.7196 - out_S_loss: 5.3920 - val_loss: 56.9997 - val_out_T_loss: 2.6999 - val_out_S_loss: 5.4300\n",
      "Epoch 22/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 56.4000 - out_T_loss: 2.6545 - out_S_loss: 5.3745 - val_loss: 56.7650 - val_out_T_loss: 2.6281 - val_out_S_loss: 5.4137\n",
      "Epoch 23/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 56.2281 - out_T_loss: 2.5592 - out_S_loss: 5.3669 - val_loss: 56.5226 - val_out_T_loss: 2.5496 - val_out_S_loss: 5.3973\n",
      "Epoch 24/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 55.9290 - out_T_loss: 2.4970 - out_S_loss: 5.3432 - val_loss: 56.2868 - val_out_T_loss: 2.4763 - val_out_S_loss: 5.3811\n",
      "Epoch 25/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 55.8183 - out_T_loss: 2.4384 - out_S_loss: 5.3380 - val_loss: 56.0586 - val_out_T_loss: 2.4161 - val_out_S_loss: 5.3642\n",
      "Epoch 26/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 55.4789 - out_T_loss: 2.3405 - out_S_loss: 5.3138 - val_loss: 55.7981 - val_out_T_loss: 2.3351 - val_out_S_loss: 5.3463\n",
      "Epoch 27/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 55.1829 - out_T_loss: 2.2849 - out_S_loss: 5.2898 - val_loss: 55.5708 - val_out_T_loss: 2.2749 - val_out_S_loss: 5.3296\n",
      "Epoch 28/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 54.7696 - out_T_loss: 2.1529 - out_S_loss: 5.2617 - val_loss: 55.3320 - val_out_T_loss: 2.2147 - val_out_S_loss: 5.3117\n",
      "Epoch 29/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 54.5747 - out_T_loss: 2.1606 - out_S_loss: 5.2414 - val_loss: 55.0642 - val_out_T_loss: 2.1291 - val_out_S_loss: 5.2935\n",
      "Epoch 30/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 54.3474 - out_T_loss: 2.1662 - out_S_loss: 5.2181 - val_loss: 54.8377 - val_out_T_loss: 2.0964 - val_out_S_loss: 5.2741\n",
      "Epoch 31/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 54.3630 - out_T_loss: 2.0885 - out_S_loss: 5.2274 - val_loss: 54.6381 - val_out_T_loss: 2.0693 - val_out_S_loss: 5.2569\n",
      "Epoch 32/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 53.7150 - out_T_loss: 1.9859 - out_S_loss: 5.1729 - val_loss: 54.3686 - val_out_T_loss: 1.9992 - val_out_S_loss: 5.2369\n",
      "Epoch 33/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 53.8198 - out_T_loss: 1.9953 - out_S_loss: 5.1824 - val_loss: 54.1270 - val_out_T_loss: 1.9496 - val_out_S_loss: 5.2177\n",
      "Epoch 34/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 53.3601 - out_T_loss: 1.9334 - out_S_loss: 5.1427 - val_loss: 53.9164 - val_out_T_loss: 1.9417 - val_out_S_loss: 5.1975\n",
      "Epoch 35/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 53.3228 - out_T_loss: 1.9388 - out_S_loss: 5.1384 - val_loss: 53.6795 - val_out_T_loss: 1.9150 - val_out_S_loss: 5.1765\n",
      "Epoch 36/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 52.6287 - out_T_loss: 1.8150 - out_S_loss: 5.0814 - val_loss: 53.3822 - val_out_T_loss: 1.8240 - val_out_S_loss: 5.1558\n",
      "Epoch 37/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 52.6594 - out_T_loss: 1.7961 - out_S_loss: 5.0863 - val_loss: 53.1213 - val_out_T_loss: 1.7728 - val_out_S_loss: 5.1348\n",
      "Epoch 38/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 52.4747 - out_T_loss: 1.7766 - out_S_loss: 5.0698 - val_loss: 52.8609 - val_out_T_loss: 1.7316 - val_out_S_loss: 5.1129\n",
      "Epoch 39/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 52.0300 - out_T_loss: 1.6926 - out_S_loss: 5.0337 - val_loss: 52.6137 - val_out_T_loss: 1.7055 - val_out_S_loss: 5.0908\n",
      "Epoch 40/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 51.8477 - out_T_loss: 1.6920 - out_S_loss: 5.0156 - val_loss: 52.4308 - val_out_T_loss: 1.7363 - val_out_S_loss: 5.0695\n",
      "Epoch 41/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 51.6261 - out_T_loss: 1.6554 - out_S_loss: 4.9971 - val_loss: 52.1234 - val_out_T_loss: 1.6473 - val_out_S_loss: 5.0476\n",
      "Epoch 42/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 51.5034 - out_T_loss: 1.5917 - out_S_loss: 4.9912 - val_loss: 51.8727 - val_out_T_loss: 1.5982 - val_out_S_loss: 5.0275\n",
      "Epoch 43/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 51.0528 - out_T_loss: 1.5540 - out_S_loss: 4.9499 - val_loss: 51.6165 - val_out_T_loss: 1.5605 - val_out_S_loss: 5.0056\n",
      "Epoch 44/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 50.8535 - out_T_loss: 1.5170 - out_S_loss: 4.9337 - val_loss: 51.3557 - val_out_T_loss: 1.5067 - val_out_S_loss: 4.9849\n",
      "Epoch 45/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 50.7233 - out_T_loss: 1.4785 - out_S_loss: 4.9245 - val_loss: 51.1245 - val_out_T_loss: 1.5122 - val_out_S_loss: 4.9612\n",
      "Epoch 46/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 50.5164 - out_T_loss: 1.4568 - out_S_loss: 4.9060 - val_loss: 50.8846 - val_out_T_loss: 1.4859 - val_out_S_loss: 4.9399\n",
      "Epoch 47/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 50.1220 - out_T_loss: 1.3932 - out_S_loss: 4.8729 - val_loss: 50.6309 - val_out_T_loss: 1.4366 - val_out_S_loss: 4.9194\n",
      "Epoch 48/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 49.9633 - out_T_loss: 1.3879 - out_S_loss: 4.8575 - val_loss: 50.3915 - val_out_T_loss: 1.4031 - val_out_S_loss: 4.8988\n",
      "Epoch 49/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 49.4728 - out_T_loss: 1.3331 - out_S_loss: 4.8140 - val_loss: 50.1439 - val_out_T_loss: 1.3756 - val_out_S_loss: 4.8768\n",
      "Epoch 50/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 49.2840 - out_T_loss: 1.3644 - out_S_loss: 4.7920 - val_loss: 49.9287 - val_out_T_loss: 1.3884 - val_out_S_loss: 4.8540\n",
      "Epoch 51/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 49.2595 - out_T_loss: 1.2970 - out_S_loss: 4.7962 - val_loss: 49.6610 - val_out_T_loss: 1.3320 - val_out_S_loss: 4.8329\n",
      "Epoch 52/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 48.9065 - out_T_loss: 1.2531 - out_S_loss: 4.7653 - val_loss: 49.4597 - val_out_T_loss: 1.3431 - val_out_S_loss: 4.8117\n",
      "Epoch 53/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 48.6814 - out_T_loss: 1.2807 - out_S_loss: 4.7401 - val_loss: 49.2137 - val_out_T_loss: 1.2941 - val_out_S_loss: 4.7920\n",
      "Epoch 54/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 48.4805 - out_T_loss: 1.1988 - out_S_loss: 4.7282 - val_loss: 48.9831 - val_out_T_loss: 1.2827 - val_out_S_loss: 4.7700\n",
      "Epoch 55/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 48.1805 - out_T_loss: 1.1999 - out_S_loss: 4.6981 - val_loss: 48.7489 - val_out_T_loss: 1.2636 - val_out_S_loss: 4.7485\n",
      "Epoch 56/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 47.8165 - out_T_loss: 1.1613 - out_S_loss: 4.6655 - val_loss: 48.4840 - val_out_T_loss: 1.2277 - val_out_S_loss: 4.7256\n",
      "Epoch 57/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 47.8390 - out_T_loss: 1.1448 - out_S_loss: 4.6694 - val_loss: 48.2523 - val_out_T_loss: 1.1955 - val_out_S_loss: 4.7057\n",
      "Epoch 58/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 47.5841 - out_T_loss: 1.0950 - out_S_loss: 4.6489 - val_loss: 47.9966 - val_out_T_loss: 1.1634 - val_out_S_loss: 4.6833\n",
      "Epoch 59/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 47.1941 - out_T_loss: 1.0850 - out_S_loss: 4.6109 - val_loss: 47.7828 - val_out_T_loss: 1.1444 - val_out_S_loss: 4.6638\n",
      "Epoch 60/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 47.1232 - out_T_loss: 1.0679 - out_S_loss: 4.6055 - val_loss: 47.5564 - val_out_T_loss: 1.1314 - val_out_S_loss: 4.6425\n",
      "Epoch 61/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 46.9762 - out_T_loss: 1.0871 - out_S_loss: 4.5889 - val_loss: 47.3684 - val_out_T_loss: 1.1412 - val_out_S_loss: 4.6227\n",
      "Epoch 62/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 46.5250 - out_T_loss: 1.0392 - out_S_loss: 4.5486 - val_loss: 47.1402 - val_out_T_loss: 1.1132 - val_out_S_loss: 4.6027\n",
      "Epoch 63/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 46.3502 - out_T_loss: 1.0500 - out_S_loss: 4.5300 - val_loss: 46.9012 - val_out_T_loss: 1.0903 - val_out_S_loss: 4.5811\n",
      "Epoch 64/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 46.1899 - out_T_loss: 0.9621 - out_S_loss: 4.5228 - val_loss: 46.7464 - val_out_T_loss: 1.1200 - val_out_S_loss: 4.5626\n",
      "Epoch 65/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 46.3614 - out_T_loss: 1.0165 - out_S_loss: 4.5345 - val_loss: 46.4846 - val_out_T_loss: 1.0532 - val_out_S_loss: 4.5431\n",
      "Epoch 66/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 45.8407 - out_T_loss: 0.9554 - out_S_loss: 4.4885 - val_loss: 46.2859 - val_out_T_loss: 1.0502 - val_out_S_loss: 4.5236\n",
      "Epoch 67/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 45.5209 - out_T_loss: 0.8944 - out_S_loss: 4.4627 - val_loss: 46.0797 - val_out_T_loss: 1.0238 - val_out_S_loss: 4.5056\n",
      "Epoch 68/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 45.2384 - out_T_loss: 0.9554 - out_S_loss: 4.4283 - val_loss: 45.8647 - val_out_T_loss: 1.0131 - val_out_S_loss: 4.4852\n",
      "Epoch 69/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 45.2613 - out_T_loss: 0.9578 - out_S_loss: 4.4303 - val_loss: 45.6467 - val_out_T_loss: 0.9968 - val_out_S_loss: 4.4650\n",
      "Epoch 70/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 45.2180 - out_T_loss: 0.9156 - out_S_loss: 4.4302 - val_loss: 45.4518 - val_out_T_loss: 0.9839 - val_out_S_loss: 4.4468\n",
      "Epoch 71/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 44.7722 - out_T_loss: 0.8351 - out_S_loss: 4.3937 - val_loss: 45.2325 - val_out_T_loss: 0.9762 - val_out_S_loss: 4.4256\n",
      "Epoch 72/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 44.7827 - out_T_loss: 0.8440 - out_S_loss: 4.3939 - val_loss: 45.0445 - val_out_T_loss: 0.9693 - val_out_S_loss: 4.4075\n",
      "Epoch 73/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 44.5989 - out_T_loss: 0.7971 - out_S_loss: 4.3802 - val_loss: 44.8467 - val_out_T_loss: 0.9561 - val_out_S_loss: 4.3891\n",
      "Epoch 74/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 44.3769 - out_T_loss: 0.7760 - out_S_loss: 4.3601 - val_loss: 44.6166 - val_out_T_loss: 0.9077 - val_out_S_loss: 4.3709\n",
      "Epoch 75/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 44.1103 - out_T_loss: 0.7915 - out_S_loss: 4.3319 - val_loss: 44.4668 - val_out_T_loss: 0.9183 - val_out_S_loss: 4.3549\n",
      "Epoch 76/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 43.9489 - out_T_loss: 0.7860 - out_S_loss: 4.3163 - val_loss: 44.2616 - val_out_T_loss: 0.9019 - val_out_S_loss: 4.3360\n",
      "Epoch 77/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 43.5589 - out_T_loss: 0.7490 - out_S_loss: 4.2810 - val_loss: 44.0608 - val_out_T_loss: 0.8846 - val_out_S_loss: 4.3176\n",
      "Epoch 78/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 43.2889 - out_T_loss: 0.7333 - out_S_loss: 4.2556 - val_loss: 43.8519 - val_out_T_loss: 0.8469 - val_out_S_loss: 4.3005\n",
      "Epoch 79/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 43.2921 - out_T_loss: 0.7222 - out_S_loss: 4.2570 - val_loss: 43.6948 - val_out_T_loss: 0.8528 - val_out_S_loss: 4.2842\n",
      "Epoch 80/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 43.2480 - out_T_loss: 0.7027 - out_S_loss: 4.2545 - val_loss: 43.4572 - val_out_T_loss: 0.8354 - val_out_S_loss: 4.2622\n",
      "Epoch 81/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 43.2623 - out_T_loss: 0.6969 - out_S_loss: 4.2565 - val_loss: 43.2700 - val_out_T_loss: 0.8274 - val_out_S_loss: 4.2443\n",
      "Epoch 82/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 42.8527 - out_T_loss: 0.7275 - out_S_loss: 4.2125 - val_loss: 43.1311 - val_out_T_loss: 0.8148 - val_out_S_loss: 4.2316\n",
      "Epoch 83/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 42.4933 - out_T_loss: 0.6691 - out_S_loss: 4.1824 - val_loss: 42.9239 - val_out_T_loss: 0.7937 - val_out_S_loss: 4.2130\n",
      "Epoch 84/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 42.5035 - out_T_loss: 0.6661 - out_S_loss: 4.1837 - val_loss: 42.6948 - val_out_T_loss: 0.7846 - val_out_S_loss: 4.1910\n",
      "Epoch 85/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 42.1889 - out_T_loss: 0.6498 - out_S_loss: 4.1539 - val_loss: 42.5217 - val_out_T_loss: 0.7798 - val_out_S_loss: 4.1742\n",
      "Epoch 86/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 42.1989 - out_T_loss: 0.6268 - out_S_loss: 4.1572 - val_loss: 42.4159 - val_out_T_loss: 0.7935 - val_out_S_loss: 4.1622\n",
      "Epoch 87/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 42.2641 - out_T_loss: 0.6108 - out_S_loss: 4.1653 - val_loss: 42.2261 - val_out_T_loss: 0.7730 - val_out_S_loss: 4.1453\n",
      "Epoch 88/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 42.4689 - out_T_loss: 0.6092 - out_S_loss: 4.1860 - val_loss: 42.0293 - val_out_T_loss: 0.7748 - val_out_S_loss: 4.1255\n",
      "Epoch 89/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 41.9698 - out_T_loss: 0.6032 - out_S_loss: 4.1367 - val_loss: 41.9077 - val_out_T_loss: 0.7822 - val_out_S_loss: 4.1126\n",
      "Epoch 90/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 41.5085 - out_T_loss: 0.5579 - out_S_loss: 4.0951 - val_loss: 41.7246 - val_out_T_loss: 0.7445 - val_out_S_loss: 4.0980\n",
      "Epoch 91/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 41.4006 - out_T_loss: 0.5787 - out_S_loss: 4.0822 - val_loss: 41.5914 - val_out_T_loss: 0.7949 - val_out_S_loss: 4.0797\n",
      "Epoch 92/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 41.2422 - out_T_loss: 0.5453 - out_S_loss: 4.0697 - val_loss: 41.3380 - val_out_T_loss: 0.7392 - val_out_S_loss: 4.0599\n",
      "Epoch 93/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 41.2343 - out_T_loss: 0.5500 - out_S_loss: 4.0684 - val_loss: 41.1652 - val_out_T_loss: 0.7197 - val_out_S_loss: 4.0446\n",
      "Epoch 94/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 40.8638 - out_T_loss: 0.5430 - out_S_loss: 4.0321 - val_loss: 41.0121 - val_out_T_loss: 0.7136 - val_out_S_loss: 4.0298\n",
      "Epoch 95/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 40.9134 - out_T_loss: 0.5499 - out_S_loss: 4.0364 - val_loss: 40.8495 - val_out_T_loss: 0.7296 - val_out_S_loss: 4.0120\n",
      "Epoch 96/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 40.9527 - out_T_loss: 0.5557 - out_S_loss: 4.0397 - val_loss: 40.6891 - val_out_T_loss: 0.7068 - val_out_S_loss: 3.9982\n",
      "Epoch 97/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 40.6177 - out_T_loss: 0.4939 - out_S_loss: 4.0124 - val_loss: 40.5727 - val_out_T_loss: 0.6837 - val_out_S_loss: 3.9889\n",
      "Epoch 98/2000\n",
      "24/24 [==============================] - 0s 20ms/step - loss: 40.1772 - out_T_loss: 0.4803 - out_S_loss: 3.9697 - val_loss: 40.4150 - val_out_T_loss: 0.7192 - val_out_S_loss: 3.9696\n",
      "Epoch 99/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 40.5114 - out_T_loss: 0.4898 - out_S_loss: 4.0022 - val_loss: 40.1839 - val_out_T_loss: 0.7066 - val_out_S_loss: 3.9477\n",
      "Epoch 100/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 39.9528 - out_T_loss: 0.4979 - out_S_loss: 3.9455 - val_loss: 39.9829 - val_out_T_loss: 0.6575 - val_out_S_loss: 3.9325\n",
      "Epoch 101/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 40.2738 - out_T_loss: 0.4742 - out_S_loss: 3.9800 - val_loss: 39.8599 - val_out_T_loss: 0.6554 - val_out_S_loss: 3.9204\n",
      "Epoch 102/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 39.7827 - out_T_loss: 0.4718 - out_S_loss: 3.9311 - val_loss: 39.7026 - val_out_T_loss: 0.6430 - val_out_S_loss: 3.9060\n",
      "Epoch 103/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 39.6565 - out_T_loss: 0.4849 - out_S_loss: 3.9172 - val_loss: 39.5570 - val_out_T_loss: 0.6408 - val_out_S_loss: 3.8916\n",
      "Epoch 104/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 39.5124 - out_T_loss: 0.4784 - out_S_loss: 3.9034 - val_loss: 39.3828 - val_out_T_loss: 0.6330 - val_out_S_loss: 3.8750\n",
      "Epoch 105/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 39.1641 - out_T_loss: 0.4263 - out_S_loss: 3.8738 - val_loss: 39.2227 - val_out_T_loss: 0.6393 - val_out_S_loss: 3.8583\n",
      "Epoch 106/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 39.1986 - out_T_loss: 0.4170 - out_S_loss: 3.8782 - val_loss: 39.1136 - val_out_T_loss: 0.6634 - val_out_S_loss: 3.8450\n",
      "Epoch 107/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 39.1347 - out_T_loss: 0.4579 - out_S_loss: 3.8677 - val_loss: 38.9823 - val_out_T_loss: 0.6510 - val_out_S_loss: 3.8331\n",
      "Epoch 108/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 38.9768 - out_T_loss: 0.4127 - out_S_loss: 3.8564 - val_loss: 38.7779 - val_out_T_loss: 0.6261 - val_out_S_loss: 3.8152\n",
      "Epoch 109/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 38.7023 - out_T_loss: 0.4258 - out_S_loss: 3.8277 - val_loss: 38.6419 - val_out_T_loss: 0.6080 - val_out_S_loss: 3.8034\n",
      "Epoch 110/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 38.5330 - out_T_loss: 0.3964 - out_S_loss: 3.8137 - val_loss: 38.5108 - val_out_T_loss: 0.6041 - val_out_S_loss: 3.7907\n",
      "Epoch 111/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 38.6886 - out_T_loss: 0.3885 - out_S_loss: 3.8300 - val_loss: 38.3048 - val_out_T_loss: 0.6248 - val_out_S_loss: 3.7680\n",
      "Epoch 112/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 38.4362 - out_T_loss: 0.3525 - out_S_loss: 3.8084 - val_loss: 38.1462 - val_out_T_loss: 0.6132 - val_out_S_loss: 3.7533\n",
      "Epoch 113/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 38.4104 - out_T_loss: 0.4015 - out_S_loss: 3.8009 - val_loss: 37.9711 - val_out_T_loss: 0.5985 - val_out_S_loss: 3.7373\n",
      "Epoch 114/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 37.8864 - out_T_loss: 0.3906 - out_S_loss: 3.7496 - val_loss: 37.8402 - val_out_T_loss: 0.5757 - val_out_S_loss: 3.7264\n",
      "Epoch 115/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 38.1875 - out_T_loss: 0.3839 - out_S_loss: 3.7804 - val_loss: 37.7660 - val_out_T_loss: 0.6286 - val_out_S_loss: 3.7137\n",
      "Epoch 116/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 37.8398 - out_T_loss: 0.3917 - out_S_loss: 3.7448 - val_loss: 37.6113 - val_out_T_loss: 0.6021 - val_out_S_loss: 3.7009\n",
      "Epoch 117/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 37.9390 - out_T_loss: 0.3634 - out_S_loss: 3.7576 - val_loss: 37.4426 - val_out_T_loss: 0.5917 - val_out_S_loss: 3.6851\n",
      "Epoch 118/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 37.2843 - out_T_loss: 0.3314 - out_S_loss: 3.6953 - val_loss: 37.2770 - val_out_T_loss: 0.5863 - val_out_S_loss: 3.6691\n",
      "Epoch 119/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 37.4867 - out_T_loss: 0.3467 - out_S_loss: 3.7140 - val_loss: 37.1055 - val_out_T_loss: 0.6095 - val_out_S_loss: 3.6496\n",
      "Epoch 120/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 37.0067 - out_T_loss: 0.3324 - out_S_loss: 3.6674 - val_loss: 36.9522 - val_out_T_loss: 0.5712 - val_out_S_loss: 3.6381\n",
      "Epoch 121/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 37.5173 - out_T_loss: 0.3519 - out_S_loss: 3.7165 - val_loss: 36.8073 - val_out_T_loss: 0.5908 - val_out_S_loss: 3.6217\n",
      "Epoch 122/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 37.3003 - out_T_loss: 0.3560 - out_S_loss: 3.6944 - val_loss: 36.6365 - val_out_T_loss: 0.5806 - val_out_S_loss: 3.6056\n",
      "Epoch 123/2000\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 36.9696 - out_T_loss: 0.3321 - out_S_loss: 3.6637 - val_loss: 36.5504 - val_out_T_loss: 0.6060 - val_out_S_loss: 3.5944\n",
      "Epoch 124/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 36.7501 - out_T_loss: 0.3350 - out_S_loss: 3.6415 - val_loss: 36.4048 - val_out_T_loss: 0.5669 - val_out_S_loss: 3.5838\n",
      "Epoch 125/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 36.5571 - out_T_loss: 0.3159 - out_S_loss: 3.6241 - val_loss: 36.2606 - val_out_T_loss: 0.5459 - val_out_S_loss: 3.5715\n",
      "Epoch 126/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 36.6156 - out_T_loss: 0.3114 - out_S_loss: 3.6304 - val_loss: 36.0875 - val_out_T_loss: 0.5482 - val_out_S_loss: 3.5539\n",
      "Epoch 127/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 36.4009 - out_T_loss: 0.2906 - out_S_loss: 3.6110 - val_loss: 35.9660 - val_out_T_loss: 0.5580 - val_out_S_loss: 3.5408\n",
      "Epoch 128/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 36.2180 - out_T_loss: 0.2949 - out_S_loss: 3.5923 - val_loss: 35.8112 - val_out_T_loss: 0.5090 - val_out_S_loss: 3.5302\n",
      "Epoch 129/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 36.0062 - out_T_loss: 0.2971 - out_S_loss: 3.5709 - val_loss: 35.7463 - val_out_T_loss: 0.5616 - val_out_S_loss: 3.5185\n",
      "Epoch 130/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 36.3625 - out_T_loss: 0.3164 - out_S_loss: 3.6046 - val_loss: 35.5809 - val_out_T_loss: 0.5740 - val_out_S_loss: 3.5007\n",
      "Epoch 131/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 35.7447 - out_T_loss: 0.2813 - out_S_loss: 3.5463 - val_loss: 35.4106 - val_out_T_loss: 0.5368 - val_out_S_loss: 3.4874\n",
      "Epoch 132/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 35.9845 - out_T_loss: 0.2669 - out_S_loss: 3.5718 - val_loss: 35.2508 - val_out_T_loss: 0.5210 - val_out_S_loss: 3.4730\n",
      "Epoch 133/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 36.0479 - out_T_loss: 0.2720 - out_S_loss: 3.5776 - val_loss: 35.1640 - val_out_T_loss: 0.5320 - val_out_S_loss: 3.4632\n",
      "Epoch 134/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 35.6170 - out_T_loss: 0.2671 - out_S_loss: 3.5350 - val_loss: 35.0391 - val_out_T_loss: 0.5332 - val_out_S_loss: 3.4506\n",
      "Epoch 135/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 35.3102 - out_T_loss: 0.2712 - out_S_loss: 3.5039 - val_loss: 34.9041 - val_out_T_loss: 0.5096 - val_out_S_loss: 3.4395\n",
      "Epoch 136/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 35.4125 - out_T_loss: 0.2710 - out_S_loss: 3.5141 - val_loss: 34.8109 - val_out_T_loss: 0.5356 - val_out_S_loss: 3.4275\n",
      "Epoch 137/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 35.6892 - out_T_loss: 0.2790 - out_S_loss: 3.5410 - val_loss: 34.6764 - val_out_T_loss: 0.5304 - val_out_S_loss: 3.4146\n",
      "Epoch 138/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 35.0569 - out_T_loss: 0.2661 - out_S_loss: 3.4791 - val_loss: 34.5770 - val_out_T_loss: 0.5166 - val_out_S_loss: 3.4060\n",
      "Epoch 139/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 35.1391 - out_T_loss: 0.2481 - out_S_loss: 3.4891 - val_loss: 34.3771 - val_out_T_loss: 0.4929 - val_out_S_loss: 3.3884\n",
      "Epoch 140/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 34.6960 - out_T_loss: 0.2555 - out_S_loss: 3.4440 - val_loss: 34.2750 - val_out_T_loss: 0.5057 - val_out_S_loss: 3.3769\n",
      "Epoch 141/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 34.9305 - out_T_loss: 0.2411 - out_S_loss: 3.4689 - val_loss: 34.1356 - val_out_T_loss: 0.5020 - val_out_S_loss: 3.3634\n",
      "Epoch 142/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 34.7462 - out_T_loss: 0.2575 - out_S_loss: 3.4489 - val_loss: 34.0072 - val_out_T_loss: 0.5197 - val_out_S_loss: 3.3487\n",
      "Epoch 143/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 34.8615 - out_T_loss: 0.2435 - out_S_loss: 3.4618 - val_loss: 33.8744 - val_out_T_loss: 0.5015 - val_out_S_loss: 3.3373\n",
      "Epoch 144/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 34.3384 - out_T_loss: 0.2272 - out_S_loss: 3.4111 - val_loss: 33.7033 - val_out_T_loss: 0.4979 - val_out_S_loss: 3.3205\n",
      "Epoch 145/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 34.6724 - out_T_loss: 0.2575 - out_S_loss: 3.4415 - val_loss: 33.6548 - val_out_T_loss: 0.4984 - val_out_S_loss: 3.3156\n",
      "Epoch 146/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 34.4439 - out_T_loss: 0.2058 - out_S_loss: 3.4238 - val_loss: 33.4794 - val_out_T_loss: 0.4710 - val_out_S_loss: 3.3008\n",
      "Epoch 147/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 34.6267 - out_T_loss: 0.2479 - out_S_loss: 3.4379 - val_loss: 33.3439 - val_out_T_loss: 0.4625 - val_out_S_loss: 3.2881\n",
      "Epoch 148/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 33.9364 - out_T_loss: 0.2336 - out_S_loss: 3.3703 - val_loss: 33.2416 - val_out_T_loss: 0.4779 - val_out_S_loss: 3.2764\n",
      "Epoch 149/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 34.2273 - out_T_loss: 0.2189 - out_S_loss: 3.4008 - val_loss: 33.1009 - val_out_T_loss: 0.4712 - val_out_S_loss: 3.2630\n",
      "Epoch 150/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 33.8799 - out_T_loss: 0.2255 - out_S_loss: 3.3654 - val_loss: 33.0297 - val_out_T_loss: 0.5156 - val_out_S_loss: 3.2514\n",
      "Epoch 151/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 33.5454 - out_T_loss: 0.2079 - out_S_loss: 3.3337 - val_loss: 32.8915 - val_out_T_loss: 0.4948 - val_out_S_loss: 3.2397\n",
      "Epoch 152/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 33.5229 - out_T_loss: 0.2015 - out_S_loss: 3.3321 - val_loss: 32.7071 - val_out_T_loss: 0.4676 - val_out_S_loss: 3.2240\n",
      "Epoch 153/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 33.8553 - out_T_loss: 0.1840 - out_S_loss: 3.3671 - val_loss: 32.6053 - val_out_T_loss: 0.4693 - val_out_S_loss: 3.2136\n",
      "Epoch 154/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 33.6298 - out_T_loss: 0.2150 - out_S_loss: 3.3415 - val_loss: 32.4643 - val_out_T_loss: 0.4572 - val_out_S_loss: 3.2007\n",
      "Epoch 155/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 33.7546 - out_T_loss: 0.1859 - out_S_loss: 3.3569 - val_loss: 32.3488 - val_out_T_loss: 0.4532 - val_out_S_loss: 3.1896\n",
      "Epoch 156/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 33.4776 - out_T_loss: 0.1872 - out_S_loss: 3.3290 - val_loss: 32.2552 - val_out_T_loss: 0.4582 - val_out_S_loss: 3.1797\n",
      "Epoch 157/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 33.6476 - out_T_loss: 0.1957 - out_S_loss: 3.3452 - val_loss: 32.1720 - val_out_T_loss: 0.4800 - val_out_S_loss: 3.1692\n",
      "Epoch 158/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 33.1855 - out_T_loss: 0.1956 - out_S_loss: 3.2990 - val_loss: 32.0561 - val_out_T_loss: 0.4689 - val_out_S_loss: 3.1587\n",
      "Epoch 159/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 33.2906 - out_T_loss: 0.1949 - out_S_loss: 3.3096 - val_loss: 31.9117 - val_out_T_loss: 0.4535 - val_out_S_loss: 3.1458\n",
      "Epoch 160/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 32.8166 - out_T_loss: 0.1887 - out_S_loss: 3.2628 - val_loss: 31.8252 - val_out_T_loss: 0.4698 - val_out_S_loss: 3.1355\n",
      "Epoch 161/2000\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 32.8118 - out_T_loss: 0.1747 - out_S_loss: 3.2637 - val_loss: 31.6974 - val_out_T_loss: 0.4405 - val_out_S_loss: 3.1257\n",
      "Epoch 162/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 32.3810 - out_T_loss: 0.1585 - out_S_loss: 3.2222 - val_loss: 31.5604 - val_out_T_loss: 0.4348 - val_out_S_loss: 3.1126\n",
      "Epoch 163/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 32.5121 - out_T_loss: 0.1587 - out_S_loss: 3.2353 - val_loss: 31.5186 - val_out_T_loss: 0.4757 - val_out_S_loss: 3.1043\n",
      "Epoch 164/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 32.2184 - out_T_loss: 0.1672 - out_S_loss: 3.2051 - val_loss: 31.3502 - val_out_T_loss: 0.4648 - val_out_S_loss: 3.0885\n",
      "Epoch 165/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 32.8485 - out_T_loss: 0.2031 - out_S_loss: 3.2645 - val_loss: 31.2334 - val_out_T_loss: 0.4637 - val_out_S_loss: 3.0770\n",
      "Epoch 166/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 31.7497 - out_T_loss: 0.1570 - out_S_loss: 3.1593 - val_loss: 31.1347 - val_out_T_loss: 0.4626 - val_out_S_loss: 3.0672\n",
      "Epoch 167/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 32.0843 - out_T_loss: 0.1665 - out_S_loss: 3.1918 - val_loss: 31.0535 - val_out_T_loss: 0.4931 - val_out_S_loss: 3.0560\n",
      "Epoch 168/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 31.8891 - out_T_loss: 0.1618 - out_S_loss: 3.1727 - val_loss: 30.9152 - val_out_T_loss: 0.4554 - val_out_S_loss: 3.0460\n",
      "Epoch 169/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 31.8448 - out_T_loss: 0.1495 - out_S_loss: 3.1695 - val_loss: 30.8098 - val_out_T_loss: 0.4327 - val_out_S_loss: 3.0377\n",
      "Epoch 170/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 31.5574 - out_T_loss: 0.1632 - out_S_loss: 3.1394 - val_loss: 30.6656 - val_out_T_loss: 0.4303 - val_out_S_loss: 3.0235\n",
      "Epoch 171/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 31.6020 - out_T_loss: 0.1507 - out_S_loss: 3.1451 - val_loss: 30.5768 - val_out_T_loss: 0.4384 - val_out_S_loss: 3.0138\n",
      "Epoch 172/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 31.5769 - out_T_loss: 0.1432 - out_S_loss: 3.1434 - val_loss: 30.4590 - val_out_T_loss: 0.4418 - val_out_S_loss: 3.0017\n",
      "Epoch 173/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 31.6034 - out_T_loss: 0.1345 - out_S_loss: 3.1469 - val_loss: 30.3512 - val_out_T_loss: 0.4562 - val_out_S_loss: 2.9895\n",
      "Epoch 174/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 31.3836 - out_T_loss: 0.1542 - out_S_loss: 3.1229 - val_loss: 30.2931 - val_out_T_loss: 0.4582 - val_out_S_loss: 2.9835\n",
      "Epoch 175/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 31.2387 - out_T_loss: 0.1594 - out_S_loss: 3.1079 - val_loss: 30.0880 - val_out_T_loss: 0.4104 - val_out_S_loss: 2.9678\n",
      "Epoch 176/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 31.0116 - out_T_loss: 0.1447 - out_S_loss: 3.0867 - val_loss: 29.9638 - val_out_T_loss: 0.4247 - val_out_S_loss: 2.9539\n",
      "Epoch 177/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 31.0831 - out_T_loss: 0.1275 - out_S_loss: 3.0956 - val_loss: 29.8657 - val_out_T_loss: 0.4371 - val_out_S_loss: 2.9429\n",
      "Epoch 178/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 31.1181 - out_T_loss: 0.1315 - out_S_loss: 3.0987 - val_loss: 29.7408 - val_out_T_loss: 0.4255 - val_out_S_loss: 2.9315\n",
      "Epoch 179/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 30.6266 - out_T_loss: 0.1232 - out_S_loss: 3.0503 - val_loss: 29.6317 - val_out_T_loss: 0.3993 - val_out_S_loss: 2.9232\n",
      "Epoch 180/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 30.6453 - out_T_loss: 0.1331 - out_S_loss: 3.0512 - val_loss: 29.5940 - val_out_T_loss: 0.4498 - val_out_S_loss: 2.9144\n",
      "Epoch 181/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 30.5810 - out_T_loss: 0.1360 - out_S_loss: 3.0445 - val_loss: 29.4870 - val_out_T_loss: 0.4331 - val_out_S_loss: 2.9054\n",
      "Epoch 182/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 30.5246 - out_T_loss: 0.1110 - out_S_loss: 3.0414 - val_loss: 29.3455 - val_out_T_loss: 0.4041 - val_out_S_loss: 2.8941\n",
      "Epoch 183/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 30.1643 - out_T_loss: 0.1084 - out_S_loss: 3.0056 - val_loss: 29.1741 - val_out_T_loss: 0.3844 - val_out_S_loss: 2.8790\n",
      "Epoch 184/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 30.4390 - out_T_loss: 0.1312 - out_S_loss: 3.0308 - val_loss: 29.1388 - val_out_T_loss: 0.4410 - val_out_S_loss: 2.8698\n",
      "Epoch 185/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 30.1161 - out_T_loss: 0.1245 - out_S_loss: 2.9992 - val_loss: 29.0380 - val_out_T_loss: 0.4441 - val_out_S_loss: 2.8594\n",
      "Epoch 186/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 30.6182 - out_T_loss: 0.1234 - out_S_loss: 3.0495 - val_loss: 28.9048 - val_out_T_loss: 0.4164 - val_out_S_loss: 2.8488\n",
      "Epoch 187/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 30.5952 - out_T_loss: 0.1195 - out_S_loss: 3.0476 - val_loss: 28.8374 - val_out_T_loss: 0.3992 - val_out_S_loss: 2.8438\n",
      "Epoch 188/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 29.9464 - out_T_loss: 0.1141 - out_S_loss: 2.9832 - val_loss: 28.6871 - val_out_T_loss: 0.4068 - val_out_S_loss: 2.8280\n",
      "Epoch 189/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.7254 - out_T_loss: 0.1325 - out_S_loss: 2.9593 - val_loss: 28.5758 - val_out_T_loss: 0.4192 - val_out_S_loss: 2.8157\n",
      "Epoch 190/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 30.2327 - out_T_loss: 0.1216 - out_S_loss: 3.0111 - val_loss: 28.4959 - val_out_T_loss: 0.4098 - val_out_S_loss: 2.8086\n",
      "Epoch 191/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.6747 - out_T_loss: 0.1119 - out_S_loss: 2.9563 - val_loss: 28.4170 - val_out_T_loss: 0.4130 - val_out_S_loss: 2.8004\n",
      "Epoch 192/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.5474 - out_T_loss: 0.0963 - out_S_loss: 2.9451 - val_loss: 28.3644 - val_out_T_loss: 0.4151 - val_out_S_loss: 2.7949\n",
      "Epoch 193/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 29.2655 - out_T_loss: 0.1059 - out_S_loss: 2.9160 - val_loss: 28.2567 - val_out_T_loss: 0.4060 - val_out_S_loss: 2.7851\n",
      "Epoch 194/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.4182 - out_T_loss: 0.1083 - out_S_loss: 2.9310 - val_loss: 28.1932 - val_out_T_loss: 0.4486 - val_out_S_loss: 2.7745\n",
      "Epoch 195/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.2873 - out_T_loss: 0.1097 - out_S_loss: 2.9178 - val_loss: 28.0451 - val_out_T_loss: 0.3933 - val_out_S_loss: 2.7652\n",
      "Epoch 196/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.8019 - out_T_loss: 0.0982 - out_S_loss: 2.9704 - val_loss: 27.9756 - val_out_T_loss: 0.4270 - val_out_S_loss: 2.7549\n",
      "Epoch 197/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 28.9021 - out_T_loss: 0.0953 - out_S_loss: 2.8807 - val_loss: 27.8388 - val_out_T_loss: 0.4210 - val_out_S_loss: 2.7418\n",
      "Epoch 198/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 29.5367 - out_T_loss: 0.0890 - out_S_loss: 2.9448 - val_loss: 27.7652 - val_out_T_loss: 0.4228 - val_out_S_loss: 2.7342\n",
      "Epoch 199/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.2291 - out_T_loss: 0.0911 - out_S_loss: 2.9138 - val_loss: 27.6723 - val_out_T_loss: 0.4415 - val_out_S_loss: 2.7231\n",
      "Epoch 200/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.1505 - out_T_loss: 0.0884 - out_S_loss: 2.9062 - val_loss: 27.5794 - val_out_T_loss: 0.4213 - val_out_S_loss: 2.7158\n",
      "Epoch 201/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 29.1387 - out_T_loss: 0.0987 - out_S_loss: 2.9040 - val_loss: 27.5355 - val_out_T_loss: 0.4484 - val_out_S_loss: 2.7087\n",
      "Epoch 202/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 29.3727 - out_T_loss: 0.0912 - out_S_loss: 2.9282 - val_loss: 27.4302 - val_out_T_loss: 0.4100 - val_out_S_loss: 2.7020\n",
      "Epoch 203/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 28.9216 - out_T_loss: 0.0839 - out_S_loss: 2.8838 - val_loss: 27.3057 - val_out_T_loss: 0.3961 - val_out_S_loss: 2.6910\n",
      "Epoch 204/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 28.5454 - out_T_loss: 0.0916 - out_S_loss: 2.8454 - val_loss: 27.1720 - val_out_T_loss: 0.3933 - val_out_S_loss: 2.6779\n",
      "Epoch 205/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 28.6420 - out_T_loss: 0.0940 - out_S_loss: 2.8548 - val_loss: 27.0662 - val_out_T_loss: 0.3830 - val_out_S_loss: 2.6683\n",
      "Epoch 206/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 28.4153 - out_T_loss: 0.0829 - out_S_loss: 2.8332 - val_loss: 26.9960 - val_out_T_loss: 0.3836 - val_out_S_loss: 2.6612\n",
      "Epoch 207/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 28.2399 - out_T_loss: 0.0928 - out_S_loss: 2.8147 - val_loss: 26.9090 - val_out_T_loss: 0.3789 - val_out_S_loss: 2.6530\n",
      "Epoch 208/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 28.7541 - out_T_loss: 0.0775 - out_S_loss: 2.8677 - val_loss: 26.8266 - val_out_T_loss: 0.4028 - val_out_S_loss: 2.6424\n",
      "Epoch 209/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 28.7221 - out_T_loss: 0.0765 - out_S_loss: 2.8646 - val_loss: 26.7246 - val_out_T_loss: 0.4164 - val_out_S_loss: 2.6308\n",
      "Epoch 210/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 28.4139 - out_T_loss: 0.0971 - out_S_loss: 2.8317 - val_loss: 26.6609 - val_out_T_loss: 0.4032 - val_out_S_loss: 2.6258\n",
      "Epoch 211/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 28.2906 - out_T_loss: 0.0756 - out_S_loss: 2.8215 - val_loss: 26.5548 - val_out_T_loss: 0.4114 - val_out_S_loss: 2.6143\n",
      "Epoch 212/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 28.4730 - out_T_loss: 0.0699 - out_S_loss: 2.8403 - val_loss: 26.4859 - val_out_T_loss: 0.4132 - val_out_S_loss: 2.6073\n",
      "Epoch 213/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 28.3948 - out_T_loss: 0.0996 - out_S_loss: 2.8295 - val_loss: 26.3656 - val_out_T_loss: 0.3841 - val_out_S_loss: 2.5982\n",
      "Epoch 214/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 28.0646 - out_T_loss: 0.0790 - out_S_loss: 2.7986 - val_loss: 26.3226 - val_out_T_loss: 0.4031 - val_out_S_loss: 2.5920\n",
      "Epoch 215/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 28.0600 - out_T_loss: 0.0644 - out_S_loss: 2.7996 - val_loss: 26.2147 - val_out_T_loss: 0.3987 - val_out_S_loss: 2.5816\n",
      "Epoch 216/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 27.7823 - out_T_loss: 0.0739 - out_S_loss: 2.7708 - val_loss: 26.0648 - val_out_T_loss: 0.3685 - val_out_S_loss: 2.5696\n",
      "Epoch 217/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 27.2493 - out_T_loss: 0.0783 - out_S_loss: 2.7171 - val_loss: 26.0435 - val_out_T_loss: 0.3952 - val_out_S_loss: 2.5648\n",
      "Epoch 218/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 27.6832 - out_T_loss: 0.0731 - out_S_loss: 2.7610 - val_loss: 25.9937 - val_out_T_loss: 0.4045 - val_out_S_loss: 2.5589\n",
      "Epoch 219/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 27.5299 - out_T_loss: 0.0583 - out_S_loss: 2.7472 - val_loss: 25.8487 - val_out_T_loss: 0.3738 - val_out_S_loss: 2.5475\n",
      "Epoch 220/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 27.4729 - out_T_loss: 0.0756 - out_S_loss: 2.7397 - val_loss: 25.8057 - val_out_T_loss: 0.3887 - val_out_S_loss: 2.5417\n",
      "Epoch 221/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 27.1042 - out_T_loss: 0.0830 - out_S_loss: 2.7021 - val_loss: 25.7283 - val_out_T_loss: 0.3780 - val_out_S_loss: 2.5350\n",
      "Epoch 222/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 27.2354 - out_T_loss: 0.0629 - out_S_loss: 2.7173 - val_loss: 25.6393 - val_out_T_loss: 0.3878 - val_out_S_loss: 2.5252\n",
      "Epoch 223/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 27.1383 - out_T_loss: 0.0674 - out_S_loss: 2.7071 - val_loss: 25.5027 - val_out_T_loss: 0.3740 - val_out_S_loss: 2.5129\n",
      "Epoch 224/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 27.1230 - out_T_loss: 0.0674 - out_S_loss: 2.7056 - val_loss: 25.4613 - val_out_T_loss: 0.3851 - val_out_S_loss: 2.5076\n",
      "Epoch 225/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 26.5142 - out_T_loss: 0.0724 - out_S_loss: 2.6442 - val_loss: 25.4103 - val_out_T_loss: 0.3936 - val_out_S_loss: 2.5017\n",
      "Epoch 226/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 26.8774 - out_T_loss: 0.0636 - out_S_loss: 2.6814 - val_loss: 25.3037 - val_out_T_loss: 0.4081 - val_out_S_loss: 2.4896\n",
      "Epoch 227/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.7316 - out_T_loss: 0.0611 - out_S_loss: 2.6671 - val_loss: 25.2521 - val_out_T_loss: 0.3935 - val_out_S_loss: 2.4859\n",
      "Epoch 228/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 26.9466 - out_T_loss: 0.0527 - out_S_loss: 2.6894 - val_loss: 25.1419 - val_out_T_loss: 0.4080 - val_out_S_loss: 2.4734\n",
      "Epoch 229/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 27.2143 - out_T_loss: 0.0579 - out_S_loss: 2.7156 - val_loss: 25.0308 - val_out_T_loss: 0.3907 - val_out_S_loss: 2.4640\n",
      "Epoch 230/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.8454 - out_T_loss: 0.0655 - out_S_loss: 2.6780 - val_loss: 25.0120 - val_out_T_loss: 0.4493 - val_out_S_loss: 2.4563\n",
      "Epoch 231/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 26.5759 - out_T_loss: 0.0639 - out_S_loss: 2.6512 - val_loss: 24.9077 - val_out_T_loss: 0.4088 - val_out_S_loss: 2.4499\n",
      "Epoch 232/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.2949 - out_T_loss: 0.0571 - out_S_loss: 2.6238 - val_loss: 24.8011 - val_out_T_loss: 0.3857 - val_out_S_loss: 2.4415\n",
      "Epoch 233/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.7356 - out_T_loss: 0.0480 - out_S_loss: 2.6688 - val_loss: 24.7258 - val_out_T_loss: 0.3876 - val_out_S_loss: 2.4338\n",
      "Epoch 234/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.2008 - out_T_loss: 0.0479 - out_S_loss: 2.6153 - val_loss: 24.6512 - val_out_T_loss: 0.3991 - val_out_S_loss: 2.4252\n",
      "Epoch 235/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.2565 - out_T_loss: 0.0462 - out_S_loss: 2.6210 - val_loss: 24.6076 - val_out_T_loss: 0.4117 - val_out_S_loss: 2.4196\n",
      "Epoch 236/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.4741 - out_T_loss: 0.0572 - out_S_loss: 2.6417 - val_loss: 24.4790 - val_out_T_loss: 0.3622 - val_out_S_loss: 2.4117\n",
      "Epoch 237/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.7116 - out_T_loss: 0.0536 - out_S_loss: 2.5658 - val_loss: 24.4717 - val_out_T_loss: 0.3787 - val_out_S_loss: 2.4093\n",
      "Epoch 238/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 26.1762 - out_T_loss: 0.0577 - out_S_loss: 2.6118 - val_loss: 24.4752 - val_out_T_loss: 0.4002 - val_out_S_loss: 2.4075\n",
      "Epoch 239/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.0985 - out_T_loss: 0.0395 - out_S_loss: 2.6059 - val_loss: 24.3261 - val_out_T_loss: 0.4242 - val_out_S_loss: 2.3902\n",
      "Epoch 240/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.0259 - out_T_loss: 0.0437 - out_S_loss: 2.5982 - val_loss: 24.2373 - val_out_T_loss: 0.4054 - val_out_S_loss: 2.3832\n",
      "Epoch 241/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 25.9335 - out_T_loss: 0.0446 - out_S_loss: 2.5889 - val_loss: 24.2165 - val_out_T_loss: 0.4162 - val_out_S_loss: 2.3800\n",
      "Epoch 242/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 26.2533 - out_T_loss: 0.0431 - out_S_loss: 2.6210 - val_loss: 24.1588 - val_out_T_loss: 0.3959 - val_out_S_loss: 2.3763\n",
      "Epoch 243/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 25.8078 - out_T_loss: 0.0516 - out_S_loss: 2.5756 - val_loss: 24.0606 - val_out_T_loss: 0.4102 - val_out_S_loss: 2.3650\n",
      "Epoch 244/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 25.6833 - out_T_loss: 0.0557 - out_S_loss: 2.5628 - val_loss: 23.9536 - val_out_T_loss: 0.4281 - val_out_S_loss: 2.3526\n",
      "Epoch 245/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.8229 - out_T_loss: 0.0443 - out_S_loss: 2.5779 - val_loss: 23.8810 - val_out_T_loss: 0.3975 - val_out_S_loss: 2.3484\n",
      "Epoch 246/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.2499 - out_T_loss: 0.0399 - out_S_loss: 2.5210 - val_loss: 23.8128 - val_out_T_loss: 0.3770 - val_out_S_loss: 2.3436\n",
      "Epoch 247/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.2143 - out_T_loss: 0.0407 - out_S_loss: 2.5174 - val_loss: 23.7023 - val_out_T_loss: 0.3876 - val_out_S_loss: 2.3315\n",
      "Epoch 248/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.5190 - out_T_loss: 0.0349 - out_S_loss: 2.5484 - val_loss: 23.6614 - val_out_T_loss: 0.3978 - val_out_S_loss: 2.3264\n",
      "Epoch 249/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.3146 - out_T_loss: 0.0412 - out_S_loss: 2.5273 - val_loss: 23.5734 - val_out_T_loss: 0.3771 - val_out_S_loss: 2.3196\n",
      "Epoch 250/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 25.5614 - out_T_loss: 0.0443 - out_S_loss: 2.5517 - val_loss: 23.4920 - val_out_T_loss: 0.3805 - val_out_S_loss: 2.3112\n",
      "Epoch 251/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.7033 - out_T_loss: 0.0498 - out_S_loss: 2.5653 - val_loss: 23.4532 - val_out_T_loss: 0.4192 - val_out_S_loss: 2.3034\n",
      "Epoch 252/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.0146 - out_T_loss: 0.0406 - out_S_loss: 2.4974 - val_loss: 23.4205 - val_out_T_loss: 0.4166 - val_out_S_loss: 2.3004\n",
      "Epoch 253/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 25.2047 - out_T_loss: 0.0356 - out_S_loss: 2.5169 - val_loss: 23.3348 - val_out_T_loss: 0.3981 - val_out_S_loss: 2.2937\n",
      "Epoch 254/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.4640 - out_T_loss: 0.0339 - out_S_loss: 2.5430 - val_loss: 23.2782 - val_out_T_loss: 0.3719 - val_out_S_loss: 2.2906\n",
      "Epoch 255/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 24.7488 - out_T_loss: 0.0398 - out_S_loss: 2.4709 - val_loss: 23.1986 - val_out_T_loss: 0.3798 - val_out_S_loss: 2.2819\n",
      "Epoch 256/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.5262 - out_T_loss: 0.0332 - out_S_loss: 2.5493 - val_loss: 23.1241 - val_out_T_loss: 0.3764 - val_out_S_loss: 2.2748\n",
      "Epoch 257/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.8084 - out_T_loss: 0.0316 - out_S_loss: 2.4777 - val_loss: 23.1026 - val_out_T_loss: 0.3916 - val_out_S_loss: 2.2711\n",
      "Epoch 258/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 25.0311 - out_T_loss: 0.0380 - out_S_loss: 2.4993 - val_loss: 23.0448 - val_out_T_loss: 0.3828 - val_out_S_loss: 2.2662\n",
      "Epoch 259/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 25.3358 - out_T_loss: 0.0291 - out_S_loss: 2.5307 - val_loss: 23.0085 - val_out_T_loss: 0.3962 - val_out_S_loss: 2.2612\n",
      "Epoch 260/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.5894 - out_T_loss: 0.0349 - out_S_loss: 2.4555 - val_loss: 22.8800 - val_out_T_loss: 0.3970 - val_out_S_loss: 2.2483\n",
      "Epoch 261/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 24.7752 - out_T_loss: 0.0437 - out_S_loss: 2.4731 - val_loss: 22.8375 - val_out_T_loss: 0.3995 - val_out_S_loss: 2.2438\n",
      "Epoch 262/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.3773 - out_T_loss: 0.0423 - out_S_loss: 2.4335 - val_loss: 22.7511 - val_out_T_loss: 0.3771 - val_out_S_loss: 2.2374\n",
      "Epoch 263/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.4001 - out_T_loss: 0.0400 - out_S_loss: 2.4360 - val_loss: 22.7519 - val_out_T_loss: 0.4280 - val_out_S_loss: 2.2324\n",
      "Epoch 264/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.5912 - out_T_loss: 0.0319 - out_S_loss: 2.4559 - val_loss: 22.6538 - val_out_T_loss: 0.3626 - val_out_S_loss: 2.2291\n",
      "Epoch 265/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.6020 - out_T_loss: 0.0384 - out_S_loss: 2.4564 - val_loss: 22.6160 - val_out_T_loss: 0.4094 - val_out_S_loss: 2.2207\n",
      "Epoch 266/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.3647 - out_T_loss: 0.0334 - out_S_loss: 2.4331 - val_loss: 22.5256 - val_out_T_loss: 0.4234 - val_out_S_loss: 2.2102\n",
      "Epoch 267/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 24.2582 - out_T_loss: 0.0379 - out_S_loss: 2.4220 - val_loss: 22.4741 - val_out_T_loss: 0.4209 - val_out_S_loss: 2.2053\n",
      "Epoch 268/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.0848 - out_T_loss: 0.0271 - out_S_loss: 2.4058 - val_loss: 22.3408 - val_out_T_loss: 0.3862 - val_out_S_loss: 2.1955\n",
      "Epoch 269/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 24.0430 - out_T_loss: 0.0265 - out_S_loss: 2.4016 - val_loss: 22.3224 - val_out_T_loss: 0.4113 - val_out_S_loss: 2.1911\n",
      "Epoch 270/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.8793 - out_T_loss: 0.0223 - out_S_loss: 2.3857 - val_loss: 22.2727 - val_out_T_loss: 0.3957 - val_out_S_loss: 2.1877\n",
      "Epoch 271/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.8431 - out_T_loss: 0.0286 - out_S_loss: 2.3815 - val_loss: 22.2051 - val_out_T_loss: 0.4179 - val_out_S_loss: 2.1787\n",
      "Epoch 272/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.2540 - out_T_loss: 0.0360 - out_S_loss: 2.4218 - val_loss: 22.1536 - val_out_T_loss: 0.4185 - val_out_S_loss: 2.1735\n",
      "Epoch 273/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.8732 - out_T_loss: 0.0300 - out_S_loss: 2.3843 - val_loss: 22.0632 - val_out_T_loss: 0.3959 - val_out_S_loss: 2.1667\n",
      "Epoch 274/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 24.2918 - out_T_loss: 0.0281 - out_S_loss: 2.4264 - val_loss: 22.0069 - val_out_T_loss: 0.3738 - val_out_S_loss: 2.1633\n",
      "Epoch 275/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.4459 - out_T_loss: 0.0224 - out_S_loss: 2.3424 - val_loss: 21.9254 - val_out_T_loss: 0.3624 - val_out_S_loss: 2.1563\n",
      "Epoch 276/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.6504 - out_T_loss: 0.0463 - out_S_loss: 2.3604 - val_loss: 21.8827 - val_out_T_loss: 0.3857 - val_out_S_loss: 2.1497\n",
      "Epoch 277/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.9269 - out_T_loss: 0.0435 - out_S_loss: 2.3883 - val_loss: 21.8759 - val_out_T_loss: 0.4103 - val_out_S_loss: 2.1466\n",
      "Epoch 278/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 24.0737 - out_T_loss: 0.0308 - out_S_loss: 2.4043 - val_loss: 21.7403 - val_out_T_loss: 0.3949 - val_out_S_loss: 2.1345\n",
      "Epoch 279/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.5693 - out_T_loss: 0.0396 - out_S_loss: 2.3530 - val_loss: 21.6849 - val_out_T_loss: 0.3261 - val_out_S_loss: 2.1359\n",
      "Epoch 280/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.5984 - out_T_loss: 0.0281 - out_S_loss: 2.3570 - val_loss: 21.5630 - val_out_T_loss: 0.3375 - val_out_S_loss: 2.1225\n",
      "Epoch 281/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.6301 - out_T_loss: 0.0309 - out_S_loss: 2.3599 - val_loss: 21.5128 - val_out_T_loss: 0.3506 - val_out_S_loss: 2.1162\n",
      "Epoch 282/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.3039 - out_T_loss: 0.0290 - out_S_loss: 2.3275 - val_loss: 21.5400 - val_out_T_loss: 0.3628 - val_out_S_loss: 2.1177\n",
      "Epoch 283/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.3648 - out_T_loss: 0.0279 - out_S_loss: 2.3337 - val_loss: 21.4870 - val_out_T_loss: 0.4018 - val_out_S_loss: 2.1085\n",
      "Epoch 284/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.2677 - out_T_loss: 0.0285 - out_S_loss: 2.3239 - val_loss: 21.4553 - val_out_T_loss: 0.4033 - val_out_S_loss: 2.1052\n",
      "Epoch 285/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.9030 - out_T_loss: 0.0318 - out_S_loss: 2.2871 - val_loss: 21.3489 - val_out_T_loss: 0.3630 - val_out_S_loss: 2.0986\n",
      "Epoch 286/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 22.9326 - out_T_loss: 0.0304 - out_S_loss: 2.2902 - val_loss: 21.3540 - val_out_T_loss: 0.3840 - val_out_S_loss: 2.0970\n",
      "Epoch 287/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.0970 - out_T_loss: 0.0285 - out_S_loss: 2.3069 - val_loss: 21.2684 - val_out_T_loss: 0.3929 - val_out_S_loss: 2.0875\n",
      "Epoch 288/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.3217 - out_T_loss: 0.0413 - out_S_loss: 2.3280 - val_loss: 21.2049 - val_out_T_loss: 0.3910 - val_out_S_loss: 2.0814\n",
      "Epoch 289/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.4276 - out_T_loss: 0.0608 - out_S_loss: 2.3367 - val_loss: 21.1976 - val_out_T_loss: 0.4471 - val_out_S_loss: 2.0751\n",
      "Epoch 290/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.2636 - out_T_loss: 0.0552 - out_S_loss: 2.3208 - val_loss: 21.1495 - val_out_T_loss: 0.4301 - val_out_S_loss: 2.0719\n",
      "Epoch 291/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.0357 - out_T_loss: 0.0429 - out_S_loss: 2.2993 - val_loss: 21.1232 - val_out_T_loss: 0.3946 - val_out_S_loss: 2.0729\n",
      "Epoch 292/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.7049 - out_T_loss: 0.0306 - out_S_loss: 2.2674 - val_loss: 21.0317 - val_out_T_loss: 0.3963 - val_out_S_loss: 2.0635\n",
      "Epoch 293/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 23.0655 - out_T_loss: 0.0255 - out_S_loss: 2.3040 - val_loss: 20.9494 - val_out_T_loss: 0.4108 - val_out_S_loss: 2.0539\n",
      "Epoch 294/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 23.1238 - out_T_loss: 0.0338 - out_S_loss: 2.3090 - val_loss: 20.9589 - val_out_T_loss: 0.4026 - val_out_S_loss: 2.0556\n",
      "Epoch 295/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.4359 - out_T_loss: 0.0310 - out_S_loss: 2.2405 - val_loss: 20.9760 - val_out_T_loss: 0.4336 - val_out_S_loss: 2.0542\n",
      "Epoch 296/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 22.5806 - out_T_loss: 0.0238 - out_S_loss: 2.2557 - val_loss: 20.8777 - val_out_T_loss: 0.4117 - val_out_S_loss: 2.0466\n",
      "Epoch 297/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.7397 - out_T_loss: 0.0280 - out_S_loss: 2.2712 - val_loss: 20.7813 - val_out_T_loss: 0.3920 - val_out_S_loss: 2.0389\n",
      "Epoch 298/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 22.3632 - out_T_loss: 0.0242 - out_S_loss: 2.2339 - val_loss: 20.7404 - val_out_T_loss: 0.4162 - val_out_S_loss: 2.0324\n",
      "Epoch 299/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.2078 - out_T_loss: 0.0229 - out_S_loss: 2.2185 - val_loss: 20.7054 - val_out_T_loss: 0.4134 - val_out_S_loss: 2.0292\n",
      "Epoch 300/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.2241 - out_T_loss: 0.0233 - out_S_loss: 2.2201 - val_loss: 20.6495 - val_out_T_loss: 0.3721 - val_out_S_loss: 2.0277\n",
      "Epoch 301/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 22.4407 - out_T_loss: 0.0204 - out_S_loss: 2.2420 - val_loss: 20.5398 - val_out_T_loss: 0.3524 - val_out_S_loss: 2.0187\n",
      "Epoch 302/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.4864 - out_T_loss: 0.0181 - out_S_loss: 2.2468 - val_loss: 20.5027 - val_out_T_loss: 0.3923 - val_out_S_loss: 2.0110\n",
      "Epoch 303/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 22.2582 - out_T_loss: 0.0177 - out_S_loss: 2.2240 - val_loss: 20.4653 - val_out_T_loss: 0.4073 - val_out_S_loss: 2.0058\n",
      "Epoch 304/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 22.2733 - out_T_loss: 0.0167 - out_S_loss: 2.2257 - val_loss: 20.3988 - val_out_T_loss: 0.3797 - val_out_S_loss: 2.0019\n",
      "Epoch 305/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.8826 - out_T_loss: 0.0186 - out_S_loss: 2.1864 - val_loss: 20.3502 - val_out_T_loss: 0.3621 - val_out_S_loss: 1.9988\n",
      "Epoch 306/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.2402 - out_T_loss: 0.0222 - out_S_loss: 2.2218 - val_loss: 20.2984 - val_out_T_loss: 0.3694 - val_out_S_loss: 1.9929\n",
      "Epoch 307/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.1759 - out_T_loss: 0.0222 - out_S_loss: 2.2154 - val_loss: 20.2629 - val_out_T_loss: 0.3790 - val_out_S_loss: 1.9884\n",
      "Epoch 308/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.8651 - out_T_loss: 0.0204 - out_S_loss: 2.1845 - val_loss: 20.1970 - val_out_T_loss: 0.3704 - val_out_S_loss: 1.9827\n",
      "Epoch 309/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.1220 - out_T_loss: 0.0192 - out_S_loss: 2.2103 - val_loss: 20.1840 - val_out_T_loss: 0.3635 - val_out_S_loss: 1.9821\n",
      "Epoch 310/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 22.3390 - out_T_loss: 0.0142 - out_S_loss: 2.2325 - val_loss: 20.1356 - val_out_T_loss: 0.3851 - val_out_S_loss: 1.9751\n",
      "Epoch 311/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 21.8663 - out_T_loss: 0.0171 - out_S_loss: 2.1849 - val_loss: 20.1276 - val_out_T_loss: 0.4119 - val_out_S_loss: 1.9716\n",
      "Epoch 312/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 22.0122 - out_T_loss: 0.0187 - out_S_loss: 2.1993 - val_loss: 20.0383 - val_out_T_loss: 0.3960 - val_out_S_loss: 1.9642\n",
      "Epoch 313/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.5571 - out_T_loss: 0.0128 - out_S_loss: 2.1544 - val_loss: 20.0235 - val_out_T_loss: 0.4150 - val_out_S_loss: 1.9609\n",
      "Epoch 314/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 21.5861 - out_T_loss: 0.0116 - out_S_loss: 2.1575 - val_loss: 19.9830 - val_out_T_loss: 0.4107 - val_out_S_loss: 1.9572\n",
      "Epoch 315/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 21.1433 - out_T_loss: 0.0118 - out_S_loss: 2.1131 - val_loss: 19.9097 - val_out_T_loss: 0.4045 - val_out_S_loss: 1.9505\n",
      "Epoch 316/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 21.8102 - out_T_loss: 0.0163 - out_S_loss: 2.1794 - val_loss: 19.9174 - val_out_T_loss: 0.4465 - val_out_S_loss: 1.9471\n",
      "Epoch 317/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 22.1445 - out_T_loss: 0.0245 - out_S_loss: 2.2120 - val_loss: 19.8956 - val_out_T_loss: 0.4674 - val_out_S_loss: 1.9428\n",
      "Epoch 318/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 21.7423 - out_T_loss: 0.0153 - out_S_loss: 2.1727 - val_loss: 19.7818 - val_out_T_loss: 0.4456 - val_out_S_loss: 1.9336\n",
      "Epoch 319/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.8439 - out_T_loss: 0.0164 - out_S_loss: 2.1827 - val_loss: 19.7224 - val_out_T_loss: 0.3913 - val_out_S_loss: 1.9331\n",
      "Epoch 320/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.5042 - out_T_loss: 0.0262 - out_S_loss: 2.1478 - val_loss: 19.7890 - val_out_T_loss: 0.4582 - val_out_S_loss: 1.9331\n",
      "Epoch 321/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 21.3327 - out_T_loss: 0.0295 - out_S_loss: 2.1303 - val_loss: 19.6742 - val_out_T_loss: 0.4247 - val_out_S_loss: 1.9249\n",
      "Epoch 322/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.5613 - out_T_loss: 0.0203 - out_S_loss: 2.1541 - val_loss: 19.6912 - val_out_T_loss: 0.4237 - val_out_S_loss: 1.9267\n",
      "Epoch 323/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.3903 - out_T_loss: 0.0171 - out_S_loss: 2.1373 - val_loss: 19.5909 - val_out_T_loss: 0.4015 - val_out_S_loss: 1.9189\n",
      "Epoch 324/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 21.7954 - out_T_loss: 0.0234 - out_S_loss: 2.1772 - val_loss: 19.4472 - val_out_T_loss: 0.3580 - val_out_S_loss: 1.9089\n",
      "Epoch 325/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.3112 - out_T_loss: 0.0239 - out_S_loss: 2.1287 - val_loss: 19.4584 - val_out_T_loss: 0.4253 - val_out_S_loss: 1.9033\n",
      "Epoch 326/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 20.9427 - out_T_loss: 0.0189 - out_S_loss: 2.0924 - val_loss: 19.4856 - val_out_T_loss: 0.4491 - val_out_S_loss: 1.9037\n",
      "Epoch 327/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.2427 - out_T_loss: 0.0183 - out_S_loss: 2.1224 - val_loss: 19.4146 - val_out_T_loss: 0.4178 - val_out_S_loss: 1.8997\n",
      "Epoch 328/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 20.7405 - out_T_loss: 0.0174 - out_S_loss: 2.0723 - val_loss: 19.3567 - val_out_T_loss: 0.4273 - val_out_S_loss: 1.8929\n",
      "Epoch 329/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 21.3462 - out_T_loss: 0.0137 - out_S_loss: 2.1333 - val_loss: 19.2926 - val_out_T_loss: 0.4254 - val_out_S_loss: 1.8867\n",
      "Epoch 330/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 20.6721 - out_T_loss: 0.0131 - out_S_loss: 2.0659 - val_loss: 19.2014 - val_out_T_loss: 0.3660 - val_out_S_loss: 1.8835\n",
      "Epoch 331/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.9476 - out_T_loss: 0.0221 - out_S_loss: 2.0925 - val_loss: 19.1843 - val_out_T_loss: 0.3962 - val_out_S_loss: 1.8788\n",
      "Epoch 332/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.6311 - out_T_loss: 0.0150 - out_S_loss: 2.0616 - val_loss: 19.1763 - val_out_T_loss: 0.3859 - val_out_S_loss: 1.8790\n",
      "Epoch 333/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 21.4396 - out_T_loss: 0.0194 - out_S_loss: 2.1420 - val_loss: 19.1294 - val_out_T_loss: 0.3943 - val_out_S_loss: 1.8735\n",
      "Epoch 334/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.0862 - out_T_loss: 0.0105 - out_S_loss: 2.1076 - val_loss: 19.0763 - val_out_T_loss: 0.3976 - val_out_S_loss: 1.8679\n",
      "Epoch 335/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 20.9398 - out_T_loss: 0.0183 - out_S_loss: 2.0922 - val_loss: 18.9900 - val_out_T_loss: 0.3815 - val_out_S_loss: 1.8609\n",
      "Epoch 336/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 21.3808 - out_T_loss: 0.0217 - out_S_loss: 2.1359 - val_loss: 19.0280 - val_out_T_loss: 0.4341 - val_out_S_loss: 1.8594\n",
      "Epoch 337/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.1454 - out_T_loss: 0.0209 - out_S_loss: 2.1124 - val_loss: 18.9720 - val_out_T_loss: 0.4127 - val_out_S_loss: 1.8559\n",
      "Epoch 338/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.8184 - out_T_loss: 0.0185 - out_S_loss: 2.0800 - val_loss: 18.8983 - val_out_T_loss: 0.3945 - val_out_S_loss: 1.8504\n",
      "Epoch 339/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.5824 - out_T_loss: 0.0227 - out_S_loss: 2.0560 - val_loss: 18.8893 - val_out_T_loss: 0.4014 - val_out_S_loss: 1.8488\n",
      "Epoch 340/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 21.0341 - out_T_loss: 0.0214 - out_S_loss: 2.1013 - val_loss: 18.8669 - val_out_T_loss: 0.4260 - val_out_S_loss: 1.8441\n",
      "Epoch 341/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.9391 - out_T_loss: 0.0162 - out_S_loss: 2.0923 - val_loss: 18.8064 - val_out_T_loss: 0.3970 - val_out_S_loss: 1.8409\n",
      "Epoch 342/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.5095 - out_T_loss: 0.0113 - out_S_loss: 2.0498 - val_loss: 18.7491 - val_out_T_loss: 0.3845 - val_out_S_loss: 1.8365\n",
      "Epoch 343/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 20.5896 - out_T_loss: 0.0150 - out_S_loss: 2.0575 - val_loss: 18.7669 - val_out_T_loss: 0.4181 - val_out_S_loss: 1.8349\n",
      "Epoch 344/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 19.9942 - out_T_loss: 0.0133 - out_S_loss: 1.9981 - val_loss: 18.7375 - val_out_T_loss: 0.4316 - val_out_S_loss: 1.8306\n",
      "Epoch 345/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.6529 - out_T_loss: 0.0128 - out_S_loss: 2.0640 - val_loss: 18.6964 - val_out_T_loss: 0.4175 - val_out_S_loss: 1.8279\n",
      "Epoch 346/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 20.4806 - out_T_loss: 0.0188 - out_S_loss: 2.0462 - val_loss: 18.5929 - val_out_T_loss: 0.3818 - val_out_S_loss: 1.8211\n",
      "Epoch 347/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 20.0534 - out_T_loss: 0.0177 - out_S_loss: 2.0036 - val_loss: 18.4930 - val_out_T_loss: 0.3450 - val_out_S_loss: 1.8148\n",
      "Epoch 348/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.1158 - out_T_loss: 0.0120 - out_S_loss: 2.0104 - val_loss: 18.4874 - val_out_T_loss: 0.3878 - val_out_S_loss: 1.8100\n",
      "Epoch 349/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.1788 - out_T_loss: 0.0096 - out_S_loss: 2.0169 - val_loss: 18.4982 - val_out_T_loss: 0.3868 - val_out_S_loss: 1.8111\n",
      "Epoch 350/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 19.6411 - out_T_loss: 0.0100 - out_S_loss: 1.9631 - val_loss: 18.4622 - val_out_T_loss: 0.4180 - val_out_S_loss: 1.8044\n",
      "Epoch 351/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 20.2147 - out_T_loss: 0.0118 - out_S_loss: 2.0203 - val_loss: 18.3810 - val_out_T_loss: 0.4096 - val_out_S_loss: 1.7971\n",
      "Epoch 352/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.8754 - out_T_loss: 0.0149 - out_S_loss: 1.9861 - val_loss: 18.3015 - val_out_T_loss: 0.3697 - val_out_S_loss: 1.7932\n",
      "Epoch 353/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 20.2028 - out_T_loss: 0.0082 - out_S_loss: 2.0195 - val_loss: 18.3074 - val_out_T_loss: 0.4093 - val_out_S_loss: 1.7898\n",
      "Epoch 354/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 20.0146 - out_T_loss: 0.0152 - out_S_loss: 1.9999 - val_loss: 18.2698 - val_out_T_loss: 0.4623 - val_out_S_loss: 1.7808\n",
      "Epoch 355/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.9804 - out_T_loss: 0.0091 - out_S_loss: 1.9971 - val_loss: 18.2062 - val_out_T_loss: 0.4543 - val_out_S_loss: 1.7752\n",
      "Epoch 356/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 19.6303 - out_T_loss: 0.0077 - out_S_loss: 1.9623 - val_loss: 18.1975 - val_out_T_loss: 0.4329 - val_out_S_loss: 1.7765\n",
      "Epoch 357/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.9757 - out_T_loss: 0.0118 - out_S_loss: 1.9964 - val_loss: 18.1543 - val_out_T_loss: 0.4314 - val_out_S_loss: 1.7723\n",
      "Epoch 358/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 19.8520 - out_T_loss: 0.0101 - out_S_loss: 1.9842 - val_loss: 18.0926 - val_out_T_loss: 0.4201 - val_out_S_loss: 1.7672\n",
      "Epoch 359/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 19.7710 - out_T_loss: 0.0124 - out_S_loss: 1.9759 - val_loss: 18.0865 - val_out_T_loss: 0.3972 - val_out_S_loss: 1.7689\n",
      "Epoch 360/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 19.8447 - out_T_loss: 0.0134 - out_S_loss: 1.9831 - val_loss: 17.9910 - val_out_T_loss: 0.3765 - val_out_S_loss: 1.7614\n",
      "Epoch 361/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 19.9602 - out_T_loss: 0.0211 - out_S_loss: 1.9939 - val_loss: 18.0123 - val_out_T_loss: 0.3812 - val_out_S_loss: 1.7631\n",
      "Epoch 362/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.8138 - out_T_loss: 0.0162 - out_S_loss: 1.9798 - val_loss: 17.9469 - val_out_T_loss: 0.3773 - val_out_S_loss: 1.7570\n",
      "Epoch 363/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.9019 - out_T_loss: 0.0129 - out_S_loss: 1.9889 - val_loss: 17.9379 - val_out_T_loss: 0.3800 - val_out_S_loss: 1.7558\n",
      "Epoch 364/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.6316 - out_T_loss: 0.0125 - out_S_loss: 1.9619 - val_loss: 17.9349 - val_out_T_loss: 0.3922 - val_out_S_loss: 1.7543\n",
      "Epoch 365/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 19.3932 - out_T_loss: 0.0103 - out_S_loss: 1.9383 - val_loss: 17.8257 - val_out_T_loss: 0.3601 - val_out_S_loss: 1.7466\n",
      "Epoch 366/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.0971 - out_T_loss: 0.0114 - out_S_loss: 1.9086 - val_loss: 17.8133 - val_out_T_loss: 0.3769 - val_out_S_loss: 1.7436\n",
      "Epoch 367/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.0349 - out_T_loss: 0.0102 - out_S_loss: 1.9025 - val_loss: 17.8822 - val_out_T_loss: 0.4340 - val_out_S_loss: 1.7448\n",
      "Epoch 368/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 18.9247 - out_T_loss: 0.0117 - out_S_loss: 1.8913 - val_loss: 17.8083 - val_out_T_loss: 0.4183 - val_out_S_loss: 1.7390\n",
      "Epoch 369/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.4481 - out_T_loss: 0.0099 - out_S_loss: 1.9438 - val_loss: 17.7795 - val_out_T_loss: 0.3917 - val_out_S_loss: 1.7388\n",
      "Epoch 370/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.2387 - out_T_loss: 0.0133 - out_S_loss: 1.9225 - val_loss: 17.7434 - val_out_T_loss: 0.4005 - val_out_S_loss: 1.7343\n",
      "Epoch 371/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.9454 - out_T_loss: 0.0117 - out_S_loss: 1.8934 - val_loss: 17.7082 - val_out_T_loss: 0.4365 - val_out_S_loss: 1.7272\n",
      "Epoch 372/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.6614 - out_T_loss: 0.0133 - out_S_loss: 1.9648 - val_loss: 17.6374 - val_out_T_loss: 0.4080 - val_out_S_loss: 1.7229\n",
      "Epoch 373/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.9296 - out_T_loss: 0.0124 - out_S_loss: 1.8917 - val_loss: 17.5499 - val_out_T_loss: 0.3705 - val_out_S_loss: 1.7179\n",
      "Epoch 374/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.3463 - out_T_loss: 0.0102 - out_S_loss: 1.9336 - val_loss: 17.4823 - val_out_T_loss: 0.3586 - val_out_S_loss: 1.7124\n",
      "Epoch 375/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.7645 - out_T_loss: 0.0077 - out_S_loss: 1.8757 - val_loss: 17.4294 - val_out_T_loss: 0.3416 - val_out_S_loss: 1.7088\n",
      "Epoch 376/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 19.0146 - out_T_loss: 0.0097 - out_S_loss: 1.9005 - val_loss: 17.3868 - val_out_T_loss: 0.3468 - val_out_S_loss: 1.7040\n",
      "Epoch 377/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.0527 - out_T_loss: 0.0067 - out_S_loss: 1.9046 - val_loss: 17.4026 - val_out_T_loss: 0.3441 - val_out_S_loss: 1.7059\n",
      "Epoch 378/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 19.0785 - out_T_loss: 0.0098 - out_S_loss: 1.9069 - val_loss: 17.3799 - val_out_T_loss: 0.3801 - val_out_S_loss: 1.7000\n",
      "Epoch 379/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.9966 - out_T_loss: 0.0186 - out_S_loss: 1.8978 - val_loss: 17.3537 - val_out_T_loss: 0.3769 - val_out_S_loss: 1.6977\n",
      "Epoch 380/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 19.1995 - out_T_loss: 0.0172 - out_S_loss: 1.9182 - val_loss: 17.3504 - val_out_T_loss: 0.4089 - val_out_S_loss: 1.6942\n",
      "Epoch 381/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.7231 - out_T_loss: 0.0173 - out_S_loss: 1.8706 - val_loss: 17.3073 - val_out_T_loss: 0.3854 - val_out_S_loss: 1.6922\n",
      "Epoch 382/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 19.0416 - out_T_loss: 0.0171 - out_S_loss: 1.9024 - val_loss: 17.3550 - val_out_T_loss: 0.4600 - val_out_S_loss: 1.6895\n",
      "Epoch 383/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.6356 - out_T_loss: 0.0250 - out_S_loss: 1.8611 - val_loss: 17.3265 - val_out_T_loss: 0.4487 - val_out_S_loss: 1.6878\n",
      "Epoch 384/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.6456 - out_T_loss: 0.0185 - out_S_loss: 1.8627 - val_loss: 17.2325 - val_out_T_loss: 0.4142 - val_out_S_loss: 1.6818\n",
      "Epoch 385/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.7102 - out_T_loss: 0.0155 - out_S_loss: 1.8695 - val_loss: 17.1700 - val_out_T_loss: 0.4061 - val_out_S_loss: 1.6764\n",
      "Epoch 386/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.6390 - out_T_loss: 0.0140 - out_S_loss: 1.8625 - val_loss: 17.1862 - val_out_T_loss: 0.4188 - val_out_S_loss: 1.6767\n",
      "Epoch 387/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 19.0286 - out_T_loss: 0.0209 - out_S_loss: 1.9008 - val_loss: 17.1550 - val_out_T_loss: 0.4435 - val_out_S_loss: 1.6712\n",
      "Epoch 388/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 18.8036 - out_T_loss: 0.0382 - out_S_loss: 1.8765 - val_loss: 17.1063 - val_out_T_loss: 0.3996 - val_out_S_loss: 1.6707\n",
      "Epoch 389/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.9278 - out_T_loss: 0.0190 - out_S_loss: 1.8909 - val_loss: 17.0767 - val_out_T_loss: 0.4411 - val_out_S_loss: 1.6636\n",
      "Epoch 390/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.7061 - out_T_loss: 0.0225 - out_S_loss: 1.8684 - val_loss: 17.0481 - val_out_T_loss: 0.4569 - val_out_S_loss: 1.6591\n",
      "Epoch 391/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 19.0328 - out_T_loss: 0.0199 - out_S_loss: 1.9013 - val_loss: 16.9438 - val_out_T_loss: 0.4150 - val_out_S_loss: 1.6529\n",
      "Epoch 392/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 18.4293 - out_T_loss: 0.0130 - out_S_loss: 1.8416 - val_loss: 16.8506 - val_out_T_loss: 0.3701 - val_out_S_loss: 1.6481\n",
      "Epoch 393/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.8825 - out_T_loss: 0.0162 - out_S_loss: 1.8866 - val_loss: 16.8795 - val_out_T_loss: 0.3950 - val_out_S_loss: 1.6484\n",
      "Epoch 394/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 18.6843 - out_T_loss: 0.0306 - out_S_loss: 1.8654 - val_loss: 16.9867 - val_out_T_loss: 0.4961 - val_out_S_loss: 1.6491\n",
      "Epoch 395/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.3553 - out_T_loss: 0.0338 - out_S_loss: 1.8321 - val_loss: 16.8961 - val_out_T_loss: 0.4281 - val_out_S_loss: 1.6468\n",
      "Epoch 396/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 18.1560 - out_T_loss: 0.0133 - out_S_loss: 1.8143 - val_loss: 16.8166 - val_out_T_loss: 0.3894 - val_out_S_loss: 1.6427\n",
      "Epoch 397/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.2666 - out_T_loss: 0.0103 - out_S_loss: 1.8256 - val_loss: 16.7465 - val_out_T_loss: 0.3766 - val_out_S_loss: 1.6370\n",
      "Epoch 398/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 18.7154 - out_T_loss: 0.0080 - out_S_loss: 1.8707 - val_loss: 16.7110 - val_out_T_loss: 0.3958 - val_out_S_loss: 1.6315\n",
      "Epoch 399/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.5445 - out_T_loss: 0.0094 - out_S_loss: 1.8535 - val_loss: 16.7634 - val_out_T_loss: 0.4182 - val_out_S_loss: 1.6345\n",
      "Epoch 400/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 18.5902 - out_T_loss: 0.0146 - out_S_loss: 1.8576 - val_loss: 16.6961 - val_out_T_loss: 0.3807 - val_out_S_loss: 1.6315\n",
      "Epoch 401/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.9887 - out_T_loss: 0.0126 - out_S_loss: 1.8976 - val_loss: 16.6829 - val_out_T_loss: 0.3720 - val_out_S_loss: 1.6311\n",
      "Epoch 402/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.2081 - out_T_loss: 0.0089 - out_S_loss: 1.8199 - val_loss: 16.6412 - val_out_T_loss: 0.3920 - val_out_S_loss: 1.6249\n",
      "Epoch 403/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.0494 - out_T_loss: 0.0085 - out_S_loss: 1.8041 - val_loss: 16.6047 - val_out_T_loss: 0.3910 - val_out_S_loss: 1.6214\n",
      "Epoch 404/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.9886 - out_T_loss: 0.0104 - out_S_loss: 1.7978 - val_loss: 16.6236 - val_out_T_loss: 0.4085 - val_out_S_loss: 1.6215\n",
      "Epoch 405/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.9239 - out_T_loss: 0.0075 - out_S_loss: 1.7916 - val_loss: 16.5778 - val_out_T_loss: 0.4280 - val_out_S_loss: 1.6150\n",
      "Epoch 406/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.0963 - out_T_loss: 0.0068 - out_S_loss: 1.8089 - val_loss: 16.5333 - val_out_T_loss: 0.3693 - val_out_S_loss: 1.6164\n",
      "Epoch 407/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 18.1123 - out_T_loss: 0.0056 - out_S_loss: 1.8107 - val_loss: 16.4989 - val_out_T_loss: 0.4059 - val_out_S_loss: 1.6093\n",
      "Epoch 408/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.6970 - out_T_loss: 0.0042 - out_S_loss: 1.7693 - val_loss: 16.5091 - val_out_T_loss: 0.4027 - val_out_S_loss: 1.6106\n",
      "Epoch 409/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.1398 - out_T_loss: 0.0065 - out_S_loss: 1.8133 - val_loss: 16.4506 - val_out_T_loss: 0.3853 - val_out_S_loss: 1.6065\n",
      "Epoch 410/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.1846 - out_T_loss: 0.0046 - out_S_loss: 1.8180 - val_loss: 16.4550 - val_out_T_loss: 0.3945 - val_out_S_loss: 1.6061\n",
      "Epoch 411/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 17.9891 - out_T_loss: 0.0054 - out_S_loss: 1.7984 - val_loss: 16.4066 - val_out_T_loss: 0.4191 - val_out_S_loss: 1.5987\n",
      "Epoch 412/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.6268 - out_T_loss: 0.0085 - out_S_loss: 1.7618 - val_loss: 16.3214 - val_out_T_loss: 0.4041 - val_out_S_loss: 1.5917\n",
      "Epoch 413/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.4803 - out_T_loss: 0.0091 - out_S_loss: 1.7471 - val_loss: 16.3177 - val_out_T_loss: 0.4208 - val_out_S_loss: 1.5897\n",
      "Epoch 414/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 17.7923 - out_T_loss: 0.0074 - out_S_loss: 1.7785 - val_loss: 16.2458 - val_out_T_loss: 0.3861 - val_out_S_loss: 1.5860\n",
      "Epoch 415/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 18.0038 - out_T_loss: 0.0046 - out_S_loss: 1.7999 - val_loss: 16.2381 - val_out_T_loss: 0.4100 - val_out_S_loss: 1.5828\n",
      "Epoch 416/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.8558 - out_T_loss: 0.0036 - out_S_loss: 1.7852 - val_loss: 16.2501 - val_out_T_loss: 0.4220 - val_out_S_loss: 1.5828\n",
      "Epoch 417/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.1386 - out_T_loss: 0.0071 - out_S_loss: 1.7131 - val_loss: 16.1581 - val_out_T_loss: 0.4042 - val_out_S_loss: 1.5754\n",
      "Epoch 418/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.9160 - out_T_loss: 0.0036 - out_S_loss: 1.7912 - val_loss: 16.1420 - val_out_T_loss: 0.3960 - val_out_S_loss: 1.5746\n",
      "Epoch 419/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.8392 - out_T_loss: 0.0061 - out_S_loss: 1.7833 - val_loss: 16.1744 - val_out_T_loss: 0.4095 - val_out_S_loss: 1.5765\n",
      "Epoch 420/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.3380 - out_T_loss: 0.0052 - out_S_loss: 1.7333 - val_loss: 16.1017 - val_out_T_loss: 0.4097 - val_out_S_loss: 1.5692\n",
      "Epoch 421/2000\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 17.5224 - out_T_loss: 0.0055 - out_S_loss: 1.7517 - val_loss: 16.0235 - val_out_T_loss: 0.3836 - val_out_S_loss: 1.5640\n",
      "Epoch 422/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.5805 - out_T_loss: 0.0065 - out_S_loss: 1.7574 - val_loss: 15.9915 - val_out_T_loss: 0.3885 - val_out_S_loss: 1.5603\n",
      "Epoch 423/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.5889 - out_T_loss: 0.0049 - out_S_loss: 1.7584 - val_loss: 15.9973 - val_out_T_loss: 0.3930 - val_out_S_loss: 1.5604\n",
      "Epoch 424/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.3233 - out_T_loss: 0.0035 - out_S_loss: 1.7320 - val_loss: 15.9722 - val_out_T_loss: 0.3992 - val_out_S_loss: 1.5573\n",
      "Epoch 425/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.5864 - out_T_loss: 0.0040 - out_S_loss: 1.7582 - val_loss: 15.9501 - val_out_T_loss: 0.3996 - val_out_S_loss: 1.5550\n",
      "Epoch 426/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.4428 - out_T_loss: 0.0102 - out_S_loss: 1.7433 - val_loss: 15.9593 - val_out_T_loss: 0.4184 - val_out_S_loss: 1.5541\n",
      "Epoch 427/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.3534 - out_T_loss: 0.0119 - out_S_loss: 1.7342 - val_loss: 15.8974 - val_out_T_loss: 0.4033 - val_out_S_loss: 1.5494\n",
      "Epoch 428/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.4262 - out_T_loss: 0.0056 - out_S_loss: 1.7421 - val_loss: 15.8991 - val_out_T_loss: 0.4130 - val_out_S_loss: 1.5486\n",
      "Epoch 429/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.5124 - out_T_loss: 0.0048 - out_S_loss: 1.7508 - val_loss: 15.8801 - val_out_T_loss: 0.4140 - val_out_S_loss: 1.5466\n",
      "Epoch 430/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.7530 - out_T_loss: 0.0027 - out_S_loss: 1.7750 - val_loss: 15.8342 - val_out_T_loss: 0.3931 - val_out_S_loss: 1.5441\n",
      "Epoch 431/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.7187 - out_T_loss: 0.0034 - out_S_loss: 1.7715 - val_loss: 15.7913 - val_out_T_loss: 0.3746 - val_out_S_loss: 1.5417\n",
      "Epoch 432/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 17.7713 - out_T_loss: 0.0063 - out_S_loss: 1.7765 - val_loss: 15.8001 - val_out_T_loss: 0.3983 - val_out_S_loss: 1.5402\n",
      "Epoch 433/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.0746 - out_T_loss: 0.0058 - out_S_loss: 1.7069 - val_loss: 15.7536 - val_out_T_loss: 0.4157 - val_out_S_loss: 1.5338\n",
      "Epoch 434/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.5340 - out_T_loss: 0.0088 - out_S_loss: 1.7525 - val_loss: 15.7803 - val_out_T_loss: 0.4502 - val_out_S_loss: 1.5330\n",
      "Epoch 435/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.1415 - out_T_loss: 0.0101 - out_S_loss: 1.7131 - val_loss: 15.7110 - val_out_T_loss: 0.4106 - val_out_S_loss: 1.5300\n",
      "Epoch 436/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.0592 - out_T_loss: 0.0108 - out_S_loss: 1.7048 - val_loss: 15.7629 - val_out_T_loss: 0.4058 - val_out_S_loss: 1.5357\n",
      "Epoch 437/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.2646 - out_T_loss: 0.0078 - out_S_loss: 1.7257 - val_loss: 15.7175 - val_out_T_loss: 0.4254 - val_out_S_loss: 1.5292\n",
      "Epoch 438/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 16.8538 - out_T_loss: 0.0064 - out_S_loss: 1.6847 - val_loss: 15.6028 - val_out_T_loss: 0.3872 - val_out_S_loss: 1.5216\n",
      "Epoch 439/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.0124 - out_T_loss: 0.0094 - out_S_loss: 1.7003 - val_loss: 15.6301 - val_out_T_loss: 0.3597 - val_out_S_loss: 1.5270\n",
      "Epoch 440/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.2807 - out_T_loss: 0.0079 - out_S_loss: 1.7273 - val_loss: 15.5875 - val_out_T_loss: 0.3816 - val_out_S_loss: 1.5206\n",
      "Epoch 441/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.9572 - out_T_loss: 0.0104 - out_S_loss: 1.6947 - val_loss: 15.5732 - val_out_T_loss: 0.4197 - val_out_S_loss: 1.5154\n",
      "Epoch 442/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.3901 - out_T_loss: 0.0094 - out_S_loss: 1.7381 - val_loss: 15.5699 - val_out_T_loss: 0.4225 - val_out_S_loss: 1.5147\n",
      "Epoch 443/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.9975 - out_T_loss: 0.0090 - out_S_loss: 1.6989 - val_loss: 15.4908 - val_out_T_loss: 0.3881 - val_out_S_loss: 1.5103\n",
      "Epoch 444/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 16.9445 - out_T_loss: 0.0066 - out_S_loss: 1.6938 - val_loss: 15.4969 - val_out_T_loss: 0.4251 - val_out_S_loss: 1.5072\n",
      "Epoch 445/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.8341 - out_T_loss: 0.0062 - out_S_loss: 1.6828 - val_loss: 15.4487 - val_out_T_loss: 0.4053 - val_out_S_loss: 1.5043\n",
      "Epoch 446/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 17.0935 - out_T_loss: 0.0080 - out_S_loss: 1.7086 - val_loss: 15.4237 - val_out_T_loss: 0.4284 - val_out_S_loss: 1.4995\n",
      "Epoch 447/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.1669 - out_T_loss: 0.0058 - out_S_loss: 1.6161 - val_loss: 15.4554 - val_out_T_loss: 0.4493 - val_out_S_loss: 1.5006\n",
      "Epoch 448/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 16.7090 - out_T_loss: 0.0055 - out_S_loss: 1.6703 - val_loss: 15.4248 - val_out_T_loss: 0.4505 - val_out_S_loss: 1.4974\n",
      "Epoch 449/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.9729 - out_T_loss: 0.0040 - out_S_loss: 1.6969 - val_loss: 15.3762 - val_out_T_loss: 0.4239 - val_out_S_loss: 1.4952\n",
      "Epoch 450/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 17.1956 - out_T_loss: 0.0022 - out_S_loss: 1.7193 - val_loss: 15.3799 - val_out_T_loss: 0.4229 - val_out_S_loss: 1.4957\n",
      "Epoch 451/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.6122 - out_T_loss: 0.0054 - out_S_loss: 1.6607 - val_loss: 15.3161 - val_out_T_loss: 0.4161 - val_out_S_loss: 1.4900\n",
      "Epoch 452/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 17.2567 - out_T_loss: 0.0075 - out_S_loss: 1.7249 - val_loss: 15.3684 - val_out_T_loss: 0.4479 - val_out_S_loss: 1.4920\n",
      "Epoch 453/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 16.3864 - out_T_loss: 0.0130 - out_S_loss: 1.6373 - val_loss: 15.2597 - val_out_T_loss: 0.3808 - val_out_S_loss: 1.4879\n",
      "Epoch 454/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.2935 - out_T_loss: 0.0098 - out_S_loss: 1.6284 - val_loss: 15.1806 - val_out_T_loss: 0.3961 - val_out_S_loss: 1.4784\n",
      "Epoch 455/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.5244 - out_T_loss: 0.0154 - out_S_loss: 1.6509 - val_loss: 15.1801 - val_out_T_loss: 0.4182 - val_out_S_loss: 1.4762\n",
      "Epoch 456/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.3927 - out_T_loss: 0.0139 - out_S_loss: 1.6379 - val_loss: 15.1943 - val_out_T_loss: 0.4271 - val_out_S_loss: 1.4767\n",
      "Epoch 457/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.6647 - out_T_loss: 0.0175 - out_S_loss: 1.6647 - val_loss: 15.1435 - val_out_T_loss: 0.4546 - val_out_S_loss: 1.4689\n",
      "Epoch 458/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 15.9422 - out_T_loss: 0.0331 - out_S_loss: 1.5909 - val_loss: 15.1304 - val_out_T_loss: 0.3955 - val_out_S_loss: 1.4735\n",
      "Epoch 459/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 16.2998 - out_T_loss: 0.0273 - out_S_loss: 1.6272 - val_loss: 15.0650 - val_out_T_loss: 0.4002 - val_out_S_loss: 1.4665\n",
      "Epoch 460/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.0350 - out_T_loss: 0.0264 - out_S_loss: 1.6009 - val_loss: 15.1230 - val_out_T_loss: 0.4660 - val_out_S_loss: 1.4657\n",
      "Epoch 461/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 15.9021 - out_T_loss: 0.0183 - out_S_loss: 1.5884 - val_loss: 15.0637 - val_out_T_loss: 0.4660 - val_out_S_loss: 1.4598\n",
      "Epoch 462/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.6516 - out_T_loss: 0.0318 - out_S_loss: 1.6620 - val_loss: 15.0992 - val_out_T_loss: 0.4887 - val_out_S_loss: 1.4611\n",
      "Epoch 463/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.1311 - out_T_loss: 0.0186 - out_S_loss: 1.6113 - val_loss: 14.9880 - val_out_T_loss: 0.4582 - val_out_S_loss: 1.4530\n",
      "Epoch 464/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.1366 - out_T_loss: 0.0257 - out_S_loss: 1.6111 - val_loss: 15.1094 - val_out_T_loss: 0.5415 - val_out_S_loss: 1.4568\n",
      "Epoch 465/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.1954 - out_T_loss: 0.0214 - out_S_loss: 1.6174 - val_loss: 15.0823 - val_out_T_loss: 0.4440 - val_out_S_loss: 1.4638\n",
      "Epoch 466/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.2661 - out_T_loss: 0.0111 - out_S_loss: 1.6255 - val_loss: 14.9785 - val_out_T_loss: 0.4426 - val_out_S_loss: 1.4536\n",
      "Epoch 467/2000\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 16.5804 - out_T_loss: 0.0096 - out_S_loss: 1.6571 - val_loss: 14.9419 - val_out_T_loss: 0.4564 - val_out_S_loss: 1.4485\n",
      "Epoch 468/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.9540 - out_T_loss: 0.0044 - out_S_loss: 1.5950 - val_loss: 14.8263 - val_out_T_loss: 0.4088 - val_out_S_loss: 1.4418\n",
      "Epoch 469/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.5606 - out_T_loss: 0.0139 - out_S_loss: 1.6547 - val_loss: 14.8578 - val_out_T_loss: 0.4635 - val_out_S_loss: 1.4394\n",
      "Epoch 470/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.9945 - out_T_loss: 0.0112 - out_S_loss: 1.5983 - val_loss: 14.8236 - val_out_T_loss: 0.4140 - val_out_S_loss: 1.4410\n",
      "Epoch 471/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.3883 - out_T_loss: 0.0054 - out_S_loss: 1.6383 - val_loss: 14.8486 - val_out_T_loss: 0.3891 - val_out_S_loss: 1.4459\n",
      "Epoch 472/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.4646 - out_T_loss: 0.0048 - out_S_loss: 1.6460 - val_loss: 14.7347 - val_out_T_loss: 0.3782 - val_out_S_loss: 1.4356\n",
      "Epoch 473/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.0523 - out_T_loss: 0.0039 - out_S_loss: 1.6048 - val_loss: 14.6805 - val_out_T_loss: 0.3683 - val_out_S_loss: 1.4312\n",
      "Epoch 474/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.5916 - out_T_loss: 0.0063 - out_S_loss: 1.6585 - val_loss: 14.7844 - val_out_T_loss: 0.4036 - val_out_S_loss: 1.4381\n",
      "Epoch 475/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.1821 - out_T_loss: 0.0096 - out_S_loss: 1.6172 - val_loss: 14.6669 - val_out_T_loss: 0.4054 - val_out_S_loss: 1.4261\n",
      "Epoch 476/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.2436 - out_T_loss: 0.0039 - out_S_loss: 1.6240 - val_loss: 14.7192 - val_out_T_loss: 0.3869 - val_out_S_loss: 1.4332\n",
      "Epoch 477/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 15.9806 - out_T_loss: 0.0071 - out_S_loss: 1.5974 - val_loss: 14.6421 - val_out_T_loss: 0.4162 - val_out_S_loss: 1.4226\n",
      "Epoch 478/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 16.0267 - out_T_loss: 0.0071 - out_S_loss: 1.6020 - val_loss: 14.6805 - val_out_T_loss: 0.4214 - val_out_S_loss: 1.4259\n",
      "Epoch 479/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.9380 - out_T_loss: 0.0064 - out_S_loss: 1.5932 - val_loss: 14.6846 - val_out_T_loss: 0.4323 - val_out_S_loss: 1.4252\n",
      "Epoch 480/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 16.3318 - out_T_loss: 0.0054 - out_S_loss: 1.6326 - val_loss: 14.6971 - val_out_T_loss: 0.4039 - val_out_S_loss: 1.4293\n",
      "Epoch 481/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.9184 - out_T_loss: 0.0024 - out_S_loss: 1.5916 - val_loss: 14.5801 - val_out_T_loss: 0.3947 - val_out_S_loss: 1.4185\n",
      "Epoch 482/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.7798 - out_T_loss: 0.0031 - out_S_loss: 1.5777 - val_loss: 14.5490 - val_out_T_loss: 0.3904 - val_out_S_loss: 1.4159\n",
      "Epoch 483/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.6333 - out_T_loss: 0.0044 - out_S_loss: 1.5629 - val_loss: 14.4600 - val_out_T_loss: 0.3674 - val_out_S_loss: 1.4093\n",
      "Epoch 484/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.4135 - out_T_loss: 0.0098 - out_S_loss: 1.5404 - val_loss: 14.5643 - val_out_T_loss: 0.3981 - val_out_S_loss: 1.4166\n",
      "Epoch 485/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 15.8623 - out_T_loss: 0.0052 - out_S_loss: 1.5857 - val_loss: 14.5625 - val_out_T_loss: 0.4304 - val_out_S_loss: 1.4132\n",
      "Epoch 486/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.6953 - out_T_loss: 0.0045 - out_S_loss: 1.5691 - val_loss: 14.4956 - val_out_T_loss: 0.4175 - val_out_S_loss: 1.4078\n",
      "Epoch 487/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.2120 - out_T_loss: 0.0043 - out_S_loss: 1.6208 - val_loss: 14.4821 - val_out_T_loss: 0.3859 - val_out_S_loss: 1.4096\n",
      "Epoch 488/2000\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 15.8962 - out_T_loss: 0.0039 - out_S_loss: 1.5892 - val_loss: 14.5040 - val_out_T_loss: 0.4034 - val_out_S_loss: 1.4101\n",
      "Epoch 489/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.9593 - out_T_loss: 0.0086 - out_S_loss: 1.5951 - val_loss: 14.4881 - val_out_T_loss: 0.4010 - val_out_S_loss: 1.4087\n",
      "Epoch 490/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.3870 - out_T_loss: 0.0174 - out_S_loss: 1.6370 - val_loss: 14.4855 - val_out_T_loss: 0.4824 - val_out_S_loss: 1.4003\n",
      "Epoch 491/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.5818 - out_T_loss: 0.0163 - out_S_loss: 1.5566 - val_loss: 14.3440 - val_out_T_loss: 0.4115 - val_out_S_loss: 1.3932\n",
      "Epoch 492/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.7785 - out_T_loss: 0.0059 - out_S_loss: 1.5773 - val_loss: 14.4086 - val_out_T_loss: 0.4195 - val_out_S_loss: 1.3989\n",
      "Epoch 493/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.5407 - out_T_loss: 0.0052 - out_S_loss: 1.5535 - val_loss: 14.3418 - val_out_T_loss: 0.4330 - val_out_S_loss: 1.3909\n",
      "Epoch 494/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 15.2519 - out_T_loss: 0.0023 - out_S_loss: 1.5250 - val_loss: 14.3480 - val_out_T_loss: 0.4266 - val_out_S_loss: 1.3921\n",
      "Epoch 495/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.4451 - out_T_loss: 0.0025 - out_S_loss: 1.5443 - val_loss: 14.3181 - val_out_T_loss: 0.4168 - val_out_S_loss: 1.3901\n",
      "Epoch 496/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.6765 - out_T_loss: 0.0018 - out_S_loss: 1.5675 - val_loss: 14.3180 - val_out_T_loss: 0.4125 - val_out_S_loss: 1.3905\n",
      "Epoch 497/2000\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 16.0077 - out_T_loss: 0.0045 - out_S_loss: 1.6003 - val_loss: 14.3354 - val_out_T_loss: 0.4217 - val_out_S_loss: 1.3914\n",
      "Epoch 498/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.4276 - out_T_loss: 0.0057 - out_S_loss: 1.5422 - val_loss: 14.2897 - val_out_T_loss: 0.4634 - val_out_S_loss: 1.3826\n",
      "Epoch 499/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 15.6098 - out_T_loss: 0.0037 - out_S_loss: 1.5606 - val_loss: 14.3031 - val_out_T_loss: 0.4505 - val_out_S_loss: 1.3853\n",
      "Epoch 500/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 15.3900 - out_T_loss: 0.0034 - out_S_loss: 1.5387 - val_loss: 14.2005 - val_out_T_loss: 0.4448 - val_out_S_loss: 1.3756\n",
      "Epoch 501/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 15.2632 - out_T_loss: 0.0041 - out_S_loss: 1.5259 - val_loss: 14.1771 - val_out_T_loss: 0.4120 - val_out_S_loss: 1.3765\n",
      "Epoch 502/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.6653 - out_T_loss: 0.0029 - out_S_loss: 1.5662 - val_loss: 14.2689 - val_out_T_loss: 0.4429 - val_out_S_loss: 1.3826\n",
      "Epoch 503/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.5421 - out_T_loss: 0.0062 - out_S_loss: 1.5536 - val_loss: 14.2434 - val_out_T_loss: 0.5227 - val_out_S_loss: 1.3721\n",
      "Epoch 504/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.9231 - out_T_loss: 0.0054 - out_S_loss: 1.4918 - val_loss: 14.1488 - val_out_T_loss: 0.4586 - val_out_S_loss: 1.3690\n",
      "Epoch 505/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.2647 - out_T_loss: 0.0044 - out_S_loss: 1.5260 - val_loss: 14.0818 - val_out_T_loss: 0.4239 - val_out_S_loss: 1.3658\n",
      "Epoch 506/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 15.3055 - out_T_loss: 0.0023 - out_S_loss: 1.5303 - val_loss: 14.0837 - val_out_T_loss: 0.4052 - val_out_S_loss: 1.3678\n",
      "Epoch 507/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 15.1070 - out_T_loss: 0.0018 - out_S_loss: 1.5105 - val_loss: 14.0405 - val_out_T_loss: 0.4133 - val_out_S_loss: 1.3627\n",
      "Epoch 508/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 15.3137 - out_T_loss: 0.0015 - out_S_loss: 1.5312 - val_loss: 14.0622 - val_out_T_loss: 0.4536 - val_out_S_loss: 1.3609\n",
      "Epoch 509/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.0619 - out_T_loss: 0.0014 - out_S_loss: 1.5061 - val_loss: 13.9970 - val_out_T_loss: 0.4467 - val_out_S_loss: 1.3550\n",
      "Epoch 510/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.0891 - out_T_loss: 0.0020 - out_S_loss: 1.5087 - val_loss: 13.9711 - val_out_T_loss: 0.4493 - val_out_S_loss: 1.3522\n",
      "Epoch 511/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.0841 - out_T_loss: 0.0022 - out_S_loss: 1.5082 - val_loss: 14.0098 - val_out_T_loss: 0.4517 - val_out_S_loss: 1.3558\n",
      "Epoch 512/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.7075 - out_T_loss: 0.0020 - out_S_loss: 1.5705 - val_loss: 14.0246 - val_out_T_loss: 0.4450 - val_out_S_loss: 1.3580\n",
      "Epoch 513/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.0777 - out_T_loss: 0.0043 - out_S_loss: 1.5073 - val_loss: 13.9434 - val_out_T_loss: 0.4216 - val_out_S_loss: 1.3522\n",
      "Epoch 514/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.2701 - out_T_loss: 0.0051 - out_S_loss: 1.5265 - val_loss: 13.9686 - val_out_T_loss: 0.4280 - val_out_S_loss: 1.3541\n",
      "Epoch 515/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.1128 - out_T_loss: 0.0055 - out_S_loss: 1.5107 - val_loss: 13.9192 - val_out_T_loss: 0.4294 - val_out_S_loss: 1.3490\n",
      "Epoch 516/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.1232 - out_T_loss: 0.0152 - out_S_loss: 1.5108 - val_loss: 13.8039 - val_out_T_loss: 0.4095 - val_out_S_loss: 1.3394\n",
      "Epoch 517/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.2963 - out_T_loss: 0.0051 - out_S_loss: 1.5291 - val_loss: 13.8212 - val_out_T_loss: 0.3923 - val_out_S_loss: 1.3429\n",
      "Epoch 518/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.9808 - out_T_loss: 0.0047 - out_S_loss: 1.4976 - val_loss: 13.8594 - val_out_T_loss: 0.4058 - val_out_S_loss: 1.3454\n",
      "Epoch 519/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.3993 - out_T_loss: 0.0045 - out_S_loss: 1.5395 - val_loss: 13.8250 - val_out_T_loss: 0.4077 - val_out_S_loss: 1.3417\n",
      "Epoch 520/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.9229 - out_T_loss: 0.0144 - out_S_loss: 1.4909 - val_loss: 13.8620 - val_out_T_loss: 0.4617 - val_out_S_loss: 1.3400\n",
      "Epoch 521/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.9125 - out_T_loss: 0.0122 - out_S_loss: 1.4900 - val_loss: 13.7999 - val_out_T_loss: 0.3811 - val_out_S_loss: 1.3419\n",
      "Epoch 522/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 15.4549 - out_T_loss: 0.0195 - out_S_loss: 1.5435 - val_loss: 13.8573 - val_out_T_loss: 0.4449 - val_out_S_loss: 1.3412\n",
      "Epoch 523/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.2110 - out_T_loss: 0.0191 - out_S_loss: 1.5192 - val_loss: 13.7727 - val_out_T_loss: 0.4475 - val_out_S_loss: 1.3325\n",
      "Epoch 524/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.6311 - out_T_loss: 0.0185 - out_S_loss: 1.4613 - val_loss: 13.7707 - val_out_T_loss: 0.4592 - val_out_S_loss: 1.3311\n",
      "Epoch 525/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 14.9817 - out_T_loss: 0.0117 - out_S_loss: 1.4970 - val_loss: 13.6818 - val_out_T_loss: 0.4305 - val_out_S_loss: 1.3251\n",
      "Epoch 526/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.3188 - out_T_loss: 0.0077 - out_S_loss: 1.5311 - val_loss: 13.6511 - val_out_T_loss: 0.3810 - val_out_S_loss: 1.3270\n",
      "Epoch 527/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.6741 - out_T_loss: 0.0146 - out_S_loss: 1.4659 - val_loss: 13.6261 - val_out_T_loss: 0.3371 - val_out_S_loss: 1.3289\n",
      "Epoch 528/2000\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 14.7606 - out_T_loss: 0.0042 - out_S_loss: 1.4756 - val_loss: 13.5182 - val_out_T_loss: 0.3303 - val_out_S_loss: 1.3188\n",
      "Epoch 529/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 15.0372 - out_T_loss: 0.0054 - out_S_loss: 1.5032 - val_loss: 13.5728 - val_out_T_loss: 0.3628 - val_out_S_loss: 1.3210\n",
      "Epoch 530/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.3676 - out_T_loss: 0.0038 - out_S_loss: 1.4364 - val_loss: 13.6001 - val_out_T_loss: 0.4009 - val_out_S_loss: 1.3199\n",
      "Epoch 531/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.7637 - out_T_loss: 0.0026 - out_S_loss: 1.4761 - val_loss: 13.5781 - val_out_T_loss: 0.4287 - val_out_S_loss: 1.3149\n",
      "Epoch 532/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.3958 - out_T_loss: 0.0021 - out_S_loss: 1.4394 - val_loss: 13.5519 - val_out_T_loss: 0.4363 - val_out_S_loss: 1.3116\n",
      "Epoch 533/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.8869 - out_T_loss: 0.0050 - out_S_loss: 1.4882 - val_loss: 13.5808 - val_out_T_loss: 0.4172 - val_out_S_loss: 1.3164\n",
      "Epoch 534/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.8549 - out_T_loss: 0.0028 - out_S_loss: 1.4852 - val_loss: 13.5895 - val_out_T_loss: 0.4284 - val_out_S_loss: 1.3161\n",
      "Epoch 535/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.7464 - out_T_loss: 0.0019 - out_S_loss: 1.4744 - val_loss: 13.5004 - val_out_T_loss: 0.4235 - val_out_S_loss: 1.3077\n",
      "Epoch 536/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.8871 - out_T_loss: 0.0051 - out_S_loss: 1.4882 - val_loss: 13.5136 - val_out_T_loss: 0.4208 - val_out_S_loss: 1.3093\n",
      "Epoch 537/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.3713 - out_T_loss: 0.0048 - out_S_loss: 1.4366 - val_loss: 13.4798 - val_out_T_loss: 0.3830 - val_out_S_loss: 1.3097\n",
      "Epoch 538/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.4726 - out_T_loss: 0.0040 - out_S_loss: 1.4469 - val_loss: 13.4372 - val_out_T_loss: 0.4102 - val_out_S_loss: 1.3027\n",
      "Epoch 539/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.9370 - out_T_loss: 0.0059 - out_S_loss: 1.4931 - val_loss: 13.5139 - val_out_T_loss: 0.4451 - val_out_S_loss: 1.3069\n",
      "Epoch 540/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.6319 - out_T_loss: 0.0048 - out_S_loss: 1.4627 - val_loss: 13.4649 - val_out_T_loss: 0.4059 - val_out_S_loss: 1.3059\n",
      "Epoch 541/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.6669 - out_T_loss: 0.0020 - out_S_loss: 1.4665 - val_loss: 13.4453 - val_out_T_loss: 0.4162 - val_out_S_loss: 1.3029\n",
      "Epoch 542/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 13.9559 - out_T_loss: 0.0052 - out_S_loss: 1.3951 - val_loss: 13.3616 - val_out_T_loss: 0.4130 - val_out_S_loss: 1.2949\n",
      "Epoch 543/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.4919 - out_T_loss: 0.0023 - out_S_loss: 1.4490 - val_loss: 13.3755 - val_out_T_loss: 0.4126 - val_out_S_loss: 1.2963\n",
      "Epoch 544/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.3360 - out_T_loss: 0.0022 - out_S_loss: 1.4334 - val_loss: 13.3922 - val_out_T_loss: 0.4044 - val_out_S_loss: 1.2988\n",
      "Epoch 545/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.7339 - out_T_loss: 0.0020 - out_S_loss: 1.4732 - val_loss: 13.3712 - val_out_T_loss: 0.4130 - val_out_S_loss: 1.2958\n",
      "Epoch 546/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.7107 - out_T_loss: 0.0094 - out_S_loss: 1.4701 - val_loss: 13.4416 - val_out_T_loss: 0.4934 - val_out_S_loss: 1.2948\n",
      "Epoch 547/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.5399 - out_T_loss: 0.0091 - out_S_loss: 1.4531 - val_loss: 13.3977 - val_out_T_loss: 0.4740 - val_out_S_loss: 1.2924\n",
      "Epoch 548/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.3150 - out_T_loss: 0.0075 - out_S_loss: 1.4308 - val_loss: 13.2989 - val_out_T_loss: 0.4157 - val_out_S_loss: 1.2883\n",
      "Epoch 549/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.2405 - out_T_loss: 0.0109 - out_S_loss: 1.4230 - val_loss: 13.2906 - val_out_T_loss: 0.4350 - val_out_S_loss: 1.2856\n",
      "Epoch 550/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.2258 - out_T_loss: 0.0110 - out_S_loss: 1.4215 - val_loss: 13.2949 - val_out_T_loss: 0.4527 - val_out_S_loss: 1.2842\n",
      "Epoch 551/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 14.5187 - out_T_loss: 0.0111 - out_S_loss: 1.4508 - val_loss: 13.2816 - val_out_T_loss: 0.4697 - val_out_S_loss: 1.2812\n",
      "Epoch 552/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.5074 - out_T_loss: 0.0083 - out_S_loss: 1.4499 - val_loss: 13.2383 - val_out_T_loss: 0.4519 - val_out_S_loss: 1.2786\n",
      "Epoch 553/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.4151 - out_T_loss: 0.0064 - out_S_loss: 1.4409 - val_loss: 13.2595 - val_out_T_loss: 0.4471 - val_out_S_loss: 1.2812\n",
      "Epoch 554/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.0651 - out_T_loss: 0.0034 - out_S_loss: 1.4062 - val_loss: 13.1952 - val_out_T_loss: 0.4152 - val_out_S_loss: 1.2780\n",
      "Epoch 555/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 14.8140 - out_T_loss: 0.0033 - out_S_loss: 1.4811 - val_loss: 13.1493 - val_out_T_loss: 0.4473 - val_out_S_loss: 1.2702\n",
      "Epoch 556/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.0908 - out_T_loss: 0.0091 - out_S_loss: 1.4082 - val_loss: 13.1503 - val_out_T_loss: 0.4362 - val_out_S_loss: 1.2714\n",
      "Epoch 557/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.0078 - out_T_loss: 0.0028 - out_S_loss: 1.4005 - val_loss: 13.1275 - val_out_T_loss: 0.4221 - val_out_S_loss: 1.2705\n",
      "Epoch 558/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.3281 - out_T_loss: 0.0050 - out_S_loss: 1.4323 - val_loss: 13.1859 - val_out_T_loss: 0.4211 - val_out_S_loss: 1.2765\n",
      "Epoch 559/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 14.4432 - out_T_loss: 0.0036 - out_S_loss: 1.4440 - val_loss: 13.0616 - val_out_T_loss: 0.4003 - val_out_S_loss: 1.2661\n",
      "Epoch 560/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.9458 - out_T_loss: 0.0049 - out_S_loss: 1.3941 - val_loss: 13.1797 - val_out_T_loss: 0.4635 - val_out_S_loss: 1.2716\n",
      "Epoch 561/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.0728 - out_T_loss: 0.0042 - out_S_loss: 1.4069 - val_loss: 13.2502 - val_out_T_loss: 0.4966 - val_out_S_loss: 1.2754\n",
      "Epoch 562/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.1724 - out_T_loss: 0.0045 - out_S_loss: 1.4168 - val_loss: 13.1892 - val_out_T_loss: 0.4597 - val_out_S_loss: 1.2729\n",
      "Epoch 563/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.1624 - out_T_loss: 0.0058 - out_S_loss: 1.4157 - val_loss: 13.0849 - val_out_T_loss: 0.4402 - val_out_S_loss: 1.2645\n",
      "Epoch 564/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 13.9496 - out_T_loss: 0.0098 - out_S_loss: 1.3940 - val_loss: 13.0660 - val_out_T_loss: 0.4283 - val_out_S_loss: 1.2638\n",
      "Epoch 565/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.7537 - out_T_loss: 0.0054 - out_S_loss: 1.3748 - val_loss: 13.0700 - val_out_T_loss: 0.4326 - val_out_S_loss: 1.2637\n",
      "Epoch 566/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 13.7581 - out_T_loss: 0.0096 - out_S_loss: 1.3748 - val_loss: 12.9717 - val_out_T_loss: 0.4288 - val_out_S_loss: 1.2543\n",
      "Epoch 567/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.0123 - out_T_loss: 0.0084 - out_S_loss: 1.4004 - val_loss: 12.9414 - val_out_T_loss: 0.4279 - val_out_S_loss: 1.2513\n",
      "Epoch 568/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 13.3940 - out_T_loss: 0.0023 - out_S_loss: 1.3392 - val_loss: 12.9282 - val_out_T_loss: 0.4441 - val_out_S_loss: 1.2484\n",
      "Epoch 569/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 13.9523 - out_T_loss: 0.0022 - out_S_loss: 1.3950 - val_loss: 12.9821 - val_out_T_loss: 0.4285 - val_out_S_loss: 1.2554\n",
      "Epoch 570/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 13.6585 - out_T_loss: 0.0016 - out_S_loss: 1.3657 - val_loss: 12.9156 - val_out_T_loss: 0.4321 - val_out_S_loss: 1.2483\n",
      "Epoch 571/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.1026 - out_T_loss: 0.0012 - out_S_loss: 1.4101 - val_loss: 12.8960 - val_out_T_loss: 0.4269 - val_out_S_loss: 1.2469\n",
      "Epoch 572/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.9168 - out_T_loss: 0.0023 - out_S_loss: 1.3914 - val_loss: 12.8659 - val_out_T_loss: 0.4207 - val_out_S_loss: 1.2445\n",
      "Epoch 573/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.2991 - out_T_loss: 0.0039 - out_S_loss: 1.4295 - val_loss: 12.8418 - val_out_T_loss: 0.4324 - val_out_S_loss: 1.2409\n",
      "Epoch 574/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 14.3618 - out_T_loss: 0.0043 - out_S_loss: 1.4357 - val_loss: 12.9036 - val_out_T_loss: 0.4499 - val_out_S_loss: 1.2454\n",
      "Epoch 575/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.9948 - out_T_loss: 0.0037 - out_S_loss: 1.3991 - val_loss: 12.8278 - val_out_T_loss: 0.4517 - val_out_S_loss: 1.2376\n",
      "Epoch 576/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.7568 - out_T_loss: 0.0064 - out_S_loss: 1.3750 - val_loss: 12.8073 - val_out_T_loss: 0.4331 - val_out_S_loss: 1.2374\n",
      "Epoch 577/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 14.0733 - out_T_loss: 0.0073 - out_S_loss: 1.4066 - val_loss: 12.7798 - val_out_T_loss: 0.4203 - val_out_S_loss: 1.2360\n",
      "Epoch 578/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.9803 - out_T_loss: 0.0059 - out_S_loss: 1.3974 - val_loss: 12.8035 - val_out_T_loss: 0.4096 - val_out_S_loss: 1.2394\n",
      "Epoch 579/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 13.8226 - out_T_loss: 0.0039 - out_S_loss: 1.3819 - val_loss: 12.8190 - val_out_T_loss: 0.4302 - val_out_S_loss: 1.2389\n",
      "Epoch 580/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.2990 - out_T_loss: 0.0091 - out_S_loss: 1.3290 - val_loss: 12.6903 - val_out_T_loss: 0.3928 - val_out_S_loss: 1.2297\n",
      "Epoch 581/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.6678 - out_T_loss: 0.0042 - out_S_loss: 1.3664 - val_loss: 12.8105 - val_out_T_loss: 0.4393 - val_out_S_loss: 1.2371\n",
      "Epoch 582/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.6503 - out_T_loss: 0.0045 - out_S_loss: 1.3646 - val_loss: 12.6558 - val_out_T_loss: 0.4255 - val_out_S_loss: 1.2230\n",
      "Epoch 583/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 13.7782 - out_T_loss: 0.0020 - out_S_loss: 1.3776 - val_loss: 12.6752 - val_out_T_loss: 0.4046 - val_out_S_loss: 1.2271\n",
      "Epoch 584/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 13.7514 - out_T_loss: 0.0026 - out_S_loss: 1.3749 - val_loss: 12.6157 - val_out_T_loss: 0.3991 - val_out_S_loss: 1.2217\n",
      "Epoch 585/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.7867 - out_T_loss: 0.0021 - out_S_loss: 1.3785 - val_loss: 12.6580 - val_out_T_loss: 0.4245 - val_out_S_loss: 1.2234\n",
      "Epoch 586/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.5630 - out_T_loss: 0.0050 - out_S_loss: 1.3558 - val_loss: 12.6996 - val_out_T_loss: 0.4391 - val_out_S_loss: 1.2260\n",
      "Epoch 587/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 14.0892 - out_T_loss: 0.0310 - out_S_loss: 1.4058 - val_loss: 12.6914 - val_out_T_loss: 0.4471 - val_out_S_loss: 1.2244\n",
      "Epoch 588/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.3040 - out_T_loss: 0.0069 - out_S_loss: 1.3297 - val_loss: 12.6400 - val_out_T_loss: 0.3907 - val_out_S_loss: 1.2249\n",
      "Epoch 589/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.4040 - out_T_loss: 0.0038 - out_S_loss: 1.3400 - val_loss: 12.5903 - val_out_T_loss: 0.3919 - val_out_S_loss: 1.2198\n",
      "Epoch 590/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.5794 - out_T_loss: 0.0033 - out_S_loss: 1.3576 - val_loss: 12.5990 - val_out_T_loss: 0.3734 - val_out_S_loss: 1.2226\n",
      "Epoch 591/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.7409 - out_T_loss: 0.0014 - out_S_loss: 1.3740 - val_loss: 12.5194 - val_out_T_loss: 0.3848 - val_out_S_loss: 1.2135\n",
      "Epoch 592/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.6146 - out_T_loss: 0.0013 - out_S_loss: 1.3613 - val_loss: 12.5066 - val_out_T_loss: 0.3807 - val_out_S_loss: 1.2126\n",
      "Epoch 593/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.5794 - out_T_loss: 9.0717e-04 - out_S_loss: 1.3579 - val_loss: 12.5523 - val_out_T_loss: 0.3825 - val_out_S_loss: 1.2170\n",
      "Epoch 594/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.6703 - out_T_loss: 9.4388e-04 - out_S_loss: 1.3669 - val_loss: 12.4636 - val_out_T_loss: 0.3915 - val_out_S_loss: 1.2072\n",
      "Epoch 595/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 13.6686 - out_T_loss: 0.0010 - out_S_loss: 1.3668 - val_loss: 12.5926 - val_out_T_loss: 0.3965 - val_out_S_loss: 1.2196\n",
      "Epoch 596/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 12.9418 - out_T_loss: 0.0012 - out_S_loss: 1.2941 - val_loss: 12.5291 - val_out_T_loss: 0.4118 - val_out_S_loss: 1.2117\n",
      "Epoch 597/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.4572 - out_T_loss: 0.0022 - out_S_loss: 1.3455 - val_loss: 12.5189 - val_out_T_loss: 0.4191 - val_out_S_loss: 1.2100\n",
      "Epoch 598/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.6472 - out_T_loss: 0.0020 - out_S_loss: 1.3645 - val_loss: 12.5504 - val_out_T_loss: 0.4248 - val_out_S_loss: 1.2126\n",
      "Epoch 599/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.6628 - out_T_loss: 0.0021 - out_S_loss: 1.3661 - val_loss: 12.5095 - val_out_T_loss: 0.4396 - val_out_S_loss: 1.2070\n",
      "Epoch 600/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.5273 - out_T_loss: 0.0013 - out_S_loss: 1.3526 - val_loss: 12.4476 - val_out_T_loss: 0.4275 - val_out_S_loss: 1.2020\n",
      "Epoch 601/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.5223 - out_T_loss: 0.0025 - out_S_loss: 1.3520 - val_loss: 12.3856 - val_out_T_loss: 0.4115 - val_out_S_loss: 1.1974\n",
      "Epoch 602/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.4986 - out_T_loss: 0.0023 - out_S_loss: 1.3496 - val_loss: 12.3812 - val_out_T_loss: 0.3997 - val_out_S_loss: 1.1981\n",
      "Epoch 603/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.2480 - out_T_loss: 0.0033 - out_S_loss: 1.3245 - val_loss: 12.3521 - val_out_T_loss: 0.4146 - val_out_S_loss: 1.1937\n",
      "Epoch 604/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.2027 - out_T_loss: 0.0053 - out_S_loss: 1.3197 - val_loss: 12.3184 - val_out_T_loss: 0.3841 - val_out_S_loss: 1.1934\n",
      "Epoch 605/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.2750 - out_T_loss: 0.0040 - out_S_loss: 1.3271 - val_loss: 12.3907 - val_out_T_loss: 0.4033 - val_out_S_loss: 1.1987\n",
      "Epoch 606/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.6094 - out_T_loss: 0.0027 - out_S_loss: 1.3607 - val_loss: 12.3942 - val_out_T_loss: 0.3894 - val_out_S_loss: 1.2005\n",
      "Epoch 607/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.0805 - out_T_loss: 0.0014 - out_S_loss: 1.3079 - val_loss: 12.3918 - val_out_T_loss: 0.3963 - val_out_S_loss: 1.1995\n",
      "Epoch 608/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.3407 - out_T_loss: 0.0016 - out_S_loss: 1.3339 - val_loss: 12.2616 - val_out_T_loss: 0.3869 - val_out_S_loss: 1.1875\n",
      "Epoch 609/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.3039 - out_T_loss: 0.0068 - out_S_loss: 1.3297 - val_loss: 12.3582 - val_out_T_loss: 0.4141 - val_out_S_loss: 1.1944\n",
      "Epoch 610/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.6773 - out_T_loss: 0.0042 - out_S_loss: 1.3673 - val_loss: 12.3126 - val_out_T_loss: 0.4362 - val_out_S_loss: 1.1876\n",
      "Epoch 611/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.1924 - out_T_loss: 0.0073 - out_S_loss: 1.3185 - val_loss: 12.2706 - val_out_T_loss: 0.3686 - val_out_S_loss: 1.1902\n",
      "Epoch 612/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 12.9796 - out_T_loss: 0.0052 - out_S_loss: 1.2974 - val_loss: 12.2249 - val_out_T_loss: 0.3916 - val_out_S_loss: 1.1833\n",
      "Epoch 613/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.6951 - out_T_loss: 0.0040 - out_S_loss: 1.3691 - val_loss: 12.2371 - val_out_T_loss: 0.4022 - val_out_S_loss: 1.1835\n",
      "Epoch 614/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.4543 - out_T_loss: 0.0042 - out_S_loss: 1.3450 - val_loss: 12.2369 - val_out_T_loss: 0.3606 - val_out_S_loss: 1.1876\n",
      "Epoch 615/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.1792 - out_T_loss: 0.0071 - out_S_loss: 1.3172 - val_loss: 12.1756 - val_out_T_loss: 0.3238 - val_out_S_loss: 1.1852\n",
      "Epoch 616/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.1796 - out_T_loss: 0.0065 - out_S_loss: 1.3173 - val_loss: 12.1543 - val_out_T_loss: 0.3931 - val_out_S_loss: 1.1761\n",
      "Epoch 617/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.3062 - out_T_loss: 0.0115 - out_S_loss: 1.3295 - val_loss: 12.2748 - val_out_T_loss: 0.3694 - val_out_S_loss: 1.1905\n",
      "Epoch 618/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.1245 - out_T_loss: 0.0033 - out_S_loss: 1.3121 - val_loss: 12.1954 - val_out_T_loss: 0.3458 - val_out_S_loss: 1.1850\n",
      "Epoch 619/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 13.7251 - out_T_loss: 0.0035 - out_S_loss: 1.3722 - val_loss: 12.0963 - val_out_T_loss: 0.3537 - val_out_S_loss: 1.1743\n",
      "Epoch 620/2000\n",
      "24/24 [==============================] - 1s 29ms/step - loss: 13.5875 - out_T_loss: 0.0064 - out_S_loss: 1.3581 - val_loss: 12.2100 - val_out_T_loss: 0.4054 - val_out_S_loss: 1.1805\n",
      "Epoch 621/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.2429 - out_T_loss: 0.0049 - out_S_loss: 1.3238 - val_loss: 12.1652 - val_out_T_loss: 0.3959 - val_out_S_loss: 1.1769\n",
      "Epoch 622/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.0589 - out_T_loss: 0.0063 - out_S_loss: 1.3053 - val_loss: 12.0860 - val_out_T_loss: 0.3594 - val_out_S_loss: 1.1727\n",
      "Epoch 623/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 12.8203 - out_T_loss: 0.0039 - out_S_loss: 1.2816 - val_loss: 12.0416 - val_out_T_loss: 0.3650 - val_out_S_loss: 1.1677\n",
      "Epoch 624/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.1836 - out_T_loss: 0.0041 - out_S_loss: 1.3179 - val_loss: 12.1527 - val_out_T_loss: 0.3535 - val_out_S_loss: 1.1799\n",
      "Epoch 625/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 12.9117 - out_T_loss: 0.0027 - out_S_loss: 1.2909 - val_loss: 12.0292 - val_out_T_loss: 0.3517 - val_out_S_loss: 1.1678\n",
      "Epoch 626/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 12.7462 - out_T_loss: 0.0287 - out_S_loss: 1.2717 - val_loss: 12.0536 - val_out_T_loss: 0.4101 - val_out_S_loss: 1.1643\n",
      "Epoch 627/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 13.0302 - out_T_loss: 0.0265 - out_S_loss: 1.3004 - val_loss: 12.1800 - val_out_T_loss: 0.4673 - val_out_S_loss: 1.1713\n",
      "Epoch 628/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.2078 - out_T_loss: 0.0061 - out_S_loss: 1.3202 - val_loss: 12.1442 - val_out_T_loss: 0.4479 - val_out_S_loss: 1.1696\n",
      "Epoch 629/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 13.0505 - out_T_loss: 0.0044 - out_S_loss: 1.3046 - val_loss: 12.0310 - val_out_T_loss: 0.4349 - val_out_S_loss: 1.1596\n",
      "Epoch 630/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 12.3359 - out_T_loss: 0.0018 - out_S_loss: 1.2334 - val_loss: 12.0434 - val_out_T_loss: 0.4236 - val_out_S_loss: 1.1620\n",
      "Epoch 631/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7448 - out_T_loss: 0.0027 - out_S_loss: 1.2742 - val_loss: 12.0484 - val_out_T_loss: 0.4122 - val_out_S_loss: 1.1636\n",
      "Epoch 632/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.9684 - out_T_loss: 0.0049 - out_S_loss: 1.2964 - val_loss: 12.1239 - val_out_T_loss: 0.4503 - val_out_S_loss: 1.1674\n",
      "Epoch 633/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 13.0118 - out_T_loss: 0.0065 - out_S_loss: 1.3005 - val_loss: 12.2127 - val_out_T_loss: 0.5515 - val_out_S_loss: 1.1661\n",
      "Epoch 634/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 12.8903 - out_T_loss: 0.0099 - out_S_loss: 1.2880 - val_loss: 12.1337 - val_out_T_loss: 0.4581 - val_out_S_loss: 1.1676\n",
      "Epoch 635/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.3862 - out_T_loss: 0.0045 - out_S_loss: 1.2382 - val_loss: 11.9970 - val_out_T_loss: 0.4159 - val_out_S_loss: 1.1581\n",
      "Epoch 636/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7369 - out_T_loss: 0.0017 - out_S_loss: 1.2735 - val_loss: 12.0141 - val_out_T_loss: 0.4134 - val_out_S_loss: 1.1601\n",
      "Epoch 637/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 13.1310 - out_T_loss: 0.0015 - out_S_loss: 1.3129 - val_loss: 12.0188 - val_out_T_loss: 0.4366 - val_out_S_loss: 1.1582\n",
      "Epoch 638/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7117 - out_T_loss: 0.0081 - out_S_loss: 1.2704 - val_loss: 11.9202 - val_out_T_loss: 0.4162 - val_out_S_loss: 1.1504\n",
      "Epoch 639/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.9921 - out_T_loss: 0.0043 - out_S_loss: 1.2988 - val_loss: 11.9081 - val_out_T_loss: 0.3971 - val_out_S_loss: 1.1511\n",
      "Epoch 640/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 13.2274 - out_T_loss: 0.0084 - out_S_loss: 1.3219 - val_loss: 11.8558 - val_out_T_loss: 0.3542 - val_out_S_loss: 1.1502\n",
      "Epoch 641/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.5797 - out_T_loss: 0.0060 - out_S_loss: 1.2574 - val_loss: 11.7999 - val_out_T_loss: 0.3429 - val_out_S_loss: 1.1457\n",
      "Epoch 642/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7184 - out_T_loss: 0.0113 - out_S_loss: 1.2707 - val_loss: 11.9620 - val_out_T_loss: 0.4112 - val_out_S_loss: 1.1551\n",
      "Epoch 643/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 13.1089 - out_T_loss: 0.0114 - out_S_loss: 1.3097 - val_loss: 11.8507 - val_out_T_loss: 0.3723 - val_out_S_loss: 1.1478\n",
      "Epoch 644/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7906 - out_T_loss: 0.0085 - out_S_loss: 1.2782 - val_loss: 11.8328 - val_out_T_loss: 0.4008 - val_out_S_loss: 1.1432\n",
      "Epoch 645/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.5778 - out_T_loss: 0.0034 - out_S_loss: 1.2574 - val_loss: 11.8833 - val_out_T_loss: 0.3585 - val_out_S_loss: 1.1525\n",
      "Epoch 646/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7826 - out_T_loss: 0.0038 - out_S_loss: 1.2779 - val_loss: 11.8197 - val_out_T_loss: 0.3474 - val_out_S_loss: 1.1472\n",
      "Epoch 647/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.4466 - out_T_loss: 0.0032 - out_S_loss: 1.2443 - val_loss: 11.8539 - val_out_T_loss: 0.3757 - val_out_S_loss: 1.1478\n",
      "Epoch 648/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7949 - out_T_loss: 0.0058 - out_S_loss: 1.2789 - val_loss: 11.7313 - val_out_T_loss: 0.3750 - val_out_S_loss: 1.1356\n",
      "Epoch 649/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 12.2222 - out_T_loss: 0.0043 - out_S_loss: 1.2218 - val_loss: 11.7961 - val_out_T_loss: 0.4103 - val_out_S_loss: 1.1386\n",
      "Epoch 650/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.8457 - out_T_loss: 0.0030 - out_S_loss: 1.2843 - val_loss: 11.8710 - val_out_T_loss: 0.4371 - val_out_S_loss: 1.1434\n",
      "Epoch 651/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7328 - out_T_loss: 0.0052 - out_S_loss: 1.2728 - val_loss: 11.8325 - val_out_T_loss: 0.4725 - val_out_S_loss: 1.1360\n",
      "Epoch 652/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.9589 - out_T_loss: 0.0215 - out_S_loss: 1.2937 - val_loss: 11.9257 - val_out_T_loss: 0.5458 - val_out_S_loss: 1.1380\n",
      "Epoch 653/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.8611 - out_T_loss: 0.0057 - out_S_loss: 1.2855 - val_loss: 11.8135 - val_out_T_loss: 0.4793 - val_out_S_loss: 1.1334\n",
      "Epoch 654/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7962 - out_T_loss: 0.0016 - out_S_loss: 1.2795 - val_loss: 11.7809 - val_out_T_loss: 0.4533 - val_out_S_loss: 1.1328\n",
      "Epoch 655/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.4816 - out_T_loss: 0.0018 - out_S_loss: 1.2480 - val_loss: 11.7842 - val_out_T_loss: 0.4479 - val_out_S_loss: 1.1336\n",
      "Epoch 656/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.1654 - out_T_loss: 9.4636e-04 - out_S_loss: 1.2164 - val_loss: 11.7627 - val_out_T_loss: 0.4445 - val_out_S_loss: 1.1318\n",
      "Epoch 657/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.5121 - out_T_loss: 0.0032 - out_S_loss: 1.2509 - val_loss: 11.7822 - val_out_T_loss: 0.4470 - val_out_S_loss: 1.1335\n",
      "Epoch 658/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7099 - out_T_loss: 0.0014 - out_S_loss: 1.2708 - val_loss: 11.7805 - val_out_T_loss: 0.4156 - val_out_S_loss: 1.1365\n",
      "Epoch 659/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.8672 - out_T_loss: 0.0022 - out_S_loss: 1.2865 - val_loss: 11.7079 - val_out_T_loss: 0.4320 - val_out_S_loss: 1.1276\n",
      "Epoch 660/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7189 - out_T_loss: 7.9892e-04 - out_S_loss: 1.2718 - val_loss: 11.7507 - val_out_T_loss: 0.4341 - val_out_S_loss: 1.1317\n",
      "Epoch 661/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.4123 - out_T_loss: 9.6727e-04 - out_S_loss: 1.2411 - val_loss: 11.7786 - val_out_T_loss: 0.4220 - val_out_S_loss: 1.1357\n",
      "Epoch 662/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.6894 - out_T_loss: 7.4020e-04 - out_S_loss: 1.2689 - val_loss: 11.6855 - val_out_T_loss: 0.4227 - val_out_S_loss: 1.1263\n",
      "Epoch 663/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.6143 - out_T_loss: 0.0019 - out_S_loss: 1.2612 - val_loss: 11.6892 - val_out_T_loss: 0.4546 - val_out_S_loss: 1.1235\n",
      "Epoch 664/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.6038 - out_T_loss: 0.0017 - out_S_loss: 1.2602 - val_loss: 11.7152 - val_out_T_loss: 0.4560 - val_out_S_loss: 1.1259\n",
      "Epoch 665/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.3470 - out_T_loss: 0.0013 - out_S_loss: 1.2346 - val_loss: 11.6505 - val_out_T_loss: 0.4321 - val_out_S_loss: 1.1218\n",
      "Epoch 666/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.3998 - out_T_loss: 0.0021 - out_S_loss: 1.2398 - val_loss: 11.6750 - val_out_T_loss: 0.4709 - val_out_S_loss: 1.1204\n",
      "Epoch 667/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 12.5417 - out_T_loss: 0.0016 - out_S_loss: 1.2540 - val_loss: 11.5868 - val_out_T_loss: 0.4343 - val_out_S_loss: 1.1153\n",
      "Epoch 668/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.1208 - out_T_loss: 0.0026 - out_S_loss: 1.2118 - val_loss: 11.6274 - val_out_T_loss: 0.4499 - val_out_S_loss: 1.1177\n",
      "Epoch 669/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.1749 - out_T_loss: 0.0021 - out_S_loss: 1.2173 - val_loss: 11.5903 - val_out_T_loss: 0.4416 - val_out_S_loss: 1.1149\n",
      "Epoch 670/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7421 - out_T_loss: 0.0019 - out_S_loss: 1.2740 - val_loss: 11.6570 - val_out_T_loss: 0.4511 - val_out_S_loss: 1.1206\n",
      "Epoch 671/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.2800 - out_T_loss: 9.0606e-04 - out_S_loss: 1.2279 - val_loss: 11.5615 - val_out_T_loss: 0.4449 - val_out_S_loss: 1.1117\n",
      "Epoch 672/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.3724 - out_T_loss: 8.2020e-04 - out_S_loss: 1.2372 - val_loss: 11.4678 - val_out_T_loss: 0.4345 - val_out_S_loss: 1.1033\n",
      "Epoch 673/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 12.2288 - out_T_loss: 8.8300e-04 - out_S_loss: 1.2228 - val_loss: 11.4379 - val_out_T_loss: 0.4269 - val_out_S_loss: 1.1011\n",
      "Epoch 674/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.5706 - out_T_loss: 0.0019 - out_S_loss: 1.2569 - val_loss: 11.5278 - val_out_T_loss: 0.4423 - val_out_S_loss: 1.1086\n",
      "Epoch 675/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.3612 - out_T_loss: 0.0209 - out_S_loss: 1.2340 - val_loss: 11.5416 - val_out_T_loss: 0.5043 - val_out_S_loss: 1.1037\n",
      "Epoch 676/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.3289 - out_T_loss: 0.0060 - out_S_loss: 1.2323 - val_loss: 11.4929 - val_out_T_loss: 0.4250 - val_out_S_loss: 1.1068\n",
      "Epoch 677/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.5214 - out_T_loss: 0.0023 - out_S_loss: 1.2519 - val_loss: 11.5057 - val_out_T_loss: 0.4132 - val_out_S_loss: 1.1092\n",
      "Epoch 678/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.5796 - out_T_loss: 0.0068 - out_S_loss: 1.2573 - val_loss: 11.4666 - val_out_T_loss: 0.4099 - val_out_S_loss: 1.1057\n",
      "Epoch 679/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.5483 - out_T_loss: 0.0018 - out_S_loss: 1.2547 - val_loss: 11.4868 - val_out_T_loss: 0.4484 - val_out_S_loss: 1.1038\n",
      "Epoch 680/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.7019 - out_T_loss: 0.0020 - out_S_loss: 1.1700 - val_loss: 11.3856 - val_out_T_loss: 0.3878 - val_out_S_loss: 1.0998\n",
      "Epoch 681/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.5488 - out_T_loss: 0.0023 - out_S_loss: 1.2546 - val_loss: 11.4525 - val_out_T_loss: 0.4493 - val_out_S_loss: 1.1003\n",
      "Epoch 682/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 12.2371 - out_T_loss: 0.0020 - out_S_loss: 1.2235 - val_loss: 11.5135 - val_out_T_loss: 0.4465 - val_out_S_loss: 1.1067\n",
      "Epoch 683/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.2145 - out_T_loss: 8.9345e-04 - out_S_loss: 1.2214 - val_loss: 11.3549 - val_out_T_loss: 0.4175 - val_out_S_loss: 1.0937\n",
      "Epoch 684/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 12.5537 - out_T_loss: 0.0014 - out_S_loss: 1.2552 - val_loss: 11.4039 - val_out_T_loss: 0.4355 - val_out_S_loss: 1.0968\n",
      "Epoch 685/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.1625 - out_T_loss: 0.0061 - out_S_loss: 1.2156 - val_loss: 11.4194 - val_out_T_loss: 0.4792 - val_out_S_loss: 1.0940\n",
      "Epoch 686/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.8203 - out_T_loss: 0.0067 - out_S_loss: 1.1814 - val_loss: 11.4004 - val_out_T_loss: 0.4856 - val_out_S_loss: 1.0915\n",
      "Epoch 687/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.3427 - out_T_loss: 0.0070 - out_S_loss: 1.2336 - val_loss: 11.4095 - val_out_T_loss: 0.4657 - val_out_S_loss: 1.0944\n",
      "Epoch 688/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 12.2519 - out_T_loss: 0.0033 - out_S_loss: 1.2249 - val_loss: 11.3711 - val_out_T_loss: 0.4281 - val_out_S_loss: 1.0943\n",
      "Epoch 689/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.0421 - out_T_loss: 0.0019 - out_S_loss: 1.2040 - val_loss: 11.2767 - val_out_T_loss: 0.4393 - val_out_S_loss: 1.0837\n",
      "Epoch 690/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 11.8563 - out_T_loss: 0.0015 - out_S_loss: 1.1855 - val_loss: 11.3799 - val_out_T_loss: 0.4679 - val_out_S_loss: 1.0912\n",
      "Epoch 691/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.0909 - out_T_loss: 0.0019 - out_S_loss: 1.2089 - val_loss: 11.4286 - val_out_T_loss: 0.4361 - val_out_S_loss: 1.0992\n",
      "Epoch 692/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.4433 - out_T_loss: 8.2016e-04 - out_S_loss: 1.2443 - val_loss: 11.3857 - val_out_T_loss: 0.4275 - val_out_S_loss: 1.0958\n",
      "Epoch 693/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.7186 - out_T_loss: 0.0020 - out_S_loss: 1.2717 - val_loss: 11.3151 - val_out_T_loss: 0.4524 - val_out_S_loss: 1.0863\n",
      "Epoch 694/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.0842 - out_T_loss: 0.0011 - out_S_loss: 1.2083 - val_loss: 11.3570 - val_out_T_loss: 0.4418 - val_out_S_loss: 1.0915\n",
      "Epoch 695/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.3464 - out_T_loss: 6.3097e-04 - out_S_loss: 1.2346 - val_loss: 11.2969 - val_out_T_loss: 0.4261 - val_out_S_loss: 1.0871\n",
      "Epoch 696/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.5477 - out_T_loss: 5.2678e-04 - out_S_loss: 1.2547 - val_loss: 11.3364 - val_out_T_loss: 0.4148 - val_out_S_loss: 1.0922\n",
      "Epoch 697/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.0086 - out_T_loss: 0.0014 - out_S_loss: 1.2007 - val_loss: 11.2186 - val_out_T_loss: 0.4290 - val_out_S_loss: 1.0790\n",
      "Epoch 698/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.1682 - out_T_loss: 0.0011 - out_S_loss: 1.2167 - val_loss: 11.3048 - val_out_T_loss: 0.4480 - val_out_S_loss: 1.0857\n",
      "Epoch 699/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.8250 - out_T_loss: 0.0025 - out_S_loss: 1.1823 - val_loss: 11.2754 - val_out_T_loss: 0.4409 - val_out_S_loss: 1.0835\n",
      "Epoch 700/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.1293 - out_T_loss: 8.3291e-04 - out_S_loss: 1.2129 - val_loss: 11.1939 - val_out_T_loss: 0.4345 - val_out_S_loss: 1.0759\n",
      "Epoch 701/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.7551 - out_T_loss: 8.8563e-04 - out_S_loss: 1.1754 - val_loss: 11.1999 - val_out_T_loss: 0.4175 - val_out_S_loss: 1.0782\n",
      "Epoch 702/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.2450 - out_T_loss: 7.8339e-04 - out_S_loss: 1.2244 - val_loss: 11.2616 - val_out_T_loss: 0.4093 - val_out_S_loss: 1.0852\n",
      "Epoch 703/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 12.0119 - out_T_loss: 0.0016 - out_S_loss: 1.2010 - val_loss: 11.2840 - val_out_T_loss: 0.4412 - val_out_S_loss: 1.0843\n",
      "Epoch 704/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.6189 - out_T_loss: 0.0087 - out_S_loss: 1.1610 - val_loss: 11.2106 - val_out_T_loss: 0.4465 - val_out_S_loss: 1.0764\n",
      "Epoch 705/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 12.1722 - out_T_loss: 0.0021 - out_S_loss: 1.2170 - val_loss: 11.1606 - val_out_T_loss: 0.3766 - val_out_S_loss: 1.0784\n",
      "Epoch 706/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.7998 - out_T_loss: 0.0016 - out_S_loss: 1.1798 - val_loss: 11.1783 - val_out_T_loss: 0.3875 - val_out_S_loss: 1.0791\n",
      "Epoch 707/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.4439 - out_T_loss: 9.9708e-04 - out_S_loss: 1.1443 - val_loss: 11.1257 - val_out_T_loss: 0.3898 - val_out_S_loss: 1.0736\n",
      "Epoch 708/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.7881 - out_T_loss: 2.6498e-04 - out_S_loss: 1.1788 - val_loss: 11.1800 - val_out_T_loss: 0.4284 - val_out_S_loss: 1.0752\n",
      "Epoch 709/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.5677 - out_T_loss: 9.5972e-04 - out_S_loss: 1.1567 - val_loss: 11.1622 - val_out_T_loss: 0.4246 - val_out_S_loss: 1.0738\n",
      "Epoch 710/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.6749 - out_T_loss: 3.6082e-04 - out_S_loss: 1.1675 - val_loss: 11.1851 - val_out_T_loss: 0.4154 - val_out_S_loss: 1.0770\n",
      "Epoch 711/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.6096 - out_T_loss: 0.0028 - out_S_loss: 1.1607 - val_loss: 11.1389 - val_out_T_loss: 0.4260 - val_out_S_loss: 1.0713\n",
      "Epoch 712/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 12.1582 - out_T_loss: 0.0049 - out_S_loss: 1.2153 - val_loss: 11.1557 - val_out_T_loss: 0.4138 - val_out_S_loss: 1.0742\n",
      "Epoch 713/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.4342 - out_T_loss: 0.0032 - out_S_loss: 1.1431 - val_loss: 11.0731 - val_out_T_loss: 0.4143 - val_out_S_loss: 1.0659\n",
      "Epoch 714/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.4940 - out_T_loss: 0.0033 - out_S_loss: 1.1491 - val_loss: 11.1615 - val_out_T_loss: 0.4433 - val_out_S_loss: 1.0718\n",
      "Epoch 715/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 12.0943 - out_T_loss: 0.0236 - out_S_loss: 1.2071 - val_loss: 11.0861 - val_out_T_loss: 0.4023 - val_out_S_loss: 1.0684\n",
      "Epoch 716/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.7264 - out_T_loss: 0.0175 - out_S_loss: 1.1709 - val_loss: 11.0820 - val_out_T_loss: 0.4322 - val_out_S_loss: 1.0650\n",
      "Epoch 717/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.3280 - out_T_loss: 0.0152 - out_S_loss: 1.2313 - val_loss: 11.1745 - val_out_T_loss: 0.4750 - val_out_S_loss: 1.0699\n",
      "Epoch 718/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.3834 - out_T_loss: 0.0270 - out_S_loss: 1.1356 - val_loss: 11.1432 - val_out_T_loss: 0.5337 - val_out_S_loss: 1.0610\n",
      "Epoch 719/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.8173 - out_T_loss: 0.0287 - out_S_loss: 1.1789 - val_loss: 11.1482 - val_out_T_loss: 0.5385 - val_out_S_loss: 1.0610\n",
      "Epoch 720/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.5013 - out_T_loss: 0.0250 - out_S_loss: 1.1476 - val_loss: 11.1327 - val_out_T_loss: 0.5298 - val_out_S_loss: 1.0603\n",
      "Epoch 721/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.7713 - out_T_loss: 0.0203 - out_S_loss: 1.1751 - val_loss: 11.1204 - val_out_T_loss: 0.4932 - val_out_S_loss: 1.0627\n",
      "Epoch 722/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.3896 - out_T_loss: 0.0117 - out_S_loss: 1.1378 - val_loss: 11.0463 - val_out_T_loss: 0.4145 - val_out_S_loss: 1.0632\n",
      "Epoch 723/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.3004 - out_T_loss: 0.0080 - out_S_loss: 1.1292 - val_loss: 10.9671 - val_out_T_loss: 0.4125 - val_out_S_loss: 1.0555\n",
      "Epoch 724/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.8046 - out_T_loss: 0.0095 - out_S_loss: 1.1795 - val_loss: 11.0253 - val_out_T_loss: 0.3958 - val_out_S_loss: 1.0630\n",
      "Epoch 725/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.5746 - out_T_loss: 0.0025 - out_S_loss: 1.1572 - val_loss: 10.9779 - val_out_T_loss: 0.3947 - val_out_S_loss: 1.0583\n",
      "Epoch 726/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.8835 - out_T_loss: 0.0020 - out_S_loss: 1.1881 - val_loss: 10.9700 - val_out_T_loss: 0.4025 - val_out_S_loss: 1.0567\n",
      "Epoch 727/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.1023 - out_T_loss: 0.0025 - out_S_loss: 1.2100 - val_loss: 11.0262 - val_out_T_loss: 0.4156 - val_out_S_loss: 1.0611\n",
      "Epoch 728/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 11.8770 - out_T_loss: 0.0013 - out_S_loss: 1.1876 - val_loss: 11.0269 - val_out_T_loss: 0.4074 - val_out_S_loss: 1.0619\n",
      "Epoch 729/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.6811 - out_T_loss: 0.0010 - out_S_loss: 1.1680 - val_loss: 10.9456 - val_out_T_loss: 0.4162 - val_out_S_loss: 1.0529\n",
      "Epoch 730/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.8066 - out_T_loss: 0.0021 - out_S_loss: 1.1804 - val_loss: 11.0339 - val_out_T_loss: 0.3918 - val_out_S_loss: 1.0642\n",
      "Epoch 731/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.5041 - out_T_loss: 8.2287e-04 - out_S_loss: 1.1503 - val_loss: 10.9672 - val_out_T_loss: 0.3728 - val_out_S_loss: 1.0594\n",
      "Epoch 732/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.7001 - out_T_loss: 6.0086e-04 - out_S_loss: 1.1700 - val_loss: 10.8790 - val_out_T_loss: 0.3749 - val_out_S_loss: 1.0504\n",
      "Epoch 733/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.2312 - out_T_loss: 6.4184e-04 - out_S_loss: 1.1231 - val_loss: 10.8999 - val_out_T_loss: 0.3774 - val_out_S_loss: 1.0522\n",
      "Epoch 734/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.0170 - out_T_loss: 6.0297e-04 - out_S_loss: 1.2016 - val_loss: 10.8325 - val_out_T_loss: 0.3784 - val_out_S_loss: 1.0454\n",
      "Epoch 735/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.5011 - out_T_loss: 0.0013 - out_S_loss: 1.1500 - val_loss: 10.8544 - val_out_T_loss: 0.3870 - val_out_S_loss: 1.0467\n",
      "Epoch 736/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.4707 - out_T_loss: 0.0023 - out_S_loss: 1.1468 - val_loss: 10.8389 - val_out_T_loss: 0.3808 - val_out_S_loss: 1.0458\n",
      "Epoch 737/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.5864 - out_T_loss: 0.0027 - out_S_loss: 1.1584 - val_loss: 10.8552 - val_out_T_loss: 0.4133 - val_out_S_loss: 1.0442\n",
      "Epoch 738/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.3670 - out_T_loss: 0.0016 - out_S_loss: 1.1365 - val_loss: 10.8477 - val_out_T_loss: 0.3990 - val_out_S_loss: 1.0449\n",
      "Epoch 739/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.3272 - out_T_loss: 0.0026 - out_S_loss: 1.1325 - val_loss: 10.8736 - val_out_T_loss: 0.3878 - val_out_S_loss: 1.0486\n",
      "Epoch 740/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 12.0756 - out_T_loss: 0.0015 - out_S_loss: 1.2074 - val_loss: 10.8572 - val_out_T_loss: 0.4071 - val_out_S_loss: 1.0450\n",
      "Epoch 741/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.5141 - out_T_loss: 8.2975e-04 - out_S_loss: 1.1513 - val_loss: 10.9216 - val_out_T_loss: 0.4107 - val_out_S_loss: 1.0511\n",
      "Epoch 742/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 11.2216 - out_T_loss: 7.7958e-04 - out_S_loss: 1.1221 - val_loss: 10.8429 - val_out_T_loss: 0.4182 - val_out_S_loss: 1.0425\n",
      "Epoch 743/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.3766 - out_T_loss: 7.5765e-04 - out_S_loss: 1.1376 - val_loss: 10.8257 - val_out_T_loss: 0.4269 - val_out_S_loss: 1.0399\n",
      "Epoch 744/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.4383 - out_T_loss: 5.4204e-04 - out_S_loss: 1.1438 - val_loss: 10.7989 - val_out_T_loss: 0.4280 - val_out_S_loss: 1.0371\n",
      "Epoch 745/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.6852 - out_T_loss: 0.0010 - out_S_loss: 1.1684 - val_loss: 10.8121 - val_out_T_loss: 0.4276 - val_out_S_loss: 1.0385\n",
      "Epoch 746/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.5923 - out_T_loss: 0.0018 - out_S_loss: 1.1590 - val_loss: 10.9053 - val_out_T_loss: 0.4643 - val_out_S_loss: 1.0441\n",
      "Epoch 747/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.6121 - out_T_loss: 0.0106 - out_S_loss: 1.1602 - val_loss: 10.8265 - val_out_T_loss: 0.4530 - val_out_S_loss: 1.0373\n",
      "Epoch 748/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.7793 - out_T_loss: 0.0132 - out_S_loss: 1.1766 - val_loss: 10.8544 - val_out_T_loss: 0.4775 - val_out_S_loss: 1.0377\n",
      "Epoch 749/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.4526 - out_T_loss: 0.0337 - out_S_loss: 1.1419 - val_loss: 10.9792 - val_out_T_loss: 0.5844 - val_out_S_loss: 1.0395\n",
      "Epoch 750/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 11.1078 - out_T_loss: 0.0124 - out_S_loss: 1.1095 - val_loss: 10.8080 - val_out_T_loss: 0.4944 - val_out_S_loss: 1.0314\n",
      "Epoch 751/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.2465 - out_T_loss: 0.0031 - out_S_loss: 1.1243 - val_loss: 10.8118 - val_out_T_loss: 0.4328 - val_out_S_loss: 1.0379\n",
      "Epoch 752/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.1603 - out_T_loss: 0.0014 - out_S_loss: 1.1159 - val_loss: 10.7918 - val_out_T_loss: 0.4092 - val_out_S_loss: 1.0383\n",
      "Epoch 753/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 11.3712 - out_T_loss: 0.0020 - out_S_loss: 1.1369 - val_loss: 10.8980 - val_out_T_loss: 0.4196 - val_out_S_loss: 1.0478\n",
      "Epoch 754/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.0867 - out_T_loss: 0.0034 - out_S_loss: 1.1083 - val_loss: 10.7515 - val_out_T_loss: 0.3714 - val_out_S_loss: 1.0380\n",
      "Epoch 755/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.5983 - out_T_loss: 0.0024 - out_S_loss: 1.1596 - val_loss: 10.6762 - val_out_T_loss: 0.3741 - val_out_S_loss: 1.0302\n",
      "Epoch 756/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.3223 - out_T_loss: 0.0026 - out_S_loss: 1.1320 - val_loss: 10.6874 - val_out_T_loss: 0.4149 - val_out_S_loss: 1.0272\n",
      "Epoch 757/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.6618 - out_T_loss: 0.0030 - out_S_loss: 1.1659 - val_loss: 10.7184 - val_out_T_loss: 0.4375 - val_out_S_loss: 1.0281\n",
      "Epoch 758/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 12.0611 - out_T_loss: 0.0027 - out_S_loss: 1.2058 - val_loss: 10.7083 - val_out_T_loss: 0.4671 - val_out_S_loss: 1.0241\n",
      "Epoch 759/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.7249 - out_T_loss: 8.9962e-04 - out_S_loss: 1.0724 - val_loss: 10.7583 - val_out_T_loss: 0.4765 - val_out_S_loss: 1.0282\n",
      "Epoch 760/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 11.5881 - out_T_loss: 0.0014 - out_S_loss: 1.1587 - val_loss: 10.7475 - val_out_T_loss: 0.4773 - val_out_S_loss: 1.0270\n",
      "Epoch 761/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.8811 - out_T_loss: 6.1826e-04 - out_S_loss: 1.0881 - val_loss: 10.7648 - val_out_T_loss: 0.4869 - val_out_S_loss: 1.0278\n",
      "Epoch 762/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 11.4300 - out_T_loss: 0.0015 - out_S_loss: 1.1429 - val_loss: 10.7324 - val_out_T_loss: 0.4459 - val_out_S_loss: 1.0287\n",
      "Epoch 763/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.9742 - out_T_loss: 0.0031 - out_S_loss: 1.0971 - val_loss: 10.7112 - val_out_T_loss: 0.4494 - val_out_S_loss: 1.0262\n",
      "Epoch 764/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 11.0277 - out_T_loss: 9.3961e-04 - out_S_loss: 1.1027 - val_loss: 10.6004 - val_out_T_loss: 0.4436 - val_out_S_loss: 1.0157\n",
      "Epoch 765/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.1329 - out_T_loss: 8.3264e-04 - out_S_loss: 1.1132 - val_loss: 10.6466 - val_out_T_loss: 0.4395 - val_out_S_loss: 1.0207\n",
      "Epoch 766/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.9186 - out_T_loss: 0.0013 - out_S_loss: 1.0917 - val_loss: 10.6822 - val_out_T_loss: 0.4495 - val_out_S_loss: 1.0233\n",
      "Epoch 767/2000\n",
      "24/24 [==============================] - 1s 50ms/step - loss: 10.6856 - out_T_loss: 8.5921e-04 - out_S_loss: 1.0685 - val_loss: 10.6603 - val_out_T_loss: 0.4608 - val_out_S_loss: 1.0199\n",
      "Epoch 768/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.0578 - out_T_loss: 9.5699e-04 - out_S_loss: 1.1057 - val_loss: 10.6706 - val_out_T_loss: 0.4598 - val_out_S_loss: 1.0211\n",
      "Epoch 769/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 11.0969 - out_T_loss: 8.5459e-04 - out_S_loss: 1.1096 - val_loss: 10.6511 - val_out_T_loss: 0.4633 - val_out_S_loss: 1.0188\n",
      "Epoch 770/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.0579 - out_T_loss: 5.9418e-04 - out_S_loss: 1.1057 - val_loss: 10.6298 - val_out_T_loss: 0.4786 - val_out_S_loss: 1.0151\n",
      "Epoch 771/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.2905 - out_T_loss: 7.3467e-04 - out_S_loss: 1.1290 - val_loss: 10.7123 - val_out_T_loss: 0.4795 - val_out_S_loss: 1.0233\n",
      "Epoch 772/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 11.5116 - out_T_loss: 6.1544e-04 - out_S_loss: 1.1511 - val_loss: 10.6583 - val_out_T_loss: 0.4784 - val_out_S_loss: 1.0180\n",
      "Epoch 773/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 11.3983 - out_T_loss: 8.1207e-04 - out_S_loss: 1.1397 - val_loss: 10.6541 - val_out_T_loss: 0.4736 - val_out_S_loss: 1.0180\n",
      "Epoch 774/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 10.7928 - out_T_loss: 0.0011 - out_S_loss: 1.0792 - val_loss: 10.6035 - val_out_T_loss: 0.4664 - val_out_S_loss: 1.0137\n",
      "Epoch 775/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.5480 - out_T_loss: 0.0074 - out_S_loss: 1.0541 - val_loss: 10.6074 - val_out_T_loss: 0.4757 - val_out_S_loss: 1.0132\n",
      "Epoch 776/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.0045 - out_T_loss: 0.0016 - out_S_loss: 1.1003 - val_loss: 10.5877 - val_out_T_loss: 0.4677 - val_out_S_loss: 1.0120\n",
      "Epoch 777/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.8784 - out_T_loss: 6.8098e-04 - out_S_loss: 1.0878 - val_loss: 10.5119 - val_out_T_loss: 0.4584 - val_out_S_loss: 1.0054\n",
      "Epoch 778/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 10.6974 - out_T_loss: 5.8400e-04 - out_S_loss: 1.0697 - val_loss: 10.5319 - val_out_T_loss: 0.4557 - val_out_S_loss: 1.0076\n",
      "Epoch 779/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.3419 - out_T_loss: 4.2622e-04 - out_S_loss: 1.1341 - val_loss: 10.5707 - val_out_T_loss: 0.4432 - val_out_S_loss: 1.0127\n",
      "Epoch 780/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 11.2579 - out_T_loss: 6.7652e-04 - out_S_loss: 1.1257 - val_loss: 10.5518 - val_out_T_loss: 0.4433 - val_out_S_loss: 1.0108\n",
      "Epoch 781/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.1508 - out_T_loss: 4.5325e-04 - out_S_loss: 1.1150 - val_loss: 10.5143 - val_out_T_loss: 0.4440 - val_out_S_loss: 1.0070\n",
      "Epoch 782/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 11.3200 - out_T_loss: 4.7347e-04 - out_S_loss: 1.1320 - val_loss: 10.4569 - val_out_T_loss: 0.4339 - val_out_S_loss: 1.0023\n",
      "Epoch 783/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.1530 - out_T_loss: 5.1623e-04 - out_S_loss: 1.1152 - val_loss: 10.5049 - val_out_T_loss: 0.4571 - val_out_S_loss: 1.0048\n",
      "Epoch 784/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 11.5625 - out_T_loss: 0.0015 - out_S_loss: 1.1561 - val_loss: 10.4450 - val_out_T_loss: 0.4292 - val_out_S_loss: 1.0016\n",
      "Epoch 785/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 11.7817 - out_T_loss: 9.4723e-04 - out_S_loss: 1.1781 - val_loss: 10.4356 - val_out_T_loss: 0.4251 - val_out_S_loss: 1.0010\n",
      "Epoch 786/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.7708 - out_T_loss: 2.9496e-04 - out_S_loss: 1.0770 - val_loss: 10.4414 - val_out_T_loss: 0.4246 - val_out_S_loss: 1.0017\n",
      "Epoch 787/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.1956 - out_T_loss: 0.0077 - out_S_loss: 1.1188 - val_loss: 10.3750 - val_out_T_loss: 0.4206 - val_out_S_loss: 0.9954\n",
      "Epoch 788/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.6761 - out_T_loss: 0.0069 - out_S_loss: 1.0669 - val_loss: 10.4155 - val_out_T_loss: 0.4344 - val_out_S_loss: 0.9981\n",
      "Epoch 789/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 11.5166 - out_T_loss: 0.0043 - out_S_loss: 1.1512 - val_loss: 10.4287 - val_out_T_loss: 0.4697 - val_out_S_loss: 0.9959\n",
      "Epoch 790/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.7304 - out_T_loss: 0.0032 - out_S_loss: 1.0727 - val_loss: 10.4104 - val_out_T_loss: 0.4325 - val_out_S_loss: 0.9978\n",
      "Epoch 791/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.8711 - out_T_loss: 9.1889e-04 - out_S_loss: 1.0870 - val_loss: 10.4565 - val_out_T_loss: 0.4593 - val_out_S_loss: 0.9997\n",
      "Epoch 792/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.8765 - out_T_loss: 0.0013 - out_S_loss: 1.0875 - val_loss: 10.4744 - val_out_T_loss: 0.4788 - val_out_S_loss: 0.9996\n",
      "Epoch 793/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 10.9842 - out_T_loss: 8.6118e-04 - out_S_loss: 1.0983 - val_loss: 10.4444 - val_out_T_loss: 0.4767 - val_out_S_loss: 0.9968\n",
      "Epoch 794/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 10.4192 - out_T_loss: 4.2907e-04 - out_S_loss: 1.0419 - val_loss: 10.4586 - val_out_T_loss: 0.4671 - val_out_S_loss: 0.9992\n",
      "Epoch 795/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 11.2526 - out_T_loss: 3.3345e-04 - out_S_loss: 1.1252 - val_loss: 10.4306 - val_out_T_loss: 0.4619 - val_out_S_loss: 0.9969\n",
      "Epoch 796/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.7645 - out_T_loss: 0.0011 - out_S_loss: 1.0763 - val_loss: 10.4463 - val_out_T_loss: 0.4590 - val_out_S_loss: 0.9987\n",
      "Epoch 797/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.9259 - out_T_loss: 0.0015 - out_S_loss: 1.0924 - val_loss: 10.4123 - val_out_T_loss: 0.4728 - val_out_S_loss: 0.9939\n",
      "Epoch 798/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.9198 - out_T_loss: 0.0031 - out_S_loss: 1.0917 - val_loss: 10.3632 - val_out_T_loss: 0.4689 - val_out_S_loss: 0.9894\n",
      "Epoch 799/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.3243 - out_T_loss: 0.0072 - out_S_loss: 1.0317 - val_loss: 10.4018 - val_out_T_loss: 0.5256 - val_out_S_loss: 0.9876\n",
      "Epoch 800/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.7000 - out_T_loss: 0.0040 - out_S_loss: 1.0696 - val_loss: 10.3756 - val_out_T_loss: 0.4207 - val_out_S_loss: 0.9955\n",
      "Epoch 801/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.3915 - out_T_loss: 0.0021 - out_S_loss: 1.0389 - val_loss: 10.3002 - val_out_T_loss: 0.4177 - val_out_S_loss: 0.9882\n",
      "Epoch 802/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.8413 - out_T_loss: 0.0019 - out_S_loss: 1.0839 - val_loss: 10.3126 - val_out_T_loss: 0.4349 - val_out_S_loss: 0.9878\n",
      "Epoch 803/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 10.5199 - out_T_loss: 0.0064 - out_S_loss: 1.0513 - val_loss: 10.3207 - val_out_T_loss: 0.4516 - val_out_S_loss: 0.9869\n",
      "Epoch 804/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.8946 - out_T_loss: 0.0017 - out_S_loss: 1.0893 - val_loss: 10.3410 - val_out_T_loss: 0.4568 - val_out_S_loss: 0.9884\n",
      "Epoch 805/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.8567 - out_T_loss: 0.0010 - out_S_loss: 1.0856 - val_loss: 10.3859 - val_out_T_loss: 0.4300 - val_out_S_loss: 0.9956\n",
      "Epoch 806/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.5028 - out_T_loss: 6.4548e-04 - out_S_loss: 1.0502 - val_loss: 10.2919 - val_out_T_loss: 0.4280 - val_out_S_loss: 0.9864\n",
      "Epoch 807/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.5790 - out_T_loss: 0.0019 - out_S_loss: 1.0577 - val_loss: 10.3101 - val_out_T_loss: 0.4352 - val_out_S_loss: 0.9875\n",
      "Epoch 808/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.7859 - out_T_loss: 0.0017 - out_S_loss: 1.0784 - val_loss: 10.2590 - val_out_T_loss: 0.4111 - val_out_S_loss: 0.9848\n",
      "Epoch 809/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.5176 - out_T_loss: 0.0010 - out_S_loss: 1.0517 - val_loss: 10.2904 - val_out_T_loss: 0.4277 - val_out_S_loss: 0.9863\n",
      "Epoch 810/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.6136 - out_T_loss: 0.0010 - out_S_loss: 1.0613 - val_loss: 10.3272 - val_out_T_loss: 0.4516 - val_out_S_loss: 0.9876\n",
      "Epoch 811/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.4562 - out_T_loss: 3.6526e-04 - out_S_loss: 1.0456 - val_loss: 10.2947 - val_out_T_loss: 0.4413 - val_out_S_loss: 0.9853\n",
      "Epoch 812/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.6003 - out_T_loss: 8.8870e-04 - out_S_loss: 1.0599 - val_loss: 10.2871 - val_out_T_loss: 0.4496 - val_out_S_loss: 0.9838\n",
      "Epoch 813/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.6023 - out_T_loss: 6.8711e-04 - out_S_loss: 1.0602 - val_loss: 10.3203 - val_out_T_loss: 0.4574 - val_out_S_loss: 0.9863\n",
      "Epoch 814/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 11.1783 - out_T_loss: 6.8634e-04 - out_S_loss: 1.1178 - val_loss: 10.3314 - val_out_T_loss: 0.4480 - val_out_S_loss: 0.9883\n",
      "Epoch 815/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.3193 - out_T_loss: 8.1744e-04 - out_S_loss: 1.0319 - val_loss: 10.3076 - val_out_T_loss: 0.4501 - val_out_S_loss: 0.9857\n",
      "Epoch 816/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.8690 - out_T_loss: 3.8321e-04 - out_S_loss: 1.0869 - val_loss: 10.3138 - val_out_T_loss: 0.4537 - val_out_S_loss: 0.9860\n",
      "Epoch 817/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.9567 - out_T_loss: 2.0841e-04 - out_S_loss: 1.0957 - val_loss: 10.2868 - val_out_T_loss: 0.4544 - val_out_S_loss: 0.9832\n",
      "Epoch 818/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.5867 - out_T_loss: 2.5493e-04 - out_S_loss: 1.0586 - val_loss: 10.1858 - val_out_T_loss: 0.4561 - val_out_S_loss: 0.9730\n",
      "Epoch 819/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.9463 - out_T_loss: 1.9085e-04 - out_S_loss: 1.0946 - val_loss: 10.2067 - val_out_T_loss: 0.4586 - val_out_S_loss: 0.9748\n",
      "Epoch 820/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 10.8240 - out_T_loss: 2.5578e-04 - out_S_loss: 1.0824 - val_loss: 10.2184 - val_out_T_loss: 0.4620 - val_out_S_loss: 0.9756\n",
      "Epoch 821/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.6317 - out_T_loss: 2.8616e-04 - out_S_loss: 1.0631 - val_loss: 10.2270 - val_out_T_loss: 0.4629 - val_out_S_loss: 0.9764\n",
      "Epoch 822/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.5261 - out_T_loss: 2.7681e-04 - out_S_loss: 1.0526 - val_loss: 10.2915 - val_out_T_loss: 0.4730 - val_out_S_loss: 0.9818\n",
      "Epoch 823/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.8028 - out_T_loss: 2.2932e-04 - out_S_loss: 1.0803 - val_loss: 10.2367 - val_out_T_loss: 0.4688 - val_out_S_loss: 0.9768\n",
      "Epoch 824/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.7078 - out_T_loss: 2.4670e-04 - out_S_loss: 1.0708 - val_loss: 10.1478 - val_out_T_loss: 0.4551 - val_out_S_loss: 0.9693\n",
      "Epoch 825/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 11.1496 - out_T_loss: 1.9166e-04 - out_S_loss: 1.1149 - val_loss: 10.1883 - val_out_T_loss: 0.4479 - val_out_S_loss: 0.9740\n",
      "Epoch 826/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 10.4712 - out_T_loss: 1.4090e-04 - out_S_loss: 1.0471 - val_loss: 10.1857 - val_out_T_loss: 0.4454 - val_out_S_loss: 0.9740\n",
      "Epoch 827/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.6695 - out_T_loss: 2.2421e-04 - out_S_loss: 1.0669 - val_loss: 10.1286 - val_out_T_loss: 0.4347 - val_out_S_loss: 0.9694\n",
      "Epoch 828/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.0067 - out_T_loss: 4.1236e-04 - out_S_loss: 1.0006 - val_loss: 10.1329 - val_out_T_loss: 0.4382 - val_out_S_loss: 0.9695\n",
      "Epoch 829/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.9270 - out_T_loss: 2.4603e-04 - out_S_loss: 1.0927 - val_loss: 10.1648 - val_out_T_loss: 0.4598 - val_out_S_loss: 0.9705\n",
      "Epoch 830/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.2273 - out_T_loss: 2.7287e-04 - out_S_loss: 1.0227 - val_loss: 10.1036 - val_out_T_loss: 0.4621 - val_out_S_loss: 0.9641\n",
      "Epoch 831/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.7065 - out_T_loss: 2.6425e-04 - out_S_loss: 1.0706 - val_loss: 10.1272 - val_out_T_loss: 0.4469 - val_out_S_loss: 0.9680\n",
      "Epoch 832/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.9432 - out_T_loss: 1.4615e-04 - out_S_loss: 1.0943 - val_loss: 10.0974 - val_out_T_loss: 0.4377 - val_out_S_loss: 0.9660\n",
      "Epoch 833/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.4179 - out_T_loss: 3.3104e-04 - out_S_loss: 1.0418 - val_loss: 10.0843 - val_out_T_loss: 0.4264 - val_out_S_loss: 0.9658\n",
      "Epoch 834/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 10.3597 - out_T_loss: 7.1694e-04 - out_S_loss: 1.0359 - val_loss: 10.0528 - val_out_T_loss: 0.4285 - val_out_S_loss: 0.9624\n",
      "Epoch 835/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.9605 - out_T_loss: 3.4390e-04 - out_S_loss: 1.0960 - val_loss: 10.0601 - val_out_T_loss: 0.4265 - val_out_S_loss: 0.9634\n",
      "Epoch 836/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.8698 - out_T_loss: 3.6791e-04 - out_S_loss: 1.0869 - val_loss: 10.0939 - val_out_T_loss: 0.4345 - val_out_S_loss: 0.9659\n",
      "Epoch 837/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.4385 - out_T_loss: 5.3304e-04 - out_S_loss: 1.0438 - val_loss: 10.1197 - val_out_T_loss: 0.4698 - val_out_S_loss: 0.9650\n",
      "Epoch 838/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.4977 - out_T_loss: 2.2930e-04 - out_S_loss: 1.0497 - val_loss: 10.1033 - val_out_T_loss: 0.4484 - val_out_S_loss: 0.9655\n",
      "Epoch 839/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.3800 - out_T_loss: 4.0879e-04 - out_S_loss: 1.0380 - val_loss: 10.0525 - val_out_T_loss: 0.4379 - val_out_S_loss: 0.9615\n",
      "Epoch 840/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.4023 - out_T_loss: 3.2606e-04 - out_S_loss: 1.0402 - val_loss: 10.0491 - val_out_T_loss: 0.4405 - val_out_S_loss: 0.9609\n",
      "Epoch 841/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 10.6736 - out_T_loss: 0.0012 - out_S_loss: 1.0672 - val_loss: 10.1271 - val_out_T_loss: 0.5215 - val_out_S_loss: 0.9606\n",
      "Epoch 842/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.2464 - out_T_loss: 8.3461e-04 - out_S_loss: 1.0246 - val_loss: 10.0353 - val_out_T_loss: 0.4317 - val_out_S_loss: 0.9604\n",
      "Epoch 843/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.3568 - out_T_loss: 0.0037 - out_S_loss: 1.0353 - val_loss: 10.0824 - val_out_T_loss: 0.4341 - val_out_S_loss: 0.9648\n",
      "Epoch 844/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.0420 - out_T_loss: 0.0074 - out_S_loss: 1.0035 - val_loss: 10.0699 - val_out_T_loss: 0.4665 - val_out_S_loss: 0.9603\n",
      "Epoch 845/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.4975 - out_T_loss: 0.0058 - out_S_loss: 1.0492 - val_loss: 10.0897 - val_out_T_loss: 0.4751 - val_out_S_loss: 0.9615\n",
      "Epoch 846/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.1022 - out_T_loss: 0.0083 - out_S_loss: 1.0094 - val_loss: 10.1290 - val_out_T_loss: 0.5968 - val_out_S_loss: 0.9532\n",
      "Epoch 847/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.4159 - out_T_loss: 0.0077 - out_S_loss: 1.0408 - val_loss: 10.0988 - val_out_T_loss: 0.5273 - val_out_S_loss: 0.9571\n",
      "Epoch 848/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.3897 - out_T_loss: 0.0168 - out_S_loss: 1.0373 - val_loss: 10.1571 - val_out_T_loss: 0.5995 - val_out_S_loss: 0.9558\n",
      "Epoch 849/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.7237 - out_T_loss: 0.0055 - out_S_loss: 1.0718 - val_loss: 9.9044 - val_out_T_loss: 0.4565 - val_out_S_loss: 0.9448\n",
      "Epoch 850/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.7750 - out_T_loss: 0.0085 - out_S_loss: 1.0767 - val_loss: 10.0202 - val_out_T_loss: 0.4481 - val_out_S_loss: 0.9572\n",
      "Epoch 851/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.8975 - out_T_loss: 0.0196 - out_S_loss: 1.0878 - val_loss: 10.0826 - val_out_T_loss: 0.5602 - val_out_S_loss: 0.9522\n",
      "Epoch 852/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.3673 - out_T_loss: 0.0138 - out_S_loss: 1.0354 - val_loss: 9.9620 - val_out_T_loss: 0.4746 - val_out_S_loss: 0.9487\n",
      "Epoch 853/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.8324 - out_T_loss: 0.0081 - out_S_loss: 1.0824 - val_loss: 9.9020 - val_out_T_loss: 0.4032 - val_out_S_loss: 0.9499\n",
      "Epoch 854/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.7012 - out_T_loss: 0.0034 - out_S_loss: 1.0698 - val_loss: 9.8482 - val_out_T_loss: 0.3719 - val_out_S_loss: 0.9476\n",
      "Epoch 855/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 10.3607 - out_T_loss: 0.0022 - out_S_loss: 1.0358 - val_loss: 9.8719 - val_out_T_loss: 0.3687 - val_out_S_loss: 0.9503\n",
      "Epoch 856/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 9.9442 - out_T_loss: 0.0011 - out_S_loss: 0.9943 - val_loss: 9.8626 - val_out_T_loss: 0.3918 - val_out_S_loss: 0.9471\n",
      "Epoch 857/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.4351 - out_T_loss: 0.0015 - out_S_loss: 1.0434 - val_loss: 9.7874 - val_out_T_loss: 0.3614 - val_out_S_loss: 0.9426\n",
      "Epoch 858/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.5694 - out_T_loss: 0.0032 - out_S_loss: 1.0566 - val_loss: 9.9360 - val_out_T_loss: 0.4312 - val_out_S_loss: 0.9505\n",
      "Epoch 859/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.4162 - out_T_loss: 0.0043 - out_S_loss: 1.0412 - val_loss: 9.9700 - val_out_T_loss: 0.3997 - val_out_S_loss: 0.9570\n",
      "Epoch 860/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.0388 - out_T_loss: 0.0017 - out_S_loss: 1.0037 - val_loss: 9.9349 - val_out_T_loss: 0.3852 - val_out_S_loss: 0.9550\n",
      "Epoch 861/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 10.4451 - out_T_loss: 0.0029 - out_S_loss: 1.0442 - val_loss: 9.8864 - val_out_T_loss: 0.3821 - val_out_S_loss: 0.9504\n",
      "Epoch 862/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 9.9803 - out_T_loss: 0.0041 - out_S_loss: 0.9976 - val_loss: 9.8826 - val_out_T_loss: 0.3669 - val_out_S_loss: 0.9516\n",
      "Epoch 863/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 9.8320 - out_T_loss: 9.1510e-04 - out_S_loss: 0.9831 - val_loss: 9.9205 - val_out_T_loss: 0.3955 - val_out_S_loss: 0.9525\n",
      "Epoch 864/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.4241 - out_T_loss: 0.0098 - out_S_loss: 1.0414 - val_loss: 10.0257 - val_out_T_loss: 0.4846 - val_out_S_loss: 0.9541\n",
      "Epoch 865/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.9630 - out_T_loss: 0.0018 - out_S_loss: 0.9961 - val_loss: 9.8744 - val_out_T_loss: 0.3769 - val_out_S_loss: 0.9498\n",
      "Epoch 866/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.6392 - out_T_loss: 0.0013 - out_S_loss: 1.0638 - val_loss: 9.8608 - val_out_T_loss: 0.3558 - val_out_S_loss: 0.9505\n",
      "Epoch 867/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.7266 - out_T_loss: 0.0011 - out_S_loss: 1.0726 - val_loss: 9.8055 - val_out_T_loss: 0.3664 - val_out_S_loss: 0.9439\n",
      "Epoch 868/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 10.1034 - out_T_loss: 4.1250e-04 - out_S_loss: 1.0103 - val_loss: 9.7491 - val_out_T_loss: 0.3731 - val_out_S_loss: 0.9376\n",
      "Epoch 869/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.3855 - out_T_loss: 6.5058e-04 - out_S_loss: 1.0385 - val_loss: 9.7951 - val_out_T_loss: 0.3698 - val_out_S_loss: 0.9425\n",
      "Epoch 870/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.1669 - out_T_loss: 4.8660e-04 - out_S_loss: 1.0166 - val_loss: 9.8241 - val_out_T_loss: 0.3782 - val_out_S_loss: 0.9446\n",
      "Epoch 871/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.1346 - out_T_loss: 2.3107e-04 - out_S_loss: 1.0134 - val_loss: 9.8914 - val_out_T_loss: 0.3842 - val_out_S_loss: 0.9507\n",
      "Epoch 872/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.5677 - out_T_loss: 7.7659e-04 - out_S_loss: 1.0567 - val_loss: 9.8573 - val_out_T_loss: 0.3908 - val_out_S_loss: 0.9466\n",
      "Epoch 873/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.0554 - out_T_loss: 0.0011 - out_S_loss: 1.0054 - val_loss: 9.8349 - val_out_T_loss: 0.4143 - val_out_S_loss: 0.9421\n",
      "Epoch 874/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 10.2631 - out_T_loss: 0.0027 - out_S_loss: 1.0260 - val_loss: 9.9504 - val_out_T_loss: 0.5238 - val_out_S_loss: 0.9427\n",
      "Epoch 875/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.6304 - out_T_loss: 0.0028 - out_S_loss: 1.0628 - val_loss: 9.9072 - val_out_T_loss: 0.5530 - val_out_S_loss: 0.9354\n",
      "Epoch 876/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.2525 - out_T_loss: 0.0167 - out_S_loss: 1.0236 - val_loss: 9.8029 - val_out_T_loss: 0.3848 - val_out_S_loss: 0.9418\n",
      "Epoch 877/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.0501 - out_T_loss: 0.0102 - out_S_loss: 1.0040 - val_loss: 9.8307 - val_out_T_loss: 0.4408 - val_out_S_loss: 0.9390\n",
      "Epoch 878/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.2941 - out_T_loss: 0.0157 - out_S_loss: 1.0278 - val_loss: 10.0231 - val_out_T_loss: 0.6014 - val_out_S_loss: 0.9422\n",
      "Epoch 879/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.3241 - out_T_loss: 0.0086 - out_S_loss: 1.0316 - val_loss: 9.8743 - val_out_T_loss: 0.4118 - val_out_S_loss: 0.9463\n",
      "Epoch 880/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 9.8952 - out_T_loss: 0.0036 - out_S_loss: 0.9892 - val_loss: 9.7798 - val_out_T_loss: 0.3696 - val_out_S_loss: 0.9410\n",
      "Epoch 881/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.4501 - out_T_loss: 0.0045 - out_S_loss: 1.0446 - val_loss: 9.7685 - val_out_T_loss: 0.4072 - val_out_S_loss: 0.9361\n",
      "Epoch 882/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 10.0452 - out_T_loss: 0.0014 - out_S_loss: 1.0044 - val_loss: 9.8387 - val_out_T_loss: 0.4525 - val_out_S_loss: 0.9386\n",
      "Epoch 883/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.2355 - out_T_loss: 0.0041 - out_S_loss: 1.0231 - val_loss: 9.7441 - val_out_T_loss: 0.4674 - val_out_S_loss: 0.9277\n",
      "Epoch 884/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.6530 - out_T_loss: 0.0078 - out_S_loss: 0.9645 - val_loss: 9.7393 - val_out_T_loss: 0.4083 - val_out_S_loss: 0.9331\n",
      "Epoch 885/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 10.0783 - out_T_loss: 0.0039 - out_S_loss: 1.0074 - val_loss: 9.7052 - val_out_T_loss: 0.3761 - val_out_S_loss: 0.9329\n",
      "Epoch 886/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.0434 - out_T_loss: 0.0021 - out_S_loss: 1.0041 - val_loss: 9.8138 - val_out_T_loss: 0.4340 - val_out_S_loss: 0.9380\n",
      "Epoch 887/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.1875 - out_T_loss: 0.0016 - out_S_loss: 1.0186 - val_loss: 9.6742 - val_out_T_loss: 0.3827 - val_out_S_loss: 0.9291\n",
      "Epoch 888/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 9.6364 - out_T_loss: 9.0708e-04 - out_S_loss: 0.9636 - val_loss: 9.6633 - val_out_T_loss: 0.3965 - val_out_S_loss: 0.9267\n",
      "Epoch 889/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.3094 - out_T_loss: 6.5561e-04 - out_S_loss: 1.0309 - val_loss: 9.6085 - val_out_T_loss: 0.4014 - val_out_S_loss: 0.9207\n",
      "Epoch 890/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 10.6158 - out_T_loss: 0.0043 - out_S_loss: 1.0612 - val_loss: 9.6540 - val_out_T_loss: 0.3931 - val_out_S_loss: 0.9261\n",
      "Epoch 891/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.8090 - out_T_loss: 0.0151 - out_S_loss: 0.9794 - val_loss: 9.7109 - val_out_T_loss: 0.4087 - val_out_S_loss: 0.9302\n",
      "Epoch 892/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.0460 - out_T_loss: 0.0037 - out_S_loss: 1.0042 - val_loss: 9.6208 - val_out_T_loss: 0.3158 - val_out_S_loss: 0.9305\n",
      "Epoch 893/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.9970 - out_T_loss: 0.0040 - out_S_loss: 0.9993 - val_loss: 9.5860 - val_out_T_loss: 0.3589 - val_out_S_loss: 0.9227\n",
      "Epoch 894/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.6326 - out_T_loss: 0.0052 - out_S_loss: 0.9627 - val_loss: 9.6095 - val_out_T_loss: 0.3648 - val_out_S_loss: 0.9245\n",
      "Epoch 895/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.0260 - out_T_loss: 0.0048 - out_S_loss: 1.0021 - val_loss: 9.6683 - val_out_T_loss: 0.3799 - val_out_S_loss: 0.9288\n",
      "Epoch 896/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 9.7914 - out_T_loss: 0.0116 - out_S_loss: 0.9780 - val_loss: 9.7966 - val_out_T_loss: 0.5230 - val_out_S_loss: 0.9274\n",
      "Epoch 897/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.7586 - out_T_loss: 0.0096 - out_S_loss: 0.9749 - val_loss: 9.6935 - val_out_T_loss: 0.4054 - val_out_S_loss: 0.9288\n",
      "Epoch 898/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.3278 - out_T_loss: 0.0084 - out_S_loss: 0.9319 - val_loss: 9.7344 - val_out_T_loss: 0.4254 - val_out_S_loss: 0.9309\n",
      "Epoch 899/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.8959 - out_T_loss: 0.0039 - out_S_loss: 0.9892 - val_loss: 9.6827 - val_out_T_loss: 0.3828 - val_out_S_loss: 0.9300\n",
      "Epoch 900/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 9.8025 - out_T_loss: 0.0042 - out_S_loss: 0.9798 - val_loss: 9.5649 - val_out_T_loss: 0.3402 - val_out_S_loss: 0.9225\n",
      "Epoch 901/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 10.1589 - out_T_loss: 0.0048 - out_S_loss: 1.0154 - val_loss: 9.5658 - val_out_T_loss: 0.3252 - val_out_S_loss: 0.9241\n",
      "Epoch 902/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.9713 - out_T_loss: 0.0019 - out_S_loss: 0.9969 - val_loss: 9.6571 - val_out_T_loss: 0.3878 - val_out_S_loss: 0.9269\n",
      "Epoch 903/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.0589 - out_T_loss: 0.0026 - out_S_loss: 1.0056 - val_loss: 9.6499 - val_out_T_loss: 0.3899 - val_out_S_loss: 0.9260\n",
      "Epoch 904/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.1513 - out_T_loss: 0.0012 - out_S_loss: 1.0150 - val_loss: 9.6265 - val_out_T_loss: 0.3560 - val_out_S_loss: 0.9271\n",
      "Epoch 905/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.7971 - out_T_loss: 6.7381e-04 - out_S_loss: 0.9796 - val_loss: 9.5584 - val_out_T_loss: 0.3517 - val_out_S_loss: 0.9207\n",
      "Epoch 906/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 9.8644 - out_T_loss: 5.0586e-04 - out_S_loss: 0.9864 - val_loss: 9.5326 - val_out_T_loss: 0.3525 - val_out_S_loss: 0.9180\n",
      "Epoch 907/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 10.1501 - out_T_loss: 5.7995e-04 - out_S_loss: 1.0150 - val_loss: 9.5452 - val_out_T_loss: 0.3399 - val_out_S_loss: 0.9205\n",
      "Epoch 908/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.1315 - out_T_loss: 5.2957e-04 - out_S_loss: 1.0131 - val_loss: 9.4772 - val_out_T_loss: 0.3367 - val_out_S_loss: 0.9140\n",
      "Epoch 909/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.4434 - out_T_loss: 3.5644e-04 - out_S_loss: 0.9443 - val_loss: 9.4760 - val_out_T_loss: 0.3495 - val_out_S_loss: 0.9126\n",
      "Epoch 910/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 9.6746 - out_T_loss: 2.9596e-04 - out_S_loss: 0.9674 - val_loss: 9.5152 - val_out_T_loss: 0.3485 - val_out_S_loss: 0.9167\n",
      "Epoch 911/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.8851 - out_T_loss: 6.4832e-04 - out_S_loss: 0.9884 - val_loss: 9.5086 - val_out_T_loss: 0.3482 - val_out_S_loss: 0.9160\n",
      "Epoch 912/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.4844 - out_T_loss: 4.1854e-04 - out_S_loss: 0.9484 - val_loss: 9.4450 - val_out_T_loss: 0.3530 - val_out_S_loss: 0.9092\n",
      "Epoch 913/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 9.8474 - out_T_loss: 1.9258e-04 - out_S_loss: 0.9847 - val_loss: 9.4812 - val_out_T_loss: 0.3546 - val_out_S_loss: 0.9127\n",
      "Epoch 914/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.7661 - out_T_loss: 5.7459e-04 - out_S_loss: 0.9766 - val_loss: 9.5184 - val_out_T_loss: 0.3582 - val_out_S_loss: 0.9160\n",
      "Epoch 915/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.6945 - out_T_loss: 4.7934e-04 - out_S_loss: 0.9694 - val_loss: 9.4725 - val_out_T_loss: 0.3613 - val_out_S_loss: 0.9111\n",
      "Epoch 916/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.5051 - out_T_loss: 1.9018e-04 - out_S_loss: 0.9505 - val_loss: 9.5738 - val_out_T_loss: 0.3620 - val_out_S_loss: 0.9212\n",
      "Epoch 917/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.7888 - out_T_loss: 3.6678e-04 - out_S_loss: 0.9788 - val_loss: 9.5820 - val_out_T_loss: 0.3644 - val_out_S_loss: 0.9218\n",
      "Epoch 918/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 9.8930 - out_T_loss: 6.2852e-04 - out_S_loss: 0.9892 - val_loss: 9.5009 - val_out_T_loss: 0.3792 - val_out_S_loss: 0.9122\n",
      "Epoch 919/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 10.1053 - out_T_loss: 2.1853e-04 - out_S_loss: 1.0105 - val_loss: 9.5098 - val_out_T_loss: 0.3809 - val_out_S_loss: 0.9129\n",
      "Epoch 920/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 10.3264 - out_T_loss: 5.1016e-04 - out_S_loss: 1.0326 - val_loss: 9.5134 - val_out_T_loss: 0.3804 - val_out_S_loss: 0.9133\n",
      "Epoch 921/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.4168 - out_T_loss: 1.9063e-04 - out_S_loss: 0.9417 - val_loss: 9.4484 - val_out_T_loss: 0.3846 - val_out_S_loss: 0.9064\n",
      "Epoch 922/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.2049 - out_T_loss: 7.9992e-04 - out_S_loss: 1.0204 - val_loss: 9.4706 - val_out_T_loss: 0.3826 - val_out_S_loss: 0.9088\n",
      "Epoch 923/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.7942 - out_T_loss: 9.2656e-04 - out_S_loss: 0.9793 - val_loss: 9.5643 - val_out_T_loss: 0.4190 - val_out_S_loss: 0.9145\n",
      "Epoch 924/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 9.8381 - out_T_loss: 0.0012 - out_S_loss: 0.9837 - val_loss: 9.4536 - val_out_T_loss: 0.3914 - val_out_S_loss: 0.9062\n",
      "Epoch 925/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.9528 - out_T_loss: 2.9778e-04 - out_S_loss: 0.9952 - val_loss: 9.4527 - val_out_T_loss: 0.3946 - val_out_S_loss: 0.9058\n",
      "Epoch 926/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 9.9380 - out_T_loss: 0.0011 - out_S_loss: 0.9937 - val_loss: 9.4019 - val_out_T_loss: 0.3772 - val_out_S_loss: 0.9025\n",
      "Epoch 927/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 9.2356 - out_T_loss: 0.0011 - out_S_loss: 0.9235 - val_loss: 9.4157 - val_out_T_loss: 0.3926 - val_out_S_loss: 0.9023\n",
      "Epoch 928/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.3178 - out_T_loss: 9.8125e-04 - out_S_loss: 1.0317 - val_loss: 9.3691 - val_out_T_loss: 0.3574 - val_out_S_loss: 0.9012\n",
      "Epoch 929/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 9.9622 - out_T_loss: 0.0022 - out_S_loss: 0.9960 - val_loss: 9.4353 - val_out_T_loss: 0.3746 - val_out_S_loss: 0.9061\n",
      "Epoch 930/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.8735 - out_T_loss: 0.0015 - out_S_loss: 0.9872 - val_loss: 9.4133 - val_out_T_loss: 0.3799 - val_out_S_loss: 0.9033\n",
      "Epoch 931/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 9.7391 - out_T_loss: 9.9562e-04 - out_S_loss: 0.9738 - val_loss: 9.4071 - val_out_T_loss: 0.3699 - val_out_S_loss: 0.9037\n",
      "Epoch 932/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.6642 - out_T_loss: 5.6944e-04 - out_S_loss: 0.9664 - val_loss: 9.3886 - val_out_T_loss: 0.3615 - val_out_S_loss: 0.9027\n",
      "Epoch 933/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.3028 - out_T_loss: 0.0014 - out_S_loss: 0.9301 - val_loss: 9.3475 - val_out_T_loss: 0.3456 - val_out_S_loss: 0.9002\n",
      "Epoch 934/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 9.6325 - out_T_loss: 5.1600e-04 - out_S_loss: 0.9632 - val_loss: 9.3827 - val_out_T_loss: 0.3874 - val_out_S_loss: 0.8995\n",
      "Epoch 935/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.9879 - out_T_loss: 0.0028 - out_S_loss: 0.9985 - val_loss: 9.4342 - val_out_T_loss: 0.4542 - val_out_S_loss: 0.8980\n",
      "Epoch 936/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.6102 - out_T_loss: 4.7751e-04 - out_S_loss: 0.9610 - val_loss: 9.4826 - val_out_T_loss: 0.4910 - val_out_S_loss: 0.8992\n",
      "Epoch 937/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.8728 - out_T_loss: 8.4953e-04 - out_S_loss: 0.9872 - val_loss: 9.4435 - val_out_T_loss: 0.4224 - val_out_S_loss: 0.9021\n",
      "Epoch 938/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 10.0438 - out_T_loss: 3.9112e-04 - out_S_loss: 1.0043 - val_loss: 9.4493 - val_out_T_loss: 0.4025 - val_out_S_loss: 0.9047\n",
      "Epoch 939/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 9.6080 - out_T_loss: 2.1794e-04 - out_S_loss: 0.9608 - val_loss: 9.4049 - val_out_T_loss: 0.4186 - val_out_S_loss: 0.8986\n",
      "Epoch 940/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.4866 - out_T_loss: 3.8166e-04 - out_S_loss: 0.9486 - val_loss: 9.3705 - val_out_T_loss: 0.4244 - val_out_S_loss: 0.8946\n",
      "Epoch 941/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 9.8030 - out_T_loss: 3.7507e-04 - out_S_loss: 0.9803 - val_loss: 9.3997 - val_out_T_loss: 0.3982 - val_out_S_loss: 0.9001\n",
      "Epoch 942/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 9.1574 - out_T_loss: 2.4289e-04 - out_S_loss: 0.9157 - val_loss: 9.3983 - val_out_T_loss: 0.3906 - val_out_S_loss: 0.9008\n",
      "Epoch 943/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.6339 - out_T_loss: 2.0549e-04 - out_S_loss: 0.9634 - val_loss: 9.3699 - val_out_T_loss: 0.3886 - val_out_S_loss: 0.8981\n",
      "Epoch 944/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 9.6589 - out_T_loss: 2.3390e-04 - out_S_loss: 0.9659 - val_loss: 9.3802 - val_out_T_loss: 0.3879 - val_out_S_loss: 0.8992\n",
      "Epoch 945/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 9.1537 - out_T_loss: 1.9179e-04 - out_S_loss: 0.9154 - val_loss: 9.3600 - val_out_T_loss: 0.3840 - val_out_S_loss: 0.8976\n",
      "Epoch 946/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 9.6642 - out_T_loss: 1.1932e-04 - out_S_loss: 0.9664 - val_loss: 9.3691 - val_out_T_loss: 0.3823 - val_out_S_loss: 0.8987\n",
      "Epoch 947/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 10.0474 - out_T_loss: 0.0013 - out_S_loss: 1.0046 - val_loss: 9.4019 - val_out_T_loss: 0.3674 - val_out_S_loss: 0.9034\n",
      "Epoch 948/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 9.7570 - out_T_loss: 0.0032 - out_S_loss: 0.9754 - val_loss: 9.4073 - val_out_T_loss: 0.3782 - val_out_S_loss: 0.9029\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, [y_train , y_train  ],  #pairTrain[:, 1]\n",
    "  validation_data = ( x_val, [y_val , y_val  ] ),  #pairVal[:, 1]\n",
    "    epochs=2000\n",
    "    ,batch_size = 64    #8192\n",
    "  , callbacks=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "id": "26fa64ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "id": "99781f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 276)"
      ]
     },
     "execution_count": 1117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "id": "eab6f311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 1118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.argmax(y_pred[0][344])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "id": "52c03152",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_T = []\n",
    "for i in range(760):\n",
    "    pred_T.append(np.argmax(y_pred[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "id": "e172ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_S = []\n",
    "for i in range(760):\n",
    "    pred_S.append(np.argmax(y_pred[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "id": "99e5cbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9289473684210526"
      ]
     },
     "execution_count": 1121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_T = accuracy_score(y_test, pred_T)\n",
    "accuracy_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "id": "462ac88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7973684210526316"
      ]
     },
     "execution_count": 1122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_S = accuracy_score(y_test, pred_S)\n",
    "accuracy_S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18684915",
   "metadata": {},
   "source": [
    "# spatial_mean_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "id": "644702c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_90\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 93, 64)]          0         \n",
      "                                                                 \n",
      " tf.reshape_161 (TFOpLambda  (None, 64)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " sequential_86 (Sequential)  (None, 1)                 2113      \n",
      "                                                                 \n",
      " tf.reshape_162 (TFOpLambda  (None, 93, 1)             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_685 (Conv1D)         (None, 93, 1)             6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2119 (8.28 KB)\n",
      "Trainable params: 2119 (8.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_91\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 44, 256)]         0         \n",
      "                                                                 \n",
      " tf.math.reduce_mean_16 (TF  (None, 44)                0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " tf.expand_dims_49 (TFOpLam  (None, 44, 1)             0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " conv1d_687 (Conv1D)         (None, 44, 1)             4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4 (16.00 Byte)\n",
      "Trainable params: 4 (16.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_545 (B  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_599 (MaxPool  (None, 97, 32)               0         ['batch_normalization_545[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " conv1d_684 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_599[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_546 (B  (None, 93, 64)               256       ['conv1d_684[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " model_90 (Functional)       (None, 93, 1)                2119      ['batch_normalization_546[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multiply_99 (Multiply)      (None, 93, 64)               0         ['model_90[0][0]',            \n",
      "                                                                     'batch_normalization_546[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_344 (Add)               (None, 93, 64)               0         ['multiply_99[0][0]',         \n",
      "                                                                     'conv1d_684[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_600 (MaxPool  (None, 46, 64)               0         ['add_344[0][0]']             \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_547 (B  (None, 46, 64)               256       ['max_pooling1d_600[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_686 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_547[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_548 (B  (None, 44, 256)              1024      ['conv1d_686[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " model_91 (Functional)       (None, 44, 1)                4         ['batch_normalization_548[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multiply_100 (Multiply)     (None, 44, 256)              0         ['model_91[0][0]',            \n",
      "                                                                     'batch_normalization_548[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_345 (Add)               (None, 44, 256)              0         ['multiply_100[0][0]',        \n",
      "                                                                     'conv1d_686[0][0]']          \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['add_345[0][0]']             \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_601 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_549 (B  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_688 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_601[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_602 (MaxPool  (None, 48, 8)                0         ['batch_normalization_549[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 512)                  0         ['conv1d_688[0][0]']          \n",
      " 53 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_259 (  (None, 512)                  0         ['conv1d_688[0][0]']          \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_602[0][0]']   \n",
      "                                                                                                  \n",
      " add_346 (Add)               (None, 512)                  0         ['global_average_pooling1d_253\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_259[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 54 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_260 (  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_355 (Dense)           (None, 300)                  153900    ['add_346[0][0]']             \n",
      "                                                                                                  \n",
      " add_347 (Add)               (None, 32)                   0         ['global_average_pooling1d_254\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_260[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_332 (Dropout)       (None, 300)                  0         ['dense_355[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_333 (Dropout)       (None, 32)                   0         ['add_347[0][0]']             \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_332[0][0]']         \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_333[0][0]']         \n",
      "                                                                                                  \n",
      " subtract_182 (Subtract)     (None, 42, 32)               0         ['bottle_T[0][0]',            \n",
      "                                                                     'bottle_S[0][0]']            \n",
      "                                                                                                  \n",
      " subtract_183 (Subtract)     (None, 276)                  0         ['out_T[0][0]',               \n",
      "                                                                     'out_S[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.abs_187 (TFOpLambd  (None, 42, 32)               0         ['subtract_182[0][0]']        \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.abs_188 (TFOpLambd  (None, 276)                  0         ['subtract_183[0][0]']        \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 386591 (1.47 MB)\n",
      "Trainable params: 385743 (1.47 MB)\n",
      "Non-trainable params: 848 (3.31 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def spatial_mean_attention(shape, kernel):\n",
    "  inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "  # CHANNEL SUMMARIZATION\n",
    "  mean_across_channels = tf.keras.backend.mean(inp, axis = -1)\n",
    "  max_across_channels = tf.keras.backend.max(inp, axis = -1)\n",
    "  cat = tf.keras.layers.concatenate([ tf.expand_dims(mean_across_channels, axis=-1), tf.expand_dims(max_across_channels, axis=-1)])\n",
    "\n",
    "  # CONV & ACTIVATION\n",
    "  out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( tf.expand_dims(mean_across_channels, axis=-1) )\n",
    "\n",
    "  model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "# def spatial_mlp_attention(shape, kernel):\n",
    "#   inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "#   # CHANNEL SUMMARIZATION\n",
    "#   w, C = shape[0], shape[1]\n",
    "#   # Reshape the feature map to (w * h, C)\n",
    "#   reshaped_feature_map = tf.reshape(inp, [-1, C])\n",
    "#   # Define the MLP architecture\n",
    "#   hidden_units = 32\n",
    "#   output_units = 1\n",
    "#   mlp = tf.keras.Sequential([\n",
    "#       tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "#       Dropout(0.4),\n",
    "#       #BatchNormalization(),      tf.keras.layers.Dense(output_units, activation='sigmoid')\n",
    "#   ])\n",
    "#   # Apply the MLP on the reshaped feature map\n",
    "#   output_mlp = mlp(reshaped_feature_map)\n",
    "#   reshaped_output = tf.reshape(output_mlp, ( -1, w, 1 ))\n",
    "#   # CONV & ACTIVATION\n",
    "#   out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( reshaped_output )\n",
    "\n",
    "#   model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "#   model.summary()\n",
    "#   return model\n",
    "\n",
    "dataInp = Input(shape = (200, 3), name='in')\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_c1 = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_T')(dataInp) # 194, 32\n",
    "x_b1 = BatchNormalization()(x_c1)\n",
    "x_m1 = MaxPooling1D(2)(x_b1) # 97, 32\n",
    "x_c2 = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_m1) # 93, 64\n",
    "x_b2 = BatchNormalization()(x_c2)\n",
    "\n",
    "at1 = spatial_mlp_attention(shape = (93, 64), kernel = 5)\n",
    "x_a1 = at1(x_b2) \n",
    "x_T = tf.keras.layers.Multiply()([x_a1, x_b2])\n",
    "# x_T = tf.multiply([x_a1, x_b2])\n",
    "x_T = Add()([x_T, x_c2])\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_c3 = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b3 = BatchNormalization()(x_c3)\n",
    "\n",
    "at2 = spatial_mean_attention(shape = (44, 256), kernel = kernel_size)\n",
    "x_a2 = at2(x_b3)\n",
    "x_T = tf.keras.layers.Multiply()([x_a2, x_b3])\n",
    "# x_T = tf.multiply([x_a2, x_b3])\n",
    "x_T = Add()([x_T, x_c3])\n",
    "\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu', name='bottle_T')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid', name='out_T')(d_T)\n",
    "\n",
    "# STUDENT MODEL\n",
    "x_S = Conv1D(kernel_size = 7, filters=8, activation='relu', name='l1_S')(dataInp) # 194, 8\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(4)(x_S) # 48, 32\n",
    "x_b_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "avg_S = GlobalAveragePooling1D()(x_b_S)\n",
    "max_S = GlobalMaxPooling1D()(x_b_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "\n",
    "#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\n",
    "# abs_sub = tf.abs(x_b_T - x_b_S)\n",
    "# l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\n",
    "abs_sub = tf.abs(tf.keras.layers.Subtract()([x_b_T, x_b_S]))  \n",
    "l2_output = tf.abs(tf.keras.layers.Subtract()([out_T, out_S]))\n",
    "\n",
    "model_a1 = Model( inputs=dataInp, outputs=[out_T, out_S, abs_sub, l2_output], name='distillationFramework' )\n",
    "model_a1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "id": "d83e6c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] building and compilation complete!\n",
      "\n",
      " Model summary:\n",
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_545 (B  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_599 (MaxPool  (None, 97, 32)               0         ['batch_normalization_545[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " conv1d_684 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_599[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_546 (B  (None, 93, 64)               256       ['conv1d_684[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " model_90 (Functional)       (None, 93, 1)                2119      ['batch_normalization_546[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multiply_99 (Multiply)      (None, 93, 64)               0         ['model_90[0][0]',            \n",
      "                                                                     'batch_normalization_546[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_344 (Add)               (None, 93, 64)               0         ['multiply_99[0][0]',         \n",
      "                                                                     'conv1d_684[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_600 (MaxPool  (None, 46, 64)               0         ['add_344[0][0]']             \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_547 (B  (None, 46, 64)               256       ['max_pooling1d_600[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_686 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_547[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_548 (B  (None, 44, 256)              1024      ['conv1d_686[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " model_91 (Functional)       (None, 44, 1)                4         ['batch_normalization_548[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multiply_100 (Multiply)     (None, 44, 256)              0         ['model_91[0][0]',            \n",
      "                                                                     'batch_normalization_548[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_345 (Add)               (None, 44, 256)              0         ['multiply_100[0][0]',        \n",
      "                                                                     'conv1d_686[0][0]']          \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['add_345[0][0]']             \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_601 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_549 (B  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_688 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_601[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_602 (MaxPool  (None, 48, 8)                0         ['batch_normalization_549[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 512)                  0         ['conv1d_688[0][0]']          \n",
      " 53 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_259 (  (None, 512)                  0         ['conv1d_688[0][0]']          \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_602[0][0]']   \n",
      "                                                                                                  \n",
      " add_346 (Add)               (None, 512)                  0         ['global_average_pooling1d_253\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_259[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 54 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_260 (  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_355 (Dense)           (None, 300)                  153900    ['add_346[0][0]']             \n",
      "                                                                                                  \n",
      " add_347 (Add)               (None, 32)                   0         ['global_average_pooling1d_254\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_260[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_332 (Dropout)       (None, 300)                  0         ['dense_355[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_333 (Dropout)       (None, 32)                   0         ['add_347[0][0]']             \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_332[0][0]']         \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_333[0][0]']         \n",
      "                                                                                                  \n",
      " subtract_182 (Subtract)     (None, 42, 32)               0         ['bottle_T[0][0]',            \n",
      "                                                                     'bottle_S[0][0]']            \n",
      "                                                                                                  \n",
      " subtract_183 (Subtract)     (None, 276)                  0         ['out_T[0][0]',               \n",
      "                                                                     'out_S[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.abs_187 (TFOpLambd  (None, 42, 32)               0         ['subtract_182[0][0]']        \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.abs_188 (TFOpLambd  (None, 276)                  0         ['subtract_183[0][0]']        \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 386591 (1.47 MB)\n",
      "Trainable params: 385743 (1.47 MB)\n",
      "Non-trainable params: 848 (3.31 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate = 0.0001) # 0.00001\n",
    "print(\"[INFO] compiling model...\")\n",
    "model_a1.compile(loss = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\", \"mse\", \"mse\"], loss_weights = [1., 10, 20, 20], optimizer= opt)\n",
    "print('[INFO] building and compilation complete!')\n",
    "print('\\n Model summary:') \n",
    "model_a1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "id": "13069ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 42, 32)\n",
      "(500, 42, 32)\n",
      "(1500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "layer_zero_train = np.zeros(shape = (y_train.shape[0], 42, 32))\n",
    "layer_zero_val = np.zeros(shape = (y_val.shape[0], 42, 32))\n",
    "\n",
    "print(layer_zero_train.shape)\n",
    "print(layer_zero_val.shape)\n",
    "\n",
    "out_zero_train = np.zeros(shape = (y_train.shape[0], ))\n",
    "out_zero_val = np.zeros(shape = (y_val.shape[0], ))\n",
    "\n",
    "print(out_zero_train.shape)\n",
    "print(out_zero_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6546f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_a1.fit(\n",
    "    x_train, [y_train , y_train , layer_zero_train, out_zero_train ],  #pairTrain[:, 1]\n",
    "  validation_data = ( x_val, [y_val , y_val , layer_zero_val, out_zero_val ] ),  #pairVal[:, 1]\n",
    "    epochs=2000\n",
    "    ,batch_size = 64    #8192\n",
    "  , callbacks=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1418,
   "id": "dce909fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_a1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1419,
   "id": "a4d81408",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_T = []\n",
    "for i in range(760):\n",
    "    pred_T.append(np.argmax(y_pred[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "id": "4dc09210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_S = []\n",
    "for i in range(760):\n",
    "    pred_S.append(np.argmax(y_pred[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1421,
   "id": "647b2b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157894736842105"
      ]
     },
     "execution_count": 1421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_T = accuracy_score(y_test, pred_T)\n",
    "accuracy_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "id": "74fd38c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8144736842105263"
      ]
     },
     "execution_count": 1422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_S = accuracy_score(y_test, pred_S)\n",
    "accuracy_S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c492e7e",
   "metadata": {},
   "source": [
    "# with attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1529e7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 93, 64)]          0         \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 64)                0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 1)                 2113      \n",
      "                                                                 \n",
      " tf.reshape_1 (TFOpLambda)   (None, 93, 1)             0         \n",
      "                                                                 \n",
      " conv1d_26 (Conv1D)          (None, 93, 1)             6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2119 (8.28 KB)\n",
      "Trainable params: 2119 (8.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Add' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m x_T \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMultiply()([x_a1, x_b2])\n\u001b[1;32m     55\u001b[0m \u001b[39m# x_T = tf.multiply([x_a1, x_b2])\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m x_T \u001b[39m=\u001b[39m Add()([x_T, x_c2])\n\u001b[1;32m     57\u001b[0m x_T \u001b[39m=\u001b[39m MaxPooling1D(\u001b[39m2\u001b[39m)(x_T) \u001b[39m# 46, 64\u001b[39;00m\n\u001b[1;32m     58\u001b[0m x_T \u001b[39m=\u001b[39m BatchNormalization()(x_T)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Add' is not defined"
     ]
    }
   ],
   "source": [
    "# def spatial_mean_attention(shape, kernel):\n",
    "#   inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "#   # CHANNEL SUMMARIZATION\n",
    "#   mean_across_channels = tf.keras.backend.mean(inp, axis = -1)\n",
    "#   max_across_channels = tf.keras.backend.max(inp, axis = -1)\n",
    "#   cat = tf.keras.layers.concatenate([ tf.expand_dims(mean_across_channels, axis=-1), tf.expand_dims(max_across_channels, axis=-1)])\n",
    "\n",
    "#   # CONV & ACTIVATION\n",
    "#   out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( tf.expand_dims(mean_across_channels, axis=-1) )\n",
    "\n",
    "#   model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "#   model.summary()\n",
    "#   return model\n",
    "\n",
    "def spatial_mlp_attention(shape, kernel):\n",
    "  inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "  # CHANNEL SUMMARIZATION\n",
    "  w, C = shape[0], shape[1]\n",
    "  # Reshape the feature map to (w * h, C)\n",
    "  reshaped_feature_map = tf.reshape(inp, [-1, C])\n",
    "  # Define the MLP architecture\n",
    "  hidden_units = 32\n",
    "  output_units = 1\n",
    "  mlp = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      Dropout(0.4),\n",
    "      #BatchNormalization(),\n",
    "      tf.keras.layers.Dense(output_units, activation='sigmoid')\n",
    "  ])\n",
    "  # Apply the MLP on the reshaped feature map\n",
    "  output_mlp = mlp(reshaped_feature_map)\n",
    "  reshaped_output = tf.reshape(output_mlp, ( -1, w, 1 ))\n",
    "  # CONV & ACTIVATION\n",
    "  out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( reshaped_output )\n",
    "\n",
    "  model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "dataInp = Input(shape = (200, 3), name='in')\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_c1 = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_T')(dataInp) # 194, 32\n",
    "x_b1 = BatchNormalization()(x_c1)\n",
    "x_m1 = MaxPooling1D(2)(x_b1) # 97, 32\n",
    "x_c2 = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_m1) # 93, 64\n",
    "x_b2 = BatchNormalization()(x_c2)\n",
    "\n",
    "at1 = spatial_mlp_attention(shape = (93, 64), kernel = 5)\n",
    "x_a1 = at1(x_b2) \n",
    "x_T = tf.keras.layers.Multiply()([x_a1, x_b2])\n",
    "# x_T = tf.multiply([x_a1, x_b2])\n",
    "x_T = Add()([x_T, x_c2])\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_c3 = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b3 = BatchNormalization()(x_c3)\n",
    "\n",
    "at2 = spatial_mlp_attention(shape = (44, 256), kernel = kernel_size)\n",
    "x_a2 = at2(x_b3)\n",
    "x_T = tf.keras.layers.Multiply()([x_a2, x_b3])\n",
    "# x_T = tf.multiply([x_a2, x_b3])\n",
    "x_T = Add()([x_T, x_c3])\n",
    "\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu', name='bottle_T')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid', name='out_T')(d_T)\n",
    "\n",
    "# STUDENT MODEL\n",
    "x_S = Conv1D(kernel_size = 7, filters=8, activation='relu', name='l1_S')(dataInp) # 194, 8\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(4)(x_S) # 48, 32\n",
    "x_b_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "avg_S = GlobalAveragePooling1D()(x_b_S)\n",
    "max_S = GlobalMaxPooling1D()(x_b_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "\n",
    "#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\n",
    "# abs_sub = tf.abs(x_b_T - x_b_S)\n",
    "# l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\n",
    "abs_sub = tf.abs(tf.keras.layers.Subtract()([x_b_T, x_b_S]))  \n",
    "l2_output = tf.abs(tf.keras.layers.Subtract()([out_T, out_S]))\n",
    "\n",
    "model_a = Model( inputs=dataInp, outputs=[out_T, out_S, abs_sub, l2_output], name='distillationFramework' )\n",
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "id": "6be11278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] building and compilation complete!\n",
      "\n",
      " Model summary:\n",
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_540 (B  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_595 (MaxPool  (None, 97, 32)               0         ['batch_normalization_540[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " conv1d_679 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_595[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_541 (B  (None, 93, 64)               256       ['conv1d_679[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " model_88 (Functional)       (None, 93, 1)                2119      ['batch_normalization_541[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multiply_97 (Multiply)      (None, 93, 64)               0         ['model_88[0][0]',            \n",
      "                                                                     'batch_normalization_541[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_340 (Add)               (None, 93, 64)               0         ['multiply_97[0][0]',         \n",
      "                                                                     'conv1d_679[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_596 (MaxPool  (None, 46, 64)               0         ['add_340[0][0]']             \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_542 (B  (None, 46, 64)               256       ['max_pooling1d_596[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_681 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_542[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_543 (B  (None, 44, 256)              1024      ['conv1d_681[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " model_89 (Functional)       (None, 44, 1)                8261      ['batch_normalization_543[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multiply_98 (Multiply)      (None, 44, 256)              0         ['model_89[0][0]',            \n",
      "                                                                     'batch_normalization_543[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_341 (Add)               (None, 44, 256)              0         ['multiply_98[0][0]',         \n",
      "                                                                     'conv1d_681[0][0]']          \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['add_341[0][0]']             \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_597 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_544 (B  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv1d_683 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_597[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_598 (MaxPool  (None, 48, 8)                0         ['batch_normalization_544[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 512)                  0         ['conv1d_683[0][0]']          \n",
      " 51 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_257 (  (None, 512)                  0         ['conv1d_683[0][0]']          \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_598[0][0]']   \n",
      "                                                                                                  \n",
      " add_342 (Add)               (None, 512)                  0         ['global_average_pooling1d_251\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_257[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 52 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_258 (  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_352 (Dense)           (None, 300)                  153900    ['add_342[0][0]']             \n",
      "                                                                                                  \n",
      " add_343 (Add)               (None, 32)                   0         ['global_average_pooling1d_252\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_258[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_329 (Dropout)       (None, 300)                  0         ['dense_352[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_330 (Dropout)       (None, 32)                   0         ['add_343[0][0]']             \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_329[0][0]']         \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_330[0][0]']         \n",
      "                                                                                                  \n",
      " subtract_180 (Subtract)     (None, 42, 32)               0         ['bottle_T[0][0]',            \n",
      "                                                                     'bottle_S[0][0]']            \n",
      "                                                                                                  \n",
      " subtract_181 (Subtract)     (None, 276)                  0         ['out_T[0][0]',               \n",
      "                                                                     'out_S[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.abs_185 (TFOpLambd  (None, 42, 32)               0         ['subtract_180[0][0]']        \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.abs_186 (TFOpLambd  (None, 276)                  0         ['subtract_181[0][0]']        \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 394848 (1.51 MB)\n",
      "Trainable params: 394000 (1.50 MB)\n",
      "Non-trainable params: 848 (3.31 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate = 0.0001) # 0.00001\n",
    "print(\"[INFO] compiling model...\")\n",
    "model_a.compile(loss = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\", \"mse\", \"mse\"], loss_weights = [1, 10, 20, 20], optimizer= opt)\n",
    "print('[INFO] building and compilation complete!')\n",
    "print('\\n Model summary:') \n",
    "model_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "id": "168a0643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 42, 32)\n",
      "(500, 42, 32)\n",
      "(1500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "ayer_zero_train = np.zeros(shape = (y_train.shape[0], 42, 32))\n",
    "layer_zero_val = np.zeros(shape = (y_val.shape[0], 42, 32))\n",
    "\n",
    "print(layer_zero_train.shape)\n",
    "print(layer_zero_val.shape)\n",
    "\n",
    "out_zero_train = np.zeros(shape = (y_train.shape[0], ))\n",
    "out_zero_val = np.zeros(shape = (y_val.shape[0], ))\n",
    "\n",
    "print(out_zero_train.shape)\n",
    "print(out_zero_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1408,
   "id": "ef73ec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/2000\n",
      "24/24 [==============================] - 6s 47ms/step - loss: 72.0361 - out_T_loss: 5.6770 - out_S_loss: 6.0054 - tf.math.abs_185_loss: 0.2799 - tf.math.abs_186_loss: 0.0354 - val_loss: 122.9408 - val_out_T_loss: 5.6182 - val_out_S_loss: 7.3446 - val_tf.math.abs_185_loss: 2.1147 - val_tf.math.abs_186_loss: 0.0791\n",
      "Epoch 2/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 68.0651 - out_T_loss: 5.6088 - out_S_loss: 5.8891 - tf.math.abs_185_loss: 0.1532 - tf.math.abs_186_loss: 0.0250 - val_loss: 81.6776 - val_out_T_loss: 5.6152 - val_out_S_loss: 6.2985 - val_tf.math.abs_185_loss: 0.6107 - val_tf.math.abs_186_loss: 0.0432\n",
      "Epoch 3/2000\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 66.3140 - out_T_loss: 5.5762 - out_S_loss: 5.8101 - tf.math.abs_185_loss: 0.1115 - tf.math.abs_186_loss: 0.0204 - val_loss: 71.3472 - val_out_T_loss: 5.5958 - val_out_S_loss: 5.9559 - val_tf.math.abs_185_loss: 0.2835 - val_tf.math.abs_186_loss: 0.0261\n",
      "Epoch 4/2000\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 64.9964 - out_T_loss: 5.5306 - out_S_loss: 5.7342 - tf.math.abs_185_loss: 0.0891 - tf.math.abs_186_loss: 0.0171 - val_loss: 67.3254 - val_out_T_loss: 5.5719 - val_out_S_loss: 5.7984 - val_tf.math.abs_185_loss: 0.1713 - val_tf.math.abs_186_loss: 0.0171\n",
      "Epoch 5/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 64.2881 - out_T_loss: 5.4866 - out_S_loss: 5.6974 - tf.math.abs_185_loss: 0.0762 - tf.math.abs_186_loss: 0.0151 - val_loss: 65.2480 - val_out_T_loss: 5.5390 - val_out_S_loss: 5.7074 - val_tf.math.abs_185_loss: 0.1197 - val_tf.math.abs_186_loss: 0.0121\n",
      "Epoch 6/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 63.5274 - out_T_loss: 5.4385 - out_S_loss: 5.6504 - tf.math.abs_185_loss: 0.0656 - tf.math.abs_186_loss: 0.0136 - val_loss: 63.9506 - val_out_T_loss: 5.4964 - val_out_S_loss: 5.6485 - val_tf.math.abs_185_loss: 0.0894 - val_tf.math.abs_186_loss: 0.0090\n",
      "Epoch 7/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 62.9716 - out_T_loss: 5.3806 - out_S_loss: 5.6194 - tf.math.abs_185_loss: 0.0571 - tf.math.abs_186_loss: 0.0127 - val_loss: 63.0434 - val_out_T_loss: 5.4469 - val_out_S_loss: 5.6059 - val_tf.math.abs_185_loss: 0.0697 - val_tf.math.abs_186_loss: 0.0072\n",
      "Epoch 8/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 62.4646 - out_T_loss: 5.3218 - out_S_loss: 5.5886 - tf.math.abs_185_loss: 0.0505 - tf.math.abs_186_loss: 0.0123 - val_loss: 62.3582 - val_out_T_loss: 5.3945 - val_out_S_loss: 5.5731 - val_tf.math.abs_185_loss: 0.0555 - val_tf.math.abs_186_loss: 0.0061\n",
      "Epoch 9/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 61.9763 - out_T_loss: 5.2571 - out_S_loss: 5.5569 - tf.math.abs_185_loss: 0.0453 - tf.math.abs_186_loss: 0.0122 - val_loss: 61.8789 - val_out_T_loss: 5.3326 - val_out_S_loss: 5.5473 - val_tf.math.abs_185_loss: 0.0481 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 10/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 61.6683 - out_T_loss: 5.1981 - out_S_loss: 5.5390 - tf.math.abs_185_loss: 0.0416 - tf.math.abs_186_loss: 0.0124 - val_loss: 61.4983 - val_out_T_loss: 5.2784 - val_out_S_loss: 5.5256 - val_tf.math.abs_185_loss: 0.0428 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 11/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 61.4510 - out_T_loss: 5.1408 - out_S_loss: 5.5286 - tf.math.abs_185_loss: 0.0387 - tf.math.abs_186_loss: 0.0125 - val_loss: 61.1682 - val_out_T_loss: 5.2204 - val_out_S_loss: 5.5063 - val_tf.math.abs_185_loss: 0.0387 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 12/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 61.1796 - out_T_loss: 5.0799 - out_S_loss: 5.5113 - tf.math.abs_185_loss: 0.0364 - tf.math.abs_186_loss: 0.0129 - val_loss: 60.8835 - val_out_T_loss: 5.1708 - val_out_S_loss: 5.4890 - val_tf.math.abs_185_loss: 0.0356 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 13/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 60.8589 - out_T_loss: 5.0265 - out_S_loss: 5.4886 - tf.math.abs_185_loss: 0.0341 - tf.math.abs_186_loss: 0.0132 - val_loss: 60.6305 - val_out_T_loss: 5.1207 - val_out_S_loss: 5.4726 - val_tf.math.abs_185_loss: 0.0334 - val_tf.math.abs_186_loss: 0.0058\n",
      "Epoch 14/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 60.5424 - out_T_loss: 4.9558 - out_S_loss: 5.4653 - tf.math.abs_185_loss: 0.0329 - tf.math.abs_186_loss: 0.0138 - val_loss: 60.3880 - val_out_T_loss: 5.0634 - val_out_S_loss: 5.4566 - val_tf.math.abs_185_loss: 0.0317 - val_tf.math.abs_186_loss: 0.0062\n",
      "Epoch 15/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 60.3781 - out_T_loss: 4.9197 - out_S_loss: 5.4539 - tf.math.abs_185_loss: 0.0317 - tf.math.abs_186_loss: 0.0143 - val_loss: 60.1436 - val_out_T_loss: 5.0043 - val_out_S_loss: 5.4408 - val_tf.math.abs_185_loss: 0.0299 - val_tf.math.abs_186_loss: 0.0067\n",
      "Epoch 16/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 60.1126 - out_T_loss: 4.8386 - out_S_loss: 5.4368 - tf.math.abs_185_loss: 0.0304 - tf.math.abs_186_loss: 0.0149 - val_loss: 59.9244 - val_out_T_loss: 4.9569 - val_out_S_loss: 5.4259 - val_tf.math.abs_185_loss: 0.0285 - val_tf.math.abs_186_loss: 0.0069\n",
      "Epoch 17/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 59.8240 - out_T_loss: 4.7966 - out_S_loss: 5.4126 - tf.math.abs_185_loss: 0.0297 - tf.math.abs_186_loss: 0.0154 - val_loss: 59.7184 - val_out_T_loss: 4.9065 - val_out_S_loss: 5.4113 - val_tf.math.abs_185_loss: 0.0276 - val_tf.math.abs_186_loss: 0.0073\n",
      "Epoch 18/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 59.4322 - out_T_loss: 4.7322 - out_S_loss: 5.3800 - tf.math.abs_185_loss: 0.0289 - tf.math.abs_186_loss: 0.0161 - val_loss: 59.5125 - val_out_T_loss: 4.8400 - val_out_S_loss: 5.3951 - val_tf.math.abs_185_loss: 0.0277 - val_tf.math.abs_186_loss: 0.0084\n",
      "Epoch 19/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 59.1633 - out_T_loss: 4.6639 - out_S_loss: 5.3591 - tf.math.abs_185_loss: 0.0284 - tf.math.abs_186_loss: 0.0170 - val_loss: 59.2877 - val_out_T_loss: 4.7994 - val_out_S_loss: 5.3792 - val_tf.math.abs_185_loss: 0.0265 - val_tf.math.abs_186_loss: 0.0083\n",
      "Epoch 20/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 59.2014 - out_T_loss: 4.5994 - out_S_loss: 5.3689 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0177 - val_loss: 59.0651 - val_out_T_loss: 4.7416 - val_out_S_loss: 5.3626 - val_tf.math.abs_185_loss: 0.0262 - val_tf.math.abs_186_loss: 0.0087\n",
      "Epoch 21/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 59.0066 - out_T_loss: 4.5672 - out_S_loss: 5.3526 - tf.math.abs_185_loss: 0.0275 - tf.math.abs_186_loss: 0.0182 - val_loss: 58.8451 - val_out_T_loss: 4.6892 - val_out_S_loss: 5.3464 - val_tf.math.abs_185_loss: 0.0257 - val_tf.math.abs_186_loss: 0.0089\n",
      "Epoch 22/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 58.4825 - out_T_loss: 4.4928 - out_S_loss: 5.3065 - tf.math.abs_185_loss: 0.0272 - tf.math.abs_186_loss: 0.0190 - val_loss: 58.6377 - val_out_T_loss: 4.6387 - val_out_S_loss: 5.3304 - val_tf.math.abs_185_loss: 0.0255 - val_tf.math.abs_186_loss: 0.0093\n",
      "Epoch 23/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 58.4492 - out_T_loss: 4.4485 - out_S_loss: 5.3069 - tf.math.abs_185_loss: 0.0269 - tf.math.abs_186_loss: 0.0197 - val_loss: 58.4184 - val_out_T_loss: 4.5786 - val_out_S_loss: 5.3136 - val_tf.math.abs_185_loss: 0.0252 - val_tf.math.abs_186_loss: 0.0099\n",
      "Epoch 24/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 58.2358 - out_T_loss: 4.3674 - out_S_loss: 5.2926 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0205 - val_loss: 58.1875 - val_out_T_loss: 4.5323 - val_out_S_loss: 5.2964 - val_tf.math.abs_185_loss: 0.0247 - val_tf.math.abs_186_loss: 0.0099\n",
      "Epoch 25/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 57.9517 - out_T_loss: 4.3341 - out_S_loss: 5.2665 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0210 - val_loss: 57.9627 - val_out_T_loss: 4.4740 - val_out_S_loss: 5.2787 - val_tf.math.abs_185_loss: 0.0245 - val_tf.math.abs_186_loss: 0.0106\n",
      "Epoch 26/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 57.4939 - out_T_loss: 4.2700 - out_S_loss: 5.2269 - tf.math.abs_185_loss: 0.0262 - tf.math.abs_186_loss: 0.0216 - val_loss: 57.7335 - val_out_T_loss: 4.4126 - val_out_S_loss: 5.2606 - val_tf.math.abs_185_loss: 0.0246 - val_tf.math.abs_186_loss: 0.0111\n",
      "Epoch 27/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 57.2908 - out_T_loss: 4.1902 - out_S_loss: 5.2127 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0226 - val_loss: 57.4862 - val_out_T_loss: 4.3548 - val_out_S_loss: 5.2419 - val_tf.math.abs_185_loss: 0.0242 - val_tf.math.abs_186_loss: 0.0115\n",
      "Epoch 28/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 57.1910 - out_T_loss: 4.1562 - out_S_loss: 5.2057 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0228 - val_loss: 57.2476 - val_out_T_loss: 4.2984 - val_out_S_loss: 5.2229 - val_tf.math.abs_185_loss: 0.0240 - val_tf.math.abs_186_loss: 0.0120\n",
      "Epoch 29/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 56.8494 - out_T_loss: 4.0774 - out_S_loss: 5.1777 - tf.math.abs_185_loss: 0.0259 - tf.math.abs_186_loss: 0.0238 - val_loss: 57.0056 - val_out_T_loss: 4.2477 - val_out_S_loss: 5.2043 - val_tf.math.abs_185_loss: 0.0238 - val_tf.math.abs_186_loss: 0.0120\n",
      "Epoch 30/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 56.6875 - out_T_loss: 4.0404 - out_S_loss: 5.1637 - tf.math.abs_185_loss: 0.0262 - tf.math.abs_186_loss: 0.0243 - val_loss: 56.7663 - val_out_T_loss: 4.1945 - val_out_S_loss: 5.1851 - val_tf.math.abs_185_loss: 0.0239 - val_tf.math.abs_186_loss: 0.0121\n",
      "Epoch 31/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 56.2716 - out_T_loss: 3.9898 - out_S_loss: 5.1268 - tf.math.abs_185_loss: 0.0258 - tf.math.abs_186_loss: 0.0249 - val_loss: 56.5291 - val_out_T_loss: 4.1247 - val_out_S_loss: 5.1657 - val_tf.math.abs_185_loss: 0.0244 - val_tf.math.abs_186_loss: 0.0130\n",
      "Epoch 32/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 56.2363 - out_T_loss: 3.9305 - out_S_loss: 5.1276 - tf.math.abs_185_loss: 0.0257 - tf.math.abs_186_loss: 0.0257 - val_loss: 56.2929 - val_out_T_loss: 4.0829 - val_out_S_loss: 5.1469 - val_tf.math.abs_185_loss: 0.0242 - val_tf.math.abs_186_loss: 0.0128\n",
      "Epoch 33/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 56.0024 - out_T_loss: 3.8524 - out_S_loss: 5.1109 - tf.math.abs_185_loss: 0.0259 - tf.math.abs_186_loss: 0.0261 - val_loss: 56.0428 - val_out_T_loss: 4.0352 - val_out_S_loss: 5.1281 - val_tf.math.abs_185_loss: 0.0235 - val_tf.math.abs_186_loss: 0.0128\n",
      "Epoch 34/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 55.7462 - out_T_loss: 3.7730 - out_S_loss: 5.0916 - tf.math.abs_185_loss: 0.0260 - tf.math.abs_186_loss: 0.0269 - val_loss: 55.7790 - val_out_T_loss: 3.9623 - val_out_S_loss: 5.1078 - val_tf.math.abs_185_loss: 0.0236 - val_tf.math.abs_186_loss: 0.0133\n",
      "Epoch 35/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 55.2264 - out_T_loss: 3.7571 - out_S_loss: 5.0410 - tf.math.abs_185_loss: 0.0259 - tf.math.abs_186_loss: 0.0271 - val_loss: 55.5257 - val_out_T_loss: 3.9221 - val_out_S_loss: 5.0874 - val_tf.math.abs_185_loss: 0.0233 - val_tf.math.abs_186_loss: 0.0131\n",
      "Epoch 36/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 55.1416 - out_T_loss: 3.6821 - out_S_loss: 5.0382 - tf.math.abs_185_loss: 0.0260 - tf.math.abs_186_loss: 0.0279 - val_loss: 55.2898 - val_out_T_loss: 3.8643 - val_out_S_loss: 5.0685 - val_tf.math.abs_185_loss: 0.0234 - val_tf.math.abs_186_loss: 0.0136\n",
      "Epoch 37/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 54.6899 - out_T_loss: 3.6059 - out_S_loss: 5.0000 - tf.math.abs_185_loss: 0.0258 - tf.math.abs_186_loss: 0.0284 - val_loss: 55.0494 - val_out_T_loss: 3.8143 - val_out_S_loss: 5.0486 - val_tf.math.abs_185_loss: 0.0236 - val_tf.math.abs_186_loss: 0.0139\n",
      "Epoch 38/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 54.4867 - out_T_loss: 3.5684 - out_S_loss: 4.9825 - tf.math.abs_185_loss: 0.0259 - tf.math.abs_186_loss: 0.0287 - val_loss: 54.7913 - val_out_T_loss: 3.7708 - val_out_S_loss: 5.0282 - val_tf.math.abs_185_loss: 0.0233 - val_tf.math.abs_186_loss: 0.0136\n",
      "Epoch 39/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 54.4752 - out_T_loss: 3.5500 - out_S_loss: 4.9817 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0293 - val_loss: 54.5355 - val_out_T_loss: 3.7300 - val_out_S_loss: 5.0073 - val_tf.math.abs_185_loss: 0.0232 - val_tf.math.abs_186_loss: 0.0134\n",
      "Epoch 40/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 53.9963 - out_T_loss: 3.4756 - out_S_loss: 4.9416 - tf.math.abs_185_loss: 0.0257 - tf.math.abs_186_loss: 0.0295 - val_loss: 54.2908 - val_out_T_loss: 3.6692 - val_out_S_loss: 4.9866 - val_tf.math.abs_185_loss: 0.0237 - val_tf.math.abs_186_loss: 0.0140\n",
      "Epoch 41/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 53.5390 - out_T_loss: 3.4617 - out_S_loss: 4.8953 - tf.math.abs_185_loss: 0.0262 - tf.math.abs_186_loss: 0.0300 - val_loss: 54.0490 - val_out_T_loss: 3.6245 - val_out_S_loss: 4.9665 - val_tf.math.abs_185_loss: 0.0236 - val_tf.math.abs_186_loss: 0.0144\n",
      "Epoch 42/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 53.6237 - out_T_loss: 3.4060 - out_S_loss: 4.9073 - tf.math.abs_185_loss: 0.0267 - tf.math.abs_186_loss: 0.0305 - val_loss: 53.7836 - val_out_T_loss: 3.5786 - val_out_S_loss: 4.9452 - val_tf.math.abs_185_loss: 0.0234 - val_tf.math.abs_186_loss: 0.0142\n",
      "Epoch 43/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 53.3494 - out_T_loss: 3.3216 - out_S_loss: 4.8887 - tf.math.abs_185_loss: 0.0263 - tf.math.abs_186_loss: 0.0308 - val_loss: 53.5204 - val_out_T_loss: 3.5324 - val_out_S_loss: 4.9235 - val_tf.math.abs_185_loss: 0.0235 - val_tf.math.abs_186_loss: 0.0142\n",
      "Epoch 44/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 53.0838 - out_T_loss: 3.2636 - out_S_loss: 4.8670 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0314 - val_loss: 53.2816 - val_out_T_loss: 3.4930 - val_out_S_loss: 4.9037 - val_tf.math.abs_185_loss: 0.0235 - val_tf.math.abs_186_loss: 0.0141\n",
      "Epoch 45/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 52.9637 - out_T_loss: 3.2412 - out_S_loss: 4.8573 - tf.math.abs_185_loss: 0.0262 - tf.math.abs_186_loss: 0.0312 - val_loss: 53.0449 - val_out_T_loss: 3.4462 - val_out_S_loss: 4.8839 - val_tf.math.abs_185_loss: 0.0236 - val_tf.math.abs_186_loss: 0.0144\n",
      "Epoch 46/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 52.7833 - out_T_loss: 3.2099 - out_S_loss: 4.8401 - tf.math.abs_185_loss: 0.0267 - tf.math.abs_186_loss: 0.0319 - val_loss: 52.8023 - val_out_T_loss: 3.4232 - val_out_S_loss: 4.8620 - val_tf.math.abs_185_loss: 0.0238 - val_tf.math.abs_186_loss: 0.0142\n",
      "Epoch 47/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 52.2057 - out_T_loss: 3.1642 - out_S_loss: 4.7877 - tf.math.abs_185_loss: 0.0264 - tf.math.abs_186_loss: 0.0318 - val_loss: 52.5671 - val_out_T_loss: 3.3756 - val_out_S_loss: 4.8414 - val_tf.math.abs_185_loss: 0.0244 - val_tf.math.abs_186_loss: 0.0145\n",
      "Epoch 48/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 52.2280 - out_T_loss: 3.1205 - out_S_loss: 4.7932 - tf.math.abs_185_loss: 0.0267 - tf.math.abs_186_loss: 0.0321 - val_loss: 52.2901 - val_out_T_loss: 3.3181 - val_out_S_loss: 4.8206 - val_tf.math.abs_185_loss: 0.0237 - val_tf.math.abs_186_loss: 0.0146\n",
      "Epoch 49/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 51.8709 - out_T_loss: 3.0795 - out_S_loss: 4.7608 - tf.math.abs_185_loss: 0.0264 - tf.math.abs_186_loss: 0.0327 - val_loss: 52.0596 - val_out_T_loss: 3.2808 - val_out_S_loss: 4.8008 - val_tf.math.abs_185_loss: 0.0241 - val_tf.math.abs_186_loss: 0.0145\n",
      "Epoch 50/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 51.6813 - out_T_loss: 3.0345 - out_S_loss: 4.7464 - tf.math.abs_185_loss: 0.0268 - tf.math.abs_186_loss: 0.0323 - val_loss: 51.8130 - val_out_T_loss: 3.2476 - val_out_S_loss: 4.7799 - val_tf.math.abs_185_loss: 0.0239 - val_tf.math.abs_186_loss: 0.0145\n",
      "Epoch 51/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 51.2599 - out_T_loss: 2.9813 - out_S_loss: 4.7086 - tf.math.abs_185_loss: 0.0267 - tf.math.abs_186_loss: 0.0329 - val_loss: 51.5680 - val_out_T_loss: 3.2110 - val_out_S_loss: 4.7586 - val_tf.math.abs_185_loss: 0.0240 - val_tf.math.abs_186_loss: 0.0146\n",
      "Epoch 52/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 51.1553 - out_T_loss: 2.9981 - out_S_loss: 4.6967 - tf.math.abs_185_loss: 0.0269 - tf.math.abs_186_loss: 0.0326 - val_loss: 51.3215 - val_out_T_loss: 3.1788 - val_out_S_loss: 4.7375 - val_tf.math.abs_185_loss: 0.0243 - val_tf.math.abs_186_loss: 0.0141\n",
      "Epoch 53/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 50.7195 - out_T_loss: 2.9213 - out_S_loss: 4.6591 - tf.math.abs_185_loss: 0.0273 - tf.math.abs_186_loss: 0.0331 - val_loss: 51.0718 - val_out_T_loss: 3.1229 - val_out_S_loss: 4.7165 - val_tf.math.abs_185_loss: 0.0245 - val_tf.math.abs_186_loss: 0.0147\n",
      "Epoch 54/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 50.7382 - out_T_loss: 2.8608 - out_S_loss: 4.6664 - tf.math.abs_185_loss: 0.0272 - tf.math.abs_186_loss: 0.0334 - val_loss: 50.8129 - val_out_T_loss: 3.0862 - val_out_S_loss: 4.6944 - val_tf.math.abs_185_loss: 0.0243 - val_tf.math.abs_186_loss: 0.0149\n",
      "Epoch 55/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 50.3173 - out_T_loss: 2.8601 - out_S_loss: 4.6254 - tf.math.abs_185_loss: 0.0269 - tf.math.abs_186_loss: 0.0333 - val_loss: 50.5458 - val_out_T_loss: 3.0460 - val_out_S_loss: 4.6714 - val_tf.math.abs_185_loss: 0.0243 - val_tf.math.abs_186_loss: 0.0149\n",
      "Epoch 56/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 50.1309 - out_T_loss: 2.8102 - out_S_loss: 4.6120 - tf.math.abs_185_loss: 0.0267 - tf.math.abs_186_loss: 0.0334 - val_loss: 50.3283 - val_out_T_loss: 3.0239 - val_out_S_loss: 4.6519 - val_tf.math.abs_185_loss: 0.0245 - val_tf.math.abs_186_loss: 0.0148\n",
      "Epoch 57/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 50.1070 - out_T_loss: 2.7814 - out_S_loss: 4.6113 - tf.math.abs_185_loss: 0.0269 - tf.math.abs_186_loss: 0.0337 - val_loss: 50.0939 - val_out_T_loss: 2.9896 - val_out_S_loss: 4.6317 - val_tf.math.abs_185_loss: 0.0245 - val_tf.math.abs_186_loss: 0.0148\n",
      "Epoch 58/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 49.7887 - out_T_loss: 2.7623 - out_S_loss: 4.5808 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0336 - val_loss: 49.8471 - val_out_T_loss: 2.9600 - val_out_S_loss: 4.6104 - val_tf.math.abs_185_loss: 0.0242 - val_tf.math.abs_186_loss: 0.0149\n",
      "Epoch 59/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 49.7987 - out_T_loss: 2.7178 - out_S_loss: 4.5852 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0340 - val_loss: 49.6178 - val_out_T_loss: 2.9242 - val_out_S_loss: 4.5904 - val_tf.math.abs_185_loss: 0.0248 - val_tf.math.abs_186_loss: 0.0147\n",
      "Epoch 60/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 49.0376 - out_T_loss: 2.6781 - out_S_loss: 4.5153 - tf.math.abs_185_loss: 0.0271 - tf.math.abs_186_loss: 0.0332 - val_loss: 49.3571 - val_out_T_loss: 2.8802 - val_out_S_loss: 4.5689 - val_tf.math.abs_185_loss: 0.0246 - val_tf.math.abs_186_loss: 0.0148\n",
      "Epoch 61/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 48.8442 - out_T_loss: 2.6238 - out_S_loss: 4.5015 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0336 - val_loss: 49.1522 - val_out_T_loss: 2.8578 - val_out_S_loss: 4.5501 - val_tf.math.abs_185_loss: 0.0251 - val_tf.math.abs_186_loss: 0.0146\n",
      "Epoch 62/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 48.9372 - out_T_loss: 2.6190 - out_S_loss: 4.5093 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0338 - val_loss: 48.8849 - val_out_T_loss: 2.8117 - val_out_S_loss: 4.5277 - val_tf.math.abs_185_loss: 0.0249 - val_tf.math.abs_186_loss: 0.0149\n",
      "Epoch 63/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 48.3889 - out_T_loss: 2.6052 - out_S_loss: 4.4549 - tf.math.abs_185_loss: 0.0276 - tf.math.abs_186_loss: 0.0342 - val_loss: 48.6680 - val_out_T_loss: 2.7904 - val_out_S_loss: 4.5069 - val_tf.math.abs_185_loss: 0.0255 - val_tf.math.abs_186_loss: 0.0149\n",
      "Epoch 64/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 48.4342 - out_T_loss: 2.5567 - out_S_loss: 4.4654 - tf.math.abs_185_loss: 0.0275 - tf.math.abs_186_loss: 0.0337 - val_loss: 48.4165 - val_out_T_loss: 2.7505 - val_out_S_loss: 4.4858 - val_tf.math.abs_185_loss: 0.0253 - val_tf.math.abs_186_loss: 0.0151\n",
      "Epoch 65/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 48.2942 - out_T_loss: 2.5149 - out_S_loss: 4.4540 - tf.math.abs_185_loss: 0.0276 - tf.math.abs_186_loss: 0.0344 - val_loss: 48.1792 - val_out_T_loss: 2.7381 - val_out_S_loss: 4.4637 - val_tf.math.abs_185_loss: 0.0256 - val_tf.math.abs_186_loss: 0.0146\n",
      "Epoch 66/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 47.8396 - out_T_loss: 2.5221 - out_S_loss: 4.4067 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0345 - val_loss: 47.9327 - val_out_T_loss: 2.7060 - val_out_S_loss: 4.4428 - val_tf.math.abs_185_loss: 0.0254 - val_tf.math.abs_186_loss: 0.0146\n",
      "Epoch 67/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 47.7132 - out_T_loss: 2.4975 - out_S_loss: 4.3991 - tf.math.abs_185_loss: 0.0276 - tf.math.abs_186_loss: 0.0337 - val_loss: 47.6574 - val_out_T_loss: 2.6740 - val_out_S_loss: 4.4191 - val_tf.math.abs_185_loss: 0.0252 - val_tf.math.abs_186_loss: 0.0144\n",
      "Epoch 68/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 47.3815 - out_T_loss: 2.4440 - out_S_loss: 4.3701 - tf.math.abs_185_loss: 0.0277 - tf.math.abs_186_loss: 0.0341 - val_loss: 47.4354 - val_out_T_loss: 2.6457 - val_out_S_loss: 4.3989 - val_tf.math.abs_185_loss: 0.0258 - val_tf.math.abs_186_loss: 0.0143\n",
      "Epoch 69/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 47.3358 - out_T_loss: 2.4503 - out_S_loss: 4.3639 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0344 - val_loss: 47.1591 - val_out_T_loss: 2.6084 - val_out_S_loss: 4.3760 - val_tf.math.abs_185_loss: 0.0254 - val_tf.math.abs_186_loss: 0.0142\n",
      "Epoch 70/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 46.6435 - out_T_loss: 2.4174 - out_S_loss: 4.3004 - tf.math.abs_185_loss: 0.0278 - tf.math.abs_186_loss: 0.0333 - val_loss: 46.9734 - val_out_T_loss: 2.5800 - val_out_S_loss: 4.3568 - val_tf.math.abs_185_loss: 0.0265 - val_tf.math.abs_186_loss: 0.0148\n",
      "Epoch 71/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 46.6177 - out_T_loss: 2.3972 - out_S_loss: 4.2986 - tf.math.abs_185_loss: 0.0283 - tf.math.abs_186_loss: 0.0334 - val_loss: 46.7466 - val_out_T_loss: 2.5464 - val_out_S_loss: 4.3376 - val_tf.math.abs_185_loss: 0.0264 - val_tf.math.abs_186_loss: 0.0148\n",
      "Epoch 72/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 46.6178 - out_T_loss: 2.3496 - out_S_loss: 4.3031 - tf.math.abs_185_loss: 0.0284 - tf.math.abs_186_loss: 0.0335 - val_loss: 46.5051 - val_out_T_loss: 2.5378 - val_out_S_loss: 4.3158 - val_tf.math.abs_185_loss: 0.0263 - val_tf.math.abs_186_loss: 0.0141\n",
      "Epoch 73/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 46.4054 - out_T_loss: 2.3156 - out_S_loss: 4.2846 - tf.math.abs_185_loss: 0.0284 - tf.math.abs_186_loss: 0.0338 - val_loss: 46.2477 - val_out_T_loss: 2.4935 - val_out_S_loss: 4.2953 - val_tf.math.abs_185_loss: 0.0260 - val_tf.math.abs_186_loss: 0.0141\n",
      "Epoch 74/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 46.1059 - out_T_loss: 2.2879 - out_S_loss: 4.2587 - tf.math.abs_185_loss: 0.0281 - tf.math.abs_186_loss: 0.0335 - val_loss: 46.0070 - val_out_T_loss: 2.4622 - val_out_S_loss: 4.2741 - val_tf.math.abs_185_loss: 0.0261 - val_tf.math.abs_186_loss: 0.0141\n",
      "Epoch 75/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 45.8195 - out_T_loss: 2.2447 - out_S_loss: 4.2322 - tf.math.abs_185_loss: 0.0283 - tf.math.abs_186_loss: 0.0343 - val_loss: 45.7983 - val_out_T_loss: 2.4465 - val_out_S_loss: 4.2544 - val_tf.math.abs_185_loss: 0.0263 - val_tf.math.abs_186_loss: 0.0140\n",
      "Epoch 76/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 45.3197 - out_T_loss: 2.2464 - out_S_loss: 4.1843 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0335 - val_loss: 45.5502 - val_out_T_loss: 2.4127 - val_out_S_loss: 4.2332 - val_tf.math.abs_185_loss: 0.0263 - val_tf.math.abs_186_loss: 0.0139\n",
      "Epoch 77/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 45.2631 - out_T_loss: 2.2348 - out_S_loss: 4.1796 - tf.math.abs_185_loss: 0.0281 - tf.math.abs_186_loss: 0.0335 - val_loss: 45.3376 - val_out_T_loss: 2.4006 - val_out_S_loss: 4.2126 - val_tf.math.abs_185_loss: 0.0268 - val_tf.math.abs_186_loss: 0.0138\n",
      "Epoch 78/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 45.3965 - out_T_loss: 2.2261 - out_S_loss: 4.1935 - tf.math.abs_185_loss: 0.0287 - tf.math.abs_186_loss: 0.0331 - val_loss: 45.1143 - val_out_T_loss: 2.3690 - val_out_S_loss: 4.1934 - val_tf.math.abs_185_loss: 0.0269 - val_tf.math.abs_186_loss: 0.0137\n",
      "Epoch 79/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 44.9061 - out_T_loss: 2.1876 - out_S_loss: 4.1481 - tf.math.abs_185_loss: 0.0286 - tf.math.abs_186_loss: 0.0333 - val_loss: 44.8771 - val_out_T_loss: 2.3445 - val_out_S_loss: 4.1726 - val_tf.math.abs_185_loss: 0.0267 - val_tf.math.abs_186_loss: 0.0136\n",
      "Epoch 80/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 45.0407 - out_T_loss: 2.1665 - out_S_loss: 4.1610 - tf.math.abs_185_loss: 0.0296 - tf.math.abs_186_loss: 0.0336 - val_loss: 44.6580 - val_out_T_loss: 2.3291 - val_out_S_loss: 4.1511 - val_tf.math.abs_185_loss: 0.0271 - val_tf.math.abs_186_loss: 0.0138\n",
      "Epoch 81/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 44.2606 - out_T_loss: 2.1402 - out_S_loss: 4.0877 - tf.math.abs_185_loss: 0.0289 - tf.math.abs_186_loss: 0.0332 - val_loss: 44.4322 - val_out_T_loss: 2.3016 - val_out_S_loss: 4.1323 - val_tf.math.abs_185_loss: 0.0270 - val_tf.math.abs_186_loss: 0.0134\n",
      "Epoch 82/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 44.1545 - out_T_loss: 2.0842 - out_S_loss: 4.0815 - tf.math.abs_185_loss: 0.0293 - tf.math.abs_186_loss: 0.0334 - val_loss: 44.2053 - val_out_T_loss: 2.2729 - val_out_S_loss: 4.1119 - val_tf.math.abs_185_loss: 0.0273 - val_tf.math.abs_186_loss: 0.0134\n",
      "Epoch 83/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 44.4418 - out_T_loss: 2.0985 - out_S_loss: 4.1104 - tf.math.abs_185_loss: 0.0290 - tf.math.abs_186_loss: 0.0330 - val_loss: 43.9729 - val_out_T_loss: 2.2557 - val_out_S_loss: 4.0902 - val_tf.math.abs_185_loss: 0.0276 - val_tf.math.abs_186_loss: 0.0131\n",
      "Epoch 84/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 44.2263 - out_T_loss: 2.0579 - out_S_loss: 4.0931 - tf.math.abs_185_loss: 0.0289 - tf.math.abs_186_loss: 0.0329 - val_loss: 43.7948 - val_out_T_loss: 2.2478 - val_out_S_loss: 4.0729 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0131\n",
      "Epoch 85/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 44.0279 - out_T_loss: 2.0666 - out_S_loss: 4.0708 - tf.math.abs_185_loss: 0.0298 - tf.math.abs_186_loss: 0.0329 - val_loss: 43.5685 - val_out_T_loss: 2.2112 - val_out_S_loss: 4.0541 - val_tf.math.abs_185_loss: 0.0277 - val_tf.math.abs_186_loss: 0.0131\n",
      "Epoch 86/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 43.8655 - out_T_loss: 2.0262 - out_S_loss: 4.0595 - tf.math.abs_185_loss: 0.0293 - tf.math.abs_186_loss: 0.0329 - val_loss: 43.3294 - val_out_T_loss: 2.1793 - val_out_S_loss: 4.0336 - val_tf.math.abs_185_loss: 0.0277 - val_tf.math.abs_186_loss: 0.0130\n",
      "Epoch 87/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 43.6327 - out_T_loss: 2.0209 - out_S_loss: 4.0366 - tf.math.abs_185_loss: 0.0296 - tf.math.abs_186_loss: 0.0327 - val_loss: 43.1132 - val_out_T_loss: 2.1633 - val_out_S_loss: 4.0138 - val_tf.math.abs_185_loss: 0.0277 - val_tf.math.abs_186_loss: 0.0129\n",
      "Epoch 88/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 43.6570 - out_T_loss: 1.9810 - out_S_loss: 4.0411 - tf.math.abs_185_loss: 0.0302 - tf.math.abs_186_loss: 0.0331 - val_loss: 42.9206 - val_out_T_loss: 2.1583 - val_out_S_loss: 3.9951 - val_tf.math.abs_185_loss: 0.0277 - val_tf.math.abs_186_loss: 0.0128\n",
      "Epoch 89/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 43.4937 - out_T_loss: 1.9689 - out_S_loss: 4.0276 - tf.math.abs_185_loss: 0.0300 - tf.math.abs_186_loss: 0.0325 - val_loss: 42.7459 - val_out_T_loss: 2.1447 - val_out_S_loss: 3.9777 - val_tf.math.abs_185_loss: 0.0282 - val_tf.math.abs_186_loss: 0.0130\n",
      "Epoch 90/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 43.2015 - out_T_loss: 1.9661 - out_S_loss: 3.9979 - tf.math.abs_185_loss: 0.0300 - tf.math.abs_186_loss: 0.0328 - val_loss: 42.4901 - val_out_T_loss: 2.0935 - val_out_S_loss: 3.9583 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0129\n",
      "Epoch 91/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 42.8418 - out_T_loss: 1.9209 - out_S_loss: 3.9684 - tf.math.abs_185_loss: 0.0293 - tf.math.abs_186_loss: 0.0325 - val_loss: 42.3046 - val_out_T_loss: 2.0815 - val_out_S_loss: 3.9415 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0126\n",
      "Epoch 92/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 42.6605 - out_T_loss: 1.9266 - out_S_loss: 3.9492 - tf.math.abs_185_loss: 0.0299 - tf.math.abs_186_loss: 0.0322 - val_loss: 42.0792 - val_out_T_loss: 2.0562 - val_out_S_loss: 3.9218 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0124\n",
      "Epoch 93/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 42.4243 - out_T_loss: 1.8742 - out_S_loss: 3.9328 - tf.math.abs_185_loss: 0.0291 - tf.math.abs_186_loss: 0.0320 - val_loss: 41.8790 - val_out_T_loss: 2.0270 - val_out_S_loss: 3.9053 - val_tf.math.abs_185_loss: 0.0275 - val_tf.math.abs_186_loss: 0.0124\n",
      "Epoch 94/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 42.1527 - out_T_loss: 1.8628 - out_S_loss: 3.9055 - tf.math.abs_185_loss: 0.0298 - tf.math.abs_186_loss: 0.0319 - val_loss: 41.7040 - val_out_T_loss: 2.0142 - val_out_S_loss: 3.8871 - val_tf.math.abs_185_loss: 0.0284 - val_tf.math.abs_186_loss: 0.0125\n",
      "Epoch 95/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 42.3587 - out_T_loss: 1.8601 - out_S_loss: 3.9253 - tf.math.abs_185_loss: 0.0303 - tf.math.abs_186_loss: 0.0320 - val_loss: 41.4964 - val_out_T_loss: 1.9884 - val_out_S_loss: 3.8700 - val_tf.math.abs_185_loss: 0.0281 - val_tf.math.abs_186_loss: 0.0123\n",
      "Epoch 96/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 41.5454 - out_T_loss: 1.8061 - out_S_loss: 3.8518 - tf.math.abs_185_loss: 0.0297 - tf.math.abs_186_loss: 0.0313 - val_loss: 41.3231 - val_out_T_loss: 1.9653 - val_out_S_loss: 3.8532 - val_tf.math.abs_185_loss: 0.0287 - val_tf.math.abs_186_loss: 0.0126\n",
      "Epoch 97/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 41.6561 - out_T_loss: 1.8163 - out_S_loss: 3.8600 - tf.math.abs_185_loss: 0.0302 - tf.math.abs_186_loss: 0.0318 - val_loss: 41.1141 - val_out_T_loss: 1.9575 - val_out_S_loss: 3.8338 - val_tf.math.abs_185_loss: 0.0286 - val_tf.math.abs_186_loss: 0.0123\n",
      "Epoch 98/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 41.7812 - out_T_loss: 1.7956 - out_S_loss: 3.8739 - tf.math.abs_185_loss: 0.0304 - tf.math.abs_186_loss: 0.0319 - val_loss: 40.8969 - val_out_T_loss: 1.9319 - val_out_S_loss: 3.8153 - val_tf.math.abs_185_loss: 0.0285 - val_tf.math.abs_186_loss: 0.0121\n",
      "Epoch 99/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 41.6801 - out_T_loss: 1.8083 - out_S_loss: 3.8631 - tf.math.abs_185_loss: 0.0303 - tf.math.abs_186_loss: 0.0317 - val_loss: 40.7115 - val_out_T_loss: 1.9309 - val_out_S_loss: 3.7967 - val_tf.math.abs_185_loss: 0.0288 - val_tf.math.abs_186_loss: 0.0119\n",
      "Epoch 100/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 41.4659 - out_T_loss: 1.7837 - out_S_loss: 3.8443 - tf.math.abs_185_loss: 0.0303 - tf.math.abs_186_loss: 0.0316 - val_loss: 40.5028 - val_out_T_loss: 1.8985 - val_out_S_loss: 3.7801 - val_tf.math.abs_185_loss: 0.0285 - val_tf.math.abs_186_loss: 0.0117\n",
      "Epoch 101/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 41.1516 - out_T_loss: 1.7368 - out_S_loss: 3.8190 - tf.math.abs_185_loss: 0.0304 - tf.math.abs_186_loss: 0.0308 - val_loss: 40.3324 - val_out_T_loss: 1.8803 - val_out_S_loss: 3.7641 - val_tf.math.abs_185_loss: 0.0288 - val_tf.math.abs_186_loss: 0.0118\n",
      "Epoch 102/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 41.1089 - out_T_loss: 1.7395 - out_S_loss: 3.8130 - tf.math.abs_185_loss: 0.0306 - tf.math.abs_186_loss: 0.0314 - val_loss: 40.1946 - val_out_T_loss: 1.8877 - val_out_S_loss: 3.7484 - val_tf.math.abs_185_loss: 0.0294 - val_tf.math.abs_186_loss: 0.0118\n",
      "Epoch 103/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 40.7915 - out_T_loss: 1.7466 - out_S_loss: 3.7804 - tf.math.abs_185_loss: 0.0311 - tf.math.abs_186_loss: 0.0309 - val_loss: 39.9718 - val_out_T_loss: 1.8589 - val_out_S_loss: 3.7300 - val_tf.math.abs_185_loss: 0.0290 - val_tf.math.abs_186_loss: 0.0116\n",
      "Epoch 104/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 40.3292 - out_T_loss: 1.7011 - out_S_loss: 3.7404 - tf.math.abs_185_loss: 0.0302 - tf.math.abs_186_loss: 0.0310 - val_loss: 39.7856 - val_out_T_loss: 1.8189 - val_out_S_loss: 3.7148 - val_tf.math.abs_185_loss: 0.0290 - val_tf.math.abs_186_loss: 0.0119\n",
      "Epoch 105/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 40.4487 - out_T_loss: 1.7449 - out_S_loss: 3.7460 - tf.math.abs_185_loss: 0.0316 - tf.math.abs_186_loss: 0.0306 - val_loss: 39.6088 - val_out_T_loss: 1.8160 - val_out_S_loss: 3.6973 - val_tf.math.abs_185_loss: 0.0294 - val_tf.math.abs_186_loss: 0.0116\n",
      "Epoch 106/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 40.1387 - out_T_loss: 1.6909 - out_S_loss: 3.7214 - tf.math.abs_185_loss: 0.0313 - tf.math.abs_186_loss: 0.0304 - val_loss: 39.4598 - val_out_T_loss: 1.8149 - val_out_S_loss: 3.6813 - val_tf.math.abs_185_loss: 0.0300 - val_tf.math.abs_186_loss: 0.0116\n",
      "Epoch 107/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 40.0989 - out_T_loss: 1.6717 - out_S_loss: 3.7185 - tf.math.abs_185_loss: 0.0311 - tf.math.abs_186_loss: 0.0310 - val_loss: 39.2416 - val_out_T_loss: 1.7968 - val_out_S_loss: 3.6630 - val_tf.math.abs_185_loss: 0.0295 - val_tf.math.abs_186_loss: 0.0112\n",
      "Epoch 108/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 40.1578 - out_T_loss: 1.6684 - out_S_loss: 3.7253 - tf.math.abs_185_loss: 0.0312 - tf.math.abs_186_loss: 0.0307 - val_loss: 39.0633 - val_out_T_loss: 1.7700 - val_out_S_loss: 3.6477 - val_tf.math.abs_185_loss: 0.0294 - val_tf.math.abs_186_loss: 0.0114\n",
      "Epoch 109/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 40.1414 - out_T_loss: 1.6442 - out_S_loss: 3.7252 - tf.math.abs_185_loss: 0.0320 - tf.math.abs_186_loss: 0.0302 - val_loss: 38.8792 - val_out_T_loss: 1.7560 - val_out_S_loss: 3.6299 - val_tf.math.abs_185_loss: 0.0299 - val_tf.math.abs_186_loss: 0.0113\n",
      "Epoch 110/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 39.6096 - out_T_loss: 1.6243 - out_S_loss: 3.6753 - tf.math.abs_185_loss: 0.0313 - tf.math.abs_186_loss: 0.0303 - val_loss: 38.7266 - val_out_T_loss: 1.7424 - val_out_S_loss: 3.6164 - val_tf.math.abs_185_loss: 0.0298 - val_tf.math.abs_186_loss: 0.0112\n",
      "Epoch 111/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 39.8803 - out_T_loss: 1.5959 - out_S_loss: 3.7051 - tf.math.abs_185_loss: 0.0315 - tf.math.abs_186_loss: 0.0302 - val_loss: 38.5697 - val_out_T_loss: 1.7366 - val_out_S_loss: 3.6014 - val_tf.math.abs_185_loss: 0.0299 - val_tf.math.abs_186_loss: 0.0110\n",
      "Epoch 112/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 39.2560 - out_T_loss: 1.6094 - out_S_loss: 3.6420 - tf.math.abs_185_loss: 0.0319 - tf.math.abs_186_loss: 0.0294 - val_loss: 38.3635 - val_out_T_loss: 1.7156 - val_out_S_loss: 3.5831 - val_tf.math.abs_185_loss: 0.0299 - val_tf.math.abs_186_loss: 0.0110\n",
      "Epoch 113/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 39.5343 - out_T_loss: 1.5841 - out_S_loss: 3.6707 - tf.math.abs_185_loss: 0.0319 - tf.math.abs_186_loss: 0.0302 - val_loss: 38.2139 - val_out_T_loss: 1.6972 - val_out_S_loss: 3.5683 - val_tf.math.abs_185_loss: 0.0306 - val_tf.math.abs_186_loss: 0.0111\n",
      "Epoch 114/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 39.2148 - out_T_loss: 1.5522 - out_S_loss: 3.6416 - tf.math.abs_185_loss: 0.0322 - tf.math.abs_186_loss: 0.0301 - val_loss: 38.0474 - val_out_T_loss: 1.6887 - val_out_S_loss: 3.5533 - val_tf.math.abs_185_loss: 0.0302 - val_tf.math.abs_186_loss: 0.0111\n",
      "Epoch 115/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 38.8650 - out_T_loss: 1.5710 - out_S_loss: 3.6063 - tf.math.abs_185_loss: 0.0317 - tf.math.abs_186_loss: 0.0299 - val_loss: 37.8496 - val_out_T_loss: 1.6581 - val_out_S_loss: 3.5385 - val_tf.math.abs_185_loss: 0.0296 - val_tf.math.abs_186_loss: 0.0108\n",
      "Epoch 116/2000\n",
      "24/24 [==============================] - 1s 30ms/step - loss: 38.7879 - out_T_loss: 1.5672 - out_S_loss: 3.5981 - tf.math.abs_185_loss: 0.0323 - tf.math.abs_186_loss: 0.0297 - val_loss: 37.7137 - val_out_T_loss: 1.6487 - val_out_S_loss: 3.5251 - val_tf.math.abs_185_loss: 0.0299 - val_tf.math.abs_186_loss: 0.0108\n",
      "Epoch 117/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 38.6080 - out_T_loss: 1.5304 - out_S_loss: 3.5842 - tf.math.abs_185_loss: 0.0323 - tf.math.abs_186_loss: 0.0295 - val_loss: 37.5610 - val_out_T_loss: 1.6211 - val_out_S_loss: 3.5107 - val_tf.math.abs_185_loss: 0.0309 - val_tf.math.abs_186_loss: 0.0108\n",
      "Epoch 118/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 38.5610 - out_T_loss: 1.5262 - out_S_loss: 3.5810 - tf.math.abs_185_loss: 0.0321 - tf.math.abs_186_loss: 0.0292 - val_loss: 37.4052 - val_out_T_loss: 1.6151 - val_out_S_loss: 3.4957 - val_tf.math.abs_185_loss: 0.0308 - val_tf.math.abs_186_loss: 0.0109\n",
      "Epoch 119/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 38.3713 - out_T_loss: 1.5327 - out_S_loss: 3.5607 - tf.math.abs_185_loss: 0.0328 - tf.math.abs_186_loss: 0.0288 - val_loss: 37.2676 - val_out_T_loss: 1.6200 - val_out_S_loss: 3.4819 - val_tf.math.abs_185_loss: 0.0309 - val_tf.math.abs_186_loss: 0.0105\n",
      "Epoch 120/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.9245 - out_T_loss: 1.4910 - out_S_loss: 3.5214 - tf.math.abs_185_loss: 0.0322 - tf.math.abs_186_loss: 0.0288 - val_loss: 37.1031 - val_out_T_loss: 1.5921 - val_out_S_loss: 3.4686 - val_tf.math.abs_185_loss: 0.0306 - val_tf.math.abs_186_loss: 0.0107\n",
      "Epoch 121/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 38.1538 - out_T_loss: 1.4788 - out_S_loss: 3.5455 - tf.math.abs_185_loss: 0.0321 - tf.math.abs_186_loss: 0.0289 - val_loss: 36.9301 - val_out_T_loss: 1.5806 - val_out_S_loss: 3.4525 - val_tf.math.abs_185_loss: 0.0307 - val_tf.math.abs_186_loss: 0.0106\n",
      "Epoch 122/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 38.3059 - out_T_loss: 1.4589 - out_S_loss: 3.5618 - tf.math.abs_185_loss: 0.0324 - tf.math.abs_186_loss: 0.0290 - val_loss: 36.7728 - val_out_T_loss: 1.5653 - val_out_S_loss: 3.4396 - val_tf.math.abs_185_loss: 0.0304 - val_tf.math.abs_186_loss: 0.0102\n",
      "Epoch 123/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.8774 - out_T_loss: 1.4625 - out_S_loss: 3.5185 - tf.math.abs_185_loss: 0.0332 - tf.math.abs_186_loss: 0.0283 - val_loss: 36.6022 - val_out_T_loss: 1.5344 - val_out_S_loss: 3.4243 - val_tf.math.abs_185_loss: 0.0309 - val_tf.math.abs_186_loss: 0.0104\n",
      "Epoch 124/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.7616 - out_T_loss: 1.4569 - out_S_loss: 3.5078 - tf.math.abs_185_loss: 0.0328 - tf.math.abs_186_loss: 0.0285 - val_loss: 36.4831 - val_out_T_loss: 1.5297 - val_out_S_loss: 3.4127 - val_tf.math.abs_185_loss: 0.0310 - val_tf.math.abs_186_loss: 0.0103\n",
      "Epoch 125/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 37.3634 - out_T_loss: 1.4758 - out_S_loss: 3.4645 - tf.math.abs_185_loss: 0.0337 - tf.math.abs_186_loss: 0.0285 - val_loss: 36.3419 - val_out_T_loss: 1.5330 - val_out_S_loss: 3.3979 - val_tf.math.abs_185_loss: 0.0313 - val_tf.math.abs_186_loss: 0.0102\n",
      "Epoch 126/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.6016 - out_T_loss: 1.4409 - out_S_loss: 3.4944 - tf.math.abs_185_loss: 0.0330 - tf.math.abs_186_loss: 0.0278 - val_loss: 36.1925 - val_out_T_loss: 1.5148 - val_out_S_loss: 3.3844 - val_tf.math.abs_185_loss: 0.0313 - val_tf.math.abs_186_loss: 0.0103\n",
      "Epoch 127/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.4932 - out_T_loss: 1.4277 - out_S_loss: 3.4842 - tf.math.abs_185_loss: 0.0328 - tf.math.abs_186_loss: 0.0284 - val_loss: 36.0610 - val_out_T_loss: 1.5302 - val_out_S_loss: 3.3704 - val_tf.math.abs_185_loss: 0.0312 - val_tf.math.abs_186_loss: 0.0102\n",
      "Epoch 128/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.2995 - out_T_loss: 1.4234 - out_S_loss: 3.4659 - tf.math.abs_185_loss: 0.0327 - tf.math.abs_186_loss: 0.0281 - val_loss: 35.8909 - val_out_T_loss: 1.4951 - val_out_S_loss: 3.3565 - val_tf.math.abs_185_loss: 0.0317 - val_tf.math.abs_186_loss: 0.0098\n",
      "Epoch 129/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.3688 - out_T_loss: 1.4065 - out_S_loss: 3.4745 - tf.math.abs_185_loss: 0.0333 - tf.math.abs_186_loss: 0.0276 - val_loss: 35.7355 - val_out_T_loss: 1.4845 - val_out_S_loss: 3.3427 - val_tf.math.abs_185_loss: 0.0315 - val_tf.math.abs_186_loss: 0.0097\n",
      "Epoch 130/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 37.1131 - out_T_loss: 1.3833 - out_S_loss: 3.4507 - tf.math.abs_185_loss: 0.0333 - tf.math.abs_186_loss: 0.0279 - val_loss: 35.6045 - val_out_T_loss: 1.4704 - val_out_S_loss: 3.3301 - val_tf.math.abs_185_loss: 0.0319 - val_tf.math.abs_186_loss: 0.0098\n",
      "Epoch 131/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.2564 - out_T_loss: 1.3755 - out_S_loss: 3.4652 - tf.math.abs_185_loss: 0.0334 - tf.math.abs_186_loss: 0.0280 - val_loss: 35.4249 - val_out_T_loss: 1.4619 - val_out_S_loss: 3.3139 - val_tf.math.abs_185_loss: 0.0316 - val_tf.math.abs_186_loss: 0.0096\n",
      "Epoch 132/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 37.1595 - out_T_loss: 1.3798 - out_S_loss: 3.4568 - tf.math.abs_185_loss: 0.0332 - tf.math.abs_186_loss: 0.0274 - val_loss: 35.3130 - val_out_T_loss: 1.4525 - val_out_S_loss: 3.3019 - val_tf.math.abs_185_loss: 0.0324 - val_tf.math.abs_186_loss: 0.0097\n",
      "Epoch 133/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 36.8875 - out_T_loss: 1.3848 - out_S_loss: 3.4262 - tf.math.abs_185_loss: 0.0341 - tf.math.abs_186_loss: 0.0279 - val_loss: 35.1587 - val_out_T_loss: 1.4417 - val_out_S_loss: 3.2889 - val_tf.math.abs_185_loss: 0.0318 - val_tf.math.abs_186_loss: 0.0096\n",
      "Epoch 134/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 36.1186 - out_T_loss: 1.3504 - out_S_loss: 3.3545 - tf.math.abs_185_loss: 0.0336 - tf.math.abs_186_loss: 0.0275 - val_loss: 35.0242 - val_out_T_loss: 1.4134 - val_out_S_loss: 3.2770 - val_tf.math.abs_185_loss: 0.0324 - val_tf.math.abs_186_loss: 0.0096\n",
      "Epoch 135/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 36.4630 - out_T_loss: 1.3161 - out_S_loss: 3.3933 - tf.math.abs_185_loss: 0.0333 - tf.math.abs_186_loss: 0.0273 - val_loss: 34.8506 - val_out_T_loss: 1.3990 - val_out_S_loss: 3.2627 - val_tf.math.abs_185_loss: 0.0317 - val_tf.math.abs_186_loss: 0.0095\n",
      "Epoch 136/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 36.1631 - out_T_loss: 1.3051 - out_S_loss: 3.3646 - tf.math.abs_185_loss: 0.0338 - tf.math.abs_186_loss: 0.0269 - val_loss: 34.7479 - val_out_T_loss: 1.4096 - val_out_S_loss: 3.2504 - val_tf.math.abs_185_loss: 0.0322 - val_tf.math.abs_186_loss: 0.0095\n",
      "Epoch 137/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 36.1776 - out_T_loss: 1.3131 - out_S_loss: 3.3645 - tf.math.abs_185_loss: 0.0344 - tf.math.abs_186_loss: 0.0265 - val_loss: 34.6438 - val_out_T_loss: 1.3946 - val_out_S_loss: 3.2411 - val_tf.math.abs_185_loss: 0.0325 - val_tf.math.abs_186_loss: 0.0094\n",
      "Epoch 138/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 36.1277 - out_T_loss: 1.3205 - out_S_loss: 3.3561 - tf.math.abs_185_loss: 0.0347 - tf.math.abs_186_loss: 0.0276 - val_loss: 34.4836 - val_out_T_loss: 1.3778 - val_out_S_loss: 3.2269 - val_tf.math.abs_185_loss: 0.0324 - val_tf.math.abs_186_loss: 0.0095\n",
      "Epoch 139/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 35.9912 - out_T_loss: 1.2977 - out_S_loss: 3.3478 - tf.math.abs_185_loss: 0.0338 - tf.math.abs_186_loss: 0.0270 - val_loss: 34.3769 - val_out_T_loss: 1.3701 - val_out_S_loss: 3.2149 - val_tf.math.abs_185_loss: 0.0332 - val_tf.math.abs_186_loss: 0.0096\n",
      "Epoch 140/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 35.8080 - out_T_loss: 1.2747 - out_S_loss: 3.3302 - tf.math.abs_185_loss: 0.0346 - tf.math.abs_186_loss: 0.0270 - val_loss: 34.2995 - val_out_T_loss: 1.3855 - val_out_S_loss: 3.2036 - val_tf.math.abs_185_loss: 0.0342 - val_tf.math.abs_186_loss: 0.0097\n",
      "Epoch 141/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 35.8503 - out_T_loss: 1.2899 - out_S_loss: 3.3338 - tf.math.abs_185_loss: 0.0347 - tf.math.abs_186_loss: 0.0265 - val_loss: 34.1258 - val_out_T_loss: 1.3745 - val_out_S_loss: 3.1905 - val_tf.math.abs_185_loss: 0.0331 - val_tf.math.abs_186_loss: 0.0093\n",
      "Epoch 142/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 35.4531 - out_T_loss: 1.2493 - out_S_loss: 3.2984 - tf.math.abs_185_loss: 0.0341 - tf.math.abs_186_loss: 0.0269 - val_loss: 33.9508 - val_out_T_loss: 1.3348 - val_out_S_loss: 3.1769 - val_tf.math.abs_185_loss: 0.0332 - val_tf.math.abs_186_loss: 0.0092\n",
      "Epoch 143/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 35.4647 - out_T_loss: 1.2393 - out_S_loss: 3.3006 - tf.math.abs_185_loss: 0.0345 - tf.math.abs_186_loss: 0.0264 - val_loss: 33.7858 - val_out_T_loss: 1.3038 - val_out_S_loss: 3.1639 - val_tf.math.abs_185_loss: 0.0329 - val_tf.math.abs_186_loss: 0.0092\n",
      "Epoch 144/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 35.4795 - out_T_loss: 1.2586 - out_S_loss: 3.2984 - tf.math.abs_185_loss: 0.0350 - tf.math.abs_186_loss: 0.0269 - val_loss: 33.6654 - val_out_T_loss: 1.3075 - val_out_S_loss: 3.1512 - val_tf.math.abs_185_loss: 0.0333 - val_tf.math.abs_186_loss: 0.0090\n",
      "Epoch 145/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 34.9655 - out_T_loss: 1.2139 - out_S_loss: 3.2533 - tf.math.abs_185_loss: 0.0345 - tf.math.abs_186_loss: 0.0264 - val_loss: 33.5316 - val_out_T_loss: 1.2972 - val_out_S_loss: 3.1398 - val_tf.math.abs_185_loss: 0.0326 - val_tf.math.abs_186_loss: 0.0092\n",
      "Epoch 146/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 35.3578 - out_T_loss: 1.2723 - out_S_loss: 3.2845 - tf.math.abs_185_loss: 0.0353 - tf.math.abs_186_loss: 0.0267 - val_loss: 33.4539 - val_out_T_loss: 1.3123 - val_out_S_loss: 3.1289 - val_tf.math.abs_185_loss: 0.0334 - val_tf.math.abs_186_loss: 0.0092\n",
      "Epoch 147/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 34.9962 - out_T_loss: 1.2310 - out_S_loss: 3.2538 - tf.math.abs_185_loss: 0.0348 - tf.math.abs_186_loss: 0.0266 - val_loss: 33.3233 - val_out_T_loss: 1.2776 - val_out_S_loss: 3.1198 - val_tf.math.abs_185_loss: 0.0332 - val_tf.math.abs_186_loss: 0.0092\n",
      "Epoch 148/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 34.9183 - out_T_loss: 1.2397 - out_S_loss: 3.2437 - tf.math.abs_185_loss: 0.0361 - tf.math.abs_186_loss: 0.0260 - val_loss: 33.2403 - val_out_T_loss: 1.2896 - val_out_S_loss: 3.1087 - val_tf.math.abs_185_loss: 0.0343 - val_tf.math.abs_186_loss: 0.0089\n",
      "Epoch 149/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 34.9313 - out_T_loss: 1.1980 - out_S_loss: 3.2500 - tf.math.abs_185_loss: 0.0358 - tf.math.abs_186_loss: 0.0259 - val_loss: 33.0917 - val_out_T_loss: 1.2665 - val_out_S_loss: 3.0964 - val_tf.math.abs_185_loss: 0.0340 - val_tf.math.abs_186_loss: 0.0091\n",
      "Epoch 150/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 34.7849 - out_T_loss: 1.2029 - out_S_loss: 3.2341 - tf.math.abs_185_loss: 0.0361 - tf.math.abs_186_loss: 0.0259 - val_loss: 32.9702 - val_out_T_loss: 1.2545 - val_out_S_loss: 3.0861 - val_tf.math.abs_185_loss: 0.0339 - val_tf.math.abs_186_loss: 0.0088\n",
      "Epoch 151/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 34.9003 - out_T_loss: 1.1926 - out_S_loss: 3.2482 - tf.math.abs_185_loss: 0.0354 - tf.math.abs_186_loss: 0.0259 - val_loss: 32.8518 - val_out_T_loss: 1.2580 - val_out_S_loss: 3.0743 - val_tf.math.abs_185_loss: 0.0338 - val_tf.math.abs_186_loss: 0.0087\n",
      "Epoch 152/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 34.4195 - out_T_loss: 1.1696 - out_S_loss: 3.2028 - tf.math.abs_185_loss: 0.0355 - tf.math.abs_186_loss: 0.0257 - val_loss: 32.7455 - val_out_T_loss: 1.2479 - val_out_S_loss: 3.0645 - val_tf.math.abs_185_loss: 0.0341 - val_tf.math.abs_186_loss: 0.0086\n",
      "Epoch 153/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 34.5507 - out_T_loss: 1.1907 - out_S_loss: 3.2138 - tf.math.abs_185_loss: 0.0359 - tf.math.abs_186_loss: 0.0252 - val_loss: 32.6120 - val_out_T_loss: 1.2305 - val_out_S_loss: 3.0528 - val_tf.math.abs_185_loss: 0.0341 - val_tf.math.abs_186_loss: 0.0086\n",
      "Epoch 154/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 34.7251 - out_T_loss: 1.1133 - out_S_loss: 3.2389 - tf.math.abs_185_loss: 0.0351 - tf.math.abs_186_loss: 0.0261 - val_loss: 32.5114 - val_out_T_loss: 1.2302 - val_out_S_loss: 3.0428 - val_tf.math.abs_185_loss: 0.0340 - val_tf.math.abs_186_loss: 0.0087\n",
      "Epoch 155/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 34.3001 - out_T_loss: 1.1432 - out_S_loss: 3.1937 - tf.math.abs_185_loss: 0.0356 - tf.math.abs_186_loss: 0.0253 - val_loss: 32.3945 - val_out_T_loss: 1.2113 - val_out_S_loss: 3.0333 - val_tf.math.abs_185_loss: 0.0338 - val_tf.math.abs_186_loss: 0.0087\n",
      "Epoch 156/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 33.8929 - out_T_loss: 1.1314 - out_S_loss: 3.1557 - tf.math.abs_185_loss: 0.0350 - tf.math.abs_186_loss: 0.0252 - val_loss: 32.2546 - val_out_T_loss: 1.1986 - val_out_S_loss: 3.0216 - val_tf.math.abs_185_loss: 0.0335 - val_tf.math.abs_186_loss: 0.0085\n",
      "Epoch 157/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 34.2463 - out_T_loss: 1.1399 - out_S_loss: 3.1873 - tf.math.abs_185_loss: 0.0362 - tf.math.abs_186_loss: 0.0255 - val_loss: 32.1969 - val_out_T_loss: 1.2205 - val_out_S_loss: 3.0110 - val_tf.math.abs_185_loss: 0.0347 - val_tf.math.abs_186_loss: 0.0087\n",
      "Epoch 158/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 33.9214 - out_T_loss: 1.1233 - out_S_loss: 3.1564 - tf.math.abs_185_loss: 0.0360 - tf.math.abs_186_loss: 0.0257 - val_loss: 32.0486 - val_out_T_loss: 1.1765 - val_out_S_loss: 3.0018 - val_tf.math.abs_185_loss: 0.0342 - val_tf.math.abs_186_loss: 0.0085\n",
      "Epoch 159/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 33.6947 - out_T_loss: 1.1267 - out_S_loss: 3.1337 - tf.math.abs_185_loss: 0.0363 - tf.math.abs_186_loss: 0.0252 - val_loss: 31.9707 - val_out_T_loss: 1.1689 - val_out_S_loss: 2.9934 - val_tf.math.abs_185_loss: 0.0350 - val_tf.math.abs_186_loss: 0.0084\n",
      "Epoch 160/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 33.8216 - out_T_loss: 1.1245 - out_S_loss: 3.1451 - tf.math.abs_185_loss: 0.0374 - tf.math.abs_186_loss: 0.0249 - val_loss: 31.7962 - val_out_T_loss: 1.1573 - val_out_S_loss: 2.9791 - val_tf.math.abs_185_loss: 0.0340 - val_tf.math.abs_186_loss: 0.0084\n",
      "Epoch 161/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 33.6775 - out_T_loss: 1.1080 - out_S_loss: 3.1345 - tf.math.abs_185_loss: 0.0358 - tf.math.abs_186_loss: 0.0254 - val_loss: 31.6929 - val_out_T_loss: 1.1614 - val_out_S_loss: 2.9680 - val_tf.math.abs_185_loss: 0.0343 - val_tf.math.abs_186_loss: 0.0083\n",
      "Epoch 162/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 33.4144 - out_T_loss: 1.0731 - out_S_loss: 3.1130 - tf.math.abs_185_loss: 0.0363 - tf.math.abs_186_loss: 0.0243 - val_loss: 31.5982 - val_out_T_loss: 1.1621 - val_out_S_loss: 2.9575 - val_tf.math.abs_185_loss: 0.0348 - val_tf.math.abs_186_loss: 0.0083\n",
      "Epoch 163/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 33.8654 - out_T_loss: 1.0536 - out_S_loss: 3.1591 - tf.math.abs_185_loss: 0.0360 - tf.math.abs_186_loss: 0.0250 - val_loss: 31.4843 - val_out_T_loss: 1.1293 - val_out_S_loss: 2.9508 - val_tf.math.abs_185_loss: 0.0341 - val_tf.math.abs_186_loss: 0.0082\n",
      "Epoch 164/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 33.5695 - out_T_loss: 1.0610 - out_S_loss: 3.1287 - tf.math.abs_185_loss: 0.0362 - tf.math.abs_186_loss: 0.0248 - val_loss: 31.3723 - val_out_T_loss: 1.1381 - val_out_S_loss: 2.9381 - val_tf.math.abs_185_loss: 0.0346 - val_tf.math.abs_186_loss: 0.0081\n",
      "Epoch 165/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 33.4034 - out_T_loss: 1.0536 - out_S_loss: 3.1125 - tf.math.abs_185_loss: 0.0361 - tf.math.abs_186_loss: 0.0251 - val_loss: 31.2486 - val_out_T_loss: 1.1139 - val_out_S_loss: 2.9282 - val_tf.math.abs_185_loss: 0.0345 - val_tf.math.abs_186_loss: 0.0081\n",
      "Epoch 166/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 33.6851 - out_T_loss: 1.0684 - out_S_loss: 3.1390 - tf.math.abs_185_loss: 0.0366 - tf.math.abs_186_loss: 0.0248 - val_loss: 31.1616 - val_out_T_loss: 1.1150 - val_out_S_loss: 2.9197 - val_tf.math.abs_185_loss: 0.0342 - val_tf.math.abs_186_loss: 0.0082\n",
      "Epoch 167/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 32.8887 - out_T_loss: 1.0232 - out_S_loss: 3.0654 - tf.math.abs_185_loss: 0.0365 - tf.math.abs_186_loss: 0.0241 - val_loss: 31.0390 - val_out_T_loss: 1.0909 - val_out_S_loss: 2.9102 - val_tf.math.abs_185_loss: 0.0343 - val_tf.math.abs_186_loss: 0.0080\n",
      "Epoch 168/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 32.9629 - out_T_loss: 1.0606 - out_S_loss: 3.0680 - tf.math.abs_185_loss: 0.0370 - tf.math.abs_186_loss: 0.0241 - val_loss: 30.9608 - val_out_T_loss: 1.0940 - val_out_S_loss: 2.8996 - val_tf.math.abs_185_loss: 0.0352 - val_tf.math.abs_186_loss: 0.0083\n",
      "Epoch 169/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 33.0011 - out_T_loss: 1.0547 - out_S_loss: 3.0706 - tf.math.abs_185_loss: 0.0378 - tf.math.abs_186_loss: 0.0242 - val_loss: 30.8481 - val_out_T_loss: 1.0811 - val_out_S_loss: 2.8902 - val_tf.math.abs_185_loss: 0.0352 - val_tf.math.abs_186_loss: 0.0081\n",
      "Epoch 170/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 33.0036 - out_T_loss: 1.0242 - out_S_loss: 3.0760 - tf.math.abs_185_loss: 0.0370 - tf.math.abs_186_loss: 0.0240 - val_loss: 30.7313 - val_out_T_loss: 1.0787 - val_out_S_loss: 2.8794 - val_tf.math.abs_185_loss: 0.0350 - val_tf.math.abs_186_loss: 0.0079\n",
      "Epoch 171/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 32.8062 - out_T_loss: 1.0533 - out_S_loss: 3.0502 - tf.math.abs_185_loss: 0.0380 - tf.math.abs_186_loss: 0.0245 - val_loss: 30.6531 - val_out_T_loss: 1.0740 - val_out_S_loss: 2.8713 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0080\n",
      "Epoch 172/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 32.8586 - out_T_loss: 1.0666 - out_S_loss: 3.0543 - tf.math.abs_185_loss: 0.0380 - tf.math.abs_186_loss: 0.0244 - val_loss: 30.5609 - val_out_T_loss: 1.0687 - val_out_S_loss: 2.8627 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0080\n",
      "Epoch 173/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 32.9591 - out_T_loss: 1.0437 - out_S_loss: 3.0683 - tf.math.abs_185_loss: 0.0377 - tf.math.abs_186_loss: 0.0239 - val_loss: 30.4487 - val_out_T_loss: 1.0677 - val_out_S_loss: 2.8514 - val_tf.math.abs_185_loss: 0.0356 - val_tf.math.abs_186_loss: 0.0078\n",
      "Epoch 174/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 32.1306 - out_T_loss: 0.9841 - out_S_loss: 2.9926 - tf.math.abs_185_loss: 0.0372 - tf.math.abs_186_loss: 0.0238 - val_loss: 30.3577 - val_out_T_loss: 1.0672 - val_out_S_loss: 2.8403 - val_tf.math.abs_185_loss: 0.0364 - val_tf.math.abs_186_loss: 0.0080\n",
      "Epoch 175/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 32.3213 - out_T_loss: 1.0215 - out_S_loss: 3.0083 - tf.math.abs_185_loss: 0.0374 - tf.math.abs_186_loss: 0.0235 - val_loss: 30.2060 - val_out_T_loss: 1.0490 - val_out_S_loss: 2.8302 - val_tf.math.abs_185_loss: 0.0352 - val_tf.math.abs_186_loss: 0.0076\n",
      "Epoch 176/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.9931 - out_T_loss: 0.9730 - out_S_loss: 2.9800 - tf.math.abs_185_loss: 0.0377 - tf.math.abs_186_loss: 0.0233 - val_loss: 30.1421 - val_out_T_loss: 1.0294 - val_out_S_loss: 2.8222 - val_tf.math.abs_185_loss: 0.0365 - val_tf.math.abs_186_loss: 0.0080\n",
      "Epoch 177/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 32.2032 - out_T_loss: 1.0002 - out_S_loss: 2.9974 - tf.math.abs_185_loss: 0.0379 - tf.math.abs_186_loss: 0.0236 - val_loss: 30.0487 - val_out_T_loss: 1.0456 - val_out_S_loss: 2.8120 - val_tf.math.abs_185_loss: 0.0363 - val_tf.math.abs_186_loss: 0.0079\n",
      "Epoch 178/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 31.9253 - out_T_loss: 0.9999 - out_S_loss: 2.9696 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0231 - val_loss: 29.9415 - val_out_T_loss: 1.0208 - val_out_S_loss: 2.8045 - val_tf.math.abs_185_loss: 0.0360 - val_tf.math.abs_186_loss: 0.0077\n",
      "Epoch 179/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 31.7233 - out_T_loss: 0.9730 - out_S_loss: 2.9544 - tf.math.abs_185_loss: 0.0371 - tf.math.abs_186_loss: 0.0232 - val_loss: 29.8332 - val_out_T_loss: 1.0236 - val_out_S_loss: 2.7951 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0076\n",
      "Epoch 180/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 32.0322 - out_T_loss: 0.9987 - out_S_loss: 2.9813 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0234 - val_loss: 29.7320 - val_out_T_loss: 1.0044 - val_out_S_loss: 2.7869 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0076\n",
      "Epoch 181/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.9287 - out_T_loss: 0.9565 - out_S_loss: 2.9748 - tf.math.abs_185_loss: 0.0375 - tf.math.abs_186_loss: 0.0237 - val_loss: 29.6433 - val_out_T_loss: 1.0037 - val_out_S_loss: 2.7780 - val_tf.math.abs_185_loss: 0.0354 - val_tf.math.abs_186_loss: 0.0076\n",
      "Epoch 182/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 32.1211 - out_T_loss: 0.9483 - out_S_loss: 2.9943 - tf.math.abs_185_loss: 0.0383 - tf.math.abs_186_loss: 0.0232 - val_loss: 29.5372 - val_out_T_loss: 0.9983 - val_out_S_loss: 2.7674 - val_tf.math.abs_185_loss: 0.0357 - val_tf.math.abs_186_loss: 0.0075\n",
      "Epoch 183/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.8246 - out_T_loss: 0.9736 - out_S_loss: 2.9650 - tf.math.abs_185_loss: 0.0370 - tf.math.abs_186_loss: 0.0231 - val_loss: 29.4383 - val_out_T_loss: 0.9915 - val_out_S_loss: 2.7577 - val_tf.math.abs_185_loss: 0.0360 - val_tf.math.abs_186_loss: 0.0074\n",
      "Epoch 184/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.4163 - out_T_loss: 0.9351 - out_S_loss: 2.9269 - tf.math.abs_185_loss: 0.0379 - tf.math.abs_186_loss: 0.0228 - val_loss: 29.3930 - val_out_T_loss: 0.9835 - val_out_S_loss: 2.7539 - val_tf.math.abs_185_loss: 0.0361 - val_tf.math.abs_186_loss: 0.0074\n",
      "Epoch 185/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.5586 - out_T_loss: 0.9500 - out_S_loss: 2.9364 - tf.math.abs_185_loss: 0.0394 - tf.math.abs_186_loss: 0.0228 - val_loss: 29.3150 - val_out_T_loss: 0.9960 - val_out_S_loss: 2.7439 - val_tf.math.abs_185_loss: 0.0364 - val_tf.math.abs_186_loss: 0.0076\n",
      "Epoch 186/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.3546 - out_T_loss: 0.9462 - out_S_loss: 2.9173 - tf.math.abs_185_loss: 0.0386 - tf.math.abs_186_loss: 0.0232 - val_loss: 29.2223 - val_out_T_loss: 0.9723 - val_out_S_loss: 2.7361 - val_tf.math.abs_185_loss: 0.0368 - val_tf.math.abs_186_loss: 0.0076\n",
      "Epoch 187/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.4849 - out_T_loss: 0.9361 - out_S_loss: 2.9321 - tf.math.abs_185_loss: 0.0387 - tf.math.abs_186_loss: 0.0227 - val_loss: 29.1093 - val_out_T_loss: 0.9672 - val_out_S_loss: 2.7257 - val_tf.math.abs_185_loss: 0.0367 - val_tf.math.abs_186_loss: 0.0076\n",
      "Epoch 188/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.6389 - out_T_loss: 0.9202 - out_S_loss: 2.9481 - tf.math.abs_185_loss: 0.0392 - tf.math.abs_186_loss: 0.0227 - val_loss: 29.0704 - val_out_T_loss: 0.9819 - val_out_S_loss: 2.7175 - val_tf.math.abs_185_loss: 0.0381 - val_tf.math.abs_186_loss: 0.0076\n",
      "Epoch 189/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.1228 - out_T_loss: 0.8817 - out_S_loss: 2.9010 - tf.math.abs_185_loss: 0.0392 - tf.math.abs_186_loss: 0.0223 - val_loss: 28.9292 - val_out_T_loss: 0.9483 - val_out_S_loss: 2.7105 - val_tf.math.abs_185_loss: 0.0365 - val_tf.math.abs_186_loss: 0.0073\n",
      "Epoch 190/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 31.1856 - out_T_loss: 0.8980 - out_S_loss: 2.9062 - tf.math.abs_185_loss: 0.0387 - tf.math.abs_186_loss: 0.0226 - val_loss: 28.8149 - val_out_T_loss: 0.9370 - val_out_S_loss: 2.7009 - val_tf.math.abs_185_loss: 0.0361 - val_tf.math.abs_186_loss: 0.0074\n",
      "Epoch 191/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 31.4017 - out_T_loss: 0.9433 - out_S_loss: 2.9227 - tf.math.abs_185_loss: 0.0390 - tf.math.abs_186_loss: 0.0225 - val_loss: 28.8433 - val_out_T_loss: 0.9905 - val_out_S_loss: 2.6919 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0078\n",
      "Epoch 192/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.8035 - out_T_loss: 0.8862 - out_S_loss: 2.8703 - tf.math.abs_185_loss: 0.0383 - tf.math.abs_186_loss: 0.0224 - val_loss: 28.6382 - val_out_T_loss: 0.9281 - val_out_S_loss: 2.6843 - val_tf.math.abs_185_loss: 0.0362 - val_tf.math.abs_186_loss: 0.0071\n",
      "Epoch 193/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.5364 - out_T_loss: 0.9220 - out_S_loss: 2.8386 - tf.math.abs_185_loss: 0.0390 - tf.math.abs_186_loss: 0.0224 - val_loss: 28.5693 - val_out_T_loss: 0.9212 - val_out_S_loss: 2.6770 - val_tf.math.abs_185_loss: 0.0366 - val_tf.math.abs_186_loss: 0.0073\n",
      "Epoch 194/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.7372 - out_T_loss: 0.8963 - out_S_loss: 2.8618 - tf.math.abs_185_loss: 0.0393 - tf.math.abs_186_loss: 0.0218 - val_loss: 28.4959 - val_out_T_loss: 0.9251 - val_out_S_loss: 2.6687 - val_tf.math.abs_185_loss: 0.0370 - val_tf.math.abs_186_loss: 0.0072\n",
      "Epoch 195/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.9912 - out_T_loss: 0.8670 - out_S_loss: 2.8896 - tf.math.abs_185_loss: 0.0390 - tf.math.abs_186_loss: 0.0224 - val_loss: 28.4030 - val_out_T_loss: 0.9282 - val_out_S_loss: 2.6593 - val_tf.math.abs_185_loss: 0.0369 - val_tf.math.abs_186_loss: 0.0072\n",
      "Epoch 196/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 30.5104 - out_T_loss: 0.8601 - out_S_loss: 2.8423 - tf.math.abs_185_loss: 0.0391 - tf.math.abs_186_loss: 0.0223 - val_loss: 28.3603 - val_out_T_loss: 0.9255 - val_out_S_loss: 2.6530 - val_tf.math.abs_185_loss: 0.0377 - val_tf.math.abs_186_loss: 0.0075\n",
      "Epoch 197/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.3372 - out_T_loss: 0.8675 - out_S_loss: 2.8246 - tf.math.abs_185_loss: 0.0389 - tf.math.abs_186_loss: 0.0223 - val_loss: 28.2124 - val_out_T_loss: 0.8992 - val_out_S_loss: 2.6431 - val_tf.math.abs_185_loss: 0.0368 - val_tf.math.abs_186_loss: 0.0073\n",
      "Epoch 198/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.1402 - out_T_loss: 0.8429 - out_S_loss: 2.8069 - tf.math.abs_185_loss: 0.0391 - tf.math.abs_186_loss: 0.0223 - val_loss: 28.1254 - val_out_T_loss: 0.8865 - val_out_S_loss: 2.6350 - val_tf.math.abs_185_loss: 0.0373 - val_tf.math.abs_186_loss: 0.0072\n",
      "Epoch 199/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 30.7948 - out_T_loss: 0.9003 - out_S_loss: 2.8662 - tf.math.abs_185_loss: 0.0395 - tf.math.abs_186_loss: 0.0221 - val_loss: 28.1034 - val_out_T_loss: 0.9190 - val_out_S_loss: 2.6261 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0073\n",
      "Epoch 200/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 29.9608 - out_T_loss: 0.8442 - out_S_loss: 2.7877 - tf.math.abs_185_loss: 0.0400 - tf.math.abs_186_loss: 0.0220 - val_loss: 27.9856 - val_out_T_loss: 0.8993 - val_out_S_loss: 2.6192 - val_tf.math.abs_185_loss: 0.0377 - val_tf.math.abs_186_loss: 0.0070\n",
      "Epoch 201/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.7110 - out_T_loss: 0.8719 - out_S_loss: 2.8573 - tf.math.abs_185_loss: 0.0413 - tf.math.abs_186_loss: 0.0220 - val_loss: 27.9211 - val_out_T_loss: 0.8942 - val_out_S_loss: 2.6116 - val_tf.math.abs_185_loss: 0.0382 - val_tf.math.abs_186_loss: 0.0074\n",
      "Epoch 202/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.0184 - out_T_loss: 0.8684 - out_S_loss: 2.7914 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0216 - val_loss: 27.8368 - val_out_T_loss: 0.8802 - val_out_S_loss: 2.6050 - val_tf.math.abs_185_loss: 0.0380 - val_tf.math.abs_186_loss: 0.0073\n",
      "Epoch 203/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.3240 - out_T_loss: 0.8209 - out_S_loss: 2.8274 - tf.math.abs_185_loss: 0.0395 - tf.math.abs_186_loss: 0.0219 - val_loss: 27.7573 - val_out_T_loss: 0.8726 - val_out_S_loss: 2.5984 - val_tf.math.abs_185_loss: 0.0379 - val_tf.math.abs_186_loss: 0.0071\n",
      "Epoch 204/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 29.8964 - out_T_loss: 0.8444 - out_S_loss: 2.7809 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0214 - val_loss: 27.7188 - val_out_T_loss: 0.8748 - val_out_S_loss: 2.5908 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0072\n",
      "Epoch 205/2000\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 30.0566 - out_T_loss: 0.8379 - out_S_loss: 2.7977 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0215 - val_loss: 27.5895 - val_out_T_loss: 0.8642 - val_out_S_loss: 2.5825 - val_tf.math.abs_185_loss: 0.0380 - val_tf.math.abs_186_loss: 0.0071\n",
      "Epoch 206/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 29.9036 - out_T_loss: 0.8397 - out_S_loss: 2.7836 - tf.math.abs_185_loss: 0.0395 - tf.math.abs_186_loss: 0.0219 - val_loss: 27.4846 - val_out_T_loss: 0.8486 - val_out_S_loss: 2.5746 - val_tf.math.abs_185_loss: 0.0376 - val_tf.math.abs_186_loss: 0.0070\n",
      "Epoch 207/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 29.7427 - out_T_loss: 0.8242 - out_S_loss: 2.7692 - tf.math.abs_185_loss: 0.0398 - tf.math.abs_186_loss: 0.0215 - val_loss: 27.4389 - val_out_T_loss: 0.8604 - val_out_S_loss: 2.5662 - val_tf.math.abs_185_loss: 0.0388 - val_tf.math.abs_186_loss: 0.0071\n",
      "Epoch 208/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 30.0671 - out_T_loss: 0.8396 - out_S_loss: 2.7991 - tf.math.abs_185_loss: 0.0401 - tf.math.abs_186_loss: 0.0217 - val_loss: 27.3370 - val_out_T_loss: 0.8442 - val_out_S_loss: 2.5596 - val_tf.math.abs_185_loss: 0.0379 - val_tf.math.abs_186_loss: 0.0069\n",
      "Epoch 209/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 29.7234 - out_T_loss: 0.8052 - out_S_loss: 2.7678 - tf.math.abs_185_loss: 0.0400 - tf.math.abs_186_loss: 0.0220 - val_loss: 27.2470 - val_out_T_loss: 0.8288 - val_out_S_loss: 2.5526 - val_tf.math.abs_185_loss: 0.0377 - val_tf.math.abs_186_loss: 0.0069\n",
      "Epoch 210/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 29.7250 - out_T_loss: 0.8072 - out_S_loss: 2.7687 - tf.math.abs_185_loss: 0.0401 - tf.math.abs_186_loss: 0.0215 - val_loss: 27.1536 - val_out_T_loss: 0.8293 - val_out_S_loss: 2.5429 - val_tf.math.abs_185_loss: 0.0379 - val_tf.math.abs_186_loss: 0.0069\n",
      "Epoch 211/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 29.9143 - out_T_loss: 0.8466 - out_S_loss: 2.7826 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0217 - val_loss: 27.1091 - val_out_T_loss: 0.8408 - val_out_S_loss: 2.5379 - val_tf.math.abs_185_loss: 0.0378 - val_tf.math.abs_186_loss: 0.0067\n",
      "Epoch 212/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 29.4960 - out_T_loss: 0.8185 - out_S_loss: 2.7454 - tf.math.abs_185_loss: 0.0401 - tf.math.abs_186_loss: 0.0211 - val_loss: 27.0387 - val_out_T_loss: 0.8298 - val_out_S_loss: 2.5292 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0069\n",
      "Epoch 213/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 29.3152 - out_T_loss: 0.7616 - out_S_loss: 2.7332 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0207 - val_loss: 26.9299 - val_out_T_loss: 0.8161 - val_out_S_loss: 2.5206 - val_tf.math.abs_185_loss: 0.0385 - val_tf.math.abs_186_loss: 0.0069\n",
      "Epoch 214/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 29.7493 - out_T_loss: 0.7542 - out_S_loss: 2.7777 - tf.math.abs_185_loss: 0.0397 - tf.math.abs_186_loss: 0.0212 - val_loss: 26.8622 - val_out_T_loss: 0.8067 - val_out_S_loss: 2.5149 - val_tf.math.abs_185_loss: 0.0385 - val_tf.math.abs_186_loss: 0.0068\n",
      "Epoch 215/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 29.1711 - out_T_loss: 0.7652 - out_S_loss: 2.7161 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0215 - val_loss: 26.8321 - val_out_T_loss: 0.8164 - val_out_S_loss: 2.5086 - val_tf.math.abs_185_loss: 0.0395 - val_tf.math.abs_186_loss: 0.0070\n",
      "Epoch 216/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 29.1837 - out_T_loss: 0.7427 - out_S_loss: 2.7221 - tf.math.abs_185_loss: 0.0400 - tf.math.abs_186_loss: 0.0211 - val_loss: 26.7301 - val_out_T_loss: 0.7911 - val_out_S_loss: 2.5037 - val_tf.math.abs_185_loss: 0.0383 - val_tf.math.abs_186_loss: 0.0069\n",
      "Epoch 217/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.8062 - out_T_loss: 0.7669 - out_S_loss: 2.6819 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0208 - val_loss: 26.6117 - val_out_T_loss: 0.7907 - val_out_S_loss: 2.4924 - val_tf.math.abs_185_loss: 0.0381 - val_tf.math.abs_186_loss: 0.0068\n",
      "Epoch 218/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.9392 - out_T_loss: 0.7494 - out_S_loss: 2.6975 - tf.math.abs_185_loss: 0.0396 - tf.math.abs_186_loss: 0.0211 - val_loss: 26.5364 - val_out_T_loss: 0.7873 - val_out_S_loss: 2.4849 - val_tf.math.abs_185_loss: 0.0383 - val_tf.math.abs_186_loss: 0.0067\n",
      "Epoch 219/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.7896 - out_T_loss: 0.7485 - out_S_loss: 2.6823 - tf.math.abs_185_loss: 0.0397 - tf.math.abs_186_loss: 0.0212 - val_loss: 26.4564 - val_out_T_loss: 0.7723 - val_out_S_loss: 2.4774 - val_tf.math.abs_185_loss: 0.0385 - val_tf.math.abs_186_loss: 0.0070\n",
      "Epoch 220/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 28.7221 - out_T_loss: 0.7897 - out_S_loss: 2.6712 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0207 - val_loss: 26.4462 - val_out_T_loss: 0.8081 - val_out_S_loss: 2.4709 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0069\n",
      "Epoch 221/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.9047 - out_T_loss: 0.7374 - out_S_loss: 2.6933 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0205 - val_loss: 26.3598 - val_out_T_loss: 0.7927 - val_out_S_loss: 2.4647 - val_tf.math.abs_185_loss: 0.0393 - val_tf.math.abs_186_loss: 0.0067\n",
      "Epoch 222/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.8797 - out_T_loss: 0.8166 - out_S_loss: 2.6822 - tf.math.abs_185_loss: 0.0410 - tf.math.abs_186_loss: 0.0211 - val_loss: 26.2644 - val_out_T_loss: 0.7728 - val_out_S_loss: 2.4581 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0066\n",
      "Epoch 223/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.9686 - out_T_loss: 0.7438 - out_S_loss: 2.6989 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0207 - val_loss: 26.2006 - val_out_T_loss: 0.7769 - val_out_S_loss: 2.4518 - val_tf.math.abs_185_loss: 0.0387 - val_tf.math.abs_186_loss: 0.0066\n",
      "Epoch 224/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.7739 - out_T_loss: 0.7420 - out_S_loss: 2.6804 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0206 - val_loss: 26.1156 - val_out_T_loss: 0.7680 - val_out_S_loss: 2.4441 - val_tf.math.abs_185_loss: 0.0387 - val_tf.math.abs_186_loss: 0.0066\n",
      "Epoch 225/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.2603 - out_T_loss: 0.7307 - out_S_loss: 2.6315 - tf.math.abs_185_loss: 0.0400 - tf.math.abs_186_loss: 0.0207 - val_loss: 26.1270 - val_out_T_loss: 0.7965 - val_out_S_loss: 2.4405 - val_tf.math.abs_185_loss: 0.0395 - val_tf.math.abs_186_loss: 0.0067\n",
      "Epoch 226/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.8162 - out_T_loss: 0.7453 - out_S_loss: 2.6837 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0206 - val_loss: 26.0382 - val_out_T_loss: 0.7867 - val_out_S_loss: 2.4347 - val_tf.math.abs_185_loss: 0.0387 - val_tf.math.abs_186_loss: 0.0065\n",
      "Epoch 227/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 28.4951 - out_T_loss: 0.7208 - out_S_loss: 2.6552 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0204 - val_loss: 25.9631 - val_out_T_loss: 0.7573 - val_out_S_loss: 2.4287 - val_tf.math.abs_185_loss: 0.0391 - val_tf.math.abs_186_loss: 0.0068\n",
      "Epoch 228/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 28.5909 - out_T_loss: 0.7158 - out_S_loss: 2.6670 - tf.math.abs_185_loss: 0.0398 - tf.math.abs_186_loss: 0.0204 - val_loss: 25.8441 - val_out_T_loss: 0.7502 - val_out_S_loss: 2.4199 - val_tf.math.abs_185_loss: 0.0382 - val_tf.math.abs_186_loss: 0.0065\n",
      "Epoch 229/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.3514 - out_T_loss: 0.7212 - out_S_loss: 2.6415 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0202 - val_loss: 25.7904 - val_out_T_loss: 0.7573 - val_out_S_loss: 2.4138 - val_tf.math.abs_185_loss: 0.0383 - val_tf.math.abs_186_loss: 0.0064\n",
      "Epoch 230/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.2029 - out_T_loss: 0.7381 - out_S_loss: 2.6246 - tf.math.abs_185_loss: 0.0413 - tf.math.abs_186_loss: 0.0197 - val_loss: 25.7486 - val_out_T_loss: 0.7446 - val_out_S_loss: 2.4072 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0068\n",
      "Epoch 231/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.9295 - out_T_loss: 0.7094 - out_S_loss: 2.5991 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0206 - val_loss: 25.6485 - val_out_T_loss: 0.7398 - val_out_S_loss: 2.4009 - val_tf.math.abs_185_loss: 0.0386 - val_tf.math.abs_186_loss: 0.0064\n",
      "Epoch 232/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.1117 - out_T_loss: 0.7374 - out_S_loss: 2.6143 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0204 - val_loss: 25.6070 - val_out_T_loss: 0.7485 - val_out_S_loss: 2.3936 - val_tf.math.abs_185_loss: 0.0395 - val_tf.math.abs_186_loss: 0.0066\n",
      "Epoch 233/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 28.3409 - out_T_loss: 0.6943 - out_S_loss: 2.6416 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0204 - val_loss: 25.5149 - val_out_T_loss: 0.7322 - val_out_S_loss: 2.3871 - val_tf.math.abs_185_loss: 0.0391 - val_tf.math.abs_186_loss: 0.0065\n",
      "Epoch 234/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.7121 - out_T_loss: 0.7199 - out_S_loss: 2.5762 - tf.math.abs_185_loss: 0.0415 - tf.math.abs_186_loss: 0.0200 - val_loss: 25.4957 - val_out_T_loss: 0.7246 - val_out_S_loss: 2.3840 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0067\n",
      "Epoch 235/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.9729 - out_T_loss: 0.6771 - out_S_loss: 2.6087 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0195 - val_loss: 25.4051 - val_out_T_loss: 0.7140 - val_out_S_loss: 2.3772 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0066\n",
      "Epoch 236/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.8795 - out_T_loss: 0.6941 - out_S_loss: 2.5974 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0200 - val_loss: 25.3141 - val_out_T_loss: 0.7132 - val_out_S_loss: 2.3692 - val_tf.math.abs_185_loss: 0.0390 - val_tf.math.abs_186_loss: 0.0064\n",
      "Epoch 237/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.7676 - out_T_loss: 0.6713 - out_S_loss: 2.5878 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0201 - val_loss: 25.2720 - val_out_T_loss: 0.7147 - val_out_S_loss: 2.3633 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0064\n",
      "Epoch 238/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.4091 - out_T_loss: 0.7076 - out_S_loss: 2.5481 - tf.math.abs_185_loss: 0.0414 - tf.math.abs_186_loss: 0.0196 - val_loss: 25.1899 - val_out_T_loss: 0.7161 - val_out_S_loss: 2.3559 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0064\n",
      "Epoch 239/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.6907 - out_T_loss: 0.6895 - out_S_loss: 2.5781 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0198 - val_loss: 25.1213 - val_out_T_loss: 0.7106 - val_out_S_loss: 2.3487 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0064\n",
      "Epoch 240/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.4792 - out_T_loss: 0.6738 - out_S_loss: 2.5596 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0195 - val_loss: 25.0197 - val_out_T_loss: 0.7026 - val_out_S_loss: 2.3402 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0063\n",
      "Epoch 241/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.2016 - out_T_loss: 0.6572 - out_S_loss: 2.5330 - tf.math.abs_185_loss: 0.0416 - tf.math.abs_186_loss: 0.0192 - val_loss: 24.9650 - val_out_T_loss: 0.7050 - val_out_S_loss: 2.3345 - val_tf.math.abs_185_loss: 0.0395 - val_tf.math.abs_186_loss: 0.0063\n",
      "Epoch 242/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.3725 - out_T_loss: 0.6925 - out_S_loss: 2.5432 - tf.math.abs_185_loss: 0.0426 - tf.math.abs_186_loss: 0.0198 - val_loss: 24.9691 - val_out_T_loss: 0.7365 - val_out_S_loss: 2.3280 - val_tf.math.abs_185_loss: 0.0411 - val_tf.math.abs_186_loss: 0.0066\n",
      "Epoch 243/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.3355 - out_T_loss: 0.6975 - out_S_loss: 2.5395 - tf.math.abs_185_loss: 0.0424 - tf.math.abs_186_loss: 0.0197 - val_loss: 24.8446 - val_out_T_loss: 0.6951 - val_out_S_loss: 2.3219 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0064\n",
      "Epoch 244/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.1390 - out_T_loss: 0.6586 - out_S_loss: 2.5275 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0191 - val_loss: 24.7475 - val_out_T_loss: 0.6887 - val_out_S_loss: 2.3147 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0062\n",
      "Epoch 245/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.0241 - out_T_loss: 0.6493 - out_S_loss: 2.5171 - tf.math.abs_185_loss: 0.0410 - tf.math.abs_186_loss: 0.0193 - val_loss: 24.7119 - val_out_T_loss: 0.6842 - val_out_S_loss: 2.3104 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0063\n",
      "Epoch 246/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.4548 - out_T_loss: 0.6748 - out_S_loss: 2.5572 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0196 - val_loss: 24.6733 - val_out_T_loss: 0.6839 - val_out_S_loss: 2.3066 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0063\n",
      "Epoch 247/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 27.4127 - out_T_loss: 0.6513 - out_S_loss: 2.5548 - tf.math.abs_185_loss: 0.0413 - tf.math.abs_186_loss: 0.0194 - val_loss: 24.5987 - val_out_T_loss: 0.6729 - val_out_S_loss: 2.3000 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0062\n",
      "Epoch 248/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 26.7615 - out_T_loss: 0.6460 - out_S_loss: 2.4904 - tf.math.abs_185_loss: 0.0418 - tf.math.abs_186_loss: 0.0188 - val_loss: 24.5689 - val_out_T_loss: 0.6844 - val_out_S_loss: 2.2921 - val_tf.math.abs_185_loss: 0.0419 - val_tf.math.abs_186_loss: 0.0063\n",
      "Epoch 249/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 26.8838 - out_T_loss: 0.6295 - out_S_loss: 2.5059 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0190 - val_loss: 24.4441 - val_out_T_loss: 0.6617 - val_out_S_loss: 2.2864 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0062\n",
      "Epoch 250/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.1795 - out_T_loss: 0.6261 - out_S_loss: 2.5346 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0193 - val_loss: 24.4211 - val_out_T_loss: 0.6637 - val_out_S_loss: 2.2807 - val_tf.math.abs_185_loss: 0.0412 - val_tf.math.abs_186_loss: 0.0063\n",
      "Epoch 251/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 26.8891 - out_T_loss: 0.6704 - out_S_loss: 2.4990 - tf.math.abs_185_loss: 0.0423 - tf.math.abs_186_loss: 0.0192 - val_loss: 24.3885 - val_out_T_loss: 0.6589 - val_out_S_loss: 2.2770 - val_tf.math.abs_185_loss: 0.0416 - val_tf.math.abs_186_loss: 0.0064\n",
      "Epoch 252/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.2349 - out_T_loss: 0.6446 - out_S_loss: 2.5377 - tf.math.abs_185_loss: 0.0413 - tf.math.abs_186_loss: 0.0194 - val_loss: 24.3092 - val_out_T_loss: 0.6674 - val_out_S_loss: 2.2722 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0062\n",
      "Epoch 253/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.0633 - out_T_loss: 0.6621 - out_S_loss: 2.5179 - tf.math.abs_185_loss: 0.0422 - tf.math.abs_186_loss: 0.0189 - val_loss: 24.2542 - val_out_T_loss: 0.6703 - val_out_S_loss: 2.2649 - val_tf.math.abs_185_loss: 0.0405 - val_tf.math.abs_186_loss: 0.0063\n",
      "Epoch 254/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 26.5996 - out_T_loss: 0.6275 - out_S_loss: 2.4772 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0188 - val_loss: 24.1558 - val_out_T_loss: 0.6603 - val_out_S_loss: 2.2586 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0061\n",
      "Epoch 255/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 26.9001 - out_T_loss: 0.6091 - out_S_loss: 2.5090 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0192 - val_loss: 24.1242 - val_out_T_loss: 0.6578 - val_out_S_loss: 2.2549 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0062\n",
      "Epoch 256/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 27.1241 - out_T_loss: 0.6212 - out_S_loss: 2.5301 - tf.math.abs_185_loss: 0.0410 - tf.math.abs_186_loss: 0.0191 - val_loss: 24.0420 - val_out_T_loss: 0.6481 - val_out_S_loss: 2.2483 - val_tf.math.abs_185_loss: 0.0395 - val_tf.math.abs_186_loss: 0.0061\n",
      "Epoch 257/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 26.8775 - out_T_loss: 0.6380 - out_S_loss: 2.5039 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0191 - val_loss: 24.0219 - val_out_T_loss: 0.6558 - val_out_S_loss: 2.2442 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0061\n",
      "Epoch 258/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 26.2868 - out_T_loss: 0.6520 - out_S_loss: 2.4442 - tf.math.abs_185_loss: 0.0410 - tf.math.abs_186_loss: 0.0187 - val_loss: 23.9877 - val_out_T_loss: 0.6534 - val_out_S_loss: 2.2405 - val_tf.math.abs_185_loss: 0.0404 - val_tf.math.abs_186_loss: 0.0060\n",
      "Epoch 259/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 26.7081 - out_T_loss: 0.6120 - out_S_loss: 2.4901 - tf.math.abs_185_loss: 0.0413 - tf.math.abs_186_loss: 0.0184 - val_loss: 23.8854 - val_out_T_loss: 0.6458 - val_out_S_loss: 2.2323 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0061\n",
      "Epoch 260/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 26.3402 - out_T_loss: 0.6066 - out_S_loss: 2.4539 - tf.math.abs_185_loss: 0.0414 - tf.math.abs_186_loss: 0.0184 - val_loss: 23.8182 - val_out_T_loss: 0.6484 - val_out_S_loss: 2.2257 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 261/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 26.0155 - out_T_loss: 0.6321 - out_S_loss: 2.4193 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0190 - val_loss: 23.7481 - val_out_T_loss: 0.6360 - val_out_S_loss: 2.2195 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0060\n",
      "Epoch 262/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 26.7691 - out_T_loss: 0.5989 - out_S_loss: 2.4961 - tf.math.abs_185_loss: 0.0415 - tf.math.abs_186_loss: 0.0189 - val_loss: 23.6936 - val_out_T_loss: 0.6258 - val_out_S_loss: 2.2152 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0061\n",
      "Epoch 263/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 26.3972 - out_T_loss: 0.5953 - out_S_loss: 2.4602 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0188 - val_loss: 23.6406 - val_out_T_loss: 0.6336 - val_out_S_loss: 2.2086 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0060\n",
      "Epoch 264/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 26.5289 - out_T_loss: 0.5993 - out_S_loss: 2.4700 - tf.math.abs_185_loss: 0.0425 - tf.math.abs_186_loss: 0.0190 - val_loss: 23.6437 - val_out_T_loss: 0.6465 - val_out_S_loss: 2.2061 - val_tf.math.abs_185_loss: 0.0407 - val_tf.math.abs_186_loss: 0.0060\n",
      "Epoch 265/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 26.1735 - out_T_loss: 0.5777 - out_S_loss: 2.4409 - tf.math.abs_185_loss: 0.0410 - tf.math.abs_186_loss: 0.0183 - val_loss: 23.5375 - val_out_T_loss: 0.6176 - val_out_S_loss: 2.1996 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 266/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 26.0242 - out_T_loss: 0.6413 - out_S_loss: 2.4154 - tf.math.abs_185_loss: 0.0428 - tf.math.abs_186_loss: 0.0186 - val_loss: 23.4865 - val_out_T_loss: 0.6311 - val_out_S_loss: 2.1934 - val_tf.math.abs_185_loss: 0.0402 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 267/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 26.1999 - out_T_loss: 0.6095 - out_S_loss: 2.4369 - tf.math.abs_185_loss: 0.0422 - tf.math.abs_186_loss: 0.0188 - val_loss: 23.4698 - val_out_T_loss: 0.6208 - val_out_S_loss: 2.1924 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0060\n",
      "Epoch 268/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 26.0195 - out_T_loss: 0.5913 - out_S_loss: 2.4220 - tf.math.abs_185_loss: 0.0418 - tf.math.abs_186_loss: 0.0186 - val_loss: 23.3678 - val_out_T_loss: 0.6278 - val_out_S_loss: 2.1812 - val_tf.math.abs_185_loss: 0.0405 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 269/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 26.2808 - out_T_loss: 0.6086 - out_S_loss: 2.4479 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0185 - val_loss: 23.3159 - val_out_T_loss: 0.6277 - val_out_S_loss: 2.1764 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 270/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.9290 - out_T_loss: 0.5923 - out_S_loss: 2.4141 - tf.math.abs_185_loss: 0.0417 - tf.math.abs_186_loss: 0.0181 - val_loss: 23.2629 - val_out_T_loss: 0.6174 - val_out_S_loss: 2.1722 - val_tf.math.abs_185_loss: 0.0402 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 271/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.6747 - out_T_loss: 0.5824 - out_S_loss: 2.3909 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0183 - val_loss: 23.2023 - val_out_T_loss: 0.6064 - val_out_S_loss: 2.1676 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 272/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.5632 - out_T_loss: 0.5975 - out_S_loss: 2.3778 - tf.math.abs_185_loss: 0.0415 - tf.math.abs_186_loss: 0.0179 - val_loss: 23.1559 - val_out_T_loss: 0.6067 - val_out_S_loss: 2.1625 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 273/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 26.0185 - out_T_loss: 0.5578 - out_S_loss: 2.4275 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0184 - val_loss: 23.1456 - val_out_T_loss: 0.6126 - val_out_S_loss: 2.1598 - val_tf.math.abs_185_loss: 0.0410 - val_tf.math.abs_186_loss: 0.0058\n",
      "Epoch 274/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 26.0040 - out_T_loss: 0.5869 - out_S_loss: 2.4216 - tf.math.abs_185_loss: 0.0415 - tf.math.abs_186_loss: 0.0185 - val_loss: 23.0743 - val_out_T_loss: 0.6081 - val_out_S_loss: 2.1522 - val_tf.math.abs_185_loss: 0.0412 - val_tf.math.abs_186_loss: 0.0060\n",
      "Epoch 275/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.6861 - out_T_loss: 0.5940 - out_S_loss: 2.3874 - tf.math.abs_185_loss: 0.0427 - tf.math.abs_186_loss: 0.0182 - val_loss: 22.9957 - val_out_T_loss: 0.6117 - val_out_S_loss: 2.1444 - val_tf.math.abs_185_loss: 0.0411 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 276/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 25.3406 - out_T_loss: 0.5890 - out_S_loss: 2.3542 - tf.math.abs_185_loss: 0.0426 - tf.math.abs_186_loss: 0.0178 - val_loss: 22.9431 - val_out_T_loss: 0.6021 - val_out_S_loss: 2.1411 - val_tf.math.abs_185_loss: 0.0406 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 277/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 25.2116 - out_T_loss: 0.5636 - out_S_loss: 2.3455 - tf.math.abs_185_loss: 0.0414 - tf.math.abs_186_loss: 0.0182 - val_loss: 22.9045 - val_out_T_loss: 0.5968 - val_out_S_loss: 2.1392 - val_tf.math.abs_185_loss: 0.0400 - val_tf.math.abs_186_loss: 0.0058\n",
      "Epoch 278/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.4473 - out_T_loss: 0.5638 - out_S_loss: 2.3713 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0178 - val_loss: 22.8353 - val_out_T_loss: 0.5973 - val_out_S_loss: 2.1316 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0058\n",
      "Epoch 279/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.2834 - out_T_loss: 0.5966 - out_S_loss: 2.3475 - tf.math.abs_185_loss: 0.0423 - tf.math.abs_186_loss: 0.0183 - val_loss: 22.8232 - val_out_T_loss: 0.6202 - val_out_S_loss: 2.1261 - val_tf.math.abs_185_loss: 0.0410 - val_tf.math.abs_186_loss: 0.0061\n",
      "Epoch 280/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 26.1405 - out_T_loss: 0.5820 - out_S_loss: 2.4330 - tf.math.abs_185_loss: 0.0428 - tf.math.abs_186_loss: 0.0186 - val_loss: 22.7927 - val_out_T_loss: 0.6084 - val_out_S_loss: 2.1252 - val_tf.math.abs_185_loss: 0.0407 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 281/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.6296 - out_T_loss: 0.5624 - out_S_loss: 2.3881 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0182 - val_loss: 22.7530 - val_out_T_loss: 0.6098 - val_out_S_loss: 2.1205 - val_tf.math.abs_185_loss: 0.0409 - val_tf.math.abs_186_loss: 0.0061\n",
      "Epoch 282/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.2143 - out_T_loss: 0.5332 - out_S_loss: 2.3506 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0180 - val_loss: 22.6386 - val_out_T_loss: 0.5774 - val_out_S_loss: 2.1151 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0058\n",
      "Epoch 283/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 25.1725 - out_T_loss: 0.5704 - out_S_loss: 2.3407 - tf.math.abs_185_loss: 0.0416 - tf.math.abs_186_loss: 0.0182 - val_loss: 22.6155 - val_out_T_loss: 0.5921 - val_out_S_loss: 2.1098 - val_tf.math.abs_185_loss: 0.0405 - val_tf.math.abs_186_loss: 0.0058\n",
      "Epoch 284/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.9184 - out_T_loss: 0.5532 - out_S_loss: 2.3193 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0179 - val_loss: 22.5677 - val_out_T_loss: 0.5840 - val_out_S_loss: 2.1064 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0057\n",
      "Epoch 285/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.9258 - out_T_loss: 0.5471 - out_S_loss: 2.3200 - tf.math.abs_185_loss: 0.0413 - tf.math.abs_186_loss: 0.0176 - val_loss: 22.4868 - val_out_T_loss: 0.5821 - val_out_S_loss: 2.0990 - val_tf.math.abs_185_loss: 0.0400 - val_tf.math.abs_186_loss: 0.0057\n",
      "Epoch 286/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 24.9265 - out_T_loss: 0.5728 - out_S_loss: 2.3183 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0179 - val_loss: 22.4432 - val_out_T_loss: 0.5886 - val_out_S_loss: 2.0950 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 287/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 25.4610 - out_T_loss: 0.5292 - out_S_loss: 2.3766 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0181 - val_loss: 22.4083 - val_out_T_loss: 0.5826 - val_out_S_loss: 2.0920 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0057\n",
      "Epoch 288/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 24.8137 - out_T_loss: 0.5354 - out_S_loss: 2.3113 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0177 - val_loss: 22.3482 - val_out_T_loss: 0.5671 - val_out_S_loss: 2.0886 - val_tf.math.abs_185_loss: 0.0392 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 289/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.9629 - out_T_loss: 0.5349 - out_S_loss: 2.3271 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0177 - val_loss: 22.3384 - val_out_T_loss: 0.5819 - val_out_S_loss: 2.0850 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0058\n",
      "Epoch 290/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.6402 - out_T_loss: 0.5163 - out_S_loss: 2.2966 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0173 - val_loss: 22.3038 - val_out_T_loss: 0.5656 - val_out_S_loss: 2.0834 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 291/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.7121 - out_T_loss: 0.5327 - out_S_loss: 2.3026 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0172 - val_loss: 22.2055 - val_out_T_loss: 0.5784 - val_out_S_loss: 2.0716 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0057\n",
      "Epoch 292/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.7193 - out_T_loss: 0.5500 - out_S_loss: 2.3012 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0172 - val_loss: 22.1598 - val_out_T_loss: 0.5771 - val_out_S_loss: 2.0671 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0057\n",
      "Epoch 293/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.8047 - out_T_loss: 0.5577 - out_S_loss: 2.3050 - tf.math.abs_185_loss: 0.0422 - tf.math.abs_186_loss: 0.0177 - val_loss: 22.1246 - val_out_T_loss: 0.5731 - val_out_S_loss: 2.0639 - val_tf.math.abs_185_loss: 0.0400 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 294/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.9683 - out_T_loss: 0.5765 - out_S_loss: 2.3214 - tf.math.abs_185_loss: 0.0417 - tf.math.abs_186_loss: 0.0172 - val_loss: 22.0904 - val_out_T_loss: 0.5728 - val_out_S_loss: 2.0565 - val_tf.math.abs_185_loss: 0.0417 - val_tf.math.abs_186_loss: 0.0059\n",
      "Epoch 295/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 24.2529 - out_T_loss: 0.5207 - out_S_loss: 2.2560 - tf.math.abs_185_loss: 0.0410 - tf.math.abs_186_loss: 0.0176 - val_loss: 22.0187 - val_out_T_loss: 0.5618 - val_out_S_loss: 2.0540 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 296/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.8853 - out_T_loss: 0.5155 - out_S_loss: 2.3171 - tf.math.abs_185_loss: 0.0423 - tf.math.abs_186_loss: 0.0177 - val_loss: 21.9630 - val_out_T_loss: 0.5569 - val_out_S_loss: 2.0463 - val_tf.math.abs_185_loss: 0.0415 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 297/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.9917 - out_T_loss: 0.5408 - out_S_loss: 2.3269 - tf.math.abs_185_loss: 0.0416 - tf.math.abs_186_loss: 0.0175 - val_loss: 21.9087 - val_out_T_loss: 0.5577 - val_out_S_loss: 2.0426 - val_tf.math.abs_185_loss: 0.0407 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 298/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 24.7612 - out_T_loss: 0.5649 - out_S_loss: 2.2993 - tf.math.abs_185_loss: 0.0424 - tf.math.abs_186_loss: 0.0178 - val_loss: 21.9188 - val_out_T_loss: 0.5663 - val_out_S_loss: 2.0420 - val_tf.math.abs_185_loss: 0.0409 - val_tf.math.abs_186_loss: 0.0057\n",
      "Epoch 299/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 24.2844 - out_T_loss: 0.5051 - out_S_loss: 2.2629 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0171 - val_loss: 21.8339 - val_out_T_loss: 0.5566 - val_out_S_loss: 2.0372 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 300/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.1894 - out_T_loss: 0.5066 - out_S_loss: 2.2517 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0172 - val_loss: 21.7477 - val_out_T_loss: 0.5535 - val_out_S_loss: 2.0280 - val_tf.math.abs_185_loss: 0.0402 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 301/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.2018 - out_T_loss: 0.4941 - out_S_loss: 2.2553 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0170 - val_loss: 21.6850 - val_out_T_loss: 0.5398 - val_out_S_loss: 2.0235 - val_tf.math.abs_185_loss: 0.0400 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 302/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.2778 - out_T_loss: 0.4990 - out_S_loss: 2.2623 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0174 - val_loss: 21.6137 - val_out_T_loss: 0.5363 - val_out_S_loss: 2.0159 - val_tf.math.abs_185_loss: 0.0404 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 303/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 23.8223 - out_T_loss: 0.4920 - out_S_loss: 2.2156 - tf.math.abs_185_loss: 0.0415 - tf.math.abs_186_loss: 0.0172 - val_loss: 21.5799 - val_out_T_loss: 0.5366 - val_out_S_loss: 2.0135 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 304/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.5994 - out_T_loss: 0.5128 - out_S_loss: 2.2932 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0172 - val_loss: 21.5293 - val_out_T_loss: 0.5464 - val_out_S_loss: 2.0078 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 305/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.7676 - out_T_loss: 0.5185 - out_S_loss: 2.3086 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0177 - val_loss: 21.5120 - val_out_T_loss: 0.5442 - val_out_S_loss: 2.0057 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 306/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.1845 - out_T_loss: 0.5324 - out_S_loss: 2.2488 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0175 - val_loss: 21.4834 - val_out_T_loss: 0.5391 - val_out_S_loss: 2.0011 - val_tf.math.abs_185_loss: 0.0410 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 307/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 24.3169 - out_T_loss: 0.4939 - out_S_loss: 2.2668 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0169 - val_loss: 21.4281 - val_out_T_loss: 0.5350 - val_out_S_loss: 1.9966 - val_tf.math.abs_185_loss: 0.0409 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 308/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.8462 - out_T_loss: 0.4945 - out_S_loss: 2.2195 - tf.math.abs_185_loss: 0.0410 - tf.math.abs_186_loss: 0.0169 - val_loss: 21.3657 - val_out_T_loss: 0.5314 - val_out_S_loss: 1.9925 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 309/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.8261 - out_T_loss: 0.5118 - out_S_loss: 2.2157 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0173 - val_loss: 21.3328 - val_out_T_loss: 0.5448 - val_out_S_loss: 1.9882 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0057\n",
      "Epoch 310/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 23.6531 - out_T_loss: 0.5009 - out_S_loss: 2.1981 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0173 - val_loss: 21.2956 - val_out_T_loss: 0.5367 - val_out_S_loss: 1.9841 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0056\n",
      "Epoch 311/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 24.1932 - out_T_loss: 0.5242 - out_S_loss: 2.2513 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0169 - val_loss: 21.2415 - val_out_T_loss: 0.5347 - val_out_S_loss: 1.9808 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 312/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 23.7433 - out_T_loss: 0.5058 - out_S_loss: 2.2075 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0169 - val_loss: 21.1496 - val_out_T_loss: 0.5251 - val_out_S_loss: 1.9710 - val_tf.math.abs_185_loss: 0.0404 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 313/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 23.9979 - out_T_loss: 0.5119 - out_S_loss: 2.2302 - tf.math.abs_185_loss: 0.0418 - tf.math.abs_186_loss: 0.0174 - val_loss: 21.1254 - val_out_T_loss: 0.5220 - val_out_S_loss: 1.9693 - val_tf.math.abs_185_loss: 0.0402 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 314/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 23.8140 - out_T_loss: 0.5163 - out_S_loss: 2.2137 - tf.math.abs_185_loss: 0.0413 - tf.math.abs_186_loss: 0.0167 - val_loss: 21.0863 - val_out_T_loss: 0.5309 - val_out_S_loss: 1.9642 - val_tf.math.abs_185_loss: 0.0402 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 315/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.8759 - out_T_loss: 0.5055 - out_S_loss: 2.2225 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0167 - val_loss: 21.0537 - val_out_T_loss: 0.5280 - val_out_S_loss: 1.9620 - val_tf.math.abs_185_loss: 0.0400 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 316/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 23.6843 - out_T_loss: 0.4830 - out_S_loss: 2.2047 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0168 - val_loss: 20.9800 - val_out_T_loss: 0.5244 - val_out_S_loss: 1.9554 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 317/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.7576 - out_T_loss: 0.4864 - out_S_loss: 2.2126 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0165 - val_loss: 20.9385 - val_out_T_loss: 0.5159 - val_out_S_loss: 1.9521 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 318/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.8464 - out_T_loss: 0.5066 - out_S_loss: 2.2175 - tf.math.abs_185_loss: 0.0414 - tf.math.abs_186_loss: 0.0168 - val_loss: 20.9477 - val_out_T_loss: 0.5228 - val_out_S_loss: 1.9523 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 319/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.3131 - out_T_loss: 0.4873 - out_S_loss: 2.1675 - tf.math.abs_185_loss: 0.0409 - tf.math.abs_186_loss: 0.0166 - val_loss: 20.8795 - val_out_T_loss: 0.5225 - val_out_S_loss: 1.9447 - val_tf.math.abs_185_loss: 0.0402 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 320/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.3224 - out_T_loss: 0.4861 - out_S_loss: 2.1690 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0165 - val_loss: 20.8100 - val_out_T_loss: 0.5171 - val_out_S_loss: 1.9380 - val_tf.math.abs_185_loss: 0.0404 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 321/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.7012 - out_T_loss: 0.4680 - out_S_loss: 2.2098 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0164 - val_loss: 20.7930 - val_out_T_loss: 0.5127 - val_out_S_loss: 1.9359 - val_tf.math.abs_185_loss: 0.0406 - val_tf.math.abs_186_loss: 0.0055\n",
      "Epoch 322/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.7369 - out_T_loss: 0.4733 - out_S_loss: 2.2104 - tf.math.abs_185_loss: 0.0416 - tf.math.abs_186_loss: 0.0164 - val_loss: 20.7598 - val_out_T_loss: 0.5185 - val_out_S_loss: 1.9312 - val_tf.math.abs_185_loss: 0.0410 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 323/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.3100 - out_T_loss: 0.4673 - out_S_loss: 2.1694 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0164 - val_loss: 20.6900 - val_out_T_loss: 0.5177 - val_out_S_loss: 1.9265 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 324/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 23.2440 - out_T_loss: 0.4715 - out_S_loss: 2.1608 - tf.math.abs_185_loss: 0.0420 - tf.math.abs_186_loss: 0.0163 - val_loss: 20.6692 - val_out_T_loss: 0.5221 - val_out_S_loss: 1.9242 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0053\n",
      "Epoch 325/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.1267 - out_T_loss: 0.4518 - out_S_loss: 2.1547 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0160 - val_loss: 20.6215 - val_out_T_loss: 0.5010 - val_out_S_loss: 1.9197 - val_tf.math.abs_185_loss: 0.0408 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 326/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.3845 - out_T_loss: 0.4664 - out_S_loss: 2.1767 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0170 - val_loss: 20.6143 - val_out_T_loss: 0.5088 - val_out_S_loss: 1.9196 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0052\n",
      "Epoch 327/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.1157 - out_T_loss: 0.4732 - out_S_loss: 2.1511 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0164 - val_loss: 20.5011 - val_out_T_loss: 0.4979 - val_out_S_loss: 1.9103 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 328/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.1698 - out_T_loss: 0.4662 - out_S_loss: 2.1569 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0163 - val_loss: 20.4975 - val_out_T_loss: 0.5100 - val_out_S_loss: 1.9094 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 329/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.7074 - out_T_loss: 0.4701 - out_S_loss: 2.2082 - tf.math.abs_185_loss: 0.0414 - tf.math.abs_186_loss: 0.0164 - val_loss: 20.4681 - val_out_T_loss: 0.4978 - val_out_S_loss: 1.9064 - val_tf.math.abs_185_loss: 0.0401 - val_tf.math.abs_186_loss: 0.0052\n",
      "Epoch 330/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 22.4919 - out_T_loss: 0.4909 - out_S_loss: 2.0856 - tf.math.abs_185_loss: 0.0413 - tf.math.abs_186_loss: 0.0159 - val_loss: 20.4257 - val_out_T_loss: 0.5055 - val_out_S_loss: 1.9025 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0052\n",
      "Epoch 331/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.9791 - out_T_loss: 0.4587 - out_S_loss: 2.1391 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0161 - val_loss: 20.3487 - val_out_T_loss: 0.4910 - val_out_S_loss: 1.8965 - val_tf.math.abs_185_loss: 0.0395 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 332/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 22.9301 - out_T_loss: 0.4734 - out_S_loss: 2.1340 - tf.math.abs_185_loss: 0.0401 - tf.math.abs_186_loss: 0.0158 - val_loss: 20.3358 - val_out_T_loss: 0.4943 - val_out_S_loss: 1.8944 - val_tf.math.abs_185_loss: 0.0398 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 333/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.9038 - out_T_loss: 0.4643 - out_S_loss: 2.1305 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0160 - val_loss: 20.2635 - val_out_T_loss: 0.4939 - val_out_S_loss: 1.8869 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 334/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.0174 - out_T_loss: 0.4549 - out_S_loss: 2.1438 - tf.math.abs_185_loss: 0.0399 - tf.math.abs_186_loss: 0.0163 - val_loss: 20.2584 - val_out_T_loss: 0.4882 - val_out_S_loss: 1.8887 - val_tf.math.abs_185_loss: 0.0391 - val_tf.math.abs_186_loss: 0.0050\n",
      "Epoch 335/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.8472 - out_T_loss: 0.4619 - out_S_loss: 2.1239 - tf.math.abs_185_loss: 0.0414 - tf.math.abs_186_loss: 0.0159 - val_loss: 20.2073 - val_out_T_loss: 0.4911 - val_out_S_loss: 1.8828 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0050\n",
      "Epoch 336/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.5135 - out_T_loss: 0.4404 - out_S_loss: 2.0947 - tf.math.abs_185_loss: 0.0401 - tf.math.abs_186_loss: 0.0163 - val_loss: 20.1610 - val_out_T_loss: 0.4959 - val_out_S_loss: 1.8753 - val_tf.math.abs_185_loss: 0.0402 - val_tf.math.abs_186_loss: 0.0054\n",
      "Epoch 337/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.1584 - out_T_loss: 0.4639 - out_S_loss: 2.1551 - tf.math.abs_185_loss: 0.0408 - tf.math.abs_186_loss: 0.0164 - val_loss: 20.1140 - val_out_T_loss: 0.4977 - val_out_S_loss: 1.8721 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 338/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.7811 - out_T_loss: 0.4600 - out_S_loss: 2.1184 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0163 - val_loss: 20.0520 - val_out_T_loss: 0.4911 - val_out_S_loss: 1.8671 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 339/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 22.8466 - out_T_loss: 0.4520 - out_S_loss: 2.1258 - tf.math.abs_185_loss: 0.0406 - tf.math.abs_186_loss: 0.0162 - val_loss: 20.0318 - val_out_T_loss: 0.4811 - val_out_S_loss: 1.8665 - val_tf.math.abs_185_loss: 0.0392 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 340/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.8125 - out_T_loss: 0.4602 - out_S_loss: 2.1209 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0165 - val_loss: 19.9775 - val_out_T_loss: 0.4891 - val_out_S_loss: 1.8608 - val_tf.math.abs_185_loss: 0.0391 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 341/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 23.0495 - out_T_loss: 0.4743 - out_S_loss: 2.1448 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0157 - val_loss: 20.0256 - val_out_T_loss: 0.4991 - val_out_S_loss: 1.8624 - val_tf.math.abs_185_loss: 0.0400 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 342/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 22.2673 - out_T_loss: 0.4477 - out_S_loss: 2.0695 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0157 - val_loss: 19.8936 - val_out_T_loss: 0.4861 - val_out_S_loss: 1.8529 - val_tf.math.abs_185_loss: 0.0390 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 343/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.4942 - out_T_loss: 0.4413 - out_S_loss: 2.0932 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0156 - val_loss: 19.8939 - val_out_T_loss: 0.4812 - val_out_S_loss: 1.8514 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0050\n",
      "Epoch 344/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.6314 - out_T_loss: 0.4455 - out_S_loss: 2.1063 - tf.math.abs_185_loss: 0.0401 - tf.math.abs_186_loss: 0.0160 - val_loss: 19.8533 - val_out_T_loss: 0.4874 - val_out_S_loss: 1.8475 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 345/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 22.5456 - out_T_loss: 0.4622 - out_S_loss: 2.0972 - tf.math.abs_185_loss: 0.0397 - tf.math.abs_186_loss: 0.0158 - val_loss: 19.8061 - val_out_T_loss: 0.4851 - val_out_S_loss: 1.8442 - val_tf.math.abs_185_loss: 0.0390 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 346/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 22.1641 - out_T_loss: 0.4410 - out_S_loss: 2.0601 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0159 - val_loss: 19.7870 - val_out_T_loss: 0.4733 - val_out_S_loss: 1.8426 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0050\n",
      "Epoch 347/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.0253 - out_T_loss: 0.4324 - out_S_loss: 2.0482 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0154 - val_loss: 19.7268 - val_out_T_loss: 0.4762 - val_out_S_loss: 1.8362 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0050\n",
      "Epoch 348/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.2997 - out_T_loss: 0.4414 - out_S_loss: 2.0745 - tf.math.abs_185_loss: 0.0401 - tf.math.abs_186_loss: 0.0156 - val_loss: 19.6826 - val_out_T_loss: 0.4814 - val_out_S_loss: 1.8322 - val_tf.math.abs_185_loss: 0.0390 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 349/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.6934 - out_T_loss: 0.4308 - out_S_loss: 2.1149 - tf.math.abs_185_loss: 0.0397 - tf.math.abs_186_loss: 0.0159 - val_loss: 19.6663 - val_out_T_loss: 0.4846 - val_out_S_loss: 1.8287 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0050\n",
      "Epoch 350/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.2873 - out_T_loss: 0.4308 - out_S_loss: 2.0739 - tf.math.abs_185_loss: 0.0400 - tf.math.abs_186_loss: 0.0159 - val_loss: 19.5927 - val_out_T_loss: 0.4711 - val_out_S_loss: 1.8232 - val_tf.math.abs_185_loss: 0.0395 - val_tf.math.abs_186_loss: 0.0050\n",
      "Epoch 351/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.9884 - out_T_loss: 0.4303 - out_S_loss: 2.0428 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0153 - val_loss: 19.5534 - val_out_T_loss: 0.4765 - val_out_S_loss: 1.8194 - val_tf.math.abs_185_loss: 0.0393 - val_tf.math.abs_186_loss: 0.0048\n",
      "Epoch 352/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 22.5347 - out_T_loss: 0.4096 - out_S_loss: 2.1010 - tf.math.abs_185_loss: 0.0400 - tf.math.abs_186_loss: 0.0158 - val_loss: 19.5563 - val_out_T_loss: 0.4759 - val_out_S_loss: 1.8186 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 353/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.4496 - out_T_loss: 0.4170 - out_S_loss: 2.0918 - tf.math.abs_185_loss: 0.0398 - tf.math.abs_186_loss: 0.0160 - val_loss: 19.4899 - val_out_T_loss: 0.4714 - val_out_S_loss: 1.8122 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 354/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 22.1513 - out_T_loss: 0.4169 - out_S_loss: 2.0622 - tf.math.abs_185_loss: 0.0395 - tf.math.abs_186_loss: 0.0161 - val_loss: 19.4530 - val_out_T_loss: 0.4744 - val_out_S_loss: 1.8096 - val_tf.math.abs_185_loss: 0.0392 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 355/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.0263 - out_T_loss: 0.3920 - out_S_loss: 2.0526 - tf.math.abs_185_loss: 0.0395 - tf.math.abs_186_loss: 0.0159 - val_loss: 19.4707 - val_out_T_loss: 0.4648 - val_out_S_loss: 1.8094 - val_tf.math.abs_185_loss: 0.0407 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 356/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.1158 - out_T_loss: 0.4197 - out_S_loss: 2.0549 - tf.math.abs_185_loss: 0.0414 - tf.math.abs_186_loss: 0.0160 - val_loss: 19.3828 - val_out_T_loss: 0.4664 - val_out_S_loss: 1.8041 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 357/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 21.9123 - out_T_loss: 0.4374 - out_S_loss: 2.0341 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0156 - val_loss: 19.4177 - val_out_T_loss: 0.4766 - val_out_S_loss: 1.8021 - val_tf.math.abs_185_loss: 0.0409 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 358/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.1035 - out_T_loss: 0.4276 - out_S_loss: 2.0580 - tf.math.abs_185_loss: 0.0392 - tf.math.abs_186_loss: 0.0156 - val_loss: 19.3403 - val_out_T_loss: 0.4657 - val_out_S_loss: 1.7984 - val_tf.math.abs_185_loss: 0.0397 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 359/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.5411 - out_T_loss: 0.4635 - out_S_loss: 2.0950 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0159 - val_loss: 19.3446 - val_out_T_loss: 0.4784 - val_out_S_loss: 1.7989 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 360/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.0738 - out_T_loss: 0.4123 - out_S_loss: 2.0569 - tf.math.abs_185_loss: 0.0396 - tf.math.abs_186_loss: 0.0150 - val_loss: 19.3137 - val_out_T_loss: 0.4696 - val_out_S_loss: 1.7962 - val_tf.math.abs_185_loss: 0.0392 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 361/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.0725 - out_T_loss: 0.4091 - out_S_loss: 2.0578 - tf.math.abs_185_loss: 0.0386 - tf.math.abs_186_loss: 0.0156 - val_loss: 19.2323 - val_out_T_loss: 0.4681 - val_out_S_loss: 1.7890 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0048\n",
      "Epoch 362/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.9155 - out_T_loss: 0.4330 - out_S_loss: 2.0365 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0154 - val_loss: 19.1844 - val_out_T_loss: 0.4563 - val_out_S_loss: 1.7856 - val_tf.math.abs_185_loss: 0.0388 - val_tf.math.abs_186_loss: 0.0048\n",
      "Epoch 363/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.8652 - out_T_loss: 0.4536 - out_S_loss: 2.0289 - tf.math.abs_185_loss: 0.0404 - tf.math.abs_186_loss: 0.0158 - val_loss: 19.2173 - val_out_T_loss: 0.4690 - val_out_S_loss: 1.7862 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 364/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.9809 - out_T_loss: 0.4098 - out_S_loss: 2.0453 - tf.math.abs_185_loss: 0.0401 - tf.math.abs_186_loss: 0.0158 - val_loss: 19.1284 - val_out_T_loss: 0.4589 - val_out_S_loss: 1.7794 - val_tf.math.abs_185_loss: 0.0390 - val_tf.math.abs_186_loss: 0.0048\n",
      "Epoch 365/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 22.1056 - out_T_loss: 0.4563 - out_S_loss: 2.0508 - tf.math.abs_185_loss: 0.0411 - tf.math.abs_186_loss: 0.0159 - val_loss: 19.1293 - val_out_T_loss: 0.4757 - val_out_S_loss: 1.7772 - val_tf.math.abs_185_loss: 0.0393 - val_tf.math.abs_186_loss: 0.0048\n",
      "Epoch 366/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 22.0538 - out_T_loss: 0.3955 - out_S_loss: 2.0557 - tf.math.abs_185_loss: 0.0393 - tf.math.abs_186_loss: 0.0157 - val_loss: 19.0298 - val_out_T_loss: 0.4564 - val_out_S_loss: 1.7703 - val_tf.math.abs_185_loss: 0.0388 - val_tf.math.abs_186_loss: 0.0047\n",
      "Epoch 367/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.2825 - out_T_loss: 0.4091 - out_S_loss: 1.9769 - tf.math.abs_185_loss: 0.0400 - tf.math.abs_186_loss: 0.0153 - val_loss: 19.0059 - val_out_T_loss: 0.4469 - val_out_S_loss: 1.7676 - val_tf.math.abs_185_loss: 0.0394 - val_tf.math.abs_186_loss: 0.0047\n",
      "Epoch 368/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 21.4680 - out_T_loss: 0.4272 - out_S_loss: 1.9937 - tf.math.abs_185_loss: 0.0398 - tf.math.abs_186_loss: 0.0153 - val_loss: 18.9535 - val_out_T_loss: 0.4564 - val_out_S_loss: 1.7601 - val_tf.math.abs_185_loss: 0.0399 - val_tf.math.abs_186_loss: 0.0049\n",
      "Epoch 369/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.5034 - out_T_loss: 0.4256 - out_S_loss: 1.9973 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0151 - val_loss: 18.9219 - val_out_T_loss: 0.4593 - val_out_S_loss: 1.7589 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0048\n",
      "Epoch 370/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 21.7396 - out_T_loss: 0.4302 - out_S_loss: 2.0195 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0155 - val_loss: 18.8759 - val_out_T_loss: 0.4584 - val_out_S_loss: 1.7539 - val_tf.math.abs_185_loss: 0.0392 - val_tf.math.abs_186_loss: 0.0047\n",
      "Epoch 371/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.5656 - out_T_loss: 0.4263 - out_S_loss: 2.0039 - tf.math.abs_185_loss: 0.0400 - tf.math.abs_186_loss: 0.0150 - val_loss: 18.8270 - val_out_T_loss: 0.4542 - val_out_S_loss: 1.7501 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0047\n",
      "Epoch 372/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.5870 - out_T_loss: 0.4216 - out_S_loss: 2.0073 - tf.math.abs_185_loss: 0.0393 - tf.math.abs_186_loss: 0.0154 - val_loss: 18.7997 - val_out_T_loss: 0.4525 - val_out_S_loss: 1.7479 - val_tf.math.abs_185_loss: 0.0387 - val_tf.math.abs_186_loss: 0.0048\n",
      "Epoch 373/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.3526 - out_T_loss: 0.4139 - out_S_loss: 1.9828 - tf.math.abs_185_loss: 0.0405 - tf.math.abs_186_loss: 0.0150 - val_loss: 18.7746 - val_out_T_loss: 0.4489 - val_out_S_loss: 1.7456 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 374/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 21.2579 - out_T_loss: 0.4197 - out_S_loss: 1.9714 - tf.math.abs_185_loss: 0.0412 - tf.math.abs_186_loss: 0.0150 - val_loss: 18.8177 - val_out_T_loss: 0.4617 - val_out_S_loss: 1.7448 - val_tf.math.abs_185_loss: 0.0403 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 375/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 21.7060 - out_T_loss: 0.3979 - out_S_loss: 2.0218 - tf.math.abs_185_loss: 0.0391 - tf.math.abs_186_loss: 0.0154 - val_loss: 18.7001 - val_out_T_loss: 0.4480 - val_out_S_loss: 1.7396 - val_tf.math.abs_185_loss: 0.0381 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 376/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 21.0687 - out_T_loss: 0.4270 - out_S_loss: 1.9552 - tf.math.abs_185_loss: 0.0396 - tf.math.abs_186_loss: 0.0149 - val_loss: 18.6973 - val_out_T_loss: 0.4424 - val_out_S_loss: 1.7377 - val_tf.math.abs_185_loss: 0.0393 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 377/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.2432 - out_T_loss: 0.3978 - out_S_loss: 1.9761 - tf.math.abs_185_loss: 0.0391 - tf.math.abs_186_loss: 0.0152 - val_loss: 18.6552 - val_out_T_loss: 0.4475 - val_out_S_loss: 1.7351 - val_tf.math.abs_185_loss: 0.0382 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 378/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 21.1796 - out_T_loss: 0.3909 - out_S_loss: 1.9717 - tf.math.abs_185_loss: 0.0386 - tf.math.abs_186_loss: 0.0150 - val_loss: 18.5929 - val_out_T_loss: 0.4399 - val_out_S_loss: 1.7295 - val_tf.math.abs_185_loss: 0.0384 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 379/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.9128 - out_T_loss: 0.3820 - out_S_loss: 1.9465 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0148 - val_loss: 18.5330 - val_out_T_loss: 0.4409 - val_out_S_loss: 1.7244 - val_tf.math.abs_185_loss: 0.0379 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 380/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.5126 - out_T_loss: 0.3955 - out_S_loss: 2.0000 - tf.math.abs_185_loss: 0.0407 - tf.math.abs_186_loss: 0.0152 - val_loss: 18.5639 - val_out_T_loss: 0.4436 - val_out_S_loss: 1.7247 - val_tf.math.abs_185_loss: 0.0390 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 381/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.6171 - out_T_loss: 0.3894 - out_S_loss: 1.9151 - tf.math.abs_185_loss: 0.0386 - tf.math.abs_186_loss: 0.0153 - val_loss: 18.4534 - val_out_T_loss: 0.4446 - val_out_S_loss: 1.7155 - val_tf.math.abs_185_loss: 0.0382 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 382/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.9454 - out_T_loss: 0.3750 - out_S_loss: 1.9498 - tf.math.abs_185_loss: 0.0386 - tf.math.abs_186_loss: 0.0150 - val_loss: 18.4251 - val_out_T_loss: 0.4470 - val_out_S_loss: 1.7112 - val_tf.math.abs_185_loss: 0.0388 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 383/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 21.0035 - out_T_loss: 0.3950 - out_S_loss: 1.9508 - tf.math.abs_185_loss: 0.0402 - tf.math.abs_186_loss: 0.0148 - val_loss: 18.3934 - val_out_T_loss: 0.4414 - val_out_S_loss: 1.7098 - val_tf.math.abs_185_loss: 0.0381 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 384/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.9330 - out_T_loss: 0.3856 - out_S_loss: 1.9471 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0155 - val_loss: 18.3574 - val_out_T_loss: 0.4432 - val_out_S_loss: 1.7065 - val_tf.math.abs_185_loss: 0.0380 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 385/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.1883 - out_T_loss: 0.3715 - out_S_loss: 1.9747 - tf.math.abs_185_loss: 0.0381 - tf.math.abs_186_loss: 0.0154 - val_loss: 18.2981 - val_out_T_loss: 0.4365 - val_out_S_loss: 1.7016 - val_tf.math.abs_185_loss: 0.0378 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 386/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.1772 - out_T_loss: 0.3977 - out_S_loss: 1.9686 - tf.math.abs_185_loss: 0.0395 - tf.math.abs_186_loss: 0.0151 - val_loss: 18.2965 - val_out_T_loss: 0.4331 - val_out_S_loss: 1.7003 - val_tf.math.abs_185_loss: 0.0385 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 387/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.0087 - out_T_loss: 0.3831 - out_S_loss: 1.9542 - tf.math.abs_185_loss: 0.0393 - tf.math.abs_186_loss: 0.0149 - val_loss: 18.2622 - val_out_T_loss: 0.4419 - val_out_S_loss: 1.6980 - val_tf.math.abs_185_loss: 0.0375 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 388/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.7528 - out_T_loss: 0.3702 - out_S_loss: 1.9317 - tf.math.abs_185_loss: 0.0383 - tf.math.abs_186_loss: 0.0150 - val_loss: 18.2018 - val_out_T_loss: 0.4329 - val_out_S_loss: 1.6927 - val_tf.math.abs_185_loss: 0.0376 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 389/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 20.8021 - out_T_loss: 0.4022 - out_S_loss: 1.9323 - tf.math.abs_185_loss: 0.0393 - tf.math.abs_186_loss: 0.0145 - val_loss: 18.2402 - val_out_T_loss: 0.4385 - val_out_S_loss: 1.6920 - val_tf.math.abs_185_loss: 0.0396 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 390/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.6609 - out_T_loss: 0.3718 - out_S_loss: 1.9221 - tf.math.abs_185_loss: 0.0390 - tf.math.abs_186_loss: 0.0144 - val_loss: 18.1777 - val_out_T_loss: 0.4325 - val_out_S_loss: 1.6888 - val_tf.math.abs_185_loss: 0.0384 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 391/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 21.2310 - out_T_loss: 0.3923 - out_S_loss: 1.9758 - tf.math.abs_185_loss: 0.0395 - tf.math.abs_186_loss: 0.0146 - val_loss: 18.1701 - val_out_T_loss: 0.4393 - val_out_S_loss: 1.6855 - val_tf.math.abs_185_loss: 0.0387 - val_tf.math.abs_186_loss: 0.0051\n",
      "Epoch 392/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.6458 - out_T_loss: 0.3869 - out_S_loss: 1.9202 - tf.math.abs_185_loss: 0.0385 - tf.math.abs_186_loss: 0.0144 - val_loss: 18.1039 - val_out_T_loss: 0.4337 - val_out_S_loss: 1.6813 - val_tf.math.abs_185_loss: 0.0382 - val_tf.math.abs_186_loss: 0.0047\n",
      "Epoch 393/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.7848 - out_T_loss: 0.3643 - out_S_loss: 1.9366 - tf.math.abs_185_loss: 0.0379 - tf.math.abs_186_loss: 0.0148 - val_loss: 18.0858 - val_out_T_loss: 0.4275 - val_out_S_loss: 1.6819 - val_tf.math.abs_185_loss: 0.0375 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 394/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.9615 - out_T_loss: 0.3863 - out_S_loss: 1.9514 - tf.math.abs_185_loss: 0.0385 - tf.math.abs_186_loss: 0.0146 - val_loss: 18.0491 - val_out_T_loss: 0.4157 - val_out_S_loss: 1.6782 - val_tf.math.abs_185_loss: 0.0382 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 395/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 20.6949 - out_T_loss: 0.3832 - out_S_loss: 1.9262 - tf.math.abs_185_loss: 0.0377 - tf.math.abs_186_loss: 0.0148 - val_loss: 18.0233 - val_out_T_loss: 0.4192 - val_out_S_loss: 1.6747 - val_tf.math.abs_185_loss: 0.0384 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 396/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 20.4305 - out_T_loss: 0.3820 - out_S_loss: 1.9007 - tf.math.abs_185_loss: 0.0381 - tf.math.abs_186_loss: 0.0140 - val_loss: 17.9821 - val_out_T_loss: 0.4268 - val_out_S_loss: 1.6692 - val_tf.math.abs_185_loss: 0.0388 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 397/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.9114 - out_T_loss: 0.3664 - out_S_loss: 1.9472 - tf.math.abs_185_loss: 0.0389 - tf.math.abs_186_loss: 0.0148 - val_loss: 17.9202 - val_out_T_loss: 0.4141 - val_out_S_loss: 1.6645 - val_tf.math.abs_185_loss: 0.0386 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 398/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.4676 - out_T_loss: 0.3645 - out_S_loss: 1.9053 - tf.math.abs_185_loss: 0.0379 - tf.math.abs_186_loss: 0.0146 - val_loss: 17.8949 - val_out_T_loss: 0.4216 - val_out_S_loss: 1.6639 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 399/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 20.9737 - out_T_loss: 0.3901 - out_S_loss: 1.9499 - tf.math.abs_185_loss: 0.0391 - tf.math.abs_186_loss: 0.0152 - val_loss: 17.8717 - val_out_T_loss: 0.4317 - val_out_S_loss: 1.6600 - val_tf.math.abs_185_loss: 0.0376 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 400/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 20.6624 - out_T_loss: 0.3739 - out_S_loss: 1.9223 - tf.math.abs_185_loss: 0.0387 - tf.math.abs_186_loss: 0.0146 - val_loss: 17.8537 - val_out_T_loss: 0.4224 - val_out_S_loss: 1.6581 - val_tf.math.abs_185_loss: 0.0380 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 401/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 20.3196 - out_T_loss: 0.3664 - out_S_loss: 1.8901 - tf.math.abs_185_loss: 0.0378 - tf.math.abs_186_loss: 0.0148 - val_loss: 17.8080 - val_out_T_loss: 0.4168 - val_out_S_loss: 1.6548 - val_tf.math.abs_185_loss: 0.0378 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 402/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.3009 - out_T_loss: 0.3936 - out_S_loss: 1.8829 - tf.math.abs_185_loss: 0.0395 - tf.math.abs_186_loss: 0.0144 - val_loss: 17.7792 - val_out_T_loss: 0.4255 - val_out_S_loss: 1.6503 - val_tf.math.abs_185_loss: 0.0381 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 403/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.5863 - out_T_loss: 0.3958 - out_S_loss: 1.9090 - tf.math.abs_185_loss: 0.0403 - tf.math.abs_186_loss: 0.0147 - val_loss: 17.7555 - val_out_T_loss: 0.4411 - val_out_S_loss: 1.6462 - val_tf.math.abs_185_loss: 0.0382 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 404/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.4001 - out_T_loss: 0.3744 - out_S_loss: 1.8958 - tf.math.abs_185_loss: 0.0390 - tf.math.abs_186_loss: 0.0144 - val_loss: 17.7250 - val_out_T_loss: 0.4188 - val_out_S_loss: 1.6455 - val_tf.math.abs_185_loss: 0.0381 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 405/2000\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 20.5754 - out_T_loss: 0.3909 - out_S_loss: 1.9132 - tf.math.abs_185_loss: 0.0382 - tf.math.abs_186_loss: 0.0144 - val_loss: 17.7104 - val_out_T_loss: 0.4183 - val_out_S_loss: 1.6452 - val_tf.math.abs_185_loss: 0.0377 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 406/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.0026 - out_T_loss: 0.3523 - out_S_loss: 1.8602 - tf.math.abs_185_loss: 0.0383 - tf.math.abs_186_loss: 0.0142 - val_loss: 17.6596 - val_out_T_loss: 0.4204 - val_out_S_loss: 1.6384 - val_tf.math.abs_185_loss: 0.0385 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 407/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 20.6279 - out_T_loss: 0.3646 - out_S_loss: 1.9192 - tf.math.abs_185_loss: 0.0392 - tf.math.abs_186_loss: 0.0143 - val_loss: 17.6152 - val_out_T_loss: 0.4233 - val_out_S_loss: 1.6346 - val_tf.math.abs_185_loss: 0.0378 - val_tf.math.abs_186_loss: 0.0045\n",
      "Epoch 408/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 20.2129 - out_T_loss: 0.3828 - out_S_loss: 1.8772 - tf.math.abs_185_loss: 0.0381 - tf.math.abs_186_loss: 0.0148 - val_loss: 17.6434 - val_out_T_loss: 0.4257 - val_out_S_loss: 1.6349 - val_tf.math.abs_185_loss: 0.0389 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 409/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.1624 - out_T_loss: 0.3712 - out_S_loss: 1.8729 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0147 - val_loss: 17.5980 - val_out_T_loss: 0.4257 - val_out_S_loss: 1.6332 - val_tf.math.abs_185_loss: 0.0376 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 410/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.9426 - out_T_loss: 0.3731 - out_S_loss: 1.8516 - tf.math.abs_185_loss: 0.0387 - tf.math.abs_186_loss: 0.0140 - val_loss: 17.5720 - val_out_T_loss: 0.4200 - val_out_S_loss: 1.6261 - val_tf.math.abs_185_loss: 0.0400 - val_tf.math.abs_186_loss: 0.0046\n",
      "Epoch 411/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.1866 - out_T_loss: 0.3642 - out_S_loss: 1.8765 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0144 - val_loss: 17.4887 - val_out_T_loss: 0.4102 - val_out_S_loss: 1.6245 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 412/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.9225 - out_T_loss: 0.3394 - out_S_loss: 1.8549 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0141 - val_loss: 17.4408 - val_out_T_loss: 0.4019 - val_out_S_loss: 1.6208 - val_tf.math.abs_185_loss: 0.0373 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 413/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 20.2230 - out_T_loss: 0.3675 - out_S_loss: 1.8800 - tf.math.abs_185_loss: 0.0380 - tf.math.abs_186_loss: 0.0147 - val_loss: 17.4857 - val_out_T_loss: 0.4118 - val_out_S_loss: 1.6235 - val_tf.math.abs_185_loss: 0.0376 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 414/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.8993 - out_T_loss: 0.3512 - out_S_loss: 1.8509 - tf.math.abs_185_loss: 0.0374 - tf.math.abs_186_loss: 0.0146 - val_loss: 17.3686 - val_out_T_loss: 0.4067 - val_out_S_loss: 1.6133 - val_tf.math.abs_185_loss: 0.0373 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 415/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.9953 - out_T_loss: 0.3756 - out_S_loss: 1.8561 - tf.math.abs_185_loss: 0.0386 - tf.math.abs_186_loss: 0.0143 - val_loss: 17.3800 - val_out_T_loss: 0.4136 - val_out_S_loss: 1.6120 - val_tf.math.abs_185_loss: 0.0380 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 416/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 19.8892 - out_T_loss: 0.3466 - out_S_loss: 1.8500 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0138 - val_loss: 17.3554 - val_out_T_loss: 0.4140 - val_out_S_loss: 1.6112 - val_tf.math.abs_185_loss: 0.0373 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 417/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 20.3720 - out_T_loss: 0.3795 - out_S_loss: 1.8923 - tf.math.abs_185_loss: 0.0392 - tf.math.abs_186_loss: 0.0143 - val_loss: 17.3511 - val_out_T_loss: 0.4159 - val_out_S_loss: 1.6092 - val_tf.math.abs_185_loss: 0.0379 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 418/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 20.0745 - out_T_loss: 0.3701 - out_S_loss: 1.8682 - tf.math.abs_185_loss: 0.0370 - tf.math.abs_186_loss: 0.0142 - val_loss: 17.3570 - val_out_T_loss: 0.4136 - val_out_S_loss: 1.6109 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 419/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.7068 - out_T_loss: 0.3444 - out_S_loss: 1.8336 - tf.math.abs_185_loss: 0.0368 - tf.math.abs_186_loss: 0.0145 - val_loss: 17.2544 - val_out_T_loss: 0.4046 - val_out_S_loss: 1.6008 - val_tf.math.abs_185_loss: 0.0377 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 420/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.6203 - out_T_loss: 0.3330 - out_S_loss: 1.8254 - tf.math.abs_185_loss: 0.0373 - tf.math.abs_186_loss: 0.0144 - val_loss: 17.1887 - val_out_T_loss: 0.3987 - val_out_S_loss: 1.5971 - val_tf.math.abs_185_loss: 0.0369 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 421/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.9643 - out_T_loss: 0.3610 - out_S_loss: 1.8557 - tf.math.abs_185_loss: 0.0374 - tf.math.abs_186_loss: 0.0149 - val_loss: 17.1937 - val_out_T_loss: 0.4008 - val_out_S_loss: 1.5962 - val_tf.math.abs_185_loss: 0.0372 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 422/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.7493 - out_T_loss: 0.3610 - out_S_loss: 1.8350 - tf.math.abs_185_loss: 0.0380 - tf.math.abs_186_loss: 0.0139 - val_loss: 17.1849 - val_out_T_loss: 0.4004 - val_out_S_loss: 1.5953 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 423/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.9569 - out_T_loss: 0.3573 - out_S_loss: 1.8557 - tf.math.abs_185_loss: 0.0383 - tf.math.abs_186_loss: 0.0139 - val_loss: 17.2124 - val_out_T_loss: 0.4111 - val_out_S_loss: 1.5971 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 424/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.5678 - out_T_loss: 0.3518 - out_S_loss: 1.8182 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0141 - val_loss: 17.1540 - val_out_T_loss: 0.4133 - val_out_S_loss: 1.5885 - val_tf.math.abs_185_loss: 0.0385 - val_tf.math.abs_186_loss: 0.0043\n",
      "Epoch 425/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 20.0103 - out_T_loss: 0.3690 - out_S_loss: 1.8588 - tf.math.abs_185_loss: 0.0385 - tf.math.abs_186_loss: 0.0141 - val_loss: 17.1113 - val_out_T_loss: 0.4119 - val_out_S_loss: 1.5873 - val_tf.math.abs_185_loss: 0.0372 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 426/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.5341 - out_T_loss: 0.3591 - out_S_loss: 1.8124 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0142 - val_loss: 17.0847 - val_out_T_loss: 0.4211 - val_out_S_loss: 1.5830 - val_tf.math.abs_185_loss: 0.0376 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 427/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.4894 - out_T_loss: 0.3399 - out_S_loss: 1.8128 - tf.math.abs_185_loss: 0.0373 - tf.math.abs_186_loss: 0.0138 - val_loss: 17.0269 - val_out_T_loss: 0.4043 - val_out_S_loss: 1.5791 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 428/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.6690 - out_T_loss: 0.3481 - out_S_loss: 1.8275 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0147 - val_loss: 16.9957 - val_out_T_loss: 0.3977 - val_out_S_loss: 1.5781 - val_tf.math.abs_185_loss: 0.0368 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 429/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.4810 - out_T_loss: 0.3341 - out_S_loss: 1.8113 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0141 - val_loss: 16.9269 - val_out_T_loss: 0.4012 - val_out_S_loss: 1.5706 - val_tf.math.abs_185_loss: 0.0369 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 430/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.4703 - out_T_loss: 0.3369 - out_S_loss: 1.8090 - tf.math.abs_185_loss: 0.0379 - tf.math.abs_186_loss: 0.0143 - val_loss: 16.8701 - val_out_T_loss: 0.3883 - val_out_S_loss: 1.5653 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 431/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.6100 - out_T_loss: 0.3477 - out_S_loss: 1.8214 - tf.math.abs_185_loss: 0.0381 - tf.math.abs_186_loss: 0.0143 - val_loss: 16.8924 - val_out_T_loss: 0.3969 - val_out_S_loss: 1.5661 - val_tf.math.abs_185_loss: 0.0375 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 432/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.3328 - out_T_loss: 0.3357 - out_S_loss: 1.7978 - tf.math.abs_185_loss: 0.0373 - tf.math.abs_186_loss: 0.0136 - val_loss: 16.9113 - val_out_T_loss: 0.4090 - val_out_S_loss: 1.5682 - val_tf.math.abs_185_loss: 0.0368 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 433/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.3307 - out_T_loss: 0.3393 - out_S_loss: 1.7963 - tf.math.abs_185_loss: 0.0371 - tf.math.abs_186_loss: 0.0143 - val_loss: 16.8324 - val_out_T_loss: 0.4015 - val_out_S_loss: 1.5613 - val_tf.math.abs_185_loss: 0.0368 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 434/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.5513 - out_T_loss: 0.3438 - out_S_loss: 1.8151 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0144 - val_loss: 16.8290 - val_out_T_loss: 0.4121 - val_out_S_loss: 1.5594 - val_tf.math.abs_185_loss: 0.0370 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 435/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 19.8078 - out_T_loss: 0.3751 - out_S_loss: 1.8380 - tf.math.abs_185_loss: 0.0381 - tf.math.abs_186_loss: 0.0146 - val_loss: 16.8263 - val_out_T_loss: 0.4123 - val_out_S_loss: 1.5593 - val_tf.math.abs_185_loss: 0.0370 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 436/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.2020 - out_T_loss: 0.3316 - out_S_loss: 1.7852 - tf.math.abs_185_loss: 0.0373 - tf.math.abs_186_loss: 0.0136 - val_loss: 16.7664 - val_out_T_loss: 0.3908 - val_out_S_loss: 1.5553 - val_tf.math.abs_185_loss: 0.0371 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 437/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.8783 - out_T_loss: 0.3536 - out_S_loss: 1.8477 - tf.math.abs_185_loss: 0.0383 - tf.math.abs_186_loss: 0.0140 - val_loss: 16.7762 - val_out_T_loss: 0.4056 - val_out_S_loss: 1.5547 - val_tf.math.abs_185_loss: 0.0370 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 438/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.2365 - out_T_loss: 0.3418 - out_S_loss: 1.7872 - tf.math.abs_185_loss: 0.0372 - tf.math.abs_186_loss: 0.0139 - val_loss: 16.7012 - val_out_T_loss: 0.3976 - val_out_S_loss: 1.5468 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0044\n",
      "Epoch 439/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.5002 - out_T_loss: 0.3492 - out_S_loss: 1.8101 - tf.math.abs_185_loss: 0.0388 - tf.math.abs_186_loss: 0.0137 - val_loss: 16.6861 - val_out_T_loss: 0.3927 - val_out_S_loss: 1.5469 - val_tf.math.abs_185_loss: 0.0371 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 440/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.2942 - out_T_loss: 0.3224 - out_S_loss: 1.7927 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0146 - val_loss: 16.6630 - val_out_T_loss: 0.3818 - val_out_S_loss: 1.5424 - val_tf.math.abs_185_loss: 0.0388 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 441/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 19.2823 - out_T_loss: 0.3323 - out_S_loss: 1.7924 - tf.math.abs_185_loss: 0.0371 - tf.math.abs_186_loss: 0.0142 - val_loss: 16.6592 - val_out_T_loss: 0.3996 - val_out_S_loss: 1.5428 - val_tf.math.abs_185_loss: 0.0375 - val_tf.math.abs_186_loss: 0.0041\n",
      "Epoch 442/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 19.2872 - out_T_loss: 0.3533 - out_S_loss: 1.7879 - tf.math.abs_185_loss: 0.0387 - tf.math.abs_186_loss: 0.0140 - val_loss: 16.6427 - val_out_T_loss: 0.3840 - val_out_S_loss: 1.5430 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 443/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.9123 - out_T_loss: 0.3263 - out_S_loss: 1.7554 - tf.math.abs_185_loss: 0.0375 - tf.math.abs_186_loss: 0.0141 - val_loss: 16.5493 - val_out_T_loss: 0.3851 - val_out_S_loss: 1.5352 - val_tf.math.abs_185_loss: 0.0367 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 444/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.2168 - out_T_loss: 0.3320 - out_S_loss: 1.7873 - tf.math.abs_185_loss: 0.0367 - tf.math.abs_186_loss: 0.0139 - val_loss: 16.5535 - val_out_T_loss: 0.3943 - val_out_S_loss: 1.5354 - val_tf.math.abs_185_loss: 0.0364 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 445/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.9201 - out_T_loss: 0.3164 - out_S_loss: 1.7611 - tf.math.abs_185_loss: 0.0360 - tf.math.abs_186_loss: 0.0136 - val_loss: 16.4969 - val_out_T_loss: 0.3844 - val_out_S_loss: 1.5303 - val_tf.math.abs_185_loss: 0.0367 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 446/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 19.3805 - out_T_loss: 0.3385 - out_S_loss: 1.8021 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0135 - val_loss: 16.4664 - val_out_T_loss: 0.3814 - val_out_S_loss: 1.5277 - val_tf.math.abs_185_loss: 0.0364 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 447/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.3289 - out_T_loss: 0.3189 - out_S_loss: 1.7993 - tf.math.abs_185_loss: 0.0372 - tf.math.abs_186_loss: 0.0136 - val_loss: 16.4697 - val_out_T_loss: 0.3873 - val_out_S_loss: 1.5249 - val_tf.math.abs_185_loss: 0.0377 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 448/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.4785 - out_T_loss: 0.3296 - out_S_loss: 1.8111 - tf.math.abs_185_loss: 0.0380 - tf.math.abs_186_loss: 0.0139 - val_loss: 16.4626 - val_out_T_loss: 0.4035 - val_out_S_loss: 1.5228 - val_tf.math.abs_185_loss: 0.0376 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 449/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.9443 - out_T_loss: 0.3183 - out_S_loss: 1.7611 - tf.math.abs_185_loss: 0.0367 - tf.math.abs_186_loss: 0.0140 - val_loss: 16.4859 - val_out_T_loss: 0.3940 - val_out_S_loss: 1.5260 - val_tf.math.abs_185_loss: 0.0374 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 450/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.7430 - out_T_loss: 0.3169 - out_S_loss: 1.7402 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0136 - val_loss: 16.3702 - val_out_T_loss: 0.3802 - val_out_S_loss: 1.5155 - val_tf.math.abs_185_loss: 0.0379 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 451/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 19.2976 - out_T_loss: 0.3171 - out_S_loss: 1.7965 - tf.math.abs_185_loss: 0.0368 - tf.math.abs_186_loss: 0.0139 - val_loss: 16.3860 - val_out_T_loss: 0.3868 - val_out_S_loss: 1.5180 - val_tf.math.abs_185_loss: 0.0371 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 452/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 18.7692 - out_T_loss: 0.3187 - out_S_loss: 1.7444 - tf.math.abs_185_loss: 0.0365 - tf.math.abs_186_loss: 0.0139 - val_loss: 16.3566 - val_out_T_loss: 0.3903 - val_out_S_loss: 1.5169 - val_tf.math.abs_185_loss: 0.0360 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 453/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.2450 - out_T_loss: 0.3294 - out_S_loss: 1.7891 - tf.math.abs_185_loss: 0.0367 - tf.math.abs_186_loss: 0.0145 - val_loss: 16.3994 - val_out_T_loss: 0.3767 - val_out_S_loss: 1.5201 - val_tf.math.abs_185_loss: 0.0372 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 454/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 18.6567 - out_T_loss: 0.3193 - out_S_loss: 1.7322 - tf.math.abs_185_loss: 0.0373 - tf.math.abs_186_loss: 0.0134 - val_loss: 16.2694 - val_out_T_loss: 0.3873 - val_out_S_loss: 1.5072 - val_tf.math.abs_185_loss: 0.0366 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 455/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 19.2384 - out_T_loss: 0.3223 - out_S_loss: 1.7883 - tf.math.abs_185_loss: 0.0373 - tf.math.abs_186_loss: 0.0144 - val_loss: 16.2488 - val_out_T_loss: 0.3811 - val_out_S_loss: 1.5046 - val_tf.math.abs_185_loss: 0.0372 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 456/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 18.5608 - out_T_loss: 0.3365 - out_S_loss: 1.7199 - tf.math.abs_185_loss: 0.0380 - tf.math.abs_186_loss: 0.0133 - val_loss: 16.2120 - val_out_T_loss: 0.3859 - val_out_S_loss: 1.4989 - val_tf.math.abs_185_loss: 0.0377 - val_tf.math.abs_186_loss: 0.0042\n",
      "Epoch 457/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.1882 - out_T_loss: 0.3564 - out_S_loss: 1.7790 - tf.math.abs_185_loss: 0.0387 - tf.math.abs_186_loss: 0.0134 - val_loss: 16.1922 - val_out_T_loss: 0.3838 - val_out_S_loss: 1.4975 - val_tf.math.abs_185_loss: 0.0378 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 458/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 18.9498 - out_T_loss: 0.3522 - out_S_loss: 1.7564 - tf.math.abs_185_loss: 0.0378 - tf.math.abs_186_loss: 0.0138 - val_loss: 16.2252 - val_out_T_loss: 0.3935 - val_out_S_loss: 1.5025 - val_tf.math.abs_185_loss: 0.0363 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 459/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 18.6919 - out_T_loss: 0.3158 - out_S_loss: 1.7368 - tf.math.abs_185_loss: 0.0368 - tf.math.abs_186_loss: 0.0136 - val_loss: 16.0868 - val_out_T_loss: 0.3737 - val_out_S_loss: 1.4928 - val_tf.math.abs_185_loss: 0.0355 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 460/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.2108 - out_T_loss: 0.3172 - out_S_loss: 1.7885 - tf.math.abs_185_loss: 0.0366 - tf.math.abs_186_loss: 0.0138 - val_loss: 16.1453 - val_out_T_loss: 0.3764 - val_out_S_loss: 1.4945 - val_tf.math.abs_185_loss: 0.0372 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 461/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.7012 - out_T_loss: 0.3266 - out_S_loss: 1.7380 - tf.math.abs_185_loss: 0.0363 - tf.math.abs_186_loss: 0.0134 - val_loss: 16.0898 - val_out_T_loss: 0.3841 - val_out_S_loss: 1.4917 - val_tf.math.abs_185_loss: 0.0356 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 462/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.8148 - out_T_loss: 0.3113 - out_S_loss: 1.7477 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0137 - val_loss: 16.0752 - val_out_T_loss: 0.3710 - val_out_S_loss: 1.4895 - val_tf.math.abs_185_loss: 0.0366 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 463/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 19.0071 - out_T_loss: 0.3162 - out_S_loss: 1.7665 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0137 - val_loss: 16.0801 - val_out_T_loss: 0.3892 - val_out_S_loss: 1.4903 - val_tf.math.abs_185_loss: 0.0355 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 464/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 18.6197 - out_T_loss: 0.3084 - out_S_loss: 1.7311 - tf.math.abs_185_loss: 0.0364 - tf.math.abs_186_loss: 0.0137 - val_loss: 16.0241 - val_out_T_loss: 0.3747 - val_out_S_loss: 1.4845 - val_tf.math.abs_185_loss: 0.0365 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 465/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.8434 - out_T_loss: 0.2947 - out_S_loss: 1.7528 - tf.math.abs_185_loss: 0.0375 - tf.math.abs_186_loss: 0.0136 - val_loss: 15.9889 - val_out_T_loss: 0.3865 - val_out_S_loss: 1.4803 - val_tf.math.abs_185_loss: 0.0360 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 466/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 19.1034 - out_T_loss: 0.3264 - out_S_loss: 1.7742 - tf.math.abs_185_loss: 0.0382 - tf.math.abs_186_loss: 0.0135 - val_loss: 16.0482 - val_out_T_loss: 0.3804 - val_out_S_loss: 1.4823 - val_tf.math.abs_185_loss: 0.0383 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 467/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.5470 - out_T_loss: 0.3166 - out_S_loss: 1.7213 - tf.math.abs_185_loss: 0.0372 - tf.math.abs_186_loss: 0.0137 - val_loss: 15.9363 - val_out_T_loss: 0.3677 - val_out_S_loss: 1.4780 - val_tf.math.abs_185_loss: 0.0356 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 468/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.5629 - out_T_loss: 0.2949 - out_S_loss: 1.7257 - tf.math.abs_185_loss: 0.0367 - tf.math.abs_186_loss: 0.0138 - val_loss: 15.9249 - val_out_T_loss: 0.3688 - val_out_S_loss: 1.4772 - val_tf.math.abs_185_loss: 0.0354 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 469/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.5448 - out_T_loss: 0.3144 - out_S_loss: 1.7219 - tf.math.abs_185_loss: 0.0370 - tf.math.abs_186_loss: 0.0136 - val_loss: 15.8576 - val_out_T_loss: 0.3627 - val_out_S_loss: 1.4711 - val_tf.math.abs_185_loss: 0.0354 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 470/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.8736 - out_T_loss: 0.3047 - out_S_loss: 1.7536 - tf.math.abs_185_loss: 0.0376 - tf.math.abs_186_loss: 0.0141 - val_loss: 15.8915 - val_out_T_loss: 0.3762 - val_out_S_loss: 1.4702 - val_tf.math.abs_185_loss: 0.0368 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 471/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 18.4750 - out_T_loss: 0.3120 - out_S_loss: 1.7177 - tf.math.abs_185_loss: 0.0358 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.8365 - val_out_T_loss: 0.3751 - val_out_S_loss: 1.4669 - val_tf.math.abs_185_loss: 0.0359 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 472/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 18.2011 - out_T_loss: 0.3067 - out_S_loss: 1.6908 - tf.math.abs_185_loss: 0.0361 - tf.math.abs_186_loss: 0.0133 - val_loss: 15.7981 - val_out_T_loss: 0.3731 - val_out_S_loss: 1.4644 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 473/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.3497 - out_T_loss: 0.3028 - out_S_loss: 1.7065 - tf.math.abs_185_loss: 0.0356 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.7788 - val_out_T_loss: 0.3725 - val_out_S_loss: 1.4614 - val_tf.math.abs_185_loss: 0.0357 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 474/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.1785 - out_T_loss: 0.3110 - out_S_loss: 1.6880 - tf.math.abs_185_loss: 0.0360 - tf.math.abs_186_loss: 0.0134 - val_loss: 15.7568 - val_out_T_loss: 0.3643 - val_out_S_loss: 1.4613 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 475/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 18.1303 - out_T_loss: 0.3050 - out_S_loss: 1.6836 - tf.math.abs_185_loss: 0.0365 - tf.math.abs_186_loss: 0.0130 - val_loss: 15.7636 - val_out_T_loss: 0.3701 - val_out_S_loss: 1.4614 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 476/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 18.2705 - out_T_loss: 0.2978 - out_S_loss: 1.6982 - tf.math.abs_185_loss: 0.0361 - tf.math.abs_186_loss: 0.0134 - val_loss: 15.7173 - val_out_T_loss: 0.3664 - val_out_S_loss: 1.4564 - val_tf.math.abs_185_loss: 0.0356 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 477/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.3133 - out_T_loss: 0.3187 - out_S_loss: 1.6980 - tf.math.abs_185_loss: 0.0373 - tf.math.abs_186_loss: 0.0134 - val_loss: 15.7225 - val_out_T_loss: 0.3776 - val_out_S_loss: 1.4526 - val_tf.math.abs_185_loss: 0.0369 - val_tf.math.abs_186_loss: 0.0040\n",
      "Epoch 478/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 18.2299 - out_T_loss: 0.2880 - out_S_loss: 1.6943 - tf.math.abs_185_loss: 0.0361 - tf.math.abs_186_loss: 0.0139 - val_loss: 15.5967 - val_out_T_loss: 0.3632 - val_out_S_loss: 1.4458 - val_tf.math.abs_185_loss: 0.0350 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 479/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.9800 - out_T_loss: 0.2913 - out_S_loss: 1.6708 - tf.math.abs_185_loss: 0.0355 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.5920 - val_out_T_loss: 0.3694 - val_out_S_loss: 1.4445 - val_tf.math.abs_185_loss: 0.0351 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 480/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.0619 - out_T_loss: 0.3027 - out_S_loss: 1.6760 - tf.math.abs_185_loss: 0.0369 - tf.math.abs_186_loss: 0.0131 - val_loss: 15.6413 - val_out_T_loss: 0.3768 - val_out_S_loss: 1.4456 - val_tf.math.abs_185_loss: 0.0365 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 481/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.3302 - out_T_loss: 0.3406 - out_S_loss: 1.6954 - tf.math.abs_185_loss: 0.0385 - tf.math.abs_186_loss: 0.0133 - val_loss: 15.6496 - val_out_T_loss: 0.3827 - val_out_S_loss: 1.4433 - val_tf.math.abs_185_loss: 0.0378 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 482/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.1256 - out_T_loss: 0.3069 - out_S_loss: 1.6812 - tf.math.abs_185_loss: 0.0368 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.5612 - val_out_T_loss: 0.3855 - val_out_S_loss: 1.4401 - val_tf.math.abs_185_loss: 0.0349 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 483/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 17.8841 - out_T_loss: 0.2912 - out_S_loss: 1.6594 - tf.math.abs_185_loss: 0.0366 - tf.math.abs_186_loss: 0.0134 - val_loss: 15.5022 - val_out_T_loss: 0.3632 - val_out_S_loss: 1.4356 - val_tf.math.abs_185_loss: 0.0355 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 484/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.0579 - out_T_loss: 0.2996 - out_S_loss: 1.6757 - tf.math.abs_185_loss: 0.0364 - tf.math.abs_186_loss: 0.0137 - val_loss: 15.5495 - val_out_T_loss: 0.3713 - val_out_S_loss: 1.4381 - val_tf.math.abs_185_loss: 0.0361 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 485/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 18.7501 - out_T_loss: 0.3150 - out_S_loss: 1.7400 - tf.math.abs_185_loss: 0.0384 - tf.math.abs_186_loss: 0.0133 - val_loss: 15.5185 - val_out_T_loss: 0.3680 - val_out_S_loss: 1.4358 - val_tf.math.abs_185_loss: 0.0358 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 486/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 18.2375 - out_T_loss: 0.3071 - out_S_loss: 1.6937 - tf.math.abs_185_loss: 0.0362 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.4842 - val_out_T_loss: 0.3676 - val_out_S_loss: 1.4303 - val_tf.math.abs_185_loss: 0.0369 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 487/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.1683 - out_T_loss: 0.2976 - out_S_loss: 1.6880 - tf.math.abs_185_loss: 0.0362 - tf.math.abs_186_loss: 0.0134 - val_loss: 15.4357 - val_out_T_loss: 0.3646 - val_out_S_loss: 1.4293 - val_tf.math.abs_185_loss: 0.0352 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 488/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 18.3112 - out_T_loss: 0.3053 - out_S_loss: 1.7003 - tf.math.abs_185_loss: 0.0367 - tf.math.abs_186_loss: 0.0134 - val_loss: 15.4501 - val_out_T_loss: 0.3765 - val_out_S_loss: 1.4297 - val_tf.math.abs_185_loss: 0.0351 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 489/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.0087 - out_T_loss: 0.3028 - out_S_loss: 1.6722 - tf.math.abs_185_loss: 0.0357 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.4211 - val_out_T_loss: 0.3709 - val_out_S_loss: 1.4271 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 490/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.8248 - out_T_loss: 0.2930 - out_S_loss: 1.6546 - tf.math.abs_185_loss: 0.0360 - tf.math.abs_186_loss: 0.0132 - val_loss: 15.3721 - val_out_T_loss: 0.3655 - val_out_S_loss: 1.4233 - val_tf.math.abs_185_loss: 0.0350 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 491/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.8242 - out_T_loss: 0.2839 - out_S_loss: 1.6571 - tf.math.abs_185_loss: 0.0352 - tf.math.abs_186_loss: 0.0133 - val_loss: 15.3708 - val_out_T_loss: 0.3645 - val_out_S_loss: 1.4226 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 492/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 18.0574 - out_T_loss: 0.2922 - out_S_loss: 1.6800 - tf.math.abs_185_loss: 0.0348 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.3721 - val_out_T_loss: 0.3646 - val_out_S_loss: 1.4250 - val_tf.math.abs_185_loss: 0.0342 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 493/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 18.3576 - out_T_loss: 0.2875 - out_S_loss: 1.7052 - tf.math.abs_185_loss: 0.0373 - tf.math.abs_186_loss: 0.0136 - val_loss: 15.3092 - val_out_T_loss: 0.3641 - val_out_S_loss: 1.4158 - val_tf.math.abs_185_loss: 0.0356 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 494/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.1971 - out_T_loss: 0.3020 - out_S_loss: 1.6887 - tf.math.abs_185_loss: 0.0372 - tf.math.abs_186_loss: 0.0132 - val_loss: 15.3148 - val_out_T_loss: 0.3750 - val_out_S_loss: 1.4167 - val_tf.math.abs_185_loss: 0.0350 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 495/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.9521 - out_T_loss: 0.2862 - out_S_loss: 1.6678 - tf.math.abs_185_loss: 0.0359 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.2906 - val_out_T_loss: 0.3656 - val_out_S_loss: 1.4147 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 496/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 17.8488 - out_T_loss: 0.2965 - out_S_loss: 1.6580 - tf.math.abs_185_loss: 0.0351 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.2258 - val_out_T_loss: 0.3592 - val_out_S_loss: 1.4109 - val_tf.math.abs_185_loss: 0.0343 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 497/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 18.2261 - out_T_loss: 0.3134 - out_S_loss: 1.6950 - tf.math.abs_185_loss: 0.0351 - tf.math.abs_186_loss: 0.0131 - val_loss: 15.2003 - val_out_T_loss: 0.3562 - val_out_S_loss: 1.4065 - val_tf.math.abs_185_loss: 0.0352 - val_tf.math.abs_186_loss: 0.0038\n",
      "Epoch 498/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 17.9706 - out_T_loss: 0.3126 - out_S_loss: 1.6697 - tf.math.abs_185_loss: 0.0348 - tf.math.abs_186_loss: 0.0133 - val_loss: 15.2238 - val_out_T_loss: 0.3729 - val_out_S_loss: 1.4058 - val_tf.math.abs_185_loss: 0.0358 - val_tf.math.abs_186_loss: 0.0039\n",
      "Epoch 499/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 17.8466 - out_T_loss: 0.2970 - out_S_loss: 1.6569 - tf.math.abs_185_loss: 0.0359 - tf.math.abs_186_loss: 0.0132 - val_loss: 15.1878 - val_out_T_loss: 0.3626 - val_out_S_loss: 1.4047 - val_tf.math.abs_185_loss: 0.0353 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 500/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.9482 - out_T_loss: 0.2921 - out_S_loss: 1.6666 - tf.math.abs_185_loss: 0.0365 - tf.math.abs_186_loss: 0.0130 - val_loss: 15.1638 - val_out_T_loss: 0.3655 - val_out_S_loss: 1.4036 - val_tf.math.abs_185_loss: 0.0345 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 501/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.7839 - out_T_loss: 0.2843 - out_S_loss: 1.6527 - tf.math.abs_185_loss: 0.0357 - tf.math.abs_186_loss: 0.0130 - val_loss: 15.1736 - val_out_T_loss: 0.3663 - val_out_S_loss: 1.4033 - val_tf.math.abs_185_loss: 0.0351 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 502/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.0067 - out_T_loss: 0.2889 - out_S_loss: 1.6760 - tf.math.abs_185_loss: 0.0343 - tf.math.abs_186_loss: 0.0135 - val_loss: 15.0806 - val_out_T_loss: 0.3543 - val_out_S_loss: 1.3979 - val_tf.math.abs_185_loss: 0.0339 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 503/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 18.1425 - out_T_loss: 0.2802 - out_S_loss: 1.6882 - tf.math.abs_185_loss: 0.0357 - tf.math.abs_186_loss: 0.0133 - val_loss: 15.0432 - val_out_T_loss: 0.3529 - val_out_S_loss: 1.3936 - val_tf.math.abs_185_loss: 0.0343 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 504/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 17.4771 - out_T_loss: 0.2899 - out_S_loss: 1.6229 - tf.math.abs_185_loss: 0.0349 - tf.math.abs_186_loss: 0.0131 - val_loss: 15.0358 - val_out_T_loss: 0.3603 - val_out_S_loss: 1.3924 - val_tf.math.abs_185_loss: 0.0340 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 505/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 17.7439 - out_T_loss: 0.3023 - out_S_loss: 1.6442 - tf.math.abs_185_loss: 0.0368 - tf.math.abs_186_loss: 0.0132 - val_loss: 15.0395 - val_out_T_loss: 0.3687 - val_out_S_loss: 1.3906 - val_tf.math.abs_185_loss: 0.0346 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 506/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 17.7274 - out_T_loss: 0.2941 - out_S_loss: 1.6448 - tf.math.abs_185_loss: 0.0364 - tf.math.abs_186_loss: 0.0128 - val_loss: 15.0136 - val_out_T_loss: 0.3631 - val_out_S_loss: 1.3890 - val_tf.math.abs_185_loss: 0.0343 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 507/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.4171 - out_T_loss: 0.2783 - out_S_loss: 1.6189 - tf.math.abs_185_loss: 0.0347 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.9243 - val_out_T_loss: 0.3543 - val_out_S_loss: 1.3813 - val_tf.math.abs_185_loss: 0.0343 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 508/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.7189 - out_T_loss: 0.2884 - out_S_loss: 1.6467 - tf.math.abs_185_loss: 0.0350 - tf.math.abs_186_loss: 0.0132 - val_loss: 14.9268 - val_out_T_loss: 0.3476 - val_out_S_loss: 1.3820 - val_tf.math.abs_185_loss: 0.0343 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 509/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.1250 - out_T_loss: 0.2774 - out_S_loss: 1.5880 - tf.math.abs_185_loss: 0.0357 - tf.math.abs_186_loss: 0.0127 - val_loss: 14.8892 - val_out_T_loss: 0.3594 - val_out_S_loss: 1.3794 - val_tf.math.abs_185_loss: 0.0334 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 510/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.3999 - out_T_loss: 0.2824 - out_S_loss: 1.6147 - tf.math.abs_185_loss: 0.0356 - tf.math.abs_186_loss: 0.0130 - val_loss: 14.8880 - val_out_T_loss: 0.3564 - val_out_S_loss: 1.3795 - val_tf.math.abs_185_loss: 0.0334 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 511/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.8232 - out_T_loss: 0.2691 - out_S_loss: 1.6584 - tf.math.abs_185_loss: 0.0354 - tf.math.abs_186_loss: 0.0132 - val_loss: 14.8439 - val_out_T_loss: 0.3653 - val_out_S_loss: 1.3737 - val_tf.math.abs_185_loss: 0.0336 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 512/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 17.5513 - out_T_loss: 0.2829 - out_S_loss: 1.6305 - tf.math.abs_185_loss: 0.0352 - tf.math.abs_186_loss: 0.0130 - val_loss: 14.8793 - val_out_T_loss: 0.3608 - val_out_S_loss: 1.3733 - val_tf.math.abs_185_loss: 0.0357 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 513/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.3628 - out_T_loss: 0.2752 - out_S_loss: 1.6135 - tf.math.abs_185_loss: 0.0348 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.8625 - val_out_T_loss: 0.3519 - val_out_S_loss: 1.3729 - val_tf.math.abs_185_loss: 0.0356 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 514/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.1929 - out_T_loss: 0.2700 - out_S_loss: 1.5972 - tf.math.abs_185_loss: 0.0347 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.8367 - val_out_T_loss: 0.3522 - val_out_S_loss: 1.3717 - val_tf.math.abs_185_loss: 0.0349 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 515/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.0772 - out_T_loss: 0.2814 - out_S_loss: 1.5842 - tf.math.abs_185_loss: 0.0351 - tf.math.abs_186_loss: 0.0126 - val_loss: 14.8180 - val_out_T_loss: 0.3586 - val_out_S_loss: 1.3699 - val_tf.math.abs_185_loss: 0.0344 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 516/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.6690 - out_T_loss: 0.2675 - out_S_loss: 1.6449 - tf.math.abs_185_loss: 0.0344 - tf.math.abs_186_loss: 0.0132 - val_loss: 14.7451 - val_out_T_loss: 0.3545 - val_out_S_loss: 1.3644 - val_tf.math.abs_185_loss: 0.0338 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 517/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 17.6803 - out_T_loss: 0.2749 - out_S_loss: 1.6431 - tf.math.abs_185_loss: 0.0356 - tf.math.abs_186_loss: 0.0132 - val_loss: 14.7422 - val_out_T_loss: 0.3537 - val_out_S_loss: 1.3638 - val_tf.math.abs_185_loss: 0.0340 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 518/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.0086 - out_T_loss: 0.2716 - out_S_loss: 1.5773 - tf.math.abs_185_loss: 0.0354 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.7213 - val_out_T_loss: 0.3535 - val_out_S_loss: 1.3610 - val_tf.math.abs_185_loss: 0.0344 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 519/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.1756 - out_T_loss: 0.2662 - out_S_loss: 1.5958 - tf.math.abs_185_loss: 0.0345 - tf.math.abs_186_loss: 0.0130 - val_loss: 14.7176 - val_out_T_loss: 0.3498 - val_out_S_loss: 1.3618 - val_tf.math.abs_185_loss: 0.0341 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 520/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.6308 - out_T_loss: 0.2964 - out_S_loss: 1.6338 - tf.math.abs_185_loss: 0.0369 - tf.math.abs_186_loss: 0.0129 - val_loss: 14.7111 - val_out_T_loss: 0.3571 - val_out_S_loss: 1.3618 - val_tf.math.abs_185_loss: 0.0333 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 521/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.4916 - out_T_loss: 0.3019 - out_S_loss: 1.6211 - tf.math.abs_185_loss: 0.0361 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.7496 - val_out_T_loss: 0.3625 - val_out_S_loss: 1.3610 - val_tf.math.abs_185_loss: 0.0352 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 522/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.7564 - out_T_loss: 0.2954 - out_S_loss: 1.5524 - tf.math.abs_185_loss: 0.0344 - tf.math.abs_186_loss: 0.0124 - val_loss: 14.6410 - val_out_T_loss: 0.3658 - val_out_S_loss: 1.3520 - val_tf.math.abs_185_loss: 0.0341 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 523/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 17.2678 - out_T_loss: 0.2723 - out_S_loss: 1.6054 - tf.math.abs_185_loss: 0.0345 - tf.math.abs_186_loss: 0.0126 - val_loss: 14.5778 - val_out_T_loss: 0.3546 - val_out_S_loss: 1.3492 - val_tf.math.abs_185_loss: 0.0331 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 524/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 17.2066 - out_T_loss: 0.2703 - out_S_loss: 1.5990 - tf.math.abs_185_loss: 0.0345 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.5892 - val_out_T_loss: 0.3528 - val_out_S_loss: 1.3480 - val_tf.math.abs_185_loss: 0.0344 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 525/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.3414 - out_T_loss: 0.2788 - out_S_loss: 1.6070 - tf.math.abs_185_loss: 0.0367 - tf.math.abs_186_loss: 0.0130 - val_loss: 14.5604 - val_out_T_loss: 0.3505 - val_out_S_loss: 1.3454 - val_tf.math.abs_185_loss: 0.0344 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 526/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.9573 - out_T_loss: 0.2631 - out_S_loss: 1.5750 - tf.math.abs_185_loss: 0.0346 - tf.math.abs_186_loss: 0.0126 - val_loss: 14.5278 - val_out_T_loss: 0.3470 - val_out_S_loss: 1.3448 - val_tf.math.abs_185_loss: 0.0333 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 527/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.2987 - out_T_loss: 0.2722 - out_S_loss: 1.6086 - tf.math.abs_185_loss: 0.0341 - tf.math.abs_186_loss: 0.0130 - val_loss: 14.5199 - val_out_T_loss: 0.3430 - val_out_S_loss: 1.3439 - val_tf.math.abs_185_loss: 0.0333 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 528/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 17.4659 - out_T_loss: 0.2864 - out_S_loss: 1.6212 - tf.math.abs_185_loss: 0.0354 - tf.math.abs_186_loss: 0.0129 - val_loss: 14.5431 - val_out_T_loss: 0.3643 - val_out_S_loss: 1.3418 - val_tf.math.abs_185_loss: 0.0345 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 529/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.0711 - out_T_loss: 0.2655 - out_S_loss: 1.5856 - tf.math.abs_185_loss: 0.0349 - tf.math.abs_186_loss: 0.0126 - val_loss: 14.5204 - val_out_T_loss: 0.3545 - val_out_S_loss: 1.3420 - val_tf.math.abs_185_loss: 0.0336 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 530/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 17.9719 - out_T_loss: 0.2997 - out_S_loss: 1.6691 - tf.math.abs_185_loss: 0.0363 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.5346 - val_out_T_loss: 0.3621 - val_out_S_loss: 1.3421 - val_tf.math.abs_185_loss: 0.0342 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 531/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.7451 - out_T_loss: 0.2650 - out_S_loss: 1.5557 - tf.math.abs_185_loss: 0.0337 - tf.math.abs_186_loss: 0.0124 - val_loss: 14.4493 - val_out_T_loss: 0.3524 - val_out_S_loss: 1.3366 - val_tf.math.abs_185_loss: 0.0331 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 532/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.0294 - out_T_loss: 0.2784 - out_S_loss: 1.5813 - tf.math.abs_185_loss: 0.0341 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.4829 - val_out_T_loss: 0.3525 - val_out_S_loss: 1.3364 - val_tf.math.abs_185_loss: 0.0348 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 533/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.8735 - out_T_loss: 0.2686 - out_S_loss: 1.5700 - tf.math.abs_185_loss: 0.0329 - tf.math.abs_186_loss: 0.0124 - val_loss: 14.4219 - val_out_T_loss: 0.3447 - val_out_S_loss: 1.3351 - val_tf.math.abs_185_loss: 0.0329 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 534/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 17.3503 - out_T_loss: 0.2637 - out_S_loss: 1.6148 - tf.math.abs_185_loss: 0.0340 - tf.math.abs_186_loss: 0.0130 - val_loss: 14.3968 - val_out_T_loss: 0.3587 - val_out_S_loss: 1.3292 - val_tf.math.abs_185_loss: 0.0339 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 535/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.9440 - out_T_loss: 0.2660 - out_S_loss: 1.5752 - tf.math.abs_185_loss: 0.0340 - tf.math.abs_186_loss: 0.0122 - val_loss: 14.3738 - val_out_T_loss: 0.3495 - val_out_S_loss: 1.3286 - val_tf.math.abs_185_loss: 0.0335 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 536/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 16.9799 - out_T_loss: 0.2634 - out_S_loss: 1.5790 - tf.math.abs_185_loss: 0.0334 - tf.math.abs_186_loss: 0.0129 - val_loss: 14.3291 - val_out_T_loss: 0.3440 - val_out_S_loss: 1.3252 - val_tf.math.abs_185_loss: 0.0332 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 537/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 17.1030 - out_T_loss: 0.2573 - out_S_loss: 1.5889 - tf.math.abs_185_loss: 0.0351 - tf.math.abs_186_loss: 0.0127 - val_loss: 14.3346 - val_out_T_loss: 0.3604 - val_out_S_loss: 1.3238 - val_tf.math.abs_185_loss: 0.0334 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 538/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.9099 - out_T_loss: 0.2659 - out_S_loss: 1.5692 - tf.math.abs_185_loss: 0.0345 - tf.math.abs_186_loss: 0.0131 - val_loss: 14.3213 - val_out_T_loss: 0.3571 - val_out_S_loss: 1.3238 - val_tf.math.abs_185_loss: 0.0330 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 539/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 17.1409 - out_T_loss: 0.2487 - out_S_loss: 1.5969 - tf.math.abs_185_loss: 0.0333 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.2787 - val_out_T_loss: 0.3525 - val_out_S_loss: 1.3211 - val_tf.math.abs_185_loss: 0.0324 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 540/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 16.7605 - out_T_loss: 0.2560 - out_S_loss: 1.5576 - tf.math.abs_185_loss: 0.0338 - tf.math.abs_186_loss: 0.0126 - val_loss: 14.2753 - val_out_T_loss: 0.3391 - val_out_S_loss: 1.3200 - val_tf.math.abs_185_loss: 0.0334 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 541/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 17.2138 - out_T_loss: 0.2667 - out_S_loss: 1.6019 - tf.math.abs_185_loss: 0.0333 - tf.math.abs_186_loss: 0.0131 - val_loss: 14.2580 - val_out_T_loss: 0.3334 - val_out_S_loss: 1.3176 - val_tf.math.abs_185_loss: 0.0341 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 542/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.4577 - out_T_loss: 0.2463 - out_S_loss: 1.5297 - tf.math.abs_185_loss: 0.0330 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.2272 - val_out_T_loss: 0.3433 - val_out_S_loss: 1.3175 - val_tf.math.abs_185_loss: 0.0321 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 543/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.9000 - out_T_loss: 0.2626 - out_S_loss: 1.5716 - tf.math.abs_185_loss: 0.0334 - tf.math.abs_186_loss: 0.0126 - val_loss: 14.2135 - val_out_T_loss: 0.3405 - val_out_S_loss: 1.3143 - val_tf.math.abs_185_loss: 0.0330 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 544/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 17.3151 - out_T_loss: 0.2635 - out_S_loss: 1.6120 - tf.math.abs_185_loss: 0.0338 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.2193 - val_out_T_loss: 0.3422 - val_out_S_loss: 1.3153 - val_tf.math.abs_185_loss: 0.0329 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 545/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 17.0526 - out_T_loss: 0.2611 - out_S_loss: 1.5869 - tf.math.abs_185_loss: 0.0333 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.1925 - val_out_T_loss: 0.3472 - val_out_S_loss: 1.3124 - val_tf.math.abs_185_loss: 0.0326 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 546/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.9440 - out_T_loss: 0.2553 - out_S_loss: 1.5737 - tf.math.abs_185_loss: 0.0342 - tf.math.abs_186_loss: 0.0134 - val_loss: 14.1770 - val_out_T_loss: 0.3386 - val_out_S_loss: 1.3081 - val_tf.math.abs_185_loss: 0.0344 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 547/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 17.1260 - out_T_loss: 0.2620 - out_S_loss: 1.5942 - tf.math.abs_185_loss: 0.0334 - tf.math.abs_186_loss: 0.0127 - val_loss: 14.1102 - val_out_T_loss: 0.3401 - val_out_S_loss: 1.3035 - val_tf.math.abs_185_loss: 0.0334 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 548/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.0652 - out_T_loss: 0.2662 - out_S_loss: 1.5871 - tf.math.abs_185_loss: 0.0338 - tf.math.abs_186_loss: 0.0126 - val_loss: 14.1955 - val_out_T_loss: 0.3686 - val_out_S_loss: 1.3097 - val_tf.math.abs_185_loss: 0.0328 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 549/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.5861 - out_T_loss: 0.2665 - out_S_loss: 1.5418 - tf.math.abs_185_loss: 0.0327 - tf.math.abs_186_loss: 0.0124 - val_loss: 14.1233 - val_out_T_loss: 0.3577 - val_out_S_loss: 1.3015 - val_tf.math.abs_185_loss: 0.0341 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 550/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.9799 - out_T_loss: 0.2812 - out_S_loss: 1.5751 - tf.math.abs_185_loss: 0.0347 - tf.math.abs_186_loss: 0.0128 - val_loss: 14.1013 - val_out_T_loss: 0.3496 - val_out_S_loss: 1.3051 - val_tf.math.abs_185_loss: 0.0317 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 551/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 16.9128 - out_T_loss: 0.2560 - out_S_loss: 1.5741 - tf.math.abs_185_loss: 0.0327 - tf.math.abs_186_loss: 0.0131 - val_loss: 14.0522 - val_out_T_loss: 0.3583 - val_out_S_loss: 1.2988 - val_tf.math.abs_185_loss: 0.0319 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 552/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.6550 - out_T_loss: 0.2625 - out_S_loss: 1.5461 - tf.math.abs_185_loss: 0.0341 - tf.math.abs_186_loss: 0.0125 - val_loss: 14.0483 - val_out_T_loss: 0.3491 - val_out_S_loss: 1.3002 - val_tf.math.abs_185_loss: 0.0315 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 553/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 16.7488 - out_T_loss: 0.2525 - out_S_loss: 1.5551 - tf.math.abs_185_loss: 0.0342 - tf.math.abs_186_loss: 0.0131 - val_loss: 14.0943 - val_out_T_loss: 0.3566 - val_out_S_loss: 1.3013 - val_tf.math.abs_185_loss: 0.0326 - val_tf.math.abs_186_loss: 0.0036\n",
      "Epoch 554/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.2931 - out_T_loss: 0.2782 - out_S_loss: 1.6063 - tf.math.abs_185_loss: 0.0347 - tf.math.abs_186_loss: 0.0129 - val_loss: 14.0221 - val_out_T_loss: 0.3430 - val_out_S_loss: 1.2959 - val_tf.math.abs_185_loss: 0.0326 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 555/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.9135 - out_T_loss: 0.2613 - out_S_loss: 1.5759 - tf.math.abs_185_loss: 0.0322 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.9879 - val_out_T_loss: 0.3504 - val_out_S_loss: 1.2916 - val_tf.math.abs_185_loss: 0.0327 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 556/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.0937 - out_T_loss: 0.2488 - out_S_loss: 1.4950 - tf.math.abs_185_loss: 0.0320 - tf.math.abs_186_loss: 0.0128 - val_loss: 13.9698 - val_out_T_loss: 0.3524 - val_out_S_loss: 1.2900 - val_tf.math.abs_185_loss: 0.0324 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 557/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.7694 - out_T_loss: 0.2576 - out_S_loss: 1.5585 - tf.math.abs_185_loss: 0.0338 - tf.math.abs_186_loss: 0.0125 - val_loss: 13.9736 - val_out_T_loss: 0.3425 - val_out_S_loss: 1.2892 - val_tf.math.abs_185_loss: 0.0333 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 558/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.8911 - out_T_loss: 0.2504 - out_S_loss: 1.5732 - tf.math.abs_185_loss: 0.0325 - tf.math.abs_186_loss: 0.0130 - val_loss: 13.9295 - val_out_T_loss: 0.3362 - val_out_S_loss: 1.2886 - val_tf.math.abs_185_loss: 0.0320 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 559/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 15.9834 - out_T_loss: 0.2471 - out_S_loss: 1.4818 - tf.math.abs_185_loss: 0.0331 - tf.math.abs_186_loss: 0.0128 - val_loss: 13.8905 - val_out_T_loss: 0.3356 - val_out_S_loss: 1.2859 - val_tf.math.abs_185_loss: 0.0316 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 560/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.1827 - out_T_loss: 0.2442 - out_S_loss: 1.5038 - tf.math.abs_185_loss: 0.0326 - tf.math.abs_186_loss: 0.0125 - val_loss: 13.8484 - val_out_T_loss: 0.3365 - val_out_S_loss: 1.2801 - val_tf.math.abs_185_loss: 0.0322 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 561/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.2510 - out_T_loss: 0.2420 - out_S_loss: 1.5096 - tf.math.abs_185_loss: 0.0332 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.7824 - val_out_T_loss: 0.3275 - val_out_S_loss: 1.2767 - val_tf.math.abs_185_loss: 0.0311 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 562/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.1758 - out_T_loss: 0.2551 - out_S_loss: 1.5021 - tf.math.abs_185_loss: 0.0326 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.8046 - val_out_T_loss: 0.3268 - val_out_S_loss: 1.2778 - val_tf.math.abs_185_loss: 0.0318 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 563/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 15.9707 - out_T_loss: 0.2275 - out_S_loss: 1.4846 - tf.math.abs_185_loss: 0.0323 - tf.math.abs_186_loss: 0.0126 - val_loss: 13.8010 - val_out_T_loss: 0.3404 - val_out_S_loss: 1.2763 - val_tf.math.abs_185_loss: 0.0315 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 564/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.2858 - out_T_loss: 0.2524 - out_S_loss: 1.5104 - tf.math.abs_185_loss: 0.0337 - tf.math.abs_186_loss: 0.0128 - val_loss: 13.7748 - val_out_T_loss: 0.3429 - val_out_S_loss: 1.2733 - val_tf.math.abs_185_loss: 0.0317 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 565/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.5570 - out_T_loss: 0.2642 - out_S_loss: 1.5395 - tf.math.abs_185_loss: 0.0327 - tf.math.abs_186_loss: 0.0122 - val_loss: 13.7273 - val_out_T_loss: 0.3354 - val_out_S_loss: 1.2700 - val_tf.math.abs_185_loss: 0.0314 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 566/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.5443 - out_T_loss: 0.2416 - out_S_loss: 1.5417 - tf.math.abs_185_loss: 0.0319 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.8042 - val_out_T_loss: 0.3494 - val_out_S_loss: 1.2748 - val_tf.math.abs_185_loss: 0.0321 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 567/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.5118 - out_T_loss: 0.2485 - out_S_loss: 1.5367 - tf.math.abs_185_loss: 0.0328 - tf.math.abs_186_loss: 0.0120 - val_loss: 13.7541 - val_out_T_loss: 0.3319 - val_out_S_loss: 1.2727 - val_tf.math.abs_185_loss: 0.0315 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 568/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 17.0716 - out_T_loss: 0.2460 - out_S_loss: 1.5896 - tf.math.abs_185_loss: 0.0338 - tf.math.abs_186_loss: 0.0127 - val_loss: 13.7635 - val_out_T_loss: 0.3359 - val_out_S_loss: 1.2715 - val_tf.math.abs_185_loss: 0.0323 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 569/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.6585 - out_T_loss: 0.2336 - out_S_loss: 1.5514 - tf.math.abs_185_loss: 0.0335 - tf.math.abs_186_loss: 0.0121 - val_loss: 13.7703 - val_out_T_loss: 0.3423 - val_out_S_loss: 1.2745 - val_tf.math.abs_185_loss: 0.0308 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 570/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 16.3121 - out_T_loss: 0.2431 - out_S_loss: 1.5134 - tf.math.abs_185_loss: 0.0337 - tf.math.abs_186_loss: 0.0131 - val_loss: 13.7898 - val_out_T_loss: 0.3390 - val_out_S_loss: 1.2755 - val_tf.math.abs_185_loss: 0.0315 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 571/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.0432 - out_T_loss: 0.2386 - out_S_loss: 1.4893 - tf.math.abs_185_loss: 0.0331 - tf.math.abs_186_loss: 0.0125 - val_loss: 13.7237 - val_out_T_loss: 0.3318 - val_out_S_loss: 1.2646 - val_tf.math.abs_185_loss: 0.0338 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 572/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.5173 - out_T_loss: 0.2482 - out_S_loss: 1.5358 - tf.math.abs_185_loss: 0.0334 - tf.math.abs_186_loss: 0.0122 - val_loss: 13.6308 - val_out_T_loss: 0.3305 - val_out_S_loss: 1.2608 - val_tf.math.abs_185_loss: 0.0313 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 573/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.0270 - out_T_loss: 0.2246 - out_S_loss: 1.4899 - tf.math.abs_185_loss: 0.0325 - tf.math.abs_186_loss: 0.0127 - val_loss: 13.6703 - val_out_T_loss: 0.3383 - val_out_S_loss: 1.2643 - val_tf.math.abs_185_loss: 0.0312 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 574/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 16.4195 - out_T_loss: 0.2566 - out_S_loss: 1.5259 - tf.math.abs_185_loss: 0.0327 - tf.math.abs_186_loss: 0.0125 - val_loss: 13.7118 - val_out_T_loss: 0.3425 - val_out_S_loss: 1.2663 - val_tf.math.abs_185_loss: 0.0321 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 575/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 16.3116 - out_T_loss: 0.2277 - out_S_loss: 1.5197 - tf.math.abs_185_loss: 0.0319 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.6362 - val_out_T_loss: 0.3348 - val_out_S_loss: 1.2624 - val_tf.math.abs_185_loss: 0.0306 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 576/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.9039 - out_T_loss: 0.2274 - out_S_loss: 1.4807 - tf.math.abs_185_loss: 0.0311 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.6272 - val_out_T_loss: 0.3388 - val_out_S_loss: 1.2604 - val_tf.math.abs_185_loss: 0.0309 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 577/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.1117 - out_T_loss: 0.2483 - out_S_loss: 1.4966 - tf.math.abs_185_loss: 0.0326 - tf.math.abs_186_loss: 0.0123 - val_loss: 13.6124 - val_out_T_loss: 0.3403 - val_out_S_loss: 1.2592 - val_tf.math.abs_185_loss: 0.0307 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 578/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.6795 - out_T_loss: 0.2601 - out_S_loss: 1.5499 - tf.math.abs_185_loss: 0.0340 - tf.math.abs_186_loss: 0.0121 - val_loss: 13.6084 - val_out_T_loss: 0.3422 - val_out_S_loss: 1.2558 - val_tf.math.abs_185_loss: 0.0321 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 579/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 15.8820 - out_T_loss: 0.2636 - out_S_loss: 1.4719 - tf.math.abs_185_loss: 0.0327 - tf.math.abs_186_loss: 0.0122 - val_loss: 13.6169 - val_out_T_loss: 0.3405 - val_out_S_loss: 1.2570 - val_tf.math.abs_185_loss: 0.0321 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 580/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 16.1559 - out_T_loss: 0.2495 - out_S_loss: 1.5004 - tf.math.abs_185_loss: 0.0326 - tf.math.abs_186_loss: 0.0125 - val_loss: 13.5653 - val_out_T_loss: 0.3534 - val_out_S_loss: 1.2521 - val_tf.math.abs_185_loss: 0.0313 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 581/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.2831 - out_T_loss: 0.2575 - out_S_loss: 1.5128 - tf.math.abs_185_loss: 0.0327 - tf.math.abs_186_loss: 0.0121 - val_loss: 13.5329 - val_out_T_loss: 0.3341 - val_out_S_loss: 1.2506 - val_tf.math.abs_185_loss: 0.0315 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 582/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.1941 - out_T_loss: 0.2329 - out_S_loss: 1.5067 - tf.math.abs_185_loss: 0.0323 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.5951 - val_out_T_loss: 0.3342 - val_out_S_loss: 1.2487 - val_tf.math.abs_185_loss: 0.0354 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 583/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.3109 - out_T_loss: 0.2368 - out_S_loss: 1.5158 - tf.math.abs_185_loss: 0.0331 - tf.math.abs_186_loss: 0.0127 - val_loss: 13.4818 - val_out_T_loss: 0.3352 - val_out_S_loss: 1.2468 - val_tf.math.abs_185_loss: 0.0307 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 584/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 16.6469 - out_T_loss: 0.2747 - out_S_loss: 1.5474 - tf.math.abs_185_loss: 0.0327 - tf.math.abs_186_loss: 0.0123 - val_loss: 13.4606 - val_out_T_loss: 0.3353 - val_out_S_loss: 1.2447 - val_tf.math.abs_185_loss: 0.0307 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 585/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.3253 - out_T_loss: 0.2397 - out_S_loss: 1.5194 - tf.math.abs_185_loss: 0.0322 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.4810 - val_out_T_loss: 0.3490 - val_out_S_loss: 1.2432 - val_tf.math.abs_185_loss: 0.0316 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 586/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 16.3653 - out_T_loss: 0.2478 - out_S_loss: 1.5206 - tf.math.abs_185_loss: 0.0330 - tf.math.abs_186_loss: 0.0126 - val_loss: 13.4522 - val_out_T_loss: 0.3257 - val_out_S_loss: 1.2441 - val_tf.math.abs_185_loss: 0.0311 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 587/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.2821 - out_T_loss: 0.2357 - out_S_loss: 1.5129 - tf.math.abs_185_loss: 0.0330 - tf.math.abs_186_loss: 0.0129 - val_loss: 13.4343 - val_out_T_loss: 0.3365 - val_out_S_loss: 1.2427 - val_tf.math.abs_185_loss: 0.0304 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 588/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.8514 - out_T_loss: 0.2459 - out_S_loss: 1.4701 - tf.math.abs_185_loss: 0.0330 - tf.math.abs_186_loss: 0.0122 - val_loss: 13.4198 - val_out_T_loss: 0.3459 - val_out_S_loss: 1.2364 - val_tf.math.abs_185_loss: 0.0322 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 589/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.2193 - out_T_loss: 0.2339 - out_S_loss: 1.5092 - tf.math.abs_185_loss: 0.0325 - tf.math.abs_186_loss: 0.0121 - val_loss: 13.3676 - val_out_T_loss: 0.3326 - val_out_S_loss: 1.2361 - val_tf.math.abs_185_loss: 0.0302 - val_tf.math.abs_186_loss: 0.0035\n",
      "Epoch 590/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.6754 - out_T_loss: 0.2251 - out_S_loss: 1.4587 - tf.math.abs_185_loss: 0.0309 - tf.math.abs_186_loss: 0.0123 - val_loss: 13.3533 - val_out_T_loss: 0.3309 - val_out_S_loss: 1.2337 - val_tf.math.abs_185_loss: 0.0310 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 591/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.7403 - out_T_loss: 0.2251 - out_S_loss: 1.4661 - tf.math.abs_185_loss: 0.0307 - tf.math.abs_186_loss: 0.0121 - val_loss: 13.3133 - val_out_T_loss: 0.3294 - val_out_S_loss: 1.2328 - val_tf.math.abs_185_loss: 0.0297 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 592/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 15.7854 - out_T_loss: 0.2379 - out_S_loss: 1.4676 - tf.math.abs_185_loss: 0.0314 - tf.math.abs_186_loss: 0.0121 - val_loss: 13.3236 - val_out_T_loss: 0.3431 - val_out_S_loss: 1.2309 - val_tf.math.abs_185_loss: 0.0302 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 593/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.0094 - out_T_loss: 0.2404 - out_S_loss: 1.4850 - tf.math.abs_185_loss: 0.0333 - tf.math.abs_186_loss: 0.0127 - val_loss: 13.3306 - val_out_T_loss: 0.3389 - val_out_S_loss: 1.2284 - val_tf.math.abs_185_loss: 0.0321 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 594/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.6770 - out_T_loss: 0.2294 - out_S_loss: 1.4555 - tf.math.abs_185_loss: 0.0329 - tf.math.abs_186_loss: 0.0118 - val_loss: 13.2473 - val_out_T_loss: 0.3215 - val_out_S_loss: 1.2253 - val_tf.math.abs_185_loss: 0.0304 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 595/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.7136 - out_T_loss: 0.2225 - out_S_loss: 1.4632 - tf.math.abs_185_loss: 0.0309 - tf.math.abs_186_loss: 0.0120 - val_loss: 13.2888 - val_out_T_loss: 0.3288 - val_out_S_loss: 1.2298 - val_tf.math.abs_185_loss: 0.0298 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 596/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 16.2709 - out_T_loss: 0.2490 - out_S_loss: 1.5143 - tf.math.abs_185_loss: 0.0315 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.3078 - val_out_T_loss: 0.3488 - val_out_S_loss: 1.2283 - val_tf.math.abs_185_loss: 0.0305 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 597/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.4540 - out_T_loss: 0.2170 - out_S_loss: 1.4371 - tf.math.abs_185_loss: 0.0310 - tf.math.abs_186_loss: 0.0123 - val_loss: 13.2898 - val_out_T_loss: 0.3293 - val_out_S_loss: 1.2301 - val_tf.math.abs_185_loss: 0.0298 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 598/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.0042 - out_T_loss: 0.2341 - out_S_loss: 1.4863 - tf.math.abs_185_loss: 0.0329 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.2320 - val_out_T_loss: 0.3269 - val_out_S_loss: 1.2247 - val_tf.math.abs_185_loss: 0.0297 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 599/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.8592 - out_T_loss: 0.2455 - out_S_loss: 1.4754 - tf.math.abs_185_loss: 0.0311 - tf.math.abs_186_loss: 0.0118 - val_loss: 13.2192 - val_out_T_loss: 0.3293 - val_out_S_loss: 1.2220 - val_tf.math.abs_185_loss: 0.0303 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 600/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.7068 - out_T_loss: 0.2292 - out_S_loss: 1.4630 - tf.math.abs_185_loss: 0.0308 - tf.math.abs_186_loss: 0.0116 - val_loss: 13.2088 - val_out_T_loss: 0.3286 - val_out_S_loss: 1.2211 - val_tf.math.abs_185_loss: 0.0303 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 601/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.6233 - out_T_loss: 0.2298 - out_S_loss: 1.4541 - tf.math.abs_185_loss: 0.0303 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.1646 - val_out_T_loss: 0.3216 - val_out_S_loss: 1.2187 - val_tf.math.abs_185_loss: 0.0295 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 602/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.7739 - out_T_loss: 0.2352 - out_S_loss: 1.4673 - tf.math.abs_185_loss: 0.0314 - tf.math.abs_186_loss: 0.0118 - val_loss: 13.1910 - val_out_T_loss: 0.3273 - val_out_S_loss: 1.2208 - val_tf.math.abs_185_loss: 0.0295 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 603/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.7546 - out_T_loss: 0.2231 - out_S_loss: 1.4657 - tf.math.abs_185_loss: 0.0314 - tf.math.abs_186_loss: 0.0124 - val_loss: 13.1570 - val_out_T_loss: 0.3209 - val_out_S_loss: 1.2167 - val_tf.math.abs_185_loss: 0.0302 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 604/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.2719 - out_T_loss: 0.2321 - out_S_loss: 1.4189 - tf.math.abs_185_loss: 0.0306 - tf.math.abs_186_loss: 0.0120 - val_loss: 13.1262 - val_out_T_loss: 0.3230 - val_out_S_loss: 1.2144 - val_tf.math.abs_185_loss: 0.0298 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 605/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 16.1920 - out_T_loss: 0.2377 - out_S_loss: 1.5083 - tf.math.abs_185_loss: 0.0318 - tf.math.abs_186_loss: 0.0118 - val_loss: 13.1445 - val_out_T_loss: 0.3239 - val_out_S_loss: 1.2170 - val_tf.math.abs_185_loss: 0.0294 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 606/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.5025 - out_T_loss: 0.2267 - out_S_loss: 1.4407 - tf.math.abs_185_loss: 0.0314 - tf.math.abs_186_loss: 0.0120 - val_loss: 13.1491 - val_out_T_loss: 0.3229 - val_out_S_loss: 1.2146 - val_tf.math.abs_185_loss: 0.0307 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 607/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.0472 - out_T_loss: 0.2177 - out_S_loss: 1.3956 - tf.math.abs_185_loss: 0.0317 - tf.math.abs_186_loss: 0.0120 - val_loss: 13.0650 - val_out_T_loss: 0.3193 - val_out_S_loss: 1.2106 - val_tf.math.abs_185_loss: 0.0288 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 608/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.1801 - out_T_loss: 0.2148 - out_S_loss: 1.4133 - tf.math.abs_185_loss: 0.0293 - tf.math.abs_186_loss: 0.0123 - val_loss: 13.0434 - val_out_T_loss: 0.3287 - val_out_S_loss: 1.2067 - val_tf.math.abs_185_loss: 0.0291 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 609/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 15.9101 - out_T_loss: 0.2522 - out_S_loss: 1.4785 - tf.math.abs_185_loss: 0.0318 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.9939 - val_out_T_loss: 0.3193 - val_out_S_loss: 1.2032 - val_tf.math.abs_185_loss: 0.0290 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 610/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 16.3203 - out_T_loss: 0.2483 - out_S_loss: 1.5188 - tf.math.abs_185_loss: 0.0316 - tf.math.abs_186_loss: 0.0126 - val_loss: 13.0654 - val_out_T_loss: 0.3306 - val_out_S_loss: 1.2087 - val_tf.math.abs_185_loss: 0.0291 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 611/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.1511 - out_T_loss: 0.2109 - out_S_loss: 1.4076 - tf.math.abs_185_loss: 0.0312 - tf.math.abs_186_loss: 0.0120 - val_loss: 12.9892 - val_out_T_loss: 0.3186 - val_out_S_loss: 1.2011 - val_tf.math.abs_185_loss: 0.0298 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 612/2000\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 15.4270 - out_T_loss: 0.2193 - out_S_loss: 1.4369 - tf.math.abs_185_loss: 0.0305 - tf.math.abs_186_loss: 0.0115 - val_loss: 13.0023 - val_out_T_loss: 0.3414 - val_out_S_loss: 1.2002 - val_tf.math.abs_185_loss: 0.0297 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 613/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.1545 - out_T_loss: 0.2063 - out_S_loss: 1.4113 - tf.math.abs_185_loss: 0.0302 - tf.math.abs_186_loss: 0.0116 - val_loss: 12.9244 - val_out_T_loss: 0.3167 - val_out_S_loss: 1.1968 - val_tf.math.abs_185_loss: 0.0289 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 614/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.9876 - out_T_loss: 0.2250 - out_S_loss: 1.4898 - tf.math.abs_185_loss: 0.0309 - tf.math.abs_186_loss: 0.0123 - val_loss: 12.9550 - val_out_T_loss: 0.3329 - val_out_S_loss: 1.1954 - val_tf.math.abs_185_loss: 0.0302 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 615/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.4959 - out_T_loss: 0.2238 - out_S_loss: 1.4408 - tf.math.abs_185_loss: 0.0310 - tf.math.abs_186_loss: 0.0122 - val_loss: 12.9059 - val_out_T_loss: 0.3196 - val_out_S_loss: 1.1924 - val_tf.math.abs_185_loss: 0.0299 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 616/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.6874 - out_T_loss: 0.2236 - out_S_loss: 1.4607 - tf.math.abs_185_loss: 0.0309 - tf.math.abs_186_loss: 0.0120 - val_loss: 12.9016 - val_out_T_loss: 0.3274 - val_out_S_loss: 1.1916 - val_tf.math.abs_185_loss: 0.0298 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 617/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 15.7116 - out_T_loss: 0.2205 - out_S_loss: 1.4630 - tf.math.abs_185_loss: 0.0308 - tf.math.abs_186_loss: 0.0123 - val_loss: 12.9427 - val_out_T_loss: 0.3335 - val_out_S_loss: 1.1898 - val_tf.math.abs_185_loss: 0.0323 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 618/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.3111 - out_T_loss: 0.2065 - out_S_loss: 1.4239 - tf.math.abs_185_loss: 0.0310 - tf.math.abs_186_loss: 0.0122 - val_loss: 12.8283 - val_out_T_loss: 0.3212 - val_out_S_loss: 1.1859 - val_tf.math.abs_185_loss: 0.0292 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 619/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.1956 - out_T_loss: 0.1989 - out_S_loss: 1.4143 - tf.math.abs_185_loss: 0.0306 - tf.math.abs_186_loss: 0.0121 - val_loss: 12.8253 - val_out_T_loss: 0.3187 - val_out_S_loss: 1.1869 - val_tf.math.abs_185_loss: 0.0286 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 620/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.4847 - out_T_loss: 0.2190 - out_S_loss: 1.4396 - tf.math.abs_185_loss: 0.0311 - tf.math.abs_186_loss: 0.0124 - val_loss: 12.8468 - val_out_T_loss: 0.3186 - val_out_S_loss: 1.1883 - val_tf.math.abs_185_loss: 0.0290 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 621/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.2952 - out_T_loss: 0.2090 - out_S_loss: 1.4235 - tf.math.abs_185_loss: 0.0301 - tf.math.abs_186_loss: 0.0124 - val_loss: 12.8307 - val_out_T_loss: 0.3233 - val_out_S_loss: 1.1878 - val_tf.math.abs_185_loss: 0.0284 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 622/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.3669 - out_T_loss: 0.2085 - out_S_loss: 1.4305 - tf.math.abs_185_loss: 0.0308 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.8200 - val_out_T_loss: 0.3143 - val_out_S_loss: 1.1873 - val_tf.math.abs_185_loss: 0.0285 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 623/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.3896 - out_T_loss: 0.1991 - out_S_loss: 1.4361 - tf.math.abs_185_loss: 0.0296 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.8513 - val_out_T_loss: 0.3173 - val_out_S_loss: 1.1861 - val_tf.math.abs_185_loss: 0.0305 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 624/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.3730 - out_T_loss: 0.2167 - out_S_loss: 1.4313 - tf.math.abs_185_loss: 0.0309 - tf.math.abs_186_loss: 0.0112 - val_loss: 12.8262 - val_out_T_loss: 0.3223 - val_out_S_loss: 1.1853 - val_tf.math.abs_185_loss: 0.0294 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 625/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.6639 - out_T_loss: 0.2198 - out_S_loss: 1.4592 - tf.math.abs_185_loss: 0.0307 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.7602 - val_out_T_loss: 0.3168 - val_out_S_loss: 1.1805 - val_tf.math.abs_185_loss: 0.0288 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 626/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.0619 - out_T_loss: 0.2034 - out_S_loss: 1.4008 - tf.math.abs_185_loss: 0.0303 - tf.math.abs_186_loss: 0.0123 - val_loss: 12.7240 - val_out_T_loss: 0.3226 - val_out_S_loss: 1.1778 - val_tf.math.abs_185_loss: 0.0281 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 627/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.6346 - out_T_loss: 0.2252 - out_S_loss: 1.4565 - tf.math.abs_185_loss: 0.0304 - tf.math.abs_186_loss: 0.0118 - val_loss: 12.7463 - val_out_T_loss: 0.3211 - val_out_S_loss: 1.1767 - val_tf.math.abs_185_loss: 0.0298 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 628/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 15.4294 - out_T_loss: 0.2094 - out_S_loss: 1.4384 - tf.math.abs_185_loss: 0.0299 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.7247 - val_out_T_loss: 0.3245 - val_out_S_loss: 1.1768 - val_tf.math.abs_185_loss: 0.0284 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 629/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.4350 - out_T_loss: 0.1962 - out_S_loss: 1.4399 - tf.math.abs_185_loss: 0.0303 - tf.math.abs_186_loss: 0.0117 - val_loss: 12.6858 - val_out_T_loss: 0.3146 - val_out_S_loss: 1.1731 - val_tf.math.abs_185_loss: 0.0289 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 630/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.4345 - out_T_loss: 0.2135 - out_S_loss: 1.4375 - tf.math.abs_185_loss: 0.0299 - tf.math.abs_186_loss: 0.0124 - val_loss: 12.7055 - val_out_T_loss: 0.3347 - val_out_S_loss: 1.1745 - val_tf.math.abs_185_loss: 0.0282 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 631/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.6232 - out_T_loss: 0.2055 - out_S_loss: 1.4564 - tf.math.abs_185_loss: 0.0307 - tf.math.abs_186_loss: 0.0120 - val_loss: 12.7183 - val_out_T_loss: 0.3305 - val_out_S_loss: 1.1747 - val_tf.math.abs_185_loss: 0.0289 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 632/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.7257 - out_T_loss: 0.2053 - out_S_loss: 1.4667 - tf.math.abs_185_loss: 0.0305 - tf.math.abs_186_loss: 0.0122 - val_loss: 12.6776 - val_out_T_loss: 0.3167 - val_out_S_loss: 1.1740 - val_tf.math.abs_185_loss: 0.0280 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 633/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.3637 - out_T_loss: 0.2104 - out_S_loss: 1.4315 - tf.math.abs_185_loss: 0.0300 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.6212 - val_out_T_loss: 0.3220 - val_out_S_loss: 1.1684 - val_tf.math.abs_185_loss: 0.0277 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 634/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 14.8237 - out_T_loss: 0.2061 - out_S_loss: 1.3802 - tf.math.abs_185_loss: 0.0290 - tf.math.abs_186_loss: 0.0118 - val_loss: 12.6072 - val_out_T_loss: 0.3245 - val_out_S_loss: 1.1649 - val_tf.math.abs_185_loss: 0.0286 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 635/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.5764 - out_T_loss: 0.2082 - out_S_loss: 1.4491 - tf.math.abs_185_loss: 0.0318 - tf.math.abs_186_loss: 0.0121 - val_loss: 12.6906 - val_out_T_loss: 0.3376 - val_out_S_loss: 1.1679 - val_tf.math.abs_185_loss: 0.0300 - val_tf.math.abs_186_loss: 0.0037\n",
      "Epoch 636/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.4351 - out_T_loss: 0.2225 - out_S_loss: 1.4332 - tf.math.abs_185_loss: 0.0316 - tf.math.abs_186_loss: 0.0124 - val_loss: 12.6257 - val_out_T_loss: 0.3364 - val_out_S_loss: 1.1668 - val_tf.math.abs_185_loss: 0.0279 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 637/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.7169 - out_T_loss: 0.2228 - out_S_loss: 1.4626 - tf.math.abs_185_loss: 0.0311 - tf.math.abs_186_loss: 0.0123 - val_loss: 12.6413 - val_out_T_loss: 0.3251 - val_out_S_loss: 1.1698 - val_tf.math.abs_185_loss: 0.0277 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 638/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.5667 - out_T_loss: 0.2040 - out_S_loss: 1.3541 - tf.math.abs_185_loss: 0.0293 - tf.math.abs_186_loss: 0.0118 - val_loss: 12.6017 - val_out_T_loss: 0.3227 - val_out_S_loss: 1.1647 - val_tf.math.abs_185_loss: 0.0284 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 639/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.1972 - out_T_loss: 0.2117 - out_S_loss: 1.4156 - tf.math.abs_185_loss: 0.0294 - tf.math.abs_186_loss: 0.0120 - val_loss: 12.5666 - val_out_T_loss: 0.3192 - val_out_S_loss: 1.1632 - val_tf.math.abs_185_loss: 0.0276 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 640/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.4980 - out_T_loss: 0.2088 - out_S_loss: 1.4452 - tf.math.abs_185_loss: 0.0297 - tf.math.abs_186_loss: 0.0122 - val_loss: 12.6145 - val_out_T_loss: 0.3247 - val_out_S_loss: 1.1652 - val_tf.math.abs_185_loss: 0.0286 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 641/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 14.9311 - out_T_loss: 0.2004 - out_S_loss: 1.3909 - tf.math.abs_185_loss: 0.0294 - tf.math.abs_186_loss: 0.0117 - val_loss: 12.5450 - val_out_T_loss: 0.3229 - val_out_S_loss: 1.1605 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 642/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.1187 - out_T_loss: 0.1901 - out_S_loss: 1.4095 - tf.math.abs_185_loss: 0.0297 - tf.math.abs_186_loss: 0.0120 - val_loss: 12.6161 - val_out_T_loss: 0.3460 - val_out_S_loss: 1.1622 - val_tf.math.abs_185_loss: 0.0291 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 643/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 15.1741 - out_T_loss: 0.2138 - out_S_loss: 1.4112 - tf.math.abs_185_loss: 0.0306 - tf.math.abs_186_loss: 0.0118 - val_loss: 12.5545 - val_out_T_loss: 0.3180 - val_out_S_loss: 1.1615 - val_tf.math.abs_185_loss: 0.0279 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 644/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.0384 - out_T_loss: 0.2056 - out_S_loss: 1.4001 - tf.math.abs_185_loss: 0.0299 - tf.math.abs_186_loss: 0.0117 - val_loss: 12.4843 - val_out_T_loss: 0.3102 - val_out_S_loss: 1.1550 - val_tf.math.abs_185_loss: 0.0282 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 645/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.1474 - out_T_loss: 0.2035 - out_S_loss: 1.4101 - tf.math.abs_185_loss: 0.0305 - tf.math.abs_186_loss: 0.0116 - val_loss: 12.4878 - val_out_T_loss: 0.3165 - val_out_S_loss: 1.1537 - val_tf.math.abs_185_loss: 0.0287 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 646/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 14.9989 - out_T_loss: 0.2124 - out_S_loss: 1.3944 - tf.math.abs_185_loss: 0.0304 - tf.math.abs_186_loss: 0.0118 - val_loss: 12.4608 - val_out_T_loss: 0.3134 - val_out_S_loss: 1.1519 - val_tf.math.abs_185_loss: 0.0285 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 647/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.0944 - out_T_loss: 0.1979 - out_S_loss: 1.4078 - tf.math.abs_185_loss: 0.0288 - tf.math.abs_186_loss: 0.0122 - val_loss: 12.4460 - val_out_T_loss: 0.3258 - val_out_S_loss: 1.1497 - val_tf.math.abs_185_loss: 0.0280 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 648/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 14.8507 - out_T_loss: 0.1916 - out_S_loss: 1.3863 - tf.math.abs_185_loss: 0.0284 - tf.math.abs_186_loss: 0.0114 - val_loss: 12.3917 - val_out_T_loss: 0.3225 - val_out_S_loss: 1.1461 - val_tf.math.abs_185_loss: 0.0273 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 649/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 15.2688 - out_T_loss: 0.2164 - out_S_loss: 1.4186 - tf.math.abs_185_loss: 0.0308 - tf.math.abs_186_loss: 0.0125 - val_loss: 12.4092 - val_out_T_loss: 0.3166 - val_out_S_loss: 1.1456 - val_tf.math.abs_185_loss: 0.0287 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 650/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.9578 - out_T_loss: 0.2102 - out_S_loss: 1.3940 - tf.math.abs_185_loss: 0.0285 - tf.math.abs_186_loss: 0.0118 - val_loss: 12.4119 - val_out_T_loss: 0.3171 - val_out_S_loss: 1.1489 - val_tf.math.abs_185_loss: 0.0272 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 651/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.9396 - out_T_loss: 0.2005 - out_S_loss: 1.3917 - tf.math.abs_185_loss: 0.0290 - tf.math.abs_186_loss: 0.0120 - val_loss: 12.4025 - val_out_T_loss: 0.3051 - val_out_S_loss: 1.1466 - val_tf.math.abs_185_loss: 0.0284 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 652/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.6589 - out_T_loss: 0.2116 - out_S_loss: 1.4587 - tf.math.abs_185_loss: 0.0308 - tf.math.abs_186_loss: 0.0122 - val_loss: 12.4250 - val_out_T_loss: 0.3155 - val_out_S_loss: 1.1450 - val_tf.math.abs_185_loss: 0.0298 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 653/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.0625 - out_T_loss: 0.2122 - out_S_loss: 1.4026 - tf.math.abs_185_loss: 0.0299 - tf.math.abs_186_loss: 0.0113 - val_loss: 12.3960 - val_out_T_loss: 0.3221 - val_out_S_loss: 1.1462 - val_tf.math.abs_185_loss: 0.0275 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 654/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 15.0371 - out_T_loss: 0.2018 - out_S_loss: 1.4024 - tf.math.abs_185_loss: 0.0289 - tf.math.abs_186_loss: 0.0117 - val_loss: 12.4139 - val_out_T_loss: 0.3263 - val_out_S_loss: 1.1459 - val_tf.math.abs_185_loss: 0.0282 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 655/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.6283 - out_T_loss: 0.1911 - out_S_loss: 1.3629 - tf.math.abs_185_loss: 0.0283 - tf.math.abs_186_loss: 0.0121 - val_loss: 12.3161 - val_out_T_loss: 0.3146 - val_out_S_loss: 1.1414 - val_tf.math.abs_185_loss: 0.0263 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 656/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.9372 - out_T_loss: 0.2024 - out_S_loss: 1.3944 - tf.math.abs_185_loss: 0.0277 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.3133 - val_out_T_loss: 0.3172 - val_out_S_loss: 1.1391 - val_tf.math.abs_185_loss: 0.0272 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 657/2000\n",
      "24/24 [==============================] - 1s 34ms/step - loss: 15.2453 - out_T_loss: 0.2151 - out_S_loss: 1.4208 - tf.math.abs_185_loss: 0.0296 - tf.math.abs_186_loss: 0.0115 - val_loss: 12.3533 - val_out_T_loss: 0.3075 - val_out_S_loss: 1.1430 - val_tf.math.abs_185_loss: 0.0276 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 658/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.8132 - out_T_loss: 0.1902 - out_S_loss: 1.3819 - tf.math.abs_185_loss: 0.0285 - tf.math.abs_186_loss: 0.0117 - val_loss: 12.3036 - val_out_T_loss: 0.3219 - val_out_S_loss: 1.1359 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 659/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.5045 - out_T_loss: 0.1938 - out_S_loss: 1.3485 - tf.math.abs_185_loss: 0.0299 - tf.math.abs_186_loss: 0.0114 - val_loss: 12.2508 - val_out_T_loss: 0.3193 - val_out_S_loss: 1.1327 - val_tf.math.abs_185_loss: 0.0270 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 660/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.5838 - out_T_loss: 0.1837 - out_S_loss: 1.3599 - tf.math.abs_185_loss: 0.0282 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.2513 - val_out_T_loss: 0.3175 - val_out_S_loss: 1.1330 - val_tf.math.abs_185_loss: 0.0268 - val_tf.math.abs_186_loss: 0.0034\n",
      "Epoch 661/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.7194 - out_T_loss: 0.1982 - out_S_loss: 1.3713 - tf.math.abs_185_loss: 0.0285 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.2458 - val_out_T_loss: 0.3091 - val_out_S_loss: 1.1339 - val_tf.math.abs_185_loss: 0.0267 - val_tf.math.abs_186_loss: 0.0033\n",
      "Epoch 662/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.8027 - out_T_loss: 0.1971 - out_S_loss: 1.3785 - tf.math.abs_185_loss: 0.0288 - tf.math.abs_186_loss: 0.0123 - val_loss: 12.2447 - val_out_T_loss: 0.3212 - val_out_S_loss: 1.1312 - val_tf.math.abs_185_loss: 0.0275 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 663/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.5550 - out_T_loss: 0.1915 - out_S_loss: 1.3544 - tf.math.abs_185_loss: 0.0292 - tf.math.abs_186_loss: 0.0117 - val_loss: 12.1922 - val_out_T_loss: 0.3201 - val_out_S_loss: 1.1282 - val_tf.math.abs_185_loss: 0.0264 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 664/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.7457 - out_T_loss: 0.2013 - out_S_loss: 1.3719 - tf.math.abs_185_loss: 0.0294 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.1820 - val_out_T_loss: 0.3324 - val_out_S_loss: 1.1251 - val_tf.math.abs_185_loss: 0.0270 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 665/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.6598 - out_T_loss: 0.1943 - out_S_loss: 1.3669 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.2326 - val_out_T_loss: 0.3117 - val_out_S_loss: 1.1303 - val_tf.math.abs_185_loss: 0.0277 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 666/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.9522 - out_T_loss: 0.2015 - out_S_loss: 1.3947 - tf.math.abs_185_loss: 0.0288 - tf.math.abs_186_loss: 0.0114 - val_loss: 12.2449 - val_out_T_loss: 0.3167 - val_out_S_loss: 1.1304 - val_tf.math.abs_185_loss: 0.0280 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 667/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.4251 - out_T_loss: 0.1775 - out_S_loss: 1.3449 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0120 - val_loss: 12.1593 - val_out_T_loss: 0.3127 - val_out_S_loss: 1.1262 - val_tf.math.abs_185_loss: 0.0262 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 668/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.2254 - out_T_loss: 0.1969 - out_S_loss: 1.3221 - tf.math.abs_185_loss: 0.0288 - tf.math.abs_186_loss: 0.0116 - val_loss: 12.1411 - val_out_T_loss: 0.3048 - val_out_S_loss: 1.1248 - val_tf.math.abs_185_loss: 0.0263 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 669/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.7922 - out_T_loss: 0.1969 - out_S_loss: 1.3763 - tf.math.abs_185_loss: 0.0297 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.1560 - val_out_T_loss: 0.3195 - val_out_S_loss: 1.1247 - val_tf.math.abs_185_loss: 0.0264 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 670/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 15.5356 - out_T_loss: 0.1935 - out_S_loss: 1.4508 - tf.math.abs_185_loss: 0.0298 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.1502 - val_out_T_loss: 0.3159 - val_out_S_loss: 1.1217 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 671/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.7689 - out_T_loss: 0.2009 - out_S_loss: 1.3742 - tf.math.abs_185_loss: 0.0294 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.0863 - val_out_T_loss: 0.3161 - val_out_S_loss: 1.1184 - val_tf.math.abs_185_loss: 0.0261 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 672/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.8598 - out_T_loss: 0.2020 - out_S_loss: 1.3862 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0117 - val_loss: 12.0971 - val_out_T_loss: 0.3204 - val_out_S_loss: 1.1189 - val_tf.math.abs_185_loss: 0.0263 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 673/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.8519 - out_T_loss: 0.1939 - out_S_loss: 1.3843 - tf.math.abs_185_loss: 0.0289 - tf.math.abs_186_loss: 0.0119 - val_loss: 12.0725 - val_out_T_loss: 0.3004 - val_out_S_loss: 1.1174 - val_tf.math.abs_185_loss: 0.0268 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 674/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.7528 - out_T_loss: 0.1844 - out_S_loss: 1.3766 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0121 - val_loss: 12.0358 - val_out_T_loss: 0.3107 - val_out_S_loss: 1.1128 - val_tf.math.abs_185_loss: 0.0268 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 675/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.4528 - out_T_loss: 0.1923 - out_S_loss: 1.3459 - tf.math.abs_185_loss: 0.0286 - tf.math.abs_186_loss: 0.0114 - val_loss: 12.0220 - val_out_T_loss: 0.3161 - val_out_S_loss: 1.1115 - val_tf.math.abs_185_loss: 0.0266 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 676/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.6288 - out_T_loss: 0.1840 - out_S_loss: 1.3656 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0116 - val_loss: 12.0328 - val_out_T_loss: 0.2940 - val_out_S_loss: 1.1161 - val_tf.math.abs_185_loss: 0.0258 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 677/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.4928 - out_T_loss: 0.1896 - out_S_loss: 1.3516 - tf.math.abs_185_loss: 0.0281 - tf.math.abs_186_loss: 0.0113 - val_loss: 12.0233 - val_out_T_loss: 0.3214 - val_out_S_loss: 1.1114 - val_tf.math.abs_185_loss: 0.0263 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 678/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.5345 - out_T_loss: 0.1996 - out_S_loss: 1.3538 - tf.math.abs_185_loss: 0.0283 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.9806 - val_out_T_loss: 0.3312 - val_out_S_loss: 1.1079 - val_tf.math.abs_185_loss: 0.0254 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 679/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.5999 - out_T_loss: 0.1954 - out_S_loss: 1.3596 - tf.math.abs_185_loss: 0.0290 - tf.math.abs_186_loss: 0.0115 - val_loss: 11.9532 - val_out_T_loss: 0.3084 - val_out_S_loss: 1.1052 - val_tf.math.abs_185_loss: 0.0266 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 680/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.7233 - out_T_loss: 0.1915 - out_S_loss: 1.3722 - tf.math.abs_185_loss: 0.0288 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.9527 - val_out_T_loss: 0.3095 - val_out_S_loss: 1.1040 - val_tf.math.abs_185_loss: 0.0270 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 681/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.7413 - out_T_loss: 0.1934 - out_S_loss: 1.3748 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0120 - val_loss: 11.9534 - val_out_T_loss: 0.3125 - val_out_S_loss: 1.1046 - val_tf.math.abs_185_loss: 0.0267 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 682/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.6391 - out_T_loss: 0.1797 - out_S_loss: 1.3662 - tf.math.abs_185_loss: 0.0285 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.9471 - val_out_T_loss: 0.3105 - val_out_S_loss: 1.1065 - val_tf.math.abs_185_loss: 0.0256 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 683/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.0696 - out_T_loss: 0.1840 - out_S_loss: 1.3110 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.9539 - val_out_T_loss: 0.3128 - val_out_S_loss: 1.1062 - val_tf.math.abs_185_loss: 0.0260 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 684/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.6327 - out_T_loss: 0.1772 - out_S_loss: 1.3679 - tf.math.abs_185_loss: 0.0273 - tf.math.abs_186_loss: 0.0115 - val_loss: 11.9318 - val_out_T_loss: 0.3172 - val_out_S_loss: 1.1043 - val_tf.math.abs_185_loss: 0.0256 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 685/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.0486 - out_T_loss: 0.1688 - out_S_loss: 1.3094 - tf.math.abs_185_loss: 0.0278 - tf.math.abs_186_loss: 0.0115 - val_loss: 11.8795 - val_out_T_loss: 0.3077 - val_out_S_loss: 1.0998 - val_tf.math.abs_185_loss: 0.0257 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 686/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.1822 - out_T_loss: 0.1902 - out_S_loss: 1.3182 - tf.math.abs_185_loss: 0.0289 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.8807 - val_out_T_loss: 0.3015 - val_out_S_loss: 1.0976 - val_tf.math.abs_185_loss: 0.0270 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 687/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.3894 - out_T_loss: 0.1722 - out_S_loss: 1.3429 - tf.math.abs_185_loss: 0.0273 - tf.math.abs_186_loss: 0.0121 - val_loss: 11.8711 - val_out_T_loss: 0.3201 - val_out_S_loss: 1.0979 - val_tf.math.abs_185_loss: 0.0255 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 688/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.6408 - out_T_loss: 0.1886 - out_S_loss: 1.3640 - tf.math.abs_185_loss: 0.0286 - tf.math.abs_186_loss: 0.0120 - val_loss: 11.8535 - val_out_T_loss: 0.2973 - val_out_S_loss: 1.0988 - val_tf.math.abs_185_loss: 0.0254 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 689/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.5198 - out_T_loss: 0.1805 - out_S_loss: 1.3537 - tf.math.abs_185_loss: 0.0282 - tf.math.abs_186_loss: 0.0119 - val_loss: 11.8707 - val_out_T_loss: 0.3066 - val_out_S_loss: 1.0966 - val_tf.math.abs_185_loss: 0.0271 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 690/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.4979 - out_T_loss: 0.1893 - out_S_loss: 1.3519 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0115 - val_loss: 11.8948 - val_out_T_loss: 0.2996 - val_out_S_loss: 1.1017 - val_tf.math.abs_185_loss: 0.0260 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 691/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.6617 - out_T_loss: 0.1826 - out_S_loss: 1.3656 - tf.math.abs_185_loss: 0.0294 - tf.math.abs_186_loss: 0.0118 - val_loss: 11.8788 - val_out_T_loss: 0.3334 - val_out_S_loss: 1.0985 - val_tf.math.abs_185_loss: 0.0250 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 692/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.0041 - out_T_loss: 0.1864 - out_S_loss: 1.3027 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.8065 - val_out_T_loss: 0.2990 - val_out_S_loss: 1.0943 - val_tf.math.abs_185_loss: 0.0254 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 693/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.2250 - out_T_loss: 0.1819 - out_S_loss: 1.3268 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.8180 - val_out_T_loss: 0.3068 - val_out_S_loss: 1.0944 - val_tf.math.abs_185_loss: 0.0254 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 694/2000\n",
      "24/24 [==============================] - 1s 35ms/step - loss: 14.4186 - out_T_loss: 0.1805 - out_S_loss: 1.3439 - tf.math.abs_185_loss: 0.0282 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.8859 - val_out_T_loss: 0.3122 - val_out_S_loss: 1.0958 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 695/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.4181 - out_T_loss: 0.1904 - out_S_loss: 1.3432 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0118 - val_loss: 11.8343 - val_out_T_loss: 0.3149 - val_out_S_loss: 1.0927 - val_tf.math.abs_185_loss: 0.0266 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 696/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.5426 - out_T_loss: 0.1862 - out_S_loss: 1.3565 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0115 - val_loss: 11.7976 - val_out_T_loss: 0.3106 - val_out_S_loss: 1.0920 - val_tf.math.abs_185_loss: 0.0254 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 697/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.1382 - out_T_loss: 0.1696 - out_S_loss: 1.3196 - tf.math.abs_185_loss: 0.0273 - tf.math.abs_186_loss: 0.0113 - val_loss: 11.7979 - val_out_T_loss: 0.3111 - val_out_S_loss: 1.0900 - val_tf.math.abs_185_loss: 0.0264 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 698/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.0869 - out_T_loss: 0.1860 - out_S_loss: 1.3108 - tf.math.abs_185_loss: 0.0288 - tf.math.abs_186_loss: 0.0109 - val_loss: 11.7980 - val_out_T_loss: 0.3067 - val_out_S_loss: 1.0895 - val_tf.math.abs_185_loss: 0.0269 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 699/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.5231 - out_T_loss: 0.2009 - out_S_loss: 1.3539 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0111 - val_loss: 11.7512 - val_out_T_loss: 0.2998 - val_out_S_loss: 1.0884 - val_tf.math.abs_185_loss: 0.0254 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 700/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 14.4949 - out_T_loss: 0.1756 - out_S_loss: 1.3532 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0113 - val_loss: 11.7659 - val_out_T_loss: 0.3086 - val_out_S_loss: 1.0884 - val_tf.math.abs_185_loss: 0.0256 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 701/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 13.8463 - out_T_loss: 0.1710 - out_S_loss: 1.2904 - tf.math.abs_185_loss: 0.0270 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.7023 - val_out_T_loss: 0.2977 - val_out_S_loss: 1.0842 - val_tf.math.abs_185_loss: 0.0251 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 702/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.4350 - out_T_loss: 0.1805 - out_S_loss: 1.3488 - tf.math.abs_185_loss: 0.0269 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.7592 - val_out_T_loss: 0.3171 - val_out_S_loss: 1.0871 - val_tf.math.abs_185_loss: 0.0255 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 703/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.4080 - out_T_loss: 0.1811 - out_S_loss: 1.3453 - tf.math.abs_185_loss: 0.0273 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.7577 - val_out_T_loss: 0.3167 - val_out_S_loss: 1.0868 - val_tf.math.abs_185_loss: 0.0255 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 704/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.9066 - out_T_loss: 0.1722 - out_S_loss: 1.2940 - tf.math.abs_185_loss: 0.0284 - tf.math.abs_186_loss: 0.0113 - val_loss: 11.6911 - val_out_T_loss: 0.3154 - val_out_S_loss: 1.0815 - val_tf.math.abs_185_loss: 0.0249 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 705/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.1390 - out_T_loss: 0.1671 - out_S_loss: 1.3186 - tf.math.abs_185_loss: 0.0281 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.7016 - val_out_T_loss: 0.3113 - val_out_S_loss: 1.0803 - val_tf.math.abs_185_loss: 0.0263 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 706/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.4595 - out_T_loss: 0.1812 - out_S_loss: 1.3476 - tf.math.abs_185_loss: 0.0284 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.6829 - val_out_T_loss: 0.2949 - val_out_S_loss: 1.0812 - val_tf.math.abs_185_loss: 0.0257 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 707/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.2972 - out_T_loss: 0.1713 - out_S_loss: 1.3345 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.6837 - val_out_T_loss: 0.2920 - val_out_S_loss: 1.0821 - val_tf.math.abs_185_loss: 0.0256 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 708/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 13.7737 - out_T_loss: 0.1812 - out_S_loss: 1.2848 - tf.math.abs_185_loss: 0.0264 - tf.math.abs_186_loss: 0.0109 - val_loss: 11.6546 - val_out_T_loss: 0.3113 - val_out_S_loss: 1.0786 - val_tf.math.abs_185_loss: 0.0250 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 709/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 14.6440 - out_T_loss: 0.1902 - out_S_loss: 1.3664 - tf.math.abs_185_loss: 0.0277 - tf.math.abs_186_loss: 0.0118 - val_loss: 11.6505 - val_out_T_loss: 0.3270 - val_out_S_loss: 1.0761 - val_tf.math.abs_185_loss: 0.0251 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 710/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 13.9395 - out_T_loss: 0.1840 - out_S_loss: 1.2991 - tf.math.abs_185_loss: 0.0268 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.6008 - val_out_T_loss: 0.3156 - val_out_S_loss: 1.0733 - val_tf.math.abs_185_loss: 0.0247 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 711/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.4764 - out_T_loss: 0.1957 - out_S_loss: 1.3486 - tf.math.abs_185_loss: 0.0287 - tf.math.abs_186_loss: 0.0110 - val_loss: 11.6600 - val_out_T_loss: 0.3114 - val_out_S_loss: 1.0793 - val_tf.math.abs_185_loss: 0.0248 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 712/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.9461 - out_T_loss: 0.1696 - out_S_loss: 1.2986 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.6959 - val_out_T_loss: 0.3217 - val_out_S_loss: 1.0757 - val_tf.math.abs_185_loss: 0.0278 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 713/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.4564 - out_T_loss: 0.2019 - out_S_loss: 1.3471 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.6061 - val_out_T_loss: 0.3010 - val_out_S_loss: 1.0738 - val_tf.math.abs_185_loss: 0.0253 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 714/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.2338 - out_T_loss: 0.1582 - out_S_loss: 1.3308 - tf.math.abs_185_loss: 0.0272 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.5497 - val_out_T_loss: 0.3180 - val_out_S_loss: 1.0687 - val_tf.math.abs_185_loss: 0.0243 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 715/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.7388 - out_T_loss: 0.1632 - out_S_loss: 1.2798 - tf.math.abs_185_loss: 0.0275 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.5126 - val_out_T_loss: 0.2977 - val_out_S_loss: 1.0663 - val_tf.math.abs_185_loss: 0.0247 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 716/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.0349 - out_T_loss: 0.1672 - out_S_loss: 1.3113 - tf.math.abs_185_loss: 0.0265 - tf.math.abs_186_loss: 0.0113 - val_loss: 11.6127 - val_out_T_loss: 0.3042 - val_out_S_loss: 1.0762 - val_tf.math.abs_185_loss: 0.0244 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 717/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 13.6961 - out_T_loss: 0.1595 - out_S_loss: 1.2779 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0118 - val_loss: 11.5235 - val_out_T_loss: 0.3140 - val_out_S_loss: 1.0658 - val_tf.math.abs_185_loss: 0.0246 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 718/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 13.6732 - out_T_loss: 0.1706 - out_S_loss: 1.2755 - tf.math.abs_185_loss: 0.0262 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.4724 - val_out_T_loss: 0.3157 - val_out_S_loss: 1.0606 - val_tf.math.abs_185_loss: 0.0246 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 719/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.7636 - out_T_loss: 0.1692 - out_S_loss: 1.2844 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0110 - val_loss: 11.4528 - val_out_T_loss: 0.3007 - val_out_S_loss: 1.0595 - val_tf.math.abs_185_loss: 0.0250 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 720/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.2935 - out_T_loss: 0.1679 - out_S_loss: 1.3344 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.4337 - val_out_T_loss: 0.3024 - val_out_S_loss: 1.0584 - val_tf.math.abs_185_loss: 0.0246 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 721/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 13.9015 - out_T_loss: 0.1688 - out_S_loss: 1.2968 - tf.math.abs_185_loss: 0.0270 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.4510 - val_out_T_loss: 0.3070 - val_out_S_loss: 1.0600 - val_tf.math.abs_185_loss: 0.0243 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 722/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 14.2213 - out_T_loss: 0.1686 - out_S_loss: 1.3256 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0118 - val_loss: 11.5219 - val_out_T_loss: 0.2959 - val_out_S_loss: 1.0596 - val_tf.math.abs_185_loss: 0.0284 - val_tf.math.abs_186_loss: 0.0032\n",
      "Epoch 723/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.1288 - out_T_loss: 0.1713 - out_S_loss: 1.3179 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0115 - val_loss: 11.4459 - val_out_T_loss: 0.2994 - val_out_S_loss: 1.0601 - val_tf.math.abs_185_loss: 0.0244 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 724/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.0757 - out_T_loss: 0.1634 - out_S_loss: 1.3145 - tf.math.abs_185_loss: 0.0270 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.4811 - val_out_T_loss: 0.2980 - val_out_S_loss: 1.0638 - val_tf.math.abs_185_loss: 0.0245 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 725/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.5132 - out_T_loss: 0.1770 - out_S_loss: 1.2548 - tf.math.abs_185_loss: 0.0279 - tf.math.abs_186_loss: 0.0115 - val_loss: 11.4101 - val_out_T_loss: 0.3030 - val_out_S_loss: 1.0555 - val_tf.math.abs_185_loss: 0.0246 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 726/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 14.1105 - out_T_loss: 0.1802 - out_S_loss: 1.3172 - tf.math.abs_185_loss: 0.0267 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.4133 - val_out_T_loss: 0.2967 - val_out_S_loss: 1.0559 - val_tf.math.abs_185_loss: 0.0251 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 727/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.4081 - out_T_loss: 0.1618 - out_S_loss: 1.2485 - tf.math.abs_185_loss: 0.0272 - tf.math.abs_186_loss: 0.0109 - val_loss: 11.3958 - val_out_T_loss: 0.2957 - val_out_S_loss: 1.0547 - val_tf.math.abs_185_loss: 0.0247 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 728/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 14.2045 - out_T_loss: 0.1811 - out_S_loss: 1.3256 - tf.math.abs_185_loss: 0.0271 - tf.math.abs_186_loss: 0.0113 - val_loss: 11.4043 - val_out_T_loss: 0.3006 - val_out_S_loss: 1.0544 - val_tf.math.abs_185_loss: 0.0251 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 729/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.7039 - out_T_loss: 0.1704 - out_S_loss: 1.2759 - tf.math.abs_185_loss: 0.0272 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.3583 - val_out_T_loss: 0.3000 - val_out_S_loss: 1.0515 - val_tf.math.abs_185_loss: 0.0243 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 730/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 13.6268 - out_T_loss: 0.1658 - out_S_loss: 1.2703 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.3873 - val_out_T_loss: 0.3053 - val_out_S_loss: 1.0538 - val_tf.math.abs_185_loss: 0.0242 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 731/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.9840 - out_T_loss: 0.1724 - out_S_loss: 1.3036 - tf.math.abs_185_loss: 0.0274 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.3416 - val_out_T_loss: 0.3028 - val_out_S_loss: 1.0489 - val_tf.math.abs_185_loss: 0.0245 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 732/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 14.0118 - out_T_loss: 0.1630 - out_S_loss: 1.3088 - tf.math.abs_185_loss: 0.0271 - tf.math.abs_186_loss: 0.0110 - val_loss: 11.3314 - val_out_T_loss: 0.3008 - val_out_S_loss: 1.0491 - val_tf.math.abs_185_loss: 0.0241 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 733/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.8228 - out_T_loss: 0.1566 - out_S_loss: 1.2912 - tf.math.abs_185_loss: 0.0260 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.3621 - val_out_T_loss: 0.3181 - val_out_S_loss: 1.0483 - val_tf.math.abs_185_loss: 0.0250 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 734/2000\n",
      "24/24 [==============================] - 1s 36ms/step - loss: 14.1652 - out_T_loss: 0.1681 - out_S_loss: 1.3229 - tf.math.abs_185_loss: 0.0268 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.3375 - val_out_T_loss: 0.3013 - val_out_S_loss: 1.0493 - val_tf.math.abs_185_loss: 0.0242 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 735/2000\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 13.7980 - out_T_loss: 0.1501 - out_S_loss: 1.2896 - tf.math.abs_185_loss: 0.0265 - tf.math.abs_186_loss: 0.0111 - val_loss: 11.3573 - val_out_T_loss: 0.2957 - val_out_S_loss: 1.0480 - val_tf.math.abs_185_loss: 0.0261 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 736/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.6932 - out_T_loss: 0.1598 - out_S_loss: 1.2809 - tf.math.abs_185_loss: 0.0252 - tf.math.abs_186_loss: 0.0110 - val_loss: 11.3024 - val_out_T_loss: 0.2984 - val_out_S_loss: 1.0460 - val_tf.math.abs_185_loss: 0.0243 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 737/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.6074 - out_T_loss: 0.1629 - out_S_loss: 1.2663 - tf.math.abs_185_loss: 0.0280 - tf.math.abs_186_loss: 0.0111 - val_loss: 11.2974 - val_out_T_loss: 0.3043 - val_out_S_loss: 1.0457 - val_tf.math.abs_185_loss: 0.0238 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 738/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 14.1628 - out_T_loss: 0.1559 - out_S_loss: 1.3214 - tf.math.abs_185_loss: 0.0281 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.3111 - val_out_T_loss: 0.3016 - val_out_S_loss: 1.0462 - val_tf.math.abs_185_loss: 0.0244 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 739/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.7957 - out_T_loss: 0.1589 - out_S_loss: 1.2890 - tf.math.abs_185_loss: 0.0263 - tf.math.abs_186_loss: 0.0110 - val_loss: 11.2984 - val_out_T_loss: 0.2925 - val_out_S_loss: 1.0434 - val_tf.math.abs_185_loss: 0.0256 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 740/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.4885 - out_T_loss: 0.1640 - out_S_loss: 1.2545 - tf.math.abs_185_loss: 0.0272 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.2671 - val_out_T_loss: 0.3175 - val_out_S_loss: 1.0419 - val_tf.math.abs_185_loss: 0.0236 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 741/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 14.1858 - out_T_loss: 0.1532 - out_S_loss: 1.3301 - tf.math.abs_185_loss: 0.0252 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.2334 - val_out_T_loss: 0.3118 - val_out_S_loss: 1.0399 - val_tf.math.abs_185_loss: 0.0232 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 742/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.4639 - out_T_loss: 0.1513 - out_S_loss: 1.2582 - tf.math.abs_185_loss: 0.0255 - tf.math.abs_186_loss: 0.0111 - val_loss: 11.2637 - val_out_T_loss: 0.2954 - val_out_S_loss: 1.0420 - val_tf.math.abs_185_loss: 0.0245 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 743/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.8212 - out_T_loss: 0.1675 - out_S_loss: 1.2861 - tf.math.abs_185_loss: 0.0283 - tf.math.abs_186_loss: 0.0113 - val_loss: 11.1510 - val_out_T_loss: 0.2992 - val_out_S_loss: 1.0330 - val_tf.math.abs_185_loss: 0.0233 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 744/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.8892 - out_T_loss: 0.1652 - out_S_loss: 1.2957 - tf.math.abs_185_loss: 0.0267 - tf.math.abs_186_loss: 0.0117 - val_loss: 11.2008 - val_out_T_loss: 0.2966 - val_out_S_loss: 1.0352 - val_tf.math.abs_185_loss: 0.0246 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 745/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 14.0039 - out_T_loss: 0.1661 - out_S_loss: 1.3042 - tf.math.abs_185_loss: 0.0285 - tf.math.abs_186_loss: 0.0113 - val_loss: 11.2144 - val_out_T_loss: 0.3098 - val_out_S_loss: 1.0360 - val_tf.math.abs_185_loss: 0.0241 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 746/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.3911 - out_T_loss: 0.1585 - out_S_loss: 1.2487 - tf.math.abs_185_loss: 0.0259 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.2079 - val_out_T_loss: 0.3060 - val_out_S_loss: 1.0361 - val_tf.math.abs_185_loss: 0.0242 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 747/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 13.7718 - out_T_loss: 0.1515 - out_S_loss: 1.2857 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.1862 - val_out_T_loss: 0.2931 - val_out_S_loss: 1.0359 - val_tf.math.abs_185_loss: 0.0238 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 748/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 13.7642 - out_T_loss: 0.1575 - out_S_loss: 1.2855 - tf.math.abs_185_loss: 0.0265 - tf.math.abs_186_loss: 0.0111 - val_loss: 11.1749 - val_out_T_loss: 0.3045 - val_out_S_loss: 1.0339 - val_tf.math.abs_185_loss: 0.0236 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 749/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.8314 - out_T_loss: 0.1664 - out_S_loss: 1.2914 - tf.math.abs_185_loss: 0.0259 - tf.math.abs_186_loss: 0.0116 - val_loss: 11.1610 - val_out_T_loss: 0.2880 - val_out_S_loss: 1.0343 - val_tf.math.abs_185_loss: 0.0237 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 750/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.7644 - out_T_loss: 0.1501 - out_S_loss: 1.2870 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.1436 - val_out_T_loss: 0.2881 - val_out_S_loss: 1.0339 - val_tf.math.abs_185_loss: 0.0230 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 751/2000\n",
      "24/24 [==============================] - 1s 37ms/step - loss: 13.7236 - out_T_loss: 0.1595 - out_S_loss: 1.2827 - tf.math.abs_185_loss: 0.0256 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.1629 - val_out_T_loss: 0.3020 - val_out_S_loss: 1.0335 - val_tf.math.abs_185_loss: 0.0233 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 752/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.4946 - out_T_loss: 0.1404 - out_S_loss: 1.2618 - tf.math.abs_185_loss: 0.0253 - tf.math.abs_186_loss: 0.0115 - val_loss: 11.1879 - val_out_T_loss: 0.3042 - val_out_S_loss: 1.0317 - val_tf.math.abs_185_loss: 0.0253 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 753/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.5036 - out_T_loss: 0.1575 - out_S_loss: 1.2585 - tf.math.abs_185_loss: 0.0268 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.1552 - val_out_T_loss: 0.2974 - val_out_S_loss: 1.0302 - val_tf.math.abs_185_loss: 0.0248 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 754/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 14.0113 - out_T_loss: 0.1547 - out_S_loss: 1.3088 - tf.math.abs_185_loss: 0.0273 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.0998 - val_out_T_loss: 0.3016 - val_out_S_loss: 1.0271 - val_tf.math.abs_185_loss: 0.0235 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 755/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.6573 - out_T_loss: 0.1598 - out_S_loss: 1.2773 - tf.math.abs_185_loss: 0.0249 - tf.math.abs_186_loss: 0.0114 - val_loss: 11.0830 - val_out_T_loss: 0.3075 - val_out_S_loss: 1.0250 - val_tf.math.abs_185_loss: 0.0233 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 756/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.2933 - out_T_loss: 0.1510 - out_S_loss: 1.2417 - tf.math.abs_185_loss: 0.0252 - tf.math.abs_186_loss: 0.0111 - val_loss: 11.0319 - val_out_T_loss: 0.2990 - val_out_S_loss: 1.0206 - val_tf.math.abs_185_loss: 0.0235 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 757/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.4655 - out_T_loss: 0.1498 - out_S_loss: 1.2570 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0112 - val_loss: 11.0375 - val_out_T_loss: 0.2948 - val_out_S_loss: 1.0224 - val_tf.math.abs_185_loss: 0.0231 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 758/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.1945 - out_T_loss: 0.1432 - out_S_loss: 1.2312 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0108 - val_loss: 11.0101 - val_out_T_loss: 0.2884 - val_out_S_loss: 1.0202 - val_tf.math.abs_185_loss: 0.0233 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 759/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.7227 - out_T_loss: 0.1459 - out_S_loss: 1.2833 - tf.math.abs_185_loss: 0.0255 - tf.math.abs_186_loss: 0.0117 - val_loss: 10.9864 - val_out_T_loss: 0.2848 - val_out_S_loss: 1.0191 - val_tf.math.abs_185_loss: 0.0228 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 760/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.4518 - out_T_loss: 0.1517 - out_S_loss: 1.2567 - tf.math.abs_185_loss: 0.0259 - tf.math.abs_186_loss: 0.0108 - val_loss: 11.0252 - val_out_T_loss: 0.2964 - val_out_S_loss: 1.0204 - val_tf.math.abs_185_loss: 0.0233 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 761/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.5013 - out_T_loss: 0.1545 - out_S_loss: 1.2606 - tf.math.abs_185_loss: 0.0260 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.9619 - val_out_T_loss: 0.2958 - val_out_S_loss: 1.0138 - val_tf.math.abs_185_loss: 0.0236 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 762/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.2893 - out_T_loss: 0.1427 - out_S_loss: 1.2427 - tf.math.abs_185_loss: 0.0250 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.9164 - val_out_T_loss: 0.2837 - val_out_S_loss: 1.0120 - val_tf.math.abs_185_loss: 0.0229 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 763/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.2239 - out_T_loss: 0.1504 - out_S_loss: 1.2346 - tf.math.abs_185_loss: 0.0253 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.9582 - val_out_T_loss: 0.2905 - val_out_S_loss: 1.0147 - val_tf.math.abs_185_loss: 0.0232 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 764/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.4335 - out_T_loss: 0.1484 - out_S_loss: 1.2546 - tf.math.abs_185_loss: 0.0257 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.9477 - val_out_T_loss: 0.2965 - val_out_S_loss: 1.0129 - val_tf.math.abs_185_loss: 0.0233 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 765/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.2110 - out_T_loss: 0.1434 - out_S_loss: 1.2352 - tf.math.abs_185_loss: 0.0249 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.9064 - val_out_T_loss: 0.2868 - val_out_S_loss: 1.0095 - val_tf.math.abs_185_loss: 0.0234 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 766/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.2514 - out_T_loss: 0.1427 - out_S_loss: 1.2369 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.8671 - val_out_T_loss: 0.2900 - val_out_S_loss: 1.0076 - val_tf.math.abs_185_loss: 0.0223 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 767/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.8174 - out_T_loss: 0.1591 - out_S_loss: 1.2900 - tf.math.abs_185_loss: 0.0272 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.8998 - val_out_T_loss: 0.2886 - val_out_S_loss: 1.0109 - val_tf.math.abs_185_loss: 0.0222 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 768/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.7352 - out_T_loss: 0.1447 - out_S_loss: 1.2850 - tf.math.abs_185_loss: 0.0259 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.9497 - val_out_T_loss: 0.3044 - val_out_S_loss: 1.0108 - val_tf.math.abs_185_loss: 0.0239 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 769/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.3961 - out_T_loss: 0.1462 - out_S_loss: 1.2507 - tf.math.abs_185_loss: 0.0263 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.9080 - val_out_T_loss: 0.2856 - val_out_S_loss: 1.0085 - val_tf.math.abs_185_loss: 0.0241 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 770/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 12.9643 - out_T_loss: 0.1394 - out_S_loss: 1.2103 - tf.math.abs_185_loss: 0.0251 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.8731 - val_out_T_loss: 0.2879 - val_out_S_loss: 1.0053 - val_tf.math.abs_185_loss: 0.0238 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 771/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.0815 - out_T_loss: 0.1387 - out_S_loss: 1.2215 - tf.math.abs_185_loss: 0.0252 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.8258 - val_out_T_loss: 0.2847 - val_out_S_loss: 1.0009 - val_tf.math.abs_185_loss: 0.0237 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 772/2000\n",
      "24/24 [==============================] - 1s 38ms/step - loss: 13.1771 - out_T_loss: 0.1298 - out_S_loss: 1.2333 - tf.math.abs_185_loss: 0.0243 - tf.math.abs_186_loss: 0.0114 - val_loss: 10.8038 - val_out_T_loss: 0.2868 - val_out_S_loss: 0.9997 - val_tf.math.abs_185_loss: 0.0231 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 773/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.5294 - out_T_loss: 0.1453 - out_S_loss: 1.2659 - tf.math.abs_185_loss: 0.0254 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.7980 - val_out_T_loss: 0.3026 - val_out_S_loss: 0.9989 - val_tf.math.abs_185_loss: 0.0224 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 774/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.5835 - out_T_loss: 0.1443 - out_S_loss: 1.2684 - tf.math.abs_185_loss: 0.0264 - tf.math.abs_186_loss: 0.0114 - val_loss: 10.8314 - val_out_T_loss: 0.2882 - val_out_S_loss: 1.0035 - val_tf.math.abs_185_loss: 0.0227 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 775/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.7153 - out_T_loss: 0.1522 - out_S_loss: 1.2818 - tf.math.abs_185_loss: 0.0256 - tf.math.abs_186_loss: 0.0116 - val_loss: 10.8775 - val_out_T_loss: 0.2992 - val_out_S_loss: 1.0018 - val_tf.math.abs_185_loss: 0.0251 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 776/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.1912 - out_T_loss: 0.1461 - out_S_loss: 1.2318 - tf.math.abs_185_loss: 0.0254 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.8229 - val_out_T_loss: 0.2873 - val_out_S_loss: 1.0018 - val_tf.math.abs_185_loss: 0.0231 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 777/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.4022 - out_T_loss: 0.1436 - out_S_loss: 1.2515 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.8303 - val_out_T_loss: 0.2955 - val_out_S_loss: 1.0009 - val_tf.math.abs_185_loss: 0.0234 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 778/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.9758 - out_T_loss: 0.1548 - out_S_loss: 1.3061 - tf.math.abs_185_loss: 0.0264 - tf.math.abs_186_loss: 0.0116 - val_loss: 10.8577 - val_out_T_loss: 0.2947 - val_out_S_loss: 1.0030 - val_tf.math.abs_185_loss: 0.0238 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 779/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.2711 - out_T_loss: 0.1331 - out_S_loss: 1.2381 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0113 - val_loss: 10.8580 - val_out_T_loss: 0.2930 - val_out_S_loss: 1.0000 - val_tf.math.abs_185_loss: 0.0252 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 780/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.3234 - out_T_loss: 0.1433 - out_S_loss: 1.2420 - tf.math.abs_185_loss: 0.0262 - tf.math.abs_186_loss: 0.0118 - val_loss: 10.8295 - val_out_T_loss: 0.3055 - val_out_S_loss: 0.9992 - val_tf.math.abs_185_loss: 0.0235 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 781/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.0565 - out_T_loss: 0.1390 - out_S_loss: 1.2200 - tf.math.abs_185_loss: 0.0244 - tf.math.abs_186_loss: 0.0115 - val_loss: 10.8092 - val_out_T_loss: 0.3044 - val_out_S_loss: 0.9989 - val_tf.math.abs_185_loss: 0.0228 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 782/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.9647 - out_T_loss: 0.1352 - out_S_loss: 1.2108 - tf.math.abs_185_loss: 0.0256 - tf.math.abs_186_loss: 0.0105 - val_loss: 10.7456 - val_out_T_loss: 0.2770 - val_out_S_loss: 0.9956 - val_tf.math.abs_185_loss: 0.0228 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 783/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.3144 - out_T_loss: 0.1351 - out_S_loss: 1.2432 - tf.math.abs_185_loss: 0.0262 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.7463 - val_out_T_loss: 0.2977 - val_out_S_loss: 0.9948 - val_tf.math.abs_185_loss: 0.0222 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 784/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.2447 - out_T_loss: 0.1529 - out_S_loss: 1.2384 - tf.math.abs_185_loss: 0.0249 - tf.math.abs_186_loss: 0.0105 - val_loss: 10.7124 - val_out_T_loss: 0.2927 - val_out_S_loss: 0.9927 - val_tf.math.abs_185_loss: 0.0219 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 785/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.4291 - out_T_loss: 0.1495 - out_S_loss: 1.2527 - tf.math.abs_185_loss: 0.0263 - tf.math.abs_186_loss: 0.0113 - val_loss: 10.7442 - val_out_T_loss: 0.2896 - val_out_S_loss: 0.9955 - val_tf.math.abs_185_loss: 0.0222 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 786/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.2768 - out_T_loss: 0.1427 - out_S_loss: 1.2433 - tf.math.abs_185_loss: 0.0238 - tf.math.abs_186_loss: 0.0113 - val_loss: 10.7441 - val_out_T_loss: 0.2903 - val_out_S_loss: 0.9955 - val_tf.math.abs_185_loss: 0.0221 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 787/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 13.1135 - out_T_loss: 0.1365 - out_S_loss: 1.2282 - tf.math.abs_185_loss: 0.0238 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.7543 - val_out_T_loss: 0.2921 - val_out_S_loss: 0.9945 - val_tf.math.abs_185_loss: 0.0230 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 788/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.0207 - out_T_loss: 0.1434 - out_S_loss: 1.2162 - tf.math.abs_185_loss: 0.0243 - tf.math.abs_186_loss: 0.0114 - val_loss: 10.7428 - val_out_T_loss: 0.3033 - val_out_S_loss: 0.9930 - val_tf.math.abs_185_loss: 0.0227 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 789/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.6937 - out_T_loss: 0.1421 - out_S_loss: 1.2795 - tf.math.abs_185_loss: 0.0265 - tf.math.abs_186_loss: 0.0114 - val_loss: 10.7221 - val_out_T_loss: 0.2980 - val_out_S_loss: 0.9922 - val_tf.math.abs_185_loss: 0.0222 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 790/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.1211 - out_T_loss: 0.1276 - out_S_loss: 1.2294 - tf.math.abs_185_loss: 0.0239 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.7172 - val_out_T_loss: 0.2904 - val_out_S_loss: 0.9913 - val_tf.math.abs_185_loss: 0.0229 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 791/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.0574 - out_T_loss: 0.1380 - out_S_loss: 1.2192 - tf.math.abs_185_loss: 0.0253 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.6892 - val_out_T_loss: 0.2909 - val_out_S_loss: 0.9882 - val_tf.math.abs_185_loss: 0.0228 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 792/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 12.6554 - out_T_loss: 0.1313 - out_S_loss: 1.1824 - tf.math.abs_185_loss: 0.0243 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.6585 - val_out_T_loss: 0.2846 - val_out_S_loss: 0.9873 - val_tf.math.abs_185_loss: 0.0223 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 793/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.8866 - out_T_loss: 0.1596 - out_S_loss: 1.3010 - tf.math.abs_185_loss: 0.0245 - tf.math.abs_186_loss: 0.0113 - val_loss: 10.6571 - val_out_T_loss: 0.2962 - val_out_S_loss: 0.9859 - val_tf.math.abs_185_loss: 0.0222 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 794/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.3616 - out_T_loss: 0.1461 - out_S_loss: 1.2480 - tf.math.abs_185_loss: 0.0254 - tf.math.abs_186_loss: 0.0114 - val_loss: 10.6565 - val_out_T_loss: 0.3006 - val_out_S_loss: 0.9857 - val_tf.math.abs_185_loss: 0.0221 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 795/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.3742 - out_T_loss: 0.1309 - out_S_loss: 1.2511 - tf.math.abs_185_loss: 0.0254 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.6874 - val_out_T_loss: 0.3011 - val_out_S_loss: 0.9844 - val_tf.math.abs_185_loss: 0.0242 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 796/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.6787 - out_T_loss: 0.1620 - out_S_loss: 1.2756 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0114 - val_loss: 10.6902 - val_out_T_loss: 0.3122 - val_out_S_loss: 0.9851 - val_tf.math.abs_185_loss: 0.0235 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 797/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.5325 - out_T_loss: 0.1412 - out_S_loss: 1.2644 - tf.math.abs_185_loss: 0.0262 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.6343 - val_out_T_loss: 0.2925 - val_out_S_loss: 0.9856 - val_tf.math.abs_185_loss: 0.0217 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 798/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8704 - out_T_loss: 0.1313 - out_S_loss: 1.2047 - tf.math.abs_185_loss: 0.0238 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.6642 - val_out_T_loss: 0.2927 - val_out_S_loss: 0.9867 - val_tf.math.abs_185_loss: 0.0226 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 799/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 13.1218 - out_T_loss: 0.1338 - out_S_loss: 1.2263 - tf.math.abs_185_loss: 0.0251 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.6507 - val_out_T_loss: 0.2910 - val_out_S_loss: 0.9868 - val_tf.math.abs_185_loss: 0.0218 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 800/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.0640 - out_T_loss: 0.1387 - out_S_loss: 1.2214 - tf.math.abs_185_loss: 0.0246 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.6515 - val_out_T_loss: 0.2912 - val_out_S_loss: 0.9865 - val_tf.math.abs_185_loss: 0.0221 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 801/2000\n",
      "24/24 [==============================] - 1s 59ms/step - loss: 13.1685 - out_T_loss: 0.1309 - out_S_loss: 1.2324 - tf.math.abs_185_loss: 0.0245 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.6325 - val_out_T_loss: 0.2847 - val_out_S_loss: 0.9869 - val_tf.math.abs_185_loss: 0.0212 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 802/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 13.1916 - out_T_loss: 0.1450 - out_S_loss: 1.2309 - tf.math.abs_185_loss: 0.0256 - tf.math.abs_186_loss: 0.0113 - val_loss: 10.6548 - val_out_T_loss: 0.3040 - val_out_S_loss: 0.9844 - val_tf.math.abs_185_loss: 0.0223 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 803/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.5772 - out_T_loss: 0.1333 - out_S_loss: 1.1722 - tf.math.abs_185_loss: 0.0250 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.6308 - val_out_T_loss: 0.3019 - val_out_S_loss: 0.9802 - val_tf.math.abs_185_loss: 0.0237 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 804/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.2000 - out_T_loss: 0.1228 - out_S_loss: 1.2357 - tf.math.abs_185_loss: 0.0249 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.6037 - val_out_T_loss: 0.2911 - val_out_S_loss: 0.9796 - val_tf.math.abs_185_loss: 0.0231 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 805/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.4201 - out_T_loss: 0.1291 - out_S_loss: 1.2535 - tf.math.abs_185_loss: 0.0266 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.5793 - val_out_T_loss: 0.2888 - val_out_S_loss: 0.9795 - val_tf.math.abs_185_loss: 0.0219 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 806/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8693 - out_T_loss: 0.1204 - out_S_loss: 1.2053 - tf.math.abs_185_loss: 0.0238 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.5409 - val_out_T_loss: 0.3015 - val_out_S_loss: 0.9752 - val_tf.math.abs_185_loss: 0.0215 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 807/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.0694 - out_T_loss: 0.1371 - out_S_loss: 1.2228 - tf.math.abs_185_loss: 0.0245 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.5459 - val_out_T_loss: 0.2882 - val_out_S_loss: 0.9740 - val_tf.math.abs_185_loss: 0.0231 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 808/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.3071 - out_T_loss: 0.1351 - out_S_loss: 1.2461 - tf.math.abs_185_loss: 0.0240 - tf.math.abs_186_loss: 0.0115 - val_loss: 10.5393 - val_out_T_loss: 0.2894 - val_out_S_loss: 0.9733 - val_tf.math.abs_185_loss: 0.0231 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 809/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.3356 - out_T_loss: 0.1237 - out_S_loss: 1.2486 - tf.math.abs_185_loss: 0.0250 - tf.math.abs_186_loss: 0.0113 - val_loss: 10.5248 - val_out_T_loss: 0.2905 - val_out_S_loss: 0.9749 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 810/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 13.1500 - out_T_loss: 0.1513 - out_S_loss: 1.2260 - tf.math.abs_185_loss: 0.0258 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.5467 - val_out_T_loss: 0.2856 - val_out_S_loss: 0.9770 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 811/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.7926 - out_T_loss: 0.1335 - out_S_loss: 1.1952 - tf.math.abs_185_loss: 0.0245 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.4921 - val_out_T_loss: 0.2985 - val_out_S_loss: 0.9709 - val_tf.math.abs_185_loss: 0.0215 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 812/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.9890 - out_T_loss: 0.1334 - out_S_loss: 1.2155 - tf.math.abs_185_loss: 0.0240 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.5538 - val_out_T_loss: 0.2862 - val_out_S_loss: 0.9758 - val_tf.math.abs_185_loss: 0.0226 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 813/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.5019 - out_T_loss: 0.1326 - out_S_loss: 1.2640 - tf.math.abs_185_loss: 0.0253 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.5432 - val_out_T_loss: 0.2924 - val_out_S_loss: 0.9738 - val_tf.math.abs_185_loss: 0.0229 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 814/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.9086 - out_T_loss: 0.1298 - out_S_loss: 1.2049 - tf.math.abs_185_loss: 0.0254 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.5524 - val_out_T_loss: 0.2808 - val_out_S_loss: 0.9713 - val_tf.math.abs_185_loss: 0.0249 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 815/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 13.3560 - out_T_loss: 0.1464 - out_S_loss: 1.2471 - tf.math.abs_185_loss: 0.0260 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.4704 - val_out_T_loss: 0.2881 - val_out_S_loss: 0.9683 - val_tf.math.abs_185_loss: 0.0222 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 816/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 13.0168 - out_T_loss: 0.1379 - out_S_loss: 1.2152 - tf.math.abs_185_loss: 0.0257 - tf.math.abs_186_loss: 0.0106 - val_loss: 10.4660 - val_out_T_loss: 0.2885 - val_out_S_loss: 0.9683 - val_tf.math.abs_185_loss: 0.0219 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 817/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.6706 - out_T_loss: 0.1203 - out_S_loss: 1.1838 - tf.math.abs_185_loss: 0.0248 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.4711 - val_out_T_loss: 0.3148 - val_out_S_loss: 0.9662 - val_tf.math.abs_185_loss: 0.0220 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 818/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.9199 - out_T_loss: 0.1272 - out_S_loss: 1.2069 - tf.math.abs_185_loss: 0.0250 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.4216 - val_out_T_loss: 0.2930 - val_out_S_loss: 0.9640 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 819/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.9864 - out_T_loss: 0.1389 - out_S_loss: 1.2105 - tf.math.abs_185_loss: 0.0261 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.4369 - val_out_T_loss: 0.2839 - val_out_S_loss: 0.9653 - val_tf.math.abs_185_loss: 0.0223 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 820/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 13.1181 - out_T_loss: 0.1393 - out_S_loss: 1.2241 - tf.math.abs_185_loss: 0.0257 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.4357 - val_out_T_loss: 0.2916 - val_out_S_loss: 0.9641 - val_tf.math.abs_185_loss: 0.0223 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 821/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.6922 - out_T_loss: 0.1251 - out_S_loss: 1.1876 - tf.math.abs_185_loss: 0.0234 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.4423 - val_out_T_loss: 0.2940 - val_out_S_loss: 0.9673 - val_tf.math.abs_185_loss: 0.0210 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 822/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8987 - out_T_loss: 0.1233 - out_S_loss: 1.2084 - tf.math.abs_185_loss: 0.0236 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.4224 - val_out_T_loss: 0.2899 - val_out_S_loss: 0.9666 - val_tf.math.abs_185_loss: 0.0207 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 823/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.5654 - out_T_loss: 0.1249 - out_S_loss: 1.1735 - tf.math.abs_185_loss: 0.0244 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.4264 - val_out_T_loss: 0.2789 - val_out_S_loss: 0.9659 - val_tf.math.abs_185_loss: 0.0217 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 824/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.5980 - out_T_loss: 0.1217 - out_S_loss: 1.1782 - tf.math.abs_185_loss: 0.0238 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.4172 - val_out_T_loss: 0.2940 - val_out_S_loss: 0.9640 - val_tf.math.abs_185_loss: 0.0214 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 825/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.9238 - out_T_loss: 0.1338 - out_S_loss: 1.2098 - tf.math.abs_185_loss: 0.0239 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.3965 - val_out_T_loss: 0.3045 - val_out_S_loss: 0.9588 - val_tf.math.abs_185_loss: 0.0224 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 826/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.1777 - out_T_loss: 0.1314 - out_S_loss: 1.2337 - tf.math.abs_185_loss: 0.0248 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.3392 - val_out_T_loss: 0.2867 - val_out_S_loss: 0.9570 - val_tf.math.abs_185_loss: 0.0214 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 827/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.5235 - out_T_loss: 0.1213 - out_S_loss: 1.1719 - tf.math.abs_185_loss: 0.0231 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.3287 - val_out_T_loss: 0.2884 - val_out_S_loss: 0.9548 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 828/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.3688 - out_T_loss: 0.1262 - out_S_loss: 1.2546 - tf.math.abs_185_loss: 0.0239 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.2936 - val_out_T_loss: 0.2884 - val_out_S_loss: 0.9519 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 829/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8724 - out_T_loss: 0.1346 - out_S_loss: 1.2044 - tf.math.abs_185_loss: 0.0240 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.2968 - val_out_T_loss: 0.2873 - val_out_S_loss: 0.9537 - val_tf.math.abs_185_loss: 0.0209 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 830/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.7976 - out_T_loss: 0.1179 - out_S_loss: 1.1954 - tf.math.abs_185_loss: 0.0249 - tf.math.abs_186_loss: 0.0114 - val_loss: 10.3077 - val_out_T_loss: 0.2917 - val_out_S_loss: 0.9536 - val_tf.math.abs_185_loss: 0.0213 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 831/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.4811 - out_T_loss: 0.1281 - out_S_loss: 1.1675 - tf.math.abs_185_loss: 0.0234 - tf.math.abs_186_loss: 0.0105 - val_loss: 10.2691 - val_out_T_loss: 0.2882 - val_out_S_loss: 0.9505 - val_tf.math.abs_185_loss: 0.0210 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 832/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8425 - out_T_loss: 0.1251 - out_S_loss: 1.2020 - tf.math.abs_185_loss: 0.0241 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.2753 - val_out_T_loss: 0.2897 - val_out_S_loss: 0.9511 - val_tf.math.abs_185_loss: 0.0210 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 833/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8471 - out_T_loss: 0.1194 - out_S_loss: 1.2027 - tf.math.abs_185_loss: 0.0245 - tf.math.abs_186_loss: 0.0105 - val_loss: 10.2637 - val_out_T_loss: 0.2956 - val_out_S_loss: 0.9490 - val_tf.math.abs_185_loss: 0.0212 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 834/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.2048 - out_T_loss: 0.1182 - out_S_loss: 1.1374 - tf.math.abs_185_loss: 0.0249 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.2669 - val_out_T_loss: 0.3052 - val_out_S_loss: 0.9492 - val_tf.math.abs_185_loss: 0.0207 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 835/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8335 - out_T_loss: 0.1235 - out_S_loss: 1.2035 - tf.math.abs_185_loss: 0.0229 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.2164 - val_out_T_loss: 0.2890 - val_out_S_loss: 0.9457 - val_tf.math.abs_185_loss: 0.0209 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 836/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8380 - out_T_loss: 0.1325 - out_S_loss: 1.2005 - tf.math.abs_185_loss: 0.0243 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.2738 - val_out_T_loss: 0.2972 - val_out_S_loss: 0.9478 - val_tf.math.abs_185_loss: 0.0221 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 837/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 13.2703 - out_T_loss: 0.1319 - out_S_loss: 1.2433 - tf.math.abs_185_loss: 0.0243 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.3357 - val_out_T_loss: 0.3056 - val_out_S_loss: 0.9500 - val_tf.math.abs_185_loss: 0.0237 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 838/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8704 - out_T_loss: 0.1215 - out_S_loss: 1.2039 - tf.math.abs_185_loss: 0.0252 - tf.math.abs_186_loss: 0.0103 - val_loss: 10.2778 - val_out_T_loss: 0.2946 - val_out_S_loss: 0.9509 - val_tf.math.abs_185_loss: 0.0209 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 839/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 12.6429 - out_T_loss: 0.1224 - out_S_loss: 1.1814 - tf.math.abs_185_loss: 0.0240 - tf.math.abs_186_loss: 0.0113 - val_loss: 10.3044 - val_out_T_loss: 0.2849 - val_out_S_loss: 0.9528 - val_tf.math.abs_185_loss: 0.0218 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 840/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 13.5160 - out_T_loss: 0.1258 - out_S_loss: 1.2660 - tf.math.abs_185_loss: 0.0254 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.2502 - val_out_T_loss: 0.2773 - val_out_S_loss: 0.9488 - val_tf.math.abs_185_loss: 0.0215 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 841/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 12.4480 - out_T_loss: 0.1245 - out_S_loss: 1.1612 - tf.math.abs_185_loss: 0.0245 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.2751 - val_out_T_loss: 0.2969 - val_out_S_loss: 0.9511 - val_tf.math.abs_185_loss: 0.0204 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 842/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.2820 - out_T_loss: 0.1274 - out_S_loss: 1.2446 - tf.math.abs_185_loss: 0.0242 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.2443 - val_out_T_loss: 0.2930 - val_out_S_loss: 0.9483 - val_tf.math.abs_185_loss: 0.0207 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 843/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 13.1739 - out_T_loss: 0.1363 - out_S_loss: 1.2326 - tf.math.abs_185_loss: 0.0248 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.2675 - val_out_T_loss: 0.2787 - val_out_S_loss: 0.9458 - val_tf.math.abs_185_loss: 0.0239 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 844/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.4952 - out_T_loss: 0.1259 - out_S_loss: 1.1650 - tf.math.abs_185_loss: 0.0251 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.2028 - val_out_T_loss: 0.2940 - val_out_S_loss: 0.9429 - val_tf.math.abs_185_loss: 0.0213 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 845/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.6601 - out_T_loss: 0.1291 - out_S_loss: 1.1815 - tf.math.abs_185_loss: 0.0246 - tf.math.abs_186_loss: 0.0113 - val_loss: 10.1844 - val_out_T_loss: 0.2789 - val_out_S_loss: 0.9408 - val_tf.math.abs_185_loss: 0.0221 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 846/2000\n",
      "24/24 [==============================] - 1s 59ms/step - loss: 12.8293 - out_T_loss: 0.1137 - out_S_loss: 1.2017 - tf.math.abs_185_loss: 0.0239 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.1913 - val_out_T_loss: 0.2801 - val_out_S_loss: 0.9397 - val_tf.math.abs_185_loss: 0.0231 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 847/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.6470 - out_T_loss: 0.1097 - out_S_loss: 1.1835 - tf.math.abs_185_loss: 0.0243 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.1823 - val_out_T_loss: 0.2809 - val_out_S_loss: 0.9409 - val_tf.math.abs_185_loss: 0.0220 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 848/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.0630 - out_T_loss: 0.1347 - out_S_loss: 1.2247 - tf.math.abs_185_loss: 0.0240 - tf.math.abs_186_loss: 0.0101 - val_loss: 10.1975 - val_out_T_loss: 0.2822 - val_out_S_loss: 0.9430 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 849/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.4465 - out_T_loss: 0.1232 - out_S_loss: 1.1637 - tf.math.abs_185_loss: 0.0235 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.1655 - val_out_T_loss: 0.2858 - val_out_S_loss: 0.9413 - val_tf.math.abs_185_loss: 0.0207 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 850/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.4126 - out_T_loss: 0.1164 - out_S_loss: 1.1602 - tf.math.abs_185_loss: 0.0237 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.1582 - val_out_T_loss: 0.2868 - val_out_S_loss: 0.9414 - val_tf.math.abs_185_loss: 0.0203 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 851/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.5776 - out_T_loss: 0.1256 - out_S_loss: 1.1788 - tf.math.abs_185_loss: 0.0229 - tf.math.abs_186_loss: 0.0103 - val_loss: 10.2002 - val_out_T_loss: 0.2944 - val_out_S_loss: 0.9413 - val_tf.math.abs_185_loss: 0.0219 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 852/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.3528 - out_T_loss: 0.1222 - out_S_loss: 1.1518 - tf.math.abs_185_loss: 0.0247 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.2159 - val_out_T_loss: 0.2927 - val_out_S_loss: 0.9382 - val_tf.math.abs_185_loss: 0.0240 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 853/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.4193 - out_T_loss: 0.1123 - out_S_loss: 1.1589 - tf.math.abs_185_loss: 0.0247 - tf.math.abs_186_loss: 0.0112 - val_loss: 10.1730 - val_out_T_loss: 0.2909 - val_out_S_loss: 0.9395 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 854/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 12.4330 - out_T_loss: 0.1148 - out_S_loss: 1.1634 - tf.math.abs_185_loss: 0.0235 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.1532 - val_out_T_loss: 0.2776 - val_out_S_loss: 0.9374 - val_tf.math.abs_185_loss: 0.0223 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 855/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 12.3874 - out_T_loss: 0.1304 - out_S_loss: 1.1545 - tf.math.abs_185_loss: 0.0249 - tf.math.abs_186_loss: 0.0106 - val_loss: 10.1047 - val_out_T_loss: 0.2748 - val_out_S_loss: 0.9354 - val_tf.math.abs_185_loss: 0.0211 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 856/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.4682 - out_T_loss: 0.1158 - out_S_loss: 1.1652 - tf.math.abs_185_loss: 0.0239 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.1648 - val_out_T_loss: 0.2898 - val_out_S_loss: 0.9392 - val_tf.math.abs_185_loss: 0.0215 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 857/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.4747 - out_T_loss: 0.1236 - out_S_loss: 1.2650 - tf.math.abs_185_loss: 0.0240 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.2313 - val_out_T_loss: 0.3023 - val_out_S_loss: 0.9426 - val_tf.math.abs_185_loss: 0.0223 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 858/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.3556 - out_T_loss: 0.1211 - out_S_loss: 1.1549 - tf.math.abs_185_loss: 0.0237 - tf.math.abs_186_loss: 0.0106 - val_loss: 10.1218 - val_out_T_loss: 0.2858 - val_out_S_loss: 0.9366 - val_tf.math.abs_185_loss: 0.0208 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 859/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.4874 - out_T_loss: 0.1080 - out_S_loss: 1.1698 - tf.math.abs_185_loss: 0.0231 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.1272 - val_out_T_loss: 0.2850 - val_out_S_loss: 0.9383 - val_tf.math.abs_185_loss: 0.0203 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 860/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 12.3018 - out_T_loss: 0.1080 - out_S_loss: 1.1506 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.1227 - val_out_T_loss: 0.2842 - val_out_S_loss: 0.9352 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 861/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.6886 - out_T_loss: 0.1160 - out_S_loss: 1.1861 - tf.math.abs_185_loss: 0.0248 - tf.math.abs_186_loss: 0.0108 - val_loss: 10.1365 - val_out_T_loss: 0.2944 - val_out_S_loss: 0.9350 - val_tf.math.abs_185_loss: 0.0218 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 862/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 12.4623 - out_T_loss: 0.1259 - out_S_loss: 1.1628 - tf.math.abs_185_loss: 0.0244 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.1176 - val_out_T_loss: 0.2901 - val_out_S_loss: 0.9343 - val_tf.math.abs_185_loss: 0.0214 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 863/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.5606 - out_T_loss: 0.1302 - out_S_loss: 1.1733 - tf.math.abs_185_loss: 0.0242 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.1559 - val_out_T_loss: 0.3175 - val_out_S_loss: 0.9354 - val_tf.math.abs_185_loss: 0.0214 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 864/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 13.2326 - out_T_loss: 0.1279 - out_S_loss: 1.2387 - tf.math.abs_185_loss: 0.0244 - tf.math.abs_186_loss: 0.0115 - val_loss: 10.0876 - val_out_T_loss: 0.2783 - val_out_S_loss: 0.9351 - val_tf.math.abs_185_loss: 0.0203 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 865/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.4251 - out_T_loss: 0.0990 - out_S_loss: 1.1647 - tf.math.abs_185_loss: 0.0231 - tf.math.abs_186_loss: 0.0109 - val_loss: 10.0632 - val_out_T_loss: 0.2846 - val_out_S_loss: 0.9322 - val_tf.math.abs_185_loss: 0.0202 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 866/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.3911 - out_T_loss: 0.1272 - out_S_loss: 1.1567 - tf.math.abs_185_loss: 0.0244 - tf.math.abs_186_loss: 0.0105 - val_loss: 10.0812 - val_out_T_loss: 0.2952 - val_out_S_loss: 0.9325 - val_tf.math.abs_185_loss: 0.0204 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 867/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.4098 - out_T_loss: 0.1099 - out_S_loss: 1.1620 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0107 - val_loss: 10.1079 - val_out_T_loss: 0.2852 - val_out_S_loss: 0.9349 - val_tf.math.abs_185_loss: 0.0208 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 868/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.5492 - out_T_loss: 0.1077 - out_S_loss: 1.1758 - tf.math.abs_185_loss: 0.0231 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.0415 - val_out_T_loss: 0.2842 - val_out_S_loss: 0.9293 - val_tf.math.abs_185_loss: 0.0205 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 869/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.5535 - out_T_loss: 0.1038 - out_S_loss: 1.1763 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0110 - val_loss: 10.0577 - val_out_T_loss: 0.2918 - val_out_S_loss: 0.9286 - val_tf.math.abs_185_loss: 0.0212 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 870/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.5028 - out_T_loss: 0.0970 - out_S_loss: 1.1727 - tf.math.abs_185_loss: 0.0234 - tf.math.abs_186_loss: 0.0106 - val_loss: 10.0336 - val_out_T_loss: 0.2769 - val_out_S_loss: 0.9305 - val_tf.math.abs_185_loss: 0.0199 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 871/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.3144 - out_T_loss: 0.1270 - out_S_loss: 1.1488 - tf.math.abs_185_loss: 0.0244 - tf.math.abs_186_loss: 0.0106 - val_loss: 10.0187 - val_out_T_loss: 0.2736 - val_out_S_loss: 0.9271 - val_tf.math.abs_185_loss: 0.0211 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 872/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.4650 - out_T_loss: 0.1229 - out_S_loss: 1.1645 - tf.math.abs_185_loss: 0.0237 - tf.math.abs_186_loss: 0.0111 - val_loss: 10.0070 - val_out_T_loss: 0.2777 - val_out_S_loss: 0.9255 - val_tf.math.abs_185_loss: 0.0210 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 873/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 12.1403 - out_T_loss: 0.1119 - out_S_loss: 1.1339 - tf.math.abs_185_loss: 0.0236 - tf.math.abs_186_loss: 0.0109 - val_loss: 9.9742 - val_out_T_loss: 0.2782 - val_out_S_loss: 0.9222 - val_tf.math.abs_185_loss: 0.0209 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 874/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.2277 - out_T_loss: 0.1154 - out_S_loss: 1.1434 - tf.math.abs_185_loss: 0.0234 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.9763 - val_out_T_loss: 0.2758 - val_out_S_loss: 0.9214 - val_tf.math.abs_185_loss: 0.0217 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 875/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.4414 - out_T_loss: 0.1163 - out_S_loss: 1.1647 - tf.math.abs_185_loss: 0.0234 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.9728 - val_out_T_loss: 0.2908 - val_out_S_loss: 0.9213 - val_tf.math.abs_185_loss: 0.0208 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 876/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 12.2874 - out_T_loss: 0.1120 - out_S_loss: 1.1486 - tf.math.abs_185_loss: 0.0239 - tf.math.abs_186_loss: 0.0106 - val_loss: 10.0076 - val_out_T_loss: 0.2692 - val_out_S_loss: 0.9231 - val_tf.math.abs_185_loss: 0.0227 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 877/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.3241 - out_T_loss: 0.1099 - out_S_loss: 1.1534 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.9436 - val_out_T_loss: 0.2930 - val_out_S_loss: 0.9190 - val_tf.math.abs_185_loss: 0.0204 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 878/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.1120 - out_T_loss: 0.1042 - out_S_loss: 1.1331 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.9948 - val_out_T_loss: 0.2936 - val_out_S_loss: 0.9188 - val_tf.math.abs_185_loss: 0.0230 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 879/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8411 - out_T_loss: 0.1182 - out_S_loss: 1.2041 - tf.math.abs_185_loss: 0.0234 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.9506 - val_out_T_loss: 0.2883 - val_out_S_loss: 0.9184 - val_tf.math.abs_185_loss: 0.0211 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 880/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.6589 - out_T_loss: 0.1175 - out_S_loss: 1.1852 - tf.math.abs_185_loss: 0.0237 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.9446 - val_out_T_loss: 0.2897 - val_out_S_loss: 0.9173 - val_tf.math.abs_185_loss: 0.0214 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 881/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8332 - out_T_loss: 0.1204 - out_S_loss: 1.2019 - tf.math.abs_185_loss: 0.0240 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.9237 - val_out_T_loss: 0.2968 - val_out_S_loss: 0.9148 - val_tf.math.abs_185_loss: 0.0213 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 882/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.9823 - out_T_loss: 0.1110 - out_S_loss: 1.1203 - tf.math.abs_185_loss: 0.0230 - tf.math.abs_186_loss: 0.0104 - val_loss: 10.0064 - val_out_T_loss: 0.2916 - val_out_S_loss: 0.9210 - val_tf.math.abs_185_loss: 0.0225 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 883/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.2801 - out_T_loss: 0.1039 - out_S_loss: 1.1500 - tf.math.abs_185_loss: 0.0232 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.9682 - val_out_T_loss: 0.2860 - val_out_S_loss: 0.9186 - val_tf.math.abs_185_loss: 0.0222 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 884/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.1466 - out_T_loss: 0.1014 - out_S_loss: 1.1403 - tf.math.abs_185_loss: 0.0215 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.8753 - val_out_T_loss: 0.2864 - val_out_S_loss: 0.9142 - val_tf.math.abs_185_loss: 0.0198 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 885/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.8955 - out_T_loss: 0.1100 - out_S_loss: 1.2100 - tf.math.abs_185_loss: 0.0234 - tf.math.abs_186_loss: 0.0109 - val_loss: 9.9107 - val_out_T_loss: 0.2846 - val_out_S_loss: 0.9138 - val_tf.math.abs_185_loss: 0.0217 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 886/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.8955 - out_T_loss: 0.1086 - out_S_loss: 1.1116 - tf.math.abs_185_loss: 0.0229 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.9147 - val_out_T_loss: 0.3035 - val_out_S_loss: 0.9144 - val_tf.math.abs_185_loss: 0.0206 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 887/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.6200 - out_T_loss: 0.1113 - out_S_loss: 1.1841 - tf.math.abs_185_loss: 0.0227 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.8940 - val_out_T_loss: 0.2843 - val_out_S_loss: 0.9122 - val_tf.math.abs_185_loss: 0.0217 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 888/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 12.7156 - out_T_loss: 0.1001 - out_S_loss: 1.1906 - tf.math.abs_185_loss: 0.0244 - tf.math.abs_186_loss: 0.0110 - val_loss: 9.9306 - val_out_T_loss: 0.2783 - val_out_S_loss: 0.9169 - val_tf.math.abs_185_loss: 0.0216 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 889/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.2575 - out_T_loss: 0.1077 - out_S_loss: 1.1469 - tf.math.abs_185_loss: 0.0235 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.8957 - val_out_T_loss: 0.2842 - val_out_S_loss: 0.9092 - val_tf.math.abs_185_loss: 0.0230 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 890/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.1665 - out_T_loss: 0.1104 - out_S_loss: 1.1383 - tf.math.abs_185_loss: 0.0232 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.8848 - val_out_T_loss: 0.2875 - val_out_S_loss: 0.9084 - val_tf.math.abs_185_loss: 0.0231 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 891/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 13.1223 - out_T_loss: 0.1157 - out_S_loss: 1.2300 - tf.math.abs_185_loss: 0.0242 - tf.math.abs_186_loss: 0.0111 - val_loss: 9.8831 - val_out_T_loss: 0.2718 - val_out_S_loss: 0.9136 - val_tf.math.abs_185_loss: 0.0210 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 892/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.9323 - out_T_loss: 0.1042 - out_S_loss: 1.1149 - tf.math.abs_185_loss: 0.0231 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.8666 - val_out_T_loss: 0.2801 - val_out_S_loss: 0.9145 - val_tf.math.abs_185_loss: 0.0195 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 893/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.9036 - out_T_loss: 0.1016 - out_S_loss: 1.1143 - tf.math.abs_185_loss: 0.0226 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.8026 - val_out_T_loss: 0.2785 - val_out_S_loss: 0.9075 - val_tf.math.abs_185_loss: 0.0200 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 894/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.1576 - out_T_loss: 0.0968 - out_S_loss: 1.1369 - tf.math.abs_185_loss: 0.0240 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.8053 - val_out_T_loss: 0.2789 - val_out_S_loss: 0.9067 - val_tf.math.abs_185_loss: 0.0204 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 895/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.5362 - out_T_loss: 0.1124 - out_S_loss: 1.1736 - tf.math.abs_185_loss: 0.0237 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.7641 - val_out_T_loss: 0.2720 - val_out_S_loss: 0.9033 - val_tf.math.abs_185_loss: 0.0203 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 896/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.9681 - out_T_loss: 0.1007 - out_S_loss: 1.1178 - tf.math.abs_185_loss: 0.0242 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.8117 - val_out_T_loss: 0.2880 - val_out_S_loss: 0.9056 - val_tf.math.abs_185_loss: 0.0207 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 897/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.2513 - out_T_loss: 0.1017 - out_S_loss: 1.1459 - tf.math.abs_185_loss: 0.0237 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.7618 - val_out_T_loss: 0.2746 - val_out_S_loss: 0.9042 - val_tf.math.abs_185_loss: 0.0196 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 898/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.7216 - out_T_loss: 0.1150 - out_S_loss: 1.1918 - tf.math.abs_185_loss: 0.0235 - tf.math.abs_186_loss: 0.0109 - val_loss: 9.7471 - val_out_T_loss: 0.2712 - val_out_S_loss: 0.9033 - val_tf.math.abs_185_loss: 0.0195 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 899/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 12.3534 - out_T_loss: 0.0951 - out_S_loss: 1.1596 - tf.math.abs_185_loss: 0.0225 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.7759 - val_out_T_loss: 0.2839 - val_out_S_loss: 0.9032 - val_tf.math.abs_185_loss: 0.0203 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 900/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.0203 - out_T_loss: 0.1052 - out_S_loss: 1.1224 - tf.math.abs_185_loss: 0.0242 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.7236 - val_out_T_loss: 0.2787 - val_out_S_loss: 0.8999 - val_tf.math.abs_185_loss: 0.0198 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 901/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.2109 - out_T_loss: 0.1085 - out_S_loss: 1.1442 - tf.math.abs_185_loss: 0.0224 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.7734 - val_out_T_loss: 0.2895 - val_out_S_loss: 0.9028 - val_tf.math.abs_185_loss: 0.0202 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 902/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.9626 - out_T_loss: 0.1068 - out_S_loss: 1.1192 - tf.math.abs_185_loss: 0.0226 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.8081 - val_out_T_loss: 0.2904 - val_out_S_loss: 0.9052 - val_tf.math.abs_185_loss: 0.0207 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 903/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.8871 - out_T_loss: 0.0998 - out_S_loss: 1.2126 - tf.math.abs_185_loss: 0.0225 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.8195 - val_out_T_loss: 0.2876 - val_out_S_loss: 0.9074 - val_tf.math.abs_185_loss: 0.0204 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 904/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.6760 - out_T_loss: 0.1026 - out_S_loss: 1.0940 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.8020 - val_out_T_loss: 0.2853 - val_out_S_loss: 0.9066 - val_tf.math.abs_185_loss: 0.0200 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 905/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.0643 - out_T_loss: 0.0997 - out_S_loss: 1.1303 - tf.math.abs_185_loss: 0.0228 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.7941 - val_out_T_loss: 0.2843 - val_out_S_loss: 0.9056 - val_tf.math.abs_185_loss: 0.0201 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 906/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.8001 - out_T_loss: 0.0913 - out_S_loss: 1.1044 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0099 - val_loss: 9.7855 - val_out_T_loss: 0.2814 - val_out_S_loss: 0.9024 - val_tf.math.abs_185_loss: 0.0215 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 907/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.6069 - out_T_loss: 0.1107 - out_S_loss: 1.1828 - tf.math.abs_185_loss: 0.0229 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.7418 - val_out_T_loss: 0.2731 - val_out_S_loss: 0.9002 - val_tf.math.abs_185_loss: 0.0207 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 908/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.3874 - out_T_loss: 0.1084 - out_S_loss: 1.1607 - tf.math.abs_185_loss: 0.0228 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.7431 - val_out_T_loss: 0.2877 - val_out_S_loss: 0.9016 - val_tf.math.abs_185_loss: 0.0194 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 909/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.9587 - out_T_loss: 0.1003 - out_S_loss: 1.1199 - tf.math.abs_185_loss: 0.0226 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.7479 - val_out_T_loss: 0.2818 - val_out_S_loss: 0.9038 - val_tf.math.abs_185_loss: 0.0189 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 910/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.7996 - out_T_loss: 0.0974 - out_S_loss: 1.1052 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.7679 - val_out_T_loss: 0.2781 - val_out_S_loss: 0.9017 - val_tf.math.abs_185_loss: 0.0211 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 911/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.3975 - out_T_loss: 0.1053 - out_S_loss: 1.1617 - tf.math.abs_185_loss: 0.0231 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.7330 - val_out_T_loss: 0.2769 - val_out_S_loss: 0.9030 - val_tf.math.abs_185_loss: 0.0188 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 912/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.7846 - out_T_loss: 0.1008 - out_S_loss: 1.2021 - tf.math.abs_185_loss: 0.0228 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.7332 - val_out_T_loss: 0.2882 - val_out_S_loss: 0.9006 - val_tf.math.abs_185_loss: 0.0194 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 913/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.7366 - out_T_loss: 0.0930 - out_S_loss: 1.1005 - tf.math.abs_185_loss: 0.0217 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.7087 - val_out_T_loss: 0.2742 - val_out_S_loss: 0.9007 - val_tf.math.abs_185_loss: 0.0189 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 914/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.5874 - out_T_loss: 0.0964 - out_S_loss: 1.0858 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.7041 - val_out_T_loss: 0.2789 - val_out_S_loss: 0.8985 - val_tf.math.abs_185_loss: 0.0195 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 915/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.8188 - out_T_loss: 0.0910 - out_S_loss: 1.1079 - tf.math.abs_185_loss: 0.0227 - tf.math.abs_186_loss: 0.0098 - val_loss: 9.7265 - val_out_T_loss: 0.2712 - val_out_S_loss: 0.8973 - val_tf.math.abs_185_loss: 0.0215 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 916/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.1357 - out_T_loss: 0.0982 - out_S_loss: 1.1381 - tf.math.abs_185_loss: 0.0228 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.6707 - val_out_T_loss: 0.2722 - val_out_S_loss: 0.8955 - val_tf.math.abs_185_loss: 0.0196 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 917/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.0922 - out_T_loss: 0.0998 - out_S_loss: 1.1318 - tf.math.abs_185_loss: 0.0230 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.6720 - val_out_T_loss: 0.2749 - val_out_S_loss: 0.8958 - val_tf.math.abs_185_loss: 0.0193 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 918/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.7887 - out_T_loss: 0.0869 - out_S_loss: 1.1067 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.6872 - val_out_T_loss: 0.2904 - val_out_S_loss: 0.8975 - val_tf.math.abs_185_loss: 0.0186 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 919/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.0227 - out_T_loss: 0.1107 - out_S_loss: 1.1238 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.6842 - val_out_T_loss: 0.2846 - val_out_S_loss: 0.8951 - val_tf.math.abs_185_loss: 0.0199 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 920/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.4276 - out_T_loss: 0.0966 - out_S_loss: 1.0689 - tf.math.abs_185_loss: 0.0220 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.6395 - val_out_T_loss: 0.2853 - val_out_S_loss: 0.8928 - val_tf.math.abs_185_loss: 0.0188 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 921/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.9292 - out_T_loss: 0.0899 - out_S_loss: 1.1198 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.6516 - val_out_T_loss: 0.2755 - val_out_S_loss: 0.8932 - val_tf.math.abs_185_loss: 0.0196 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 922/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 12.1516 - out_T_loss: 0.1015 - out_S_loss: 1.1379 - tf.math.abs_185_loss: 0.0227 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.6905 - val_out_T_loss: 0.2839 - val_out_S_loss: 0.8949 - val_tf.math.abs_185_loss: 0.0201 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 923/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 12.2616 - out_T_loss: 0.0993 - out_S_loss: 1.1482 - tf.math.abs_185_loss: 0.0235 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.6109 - val_out_T_loss: 0.2733 - val_out_S_loss: 0.8881 - val_tf.math.abs_185_loss: 0.0204 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 924/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.9348 - out_T_loss: 0.0981 - out_S_loss: 1.1172 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0109 - val_loss: 9.6621 - val_out_T_loss: 0.2705 - val_out_S_loss: 0.8902 - val_tf.math.abs_185_loss: 0.0215 - val_tf.math.abs_186_loss: 0.0029\n",
      "Epoch 925/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 12.1523 - out_T_loss: 0.0908 - out_S_loss: 1.1389 - tf.math.abs_185_loss: 0.0229 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.6196 - val_out_T_loss: 0.2761 - val_out_S_loss: 0.8898 - val_tf.math.abs_185_loss: 0.0198 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 926/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.9399 - out_T_loss: 0.0908 - out_S_loss: 1.1195 - tf.math.abs_185_loss: 0.0221 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.5935 - val_out_T_loss: 0.2809 - val_out_S_loss: 0.8881 - val_tf.math.abs_185_loss: 0.0191 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 927/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.1311 - out_T_loss: 0.1053 - out_S_loss: 1.1361 - tf.math.abs_185_loss: 0.0228 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.6122 - val_out_T_loss: 0.2739 - val_out_S_loss: 0.8895 - val_tf.math.abs_185_loss: 0.0196 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 928/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.3632 - out_T_loss: 0.0956 - out_S_loss: 1.1614 - tf.math.abs_185_loss: 0.0224 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.6646 - val_out_T_loss: 0.2783 - val_out_S_loss: 0.8930 - val_tf.math.abs_185_loss: 0.0202 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 929/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.0398 - out_T_loss: 0.0857 - out_S_loss: 1.1302 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.6633 - val_out_T_loss: 0.2856 - val_out_S_loss: 0.8898 - val_tf.math.abs_185_loss: 0.0214 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 930/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 11.9830 - out_T_loss: 0.1060 - out_S_loss: 1.1191 - tf.math.abs_185_loss: 0.0236 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.6221 - val_out_T_loss: 0.2879 - val_out_S_loss: 0.8889 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 931/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.5832 - out_T_loss: 0.0959 - out_S_loss: 1.0844 - tf.math.abs_185_loss: 0.0220 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.5695 - val_out_T_loss: 0.2725 - val_out_S_loss: 0.8869 - val_tf.math.abs_185_loss: 0.0188 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 932/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.1685 - out_T_loss: 0.0975 - out_S_loss: 1.1419 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.6554 - val_out_T_loss: 0.2745 - val_out_S_loss: 0.8871 - val_tf.math.abs_185_loss: 0.0224 - val_tf.math.abs_186_loss: 0.0031\n",
      "Epoch 933/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.1974 - out_T_loss: 0.0998 - out_S_loss: 1.1412 - tf.math.abs_185_loss: 0.0230 - tf.math.abs_186_loss: 0.0112 - val_loss: 9.6502 - val_out_T_loss: 0.2840 - val_out_S_loss: 0.8918 - val_tf.math.abs_185_loss: 0.0198 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 934/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.7192 - out_T_loss: 0.0922 - out_S_loss: 1.0964 - tf.math.abs_185_loss: 0.0227 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.6489 - val_out_T_loss: 0.2799 - val_out_S_loss: 0.8881 - val_tf.math.abs_185_loss: 0.0219 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 935/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.8819 - out_T_loss: 0.0824 - out_S_loss: 1.1157 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.6164 - val_out_T_loss: 0.2882 - val_out_S_loss: 0.8877 - val_tf.math.abs_185_loss: 0.0198 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 936/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.2863 - out_T_loss: 0.0986 - out_S_loss: 1.1507 - tf.math.abs_185_loss: 0.0232 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.5924 - val_out_T_loss: 0.2831 - val_out_S_loss: 0.8865 - val_tf.math.abs_185_loss: 0.0195 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 937/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.7400 - out_T_loss: 0.0866 - out_S_loss: 1.0998 - tf.math.abs_185_loss: 0.0221 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.6291 - val_out_T_loss: 0.2910 - val_out_S_loss: 0.8867 - val_tf.math.abs_185_loss: 0.0210 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 938/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.4440 - out_T_loss: 0.0846 - out_S_loss: 1.0704 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.5420 - val_out_T_loss: 0.2878 - val_out_S_loss: 0.8808 - val_tf.math.abs_185_loss: 0.0198 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 939/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.8934 - out_T_loss: 0.0921 - out_S_loss: 1.1128 - tf.math.abs_185_loss: 0.0232 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.4994 - val_out_T_loss: 0.2706 - val_out_S_loss: 0.8776 - val_tf.math.abs_185_loss: 0.0200 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 940/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.5207 - out_T_loss: 0.0898 - out_S_loss: 1.0794 - tf.math.abs_185_loss: 0.0218 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.4773 - val_out_T_loss: 0.2773 - val_out_S_loss: 0.8760 - val_tf.math.abs_185_loss: 0.0194 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 941/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.1638 - out_T_loss: 0.0886 - out_S_loss: 1.1428 - tf.math.abs_185_loss: 0.0220 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.5057 - val_out_T_loss: 0.2835 - val_out_S_loss: 0.8772 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 942/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.0121 - out_T_loss: 0.0983 - out_S_loss: 1.1230 - tf.math.abs_185_loss: 0.0231 - tf.math.abs_186_loss: 0.0110 - val_loss: 9.5817 - val_out_T_loss: 0.2772 - val_out_S_loss: 0.8765 - val_tf.math.abs_185_loss: 0.0240 - val_tf.math.abs_186_loss: 0.0030\n",
      "Epoch 943/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 12.5341 - out_T_loss: 0.0996 - out_S_loss: 1.1751 - tf.math.abs_185_loss: 0.0235 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.5324 - val_out_T_loss: 0.2699 - val_out_S_loss: 0.8790 - val_tf.math.abs_185_loss: 0.0209 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 944/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.9841 - out_T_loss: 0.0985 - out_S_loss: 1.1181 - tf.math.abs_185_loss: 0.0246 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.5385 - val_out_T_loss: 0.2787 - val_out_S_loss: 0.8802 - val_tf.math.abs_185_loss: 0.0202 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 945/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.6735 - out_T_loss: 0.0825 - out_S_loss: 1.0935 - tf.math.abs_185_loss: 0.0220 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.4975 - val_out_T_loss: 0.2583 - val_out_S_loss: 0.8786 - val_tf.math.abs_185_loss: 0.0201 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 946/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.7203 - out_T_loss: 0.0913 - out_S_loss: 1.0945 - tf.math.abs_185_loss: 0.0237 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.4685 - val_out_T_loss: 0.2804 - val_out_S_loss: 0.8728 - val_tf.math.abs_185_loss: 0.0204 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 947/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.6465 - out_T_loss: 0.0930 - out_S_loss: 1.0914 - tf.math.abs_185_loss: 0.0217 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.4391 - val_out_T_loss: 0.2748 - val_out_S_loss: 0.8730 - val_tf.math.abs_185_loss: 0.0192 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 948/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.8988 - out_T_loss: 0.0871 - out_S_loss: 1.1152 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.4774 - val_out_T_loss: 0.2896 - val_out_S_loss: 0.8740 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 949/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.8511 - out_T_loss: 0.0980 - out_S_loss: 1.1095 - tf.math.abs_185_loss: 0.0220 - tf.math.abs_186_loss: 0.0109 - val_loss: 9.4272 - val_out_T_loss: 0.2656 - val_out_S_loss: 0.8735 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 950/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.5307 - out_T_loss: 0.0890 - out_S_loss: 1.0756 - tf.math.abs_185_loss: 0.0239 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.4059 - val_out_T_loss: 0.2740 - val_out_S_loss: 0.8712 - val_tf.math.abs_185_loss: 0.0186 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 951/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.9764 - out_T_loss: 0.0909 - out_S_loss: 1.1212 - tf.math.abs_185_loss: 0.0232 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.4106 - val_out_T_loss: 0.2767 - val_out_S_loss: 0.8714 - val_tf.math.abs_185_loss: 0.0185 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 952/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.6726 - out_T_loss: 0.0850 - out_S_loss: 1.0920 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.4851 - val_out_T_loss: 0.2685 - val_out_S_loss: 0.8722 - val_tf.math.abs_185_loss: 0.0222 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 953/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.7177 - out_T_loss: 0.0893 - out_S_loss: 1.0974 - tf.math.abs_185_loss: 0.0222 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.4466 - val_out_T_loss: 0.2775 - val_out_S_loss: 0.8716 - val_tf.math.abs_185_loss: 0.0201 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 954/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.3560 - out_T_loss: 0.0858 - out_S_loss: 1.0635 - tf.math.abs_185_loss: 0.0216 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.3803 - val_out_T_loss: 0.2788 - val_out_S_loss: 0.8683 - val_tf.math.abs_185_loss: 0.0185 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 955/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.5870 - out_T_loss: 0.0926 - out_S_loss: 1.0872 - tf.math.abs_185_loss: 0.0211 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.3690 - val_out_T_loss: 0.2704 - val_out_S_loss: 0.8661 - val_tf.math.abs_185_loss: 0.0194 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 956/2000\n",
      "24/24 [==============================] - 1s 39ms/step - loss: 11.8134 - out_T_loss: 0.0730 - out_S_loss: 1.1095 - tf.math.abs_185_loss: 0.0216 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.4577 - val_out_T_loss: 0.2837 - val_out_S_loss: 0.8697 - val_tf.math.abs_185_loss: 0.0211 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 957/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.6020 - out_T_loss: 0.0910 - out_S_loss: 1.0868 - tf.math.abs_185_loss: 0.0219 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.4099 - val_out_T_loss: 0.2829 - val_out_S_loss: 0.8681 - val_tf.math.abs_185_loss: 0.0195 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 958/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.4659 - out_T_loss: 0.0717 - out_S_loss: 1.0740 - tf.math.abs_185_loss: 0.0219 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.4366 - val_out_T_loss: 0.2795 - val_out_S_loss: 0.8693 - val_tf.math.abs_185_loss: 0.0206 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 959/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.4109 - out_T_loss: 0.0811 - out_S_loss: 1.0709 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.3979 - val_out_T_loss: 0.2842 - val_out_S_loss: 0.8704 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 960/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.3738 - out_T_loss: 0.0841 - out_S_loss: 1.0670 - tf.math.abs_185_loss: 0.0211 - tf.math.abs_186_loss: 0.0099 - val_loss: 9.4009 - val_out_T_loss: 0.2778 - val_out_S_loss: 0.8700 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 961/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.8108 - out_T_loss: 0.0904 - out_S_loss: 1.1072 - tf.math.abs_185_loss: 0.0218 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.4034 - val_out_T_loss: 0.2640 - val_out_S_loss: 0.8698 - val_tf.math.abs_185_loss: 0.0195 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 962/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.8396 - out_T_loss: 0.0973 - out_S_loss: 1.1093 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.4550 - val_out_T_loss: 0.2840 - val_out_S_loss: 0.8728 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 963/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 12.0462 - out_T_loss: 0.0960 - out_S_loss: 1.1264 - tf.math.abs_185_loss: 0.0232 - tf.math.abs_186_loss: 0.0111 - val_loss: 9.3741 - val_out_T_loss: 0.2732 - val_out_S_loss: 0.8675 - val_tf.math.abs_185_loss: 0.0188 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 964/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.4717 - out_T_loss: 0.0839 - out_S_loss: 1.0737 - tf.math.abs_185_loss: 0.0221 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.4119 - val_out_T_loss: 0.2841 - val_out_S_loss: 0.8684 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 965/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.6876 - out_T_loss: 0.0848 - out_S_loss: 1.0935 - tf.math.abs_185_loss: 0.0230 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.3540 - val_out_T_loss: 0.2726 - val_out_S_loss: 0.8659 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 966/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.5942 - out_T_loss: 0.0970 - out_S_loss: 1.0811 - tf.math.abs_185_loss: 0.0237 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.3467 - val_out_T_loss: 0.2671 - val_out_S_loss: 0.8663 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 967/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.6188 - out_T_loss: 0.0868 - out_S_loss: 1.0845 - tf.math.abs_185_loss: 0.0233 - tf.math.abs_186_loss: 0.0111 - val_loss: 9.3542 - val_out_T_loss: 0.2704 - val_out_S_loss: 0.8638 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 968/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.5303 - out_T_loss: 0.0868 - out_S_loss: 1.0813 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.3310 - val_out_T_loss: 0.2681 - val_out_S_loss: 0.8643 - val_tf.math.abs_185_loss: 0.0185 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 969/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.5074 - out_T_loss: 0.0888 - out_S_loss: 1.0781 - tf.math.abs_185_loss: 0.0215 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.3760 - val_out_T_loss: 0.2895 - val_out_S_loss: 0.8685 - val_tf.math.abs_185_loss: 0.0175 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 970/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.4726 - out_T_loss: 0.0836 - out_S_loss: 1.0763 - tf.math.abs_185_loss: 0.0209 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.4109 - val_out_T_loss: 0.2938 - val_out_S_loss: 0.8693 - val_tf.math.abs_185_loss: 0.0186 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 971/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.3211 - out_T_loss: 0.0884 - out_S_loss: 1.0596 - tf.math.abs_185_loss: 0.0218 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.4069 - val_out_T_loss: 0.2733 - val_out_S_loss: 0.8690 - val_tf.math.abs_185_loss: 0.0196 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 972/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.0524 - out_T_loss: 0.1051 - out_S_loss: 1.1272 - tf.math.abs_185_loss: 0.0230 - tf.math.abs_186_loss: 0.0108 - val_loss: 9.3225 - val_out_T_loss: 0.2631 - val_out_S_loss: 0.8608 - val_tf.math.abs_185_loss: 0.0200 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 973/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.2716 - out_T_loss: 0.0790 - out_S_loss: 1.0560 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.3137 - val_out_T_loss: 0.2790 - val_out_S_loss: 0.8616 - val_tf.math.abs_185_loss: 0.0185 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 974/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.7201 - out_T_loss: 0.0799 - out_S_loss: 1.1013 - tf.math.abs_185_loss: 0.0209 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.3010 - val_out_T_loss: 0.2606 - val_out_S_loss: 0.8594 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 975/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.0015 - out_T_loss: 0.0866 - out_S_loss: 1.1256 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.3292 - val_out_T_loss: 0.2691 - val_out_S_loss: 0.8600 - val_tf.math.abs_185_loss: 0.0204 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 976/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.6023 - out_T_loss: 0.0811 - out_S_loss: 1.0865 - tf.math.abs_185_loss: 0.0222 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.3218 - val_out_T_loss: 0.2727 - val_out_S_loss: 0.8622 - val_tf.math.abs_185_loss: 0.0188 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 977/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.6818 - out_T_loss: 0.0733 - out_S_loss: 1.0960 - tf.math.abs_185_loss: 0.0217 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.2472 - val_out_T_loss: 0.2758 - val_out_S_loss: 0.8558 - val_tf.math.abs_185_loss: 0.0181 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 978/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.4884 - out_T_loss: 0.0762 - out_S_loss: 1.0766 - tf.math.abs_185_loss: 0.0217 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.2418 - val_out_T_loss: 0.2770 - val_out_S_loss: 0.8553 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 979/2000\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 11.7714 - out_T_loss: 0.0876 - out_S_loss: 1.1030 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.2265 - val_out_T_loss: 0.2696 - val_out_S_loss: 0.8523 - val_tf.math.abs_185_loss: 0.0193 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 980/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.2501 - out_T_loss: 0.0891 - out_S_loss: 1.0506 - tf.math.abs_185_loss: 0.0226 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.2037 - val_out_T_loss: 0.2715 - val_out_S_loss: 0.8530 - val_tf.math.abs_185_loss: 0.0177 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 981/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.3328 - out_T_loss: 0.0652 - out_S_loss: 1.0640 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.2374 - val_out_T_loss: 0.2708 - val_out_S_loss: 0.8543 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 982/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.2707 - out_T_loss: 0.0789 - out_S_loss: 1.0550 - tf.math.abs_185_loss: 0.0217 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.2960 - val_out_T_loss: 0.2643 - val_out_S_loss: 0.8540 - val_tf.math.abs_185_loss: 0.0221 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 983/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.3224 - out_T_loss: 0.0781 - out_S_loss: 1.0608 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.2491 - val_out_T_loss: 0.2837 - val_out_S_loss: 0.8530 - val_tf.math.abs_185_loss: 0.0193 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 984/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 12.2168 - out_T_loss: 0.0839 - out_S_loss: 1.1489 - tf.math.abs_185_loss: 0.0216 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.2858 - val_out_T_loss: 0.2785 - val_out_S_loss: 0.8538 - val_tf.math.abs_185_loss: 0.0210 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 985/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.4429 - out_T_loss: 0.0818 - out_S_loss: 1.0714 - tf.math.abs_185_loss: 0.0221 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.2265 - val_out_T_loss: 0.2692 - val_out_S_loss: 0.8558 - val_tf.math.abs_185_loss: 0.0175 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 986/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.2652 - out_T_loss: 0.0786 - out_S_loss: 1.0554 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.2953 - val_out_T_loss: 0.2687 - val_out_S_loss: 0.8568 - val_tf.math.abs_185_loss: 0.0205 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 987/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.2898 - out_T_loss: 0.0868 - out_S_loss: 1.0564 - tf.math.abs_185_loss: 0.0222 - tf.math.abs_186_loss: 0.0098 - val_loss: 9.2511 - val_out_T_loss: 0.2608 - val_out_S_loss: 0.8566 - val_tf.math.abs_185_loss: 0.0188 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 988/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.4377 - out_T_loss: 0.0897 - out_S_loss: 1.0729 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.2372 - val_out_T_loss: 0.2676 - val_out_S_loss: 0.8563 - val_tf.math.abs_185_loss: 0.0179 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 989/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 11.1150 - out_T_loss: 0.0772 - out_S_loss: 1.0408 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.2753 - val_out_T_loss: 0.2768 - val_out_S_loss: 0.8581 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 990/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.4265 - out_T_loss: 0.0805 - out_S_loss: 1.0710 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.2538 - val_out_T_loss: 0.2713 - val_out_S_loss: 0.8545 - val_tf.math.abs_185_loss: 0.0193 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 991/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.3757 - out_T_loss: 0.0774 - out_S_loss: 1.0671 - tf.math.abs_185_loss: 0.0211 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.2402 - val_out_T_loss: 0.2921 - val_out_S_loss: 0.8530 - val_tf.math.abs_185_loss: 0.0184 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 992/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.4126 - out_T_loss: 0.0742 - out_S_loss: 1.0687 - tf.math.abs_185_loss: 0.0226 - tf.math.abs_186_loss: 0.0100 - val_loss: 9.2325 - val_out_T_loss: 0.2720 - val_out_S_loss: 0.8522 - val_tf.math.abs_185_loss: 0.0194 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 993/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.0227 - out_T_loss: 0.0653 - out_S_loss: 1.0343 - tf.math.abs_185_loss: 0.0201 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.1787 - val_out_T_loss: 0.2711 - val_out_S_loss: 0.8498 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 994/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.7101 - out_T_loss: 0.0759 - out_S_loss: 1.0031 - tf.math.abs_185_loss: 0.0201 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.1611 - val_out_T_loss: 0.2707 - val_out_S_loss: 0.8486 - val_tf.math.abs_185_loss: 0.0177 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 995/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.6814 - out_T_loss: 0.0759 - out_S_loss: 1.0964 - tf.math.abs_185_loss: 0.0217 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.1827 - val_out_T_loss: 0.2765 - val_out_S_loss: 0.8500 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 996/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.9722 - out_T_loss: 0.0780 - out_S_loss: 1.0274 - tf.math.abs_185_loss: 0.0209 - tf.math.abs_186_loss: 0.0101 - val_loss: 9.1527 - val_out_T_loss: 0.2703 - val_out_S_loss: 0.8478 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 997/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.5294 - out_T_loss: 0.0763 - out_S_loss: 1.0784 - tf.math.abs_185_loss: 0.0232 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.1430 - val_out_T_loss: 0.2652 - val_out_S_loss: 0.8475 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 998/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.0273 - out_T_loss: 0.0795 - out_S_loss: 1.0323 - tf.math.abs_185_loss: 0.0212 - tf.math.abs_186_loss: 0.0100 - val_loss: 9.2220 - val_out_T_loss: 0.2761 - val_out_S_loss: 0.8498 - val_tf.math.abs_185_loss: 0.0200 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 999/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.6947 - out_T_loss: 0.0819 - out_S_loss: 1.0971 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.1687 - val_out_T_loss: 0.2868 - val_out_S_loss: 0.8473 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1000/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.0526 - out_T_loss: 0.0749 - out_S_loss: 1.0362 - tf.math.abs_185_loss: 0.0209 - tf.math.abs_186_loss: 0.0099 - val_loss: 9.1200 - val_out_T_loss: 0.2712 - val_out_S_loss: 0.8439 - val_tf.math.abs_185_loss: 0.0182 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1001/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.2905 - out_T_loss: 0.0681 - out_S_loss: 1.0559 - tf.math.abs_185_loss: 0.0227 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.1650 - val_out_T_loss: 0.2696 - val_out_S_loss: 0.8463 - val_tf.math.abs_185_loss: 0.0193 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1002/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.2584 - out_T_loss: 0.0741 - out_S_loss: 1.0557 - tf.math.abs_185_loss: 0.0211 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.2170 - val_out_T_loss: 0.2749 - val_out_S_loss: 0.8453 - val_tf.math.abs_185_loss: 0.0218 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1003/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.9704 - out_T_loss: 0.0883 - out_S_loss: 1.1233 - tf.math.abs_185_loss: 0.0219 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.2007 - val_out_T_loss: 0.2685 - val_out_S_loss: 0.8471 - val_tf.math.abs_185_loss: 0.0207 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1004/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.5522 - out_T_loss: 0.0825 - out_S_loss: 1.0818 - tf.math.abs_185_loss: 0.0226 - tf.math.abs_186_loss: 0.0100 - val_loss: 9.1462 - val_out_T_loss: 0.2743 - val_out_S_loss: 0.8420 - val_tf.math.abs_185_loss: 0.0202 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1005/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.0266 - out_T_loss: 0.0793 - out_S_loss: 1.0317 - tf.math.abs_185_loss: 0.0212 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.1618 - val_out_T_loss: 0.2758 - val_out_S_loss: 0.8450 - val_tf.math.abs_185_loss: 0.0192 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1006/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.3759 - out_T_loss: 0.0834 - out_S_loss: 1.0651 - tf.math.abs_185_loss: 0.0215 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.1662 - val_out_T_loss: 0.2902 - val_out_S_loss: 0.8447 - val_tf.math.abs_185_loss: 0.0190 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1007/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.1919 - out_T_loss: 0.0828 - out_S_loss: 1.0477 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.1178 - val_out_T_loss: 0.2651 - val_out_S_loss: 0.8414 - val_tf.math.abs_185_loss: 0.0195 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1008/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.5655 - out_T_loss: 0.0705 - out_S_loss: 1.0860 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.0667 - val_out_T_loss: 0.2661 - val_out_S_loss: 0.8388 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1009/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.8701 - out_T_loss: 0.0789 - out_S_loss: 1.1149 - tf.math.abs_185_loss: 0.0215 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.1241 - val_out_T_loss: 0.2705 - val_out_S_loss: 0.8433 - val_tf.math.abs_185_loss: 0.0185 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1010/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.0004 - out_T_loss: 0.0699 - out_S_loss: 1.0319 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0099 - val_loss: 9.0939 - val_out_T_loss: 0.2716 - val_out_S_loss: 0.8419 - val_tf.math.abs_185_loss: 0.0176 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1011/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.2955 - out_T_loss: 0.0652 - out_S_loss: 1.0597 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.0425 - val_out_T_loss: 0.2716 - val_out_S_loss: 0.8375 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1012/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.3592 - out_T_loss: 0.0619 - out_S_loss: 1.0682 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.0370 - val_out_T_loss: 0.2723 - val_out_S_loss: 0.8359 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1013/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.2080 - out_T_loss: 0.0703 - out_S_loss: 1.0528 - tf.math.abs_185_loss: 0.0202 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.0737 - val_out_T_loss: 0.2703 - val_out_S_loss: 0.8379 - val_tf.math.abs_185_loss: 0.0188 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1014/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 12.0826 - out_T_loss: 0.0768 - out_S_loss: 1.1366 - tf.math.abs_185_loss: 0.0215 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.2077 - val_out_T_loss: 0.2571 - val_out_S_loss: 0.8499 - val_tf.math.abs_185_loss: 0.0200 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1015/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.4907 - out_T_loss: 0.0715 - out_S_loss: 1.0787 - tf.math.abs_185_loss: 0.0209 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.0700 - val_out_T_loss: 0.2677 - val_out_S_loss: 0.8396 - val_tf.math.abs_185_loss: 0.0179 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1016/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.2510 - out_T_loss: 0.0787 - out_S_loss: 1.0542 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.0655 - val_out_T_loss: 0.2747 - val_out_S_loss: 0.8380 - val_tf.math.abs_185_loss: 0.0182 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1017/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.8579 - out_T_loss: 0.0690 - out_S_loss: 1.0145 - tf.math.abs_185_loss: 0.0221 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.0640 - val_out_T_loss: 0.2843 - val_out_S_loss: 0.8349 - val_tf.math.abs_185_loss: 0.0190 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1018/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.7776 - out_T_loss: 0.0748 - out_S_loss: 1.1072 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0107 - val_loss: 9.0643 - val_out_T_loss: 0.2736 - val_out_S_loss: 0.8354 - val_tf.math.abs_185_loss: 0.0193 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1019/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.5695 - out_T_loss: 0.0875 - out_S_loss: 1.0831 - tf.math.abs_185_loss: 0.0223 - tf.math.abs_186_loss: 0.0102 - val_loss: 9.0314 - val_out_T_loss: 0.2703 - val_out_S_loss: 0.8335 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1020/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.7921 - out_T_loss: 0.0813 - out_S_loss: 1.1042 - tf.math.abs_185_loss: 0.0228 - tf.math.abs_186_loss: 0.0106 - val_loss: 9.0561 - val_out_T_loss: 0.2972 - val_out_S_loss: 0.8321 - val_tf.math.abs_185_loss: 0.0191 - val_tf.math.abs_186_loss: 0.0028\n",
      "Epoch 1021/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.5504 - out_T_loss: 0.0759 - out_S_loss: 1.0831 - tf.math.abs_185_loss: 0.0218 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.0312 - val_out_T_loss: 0.2738 - val_out_S_loss: 0.8333 - val_tf.math.abs_185_loss: 0.0186 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1022/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.6192 - out_T_loss: 0.0781 - out_S_loss: 1.0907 - tf.math.abs_185_loss: 0.0212 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.0571 - val_out_T_loss: 0.2776 - val_out_S_loss: 0.8350 - val_tf.math.abs_185_loss: 0.0191 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1023/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.2236 - out_T_loss: 0.0827 - out_S_loss: 1.0521 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.0291 - val_out_T_loss: 0.2715 - val_out_S_loss: 0.8330 - val_tf.math.abs_185_loss: 0.0190 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1024/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.0858 - out_T_loss: 0.0657 - out_S_loss: 1.0388 - tf.math.abs_185_loss: 0.0212 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.0276 - val_out_T_loss: 0.2719 - val_out_S_loss: 0.8315 - val_tf.math.abs_185_loss: 0.0196 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1025/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.2987 - out_T_loss: 0.0857 - out_S_loss: 1.0573 - tf.math.abs_185_loss: 0.0217 - tf.math.abs_186_loss: 0.0103 - val_loss: 9.0688 - val_out_T_loss: 0.2763 - val_out_S_loss: 0.8357 - val_tf.math.abs_185_loss: 0.0193 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1026/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.3183 - out_T_loss: 0.0824 - out_S_loss: 1.0596 - tf.math.abs_185_loss: 0.0216 - tf.math.abs_186_loss: 0.0104 - val_loss: 9.0416 - val_out_T_loss: 0.2903 - val_out_S_loss: 0.8328 - val_tf.math.abs_185_loss: 0.0186 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1027/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.1408 - out_T_loss: 0.0726 - out_S_loss: 1.0459 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0105 - val_loss: 9.0125 - val_out_T_loss: 0.2840 - val_out_S_loss: 0.8321 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1028/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.7936 - out_T_loss: 0.0749 - out_S_loss: 1.1095 - tf.math.abs_185_loss: 0.0207 - tf.math.abs_186_loss: 0.0105 - val_loss: 8.9885 - val_out_T_loss: 0.2835 - val_out_S_loss: 0.8296 - val_tf.math.abs_185_loss: 0.0181 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1029/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.9309 - out_T_loss: 0.0755 - out_S_loss: 1.0232 - tf.math.abs_185_loss: 0.0210 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9659 - val_out_T_loss: 0.2616 - val_out_S_loss: 0.8306 - val_tf.math.abs_185_loss: 0.0175 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1030/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 11.2753 - out_T_loss: 0.0653 - out_S_loss: 1.0600 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9266 - val_out_T_loss: 0.2694 - val_out_S_loss: 0.8254 - val_tf.math.abs_185_loss: 0.0176 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1031/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.2698 - out_T_loss: 0.0707 - out_S_loss: 1.0561 - tf.math.abs_185_loss: 0.0219 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.9652 - val_out_T_loss: 0.2875 - val_out_S_loss: 0.8231 - val_tf.math.abs_185_loss: 0.0198 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1032/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.3625 - out_T_loss: 0.0679 - out_S_loss: 1.0669 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.9369 - val_out_T_loss: 0.2682 - val_out_S_loss: 0.8259 - val_tf.math.abs_185_loss: 0.0181 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1033/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.1904 - out_T_loss: 0.0802 - out_S_loss: 1.0460 - tf.math.abs_185_loss: 0.0222 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.9445 - val_out_T_loss: 0.2687 - val_out_S_loss: 0.8241 - val_tf.math.abs_185_loss: 0.0194 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1034/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.9606 - out_T_loss: 0.0606 - out_S_loss: 1.0255 - tf.math.abs_185_loss: 0.0219 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.9708 - val_out_T_loss: 0.2849 - val_out_S_loss: 0.8257 - val_tf.math.abs_185_loss: 0.0190 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1035/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.3149 - out_T_loss: 0.0679 - out_S_loss: 1.0598 - tf.math.abs_185_loss: 0.0222 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9331 - val_out_T_loss: 0.2778 - val_out_S_loss: 0.8250 - val_tf.math.abs_185_loss: 0.0179 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1036/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.7828 - out_T_loss: 0.0758 - out_S_loss: 1.0097 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9875 - val_out_T_loss: 0.2749 - val_out_S_loss: 0.8258 - val_tf.math.abs_185_loss: 0.0201 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1037/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.1494 - out_T_loss: 0.0683 - out_S_loss: 1.0467 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0104 - val_loss: 8.9555 - val_out_T_loss: 0.2751 - val_out_S_loss: 0.8279 - val_tf.math.abs_185_loss: 0.0176 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1038/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.8779 - out_T_loss: 0.0695 - out_S_loss: 1.0180 - tf.math.abs_185_loss: 0.0216 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.9580 - val_out_T_loss: 0.2757 - val_out_S_loss: 0.8259 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1039/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.5720 - out_T_loss: 0.0785 - out_S_loss: 1.0821 - tf.math.abs_185_loss: 0.0234 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9415 - val_out_T_loss: 0.2752 - val_out_S_loss: 0.8254 - val_tf.math.abs_185_loss: 0.0182 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1040/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.9760 - out_T_loss: 0.0716 - out_S_loss: 1.0300 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.9527 - val_out_T_loss: 0.2757 - val_out_S_loss: 0.8266 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1041/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.9046 - out_T_loss: 0.0668 - out_S_loss: 1.0236 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0104 - val_loss: 8.9034 - val_out_T_loss: 0.2810 - val_out_S_loss: 0.8207 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1042/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.9666 - out_T_loss: 0.0623 - out_S_loss: 1.0300 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.8858 - val_out_T_loss: 0.2764 - val_out_S_loss: 0.8201 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1043/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.8121 - out_T_loss: 0.0636 - out_S_loss: 1.0139 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.9159 - val_out_T_loss: 0.2821 - val_out_S_loss: 0.8205 - val_tf.math.abs_185_loss: 0.0191 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1044/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.7219 - out_T_loss: 0.0596 - out_S_loss: 1.0061 - tf.math.abs_185_loss: 0.0201 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.8982 - val_out_T_loss: 0.2804 - val_out_S_loss: 0.8226 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1045/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.5398 - out_T_loss: 0.0663 - out_S_loss: 1.0827 - tf.math.abs_185_loss: 0.0222 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.9697 - val_out_T_loss: 0.2718 - val_out_S_loss: 0.8287 - val_tf.math.abs_185_loss: 0.0182 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1046/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.2067 - out_T_loss: 0.0695 - out_S_loss: 1.0518 - tf.math.abs_185_loss: 0.0207 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9062 - val_out_T_loss: 0.2812 - val_out_S_loss: 0.8237 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1047/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.7173 - out_T_loss: 0.0705 - out_S_loss: 1.0037 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.9313 - val_out_T_loss: 0.2877 - val_out_S_loss: 0.8217 - val_tf.math.abs_185_loss: 0.0189 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1048/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.3406 - out_T_loss: 0.0706 - out_S_loss: 1.0651 - tf.math.abs_185_loss: 0.0207 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9233 - val_out_T_loss: 0.2931 - val_out_S_loss: 0.8220 - val_tf.math.abs_185_loss: 0.0181 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1049/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.3559 - out_T_loss: 0.0651 - out_S_loss: 1.0658 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9034 - val_out_T_loss: 0.2751 - val_out_S_loss: 0.8226 - val_tf.math.abs_185_loss: 0.0176 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1050/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.8058 - out_T_loss: 0.0648 - out_S_loss: 1.0130 - tf.math.abs_185_loss: 0.0210 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.9155 - val_out_T_loss: 0.2822 - val_out_S_loss: 0.8201 - val_tf.math.abs_185_loss: 0.0190 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1051/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 11.4560 - out_T_loss: 0.0772 - out_S_loss: 1.0768 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.9415 - val_out_T_loss: 0.2883 - val_out_S_loss: 0.8234 - val_tf.math.abs_185_loss: 0.0186 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1052/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.6515 - out_T_loss: 0.0794 - out_S_loss: 1.0933 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0106 - val_loss: 8.9243 - val_out_T_loss: 0.2864 - val_out_S_loss: 0.8229 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1053/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.0406 - out_T_loss: 0.0712 - out_S_loss: 1.0343 - tf.math.abs_185_loss: 0.0211 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.8973 - val_out_T_loss: 0.3001 - val_out_S_loss: 0.8194 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1054/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.9431 - out_T_loss: 0.0617 - out_S_loss: 1.0260 - tf.math.abs_185_loss: 0.0211 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.8823 - val_out_T_loss: 0.2725 - val_out_S_loss: 0.8203 - val_tf.math.abs_185_loss: 0.0179 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1055/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.9825 - out_T_loss: 0.0790 - out_S_loss: 1.0277 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.9198 - val_out_T_loss: 0.2829 - val_out_S_loss: 0.8224 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1056/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.0127 - out_T_loss: 0.0653 - out_S_loss: 1.0321 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.9044 - val_out_T_loss: 0.2679 - val_out_S_loss: 0.8216 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1057/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.4501 - out_T_loss: 0.0660 - out_S_loss: 1.0790 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0106 - val_loss: 8.8825 - val_out_T_loss: 0.2763 - val_out_S_loss: 0.8202 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1058/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.2153 - out_T_loss: 0.0633 - out_S_loss: 1.0535 - tf.math.abs_185_loss: 0.0209 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.9411 - val_out_T_loss: 0.2802 - val_out_S_loss: 0.8204 - val_tf.math.abs_185_loss: 0.0206 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1059/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.9818 - out_T_loss: 0.0694 - out_S_loss: 1.0314 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.8663 - val_out_T_loss: 0.2883 - val_out_S_loss: 0.8179 - val_tf.math.abs_185_loss: 0.0177 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1060/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.8882 - out_T_loss: 0.0758 - out_S_loss: 1.0185 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.8370 - val_out_T_loss: 0.2782 - val_out_S_loss: 0.8164 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1061/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 11.4364 - out_T_loss: 0.0790 - out_S_loss: 1.0737 - tf.math.abs_185_loss: 0.0209 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.9245 - val_out_T_loss: 0.2764 - val_out_S_loss: 0.8215 - val_tf.math.abs_185_loss: 0.0193 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1062/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.9317 - out_T_loss: 0.0613 - out_S_loss: 1.0253 - tf.math.abs_185_loss: 0.0210 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.8637 - val_out_T_loss: 0.2599 - val_out_S_loss: 0.8162 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1063/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.3926 - out_T_loss: 0.0714 - out_S_loss: 1.0711 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.8392 - val_out_T_loss: 0.2755 - val_out_S_loss: 0.8157 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1064/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.0108 - out_T_loss: 0.0597 - out_S_loss: 1.0350 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.8388 - val_out_T_loss: 0.2633 - val_out_S_loss: 0.8179 - val_tf.math.abs_185_loss: 0.0175 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1065/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.1467 - out_T_loss: 0.0694 - out_S_loss: 1.0474 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.8325 - val_out_T_loss: 0.2689 - val_out_S_loss: 0.8176 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1066/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.8678 - out_T_loss: 0.0749 - out_S_loss: 1.0164 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.8481 - val_out_T_loss: 0.2903 - val_out_S_loss: 0.8116 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1067/2000\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 10.4930 - out_T_loss: 0.0584 - out_S_loss: 0.9823 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.7819 - val_out_T_loss: 0.2759 - val_out_S_loss: 0.8101 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1068/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.7505 - out_T_loss: 0.0621 - out_S_loss: 1.0094 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.7526 - val_out_T_loss: 0.2629 - val_out_S_loss: 0.8113 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1069/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.7808 - out_T_loss: 0.0584 - out_S_loss: 1.0107 - tf.math.abs_185_loss: 0.0207 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.7973 - val_out_T_loss: 0.2734 - val_out_S_loss: 0.8090 - val_tf.math.abs_185_loss: 0.0192 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1070/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.8978 - out_T_loss: 0.0595 - out_S_loss: 1.0221 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.7411 - val_out_T_loss: 0.2665 - val_out_S_loss: 0.8083 - val_tf.math.abs_185_loss: 0.0173 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1071/2000\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 11.0199 - out_T_loss: 0.0663 - out_S_loss: 1.0329 - tf.math.abs_185_loss: 0.0212 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.7582 - val_out_T_loss: 0.2697 - val_out_S_loss: 0.8069 - val_tf.math.abs_185_loss: 0.0185 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1072/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 10.9220 - out_T_loss: 0.0727 - out_S_loss: 1.0231 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.7635 - val_out_T_loss: 0.2684 - val_out_S_loss: 0.8098 - val_tf.math.abs_185_loss: 0.0175 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1073/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.0037 - out_T_loss: 0.0654 - out_S_loss: 1.0349 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.7288 - val_out_T_loss: 0.2691 - val_out_S_loss: 0.8079 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1074/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.5867 - out_T_loss: 0.0623 - out_S_loss: 0.9885 - tf.math.abs_185_loss: 0.0218 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.6883 - val_out_T_loss: 0.2674 - val_out_S_loss: 0.8045 - val_tf.math.abs_185_loss: 0.0165 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1075/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.8318 - out_T_loss: 0.0589 - out_S_loss: 1.0163 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.7379 - val_out_T_loss: 0.2614 - val_out_S_loss: 0.8073 - val_tf.math.abs_185_loss: 0.0177 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1076/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.6849 - out_T_loss: 0.0609 - out_S_loss: 1.0026 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.7516 - val_out_T_loss: 0.2792 - val_out_S_loss: 0.8085 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1077/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.0334 - out_T_loss: 0.0547 - out_S_loss: 1.0370 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.7054 - val_out_T_loss: 0.2615 - val_out_S_loss: 0.8068 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1078/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.4226 - out_T_loss: 0.0539 - out_S_loss: 0.9769 - tf.math.abs_185_loss: 0.0198 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.7016 - val_out_T_loss: 0.2753 - val_out_S_loss: 0.8040 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1079/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.9153 - out_T_loss: 0.0606 - out_S_loss: 1.0263 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.6827 - val_out_T_loss: 0.2616 - val_out_S_loss: 0.8037 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1080/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.8726 - out_T_loss: 0.0677 - out_S_loss: 1.0180 - tf.math.abs_185_loss: 0.0213 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.6899 - val_out_T_loss: 0.2757 - val_out_S_loss: 0.8023 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1081/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.0910 - out_T_loss: 0.0643 - out_S_loss: 1.0409 - tf.math.abs_185_loss: 0.0207 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.6918 - val_out_T_loss: 0.2696 - val_out_S_loss: 0.7996 - val_tf.math.abs_185_loss: 0.0190 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1082/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 11.4129 - out_T_loss: 0.0576 - out_S_loss: 1.0744 - tf.math.abs_185_loss: 0.0202 - tf.math.abs_186_loss: 0.0104 - val_loss: 8.6838 - val_out_T_loss: 0.2801 - val_out_S_loss: 0.8013 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1083/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 11.2899 - out_T_loss: 0.0585 - out_S_loss: 1.0620 - tf.math.abs_185_loss: 0.0207 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.7052 - val_out_T_loss: 0.2744 - val_out_S_loss: 0.8036 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1084/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.6310 - out_T_loss: 0.0665 - out_S_loss: 0.9959 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.7120 - val_out_T_loss: 0.2686 - val_out_S_loss: 0.8028 - val_tf.math.abs_185_loss: 0.0184 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1085/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.8567 - out_T_loss: 0.0630 - out_S_loss: 1.0185 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.6818 - val_out_T_loss: 0.2680 - val_out_S_loss: 0.8021 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1086/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 11.1191 - out_T_loss: 0.0606 - out_S_loss: 1.0466 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.7382 - val_out_T_loss: 0.2854 - val_out_S_loss: 0.8046 - val_tf.math.abs_185_loss: 0.0179 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1087/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.7561 - out_T_loss: 0.0628 - out_S_loss: 1.0100 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.6748 - val_out_T_loss: 0.2827 - val_out_S_loss: 0.8005 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1088/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.6073 - out_T_loss: 0.0551 - out_S_loss: 0.9949 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.6805 - val_out_T_loss: 0.2775 - val_out_S_loss: 0.7998 - val_tf.math.abs_185_loss: 0.0179 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1089/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.8514 - out_T_loss: 0.0571 - out_S_loss: 1.0201 - tf.math.abs_185_loss: 0.0195 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.6493 - val_out_T_loss: 0.2824 - val_out_S_loss: 0.7992 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1090/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.0433 - out_T_loss: 0.0691 - out_S_loss: 1.0367 - tf.math.abs_185_loss: 0.0207 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.6352 - val_out_T_loss: 0.2743 - val_out_S_loss: 0.7956 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1091/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.0509 - out_T_loss: 0.0638 - out_S_loss: 1.0388 - tf.math.abs_185_loss: 0.0200 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.6297 - val_out_T_loss: 0.2802 - val_out_S_loss: 0.7968 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1092/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.8071 - out_T_loss: 0.0546 - out_S_loss: 1.0134 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.6217 - val_out_T_loss: 0.2830 - val_out_S_loss: 0.7950 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1093/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.1100 - out_T_loss: 0.0536 - out_S_loss: 1.0461 - tf.math.abs_185_loss: 0.0198 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.6409 - val_out_T_loss: 0.2870 - val_out_S_loss: 0.7958 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1094/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.7352 - out_T_loss: 0.0528 - out_S_loss: 1.0083 - tf.math.abs_185_loss: 0.0196 - tf.math.abs_186_loss: 0.0104 - val_loss: 8.6011 - val_out_T_loss: 0.2692 - val_out_S_loss: 0.7952 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1095/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.7068 - out_T_loss: 0.0595 - out_S_loss: 1.0045 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.6178 - val_out_T_loss: 0.2740 - val_out_S_loss: 0.7960 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1096/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.6473 - out_T_loss: 0.0677 - out_S_loss: 0.9969 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.6607 - val_out_T_loss: 0.2889 - val_out_S_loss: 0.7950 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1097/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.9492 - out_T_loss: 0.0649 - out_S_loss: 1.0289 - tf.math.abs_185_loss: 0.0196 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.6496 - val_out_T_loss: 0.2982 - val_out_S_loss: 0.7931 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1098/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4952 - out_T_loss: 0.0604 - out_S_loss: 0.9830 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.6568 - val_out_T_loss: 0.2793 - val_out_S_loss: 0.7955 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1099/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.7086 - out_T_loss: 0.0554 - out_S_loss: 1.0049 - tf.math.abs_185_loss: 0.0200 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.6538 - val_out_T_loss: 0.2908 - val_out_S_loss: 0.7939 - val_tf.math.abs_185_loss: 0.0187 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1100/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.6219 - out_T_loss: 0.0629 - out_S_loss: 0.9954 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.6531 - val_out_T_loss: 0.2792 - val_out_S_loss: 0.7969 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1101/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.4227 - out_T_loss: 0.0513 - out_S_loss: 0.9788 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.6544 - val_out_T_loss: 0.2892 - val_out_S_loss: 0.7972 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1102/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.1390 - out_T_loss: 0.0625 - out_S_loss: 1.0462 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.5870 - val_out_T_loss: 0.2729 - val_out_S_loss: 0.7909 - val_tf.math.abs_185_loss: 0.0179 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1103/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.7468 - out_T_loss: 0.0623 - out_S_loss: 1.0083 - tf.math.abs_185_loss: 0.0201 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.5993 - val_out_T_loss: 0.2876 - val_out_S_loss: 0.7934 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1104/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.9536 - out_T_loss: 0.0606 - out_S_loss: 1.0289 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.5902 - val_out_T_loss: 0.2888 - val_out_S_loss: 0.7915 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1105/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.7256 - out_T_loss: 0.0606 - out_S_loss: 1.0054 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.5637 - val_out_T_loss: 0.2660 - val_out_S_loss: 0.7910 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1106/2000\n",
      "24/24 [==============================] - 1s 62ms/step - loss: 10.6825 - out_T_loss: 0.0564 - out_S_loss: 1.0011 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.6149 - val_out_T_loss: 0.2820 - val_out_S_loss: 0.7944 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1107/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 11.0909 - out_T_loss: 0.0641 - out_S_loss: 1.0415 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.6406 - val_out_T_loss: 0.2942 - val_out_S_loss: 0.7949 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1108/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.6657 - out_T_loss: 0.0554 - out_S_loss: 1.0005 - tf.math.abs_185_loss: 0.0200 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.6092 - val_out_T_loss: 0.2859 - val_out_S_loss: 0.7943 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1109/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.5970 - out_T_loss: 0.0472 - out_S_loss: 0.9966 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.6045 - val_out_T_loss: 0.2817 - val_out_S_loss: 0.7946 - val_tf.math.abs_185_loss: 0.0165 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1110/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.6541 - out_T_loss: 0.0487 - out_S_loss: 1.0019 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.6193 - val_out_T_loss: 0.2740 - val_out_S_loss: 0.7952 - val_tf.math.abs_185_loss: 0.0173 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1111/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.4415 - out_T_loss: 0.0507 - out_S_loss: 0.9813 - tf.math.abs_185_loss: 0.0190 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.5853 - val_out_T_loss: 0.2872 - val_out_S_loss: 0.7919 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1112/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.4033 - out_T_loss: 0.0488 - out_S_loss: 0.9770 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.5871 - val_out_T_loss: 0.2836 - val_out_S_loss: 0.7908 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1113/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3080 - out_T_loss: 0.0445 - out_S_loss: 0.9695 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.5148 - val_out_T_loss: 0.2691 - val_out_S_loss: 0.7870 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1114/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.8217 - out_T_loss: 0.0565 - out_S_loss: 1.0159 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.6500 - val_out_T_loss: 0.2836 - val_out_S_loss: 0.7898 - val_tf.math.abs_185_loss: 0.0210 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1115/2000\n",
      "24/24 [==============================] - 1s 48ms/step - loss: 11.0754 - out_T_loss: 0.0569 - out_S_loss: 1.0423 - tf.math.abs_185_loss: 0.0195 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.5453 - val_out_T_loss: 0.2828 - val_out_S_loss: 0.7878 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1116/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.5321 - out_T_loss: 0.0532 - out_S_loss: 0.9879 - tf.math.abs_185_loss: 0.0200 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.5374 - val_out_T_loss: 0.2690 - val_out_S_loss: 0.7885 - val_tf.math.abs_185_loss: 0.0169 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1117/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3328 - out_T_loss: 0.0565 - out_S_loss: 0.9682 - tf.math.abs_185_loss: 0.0201 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.5520 - val_out_T_loss: 0.2779 - val_out_S_loss: 0.7896 - val_tf.math.abs_185_loss: 0.0167 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1118/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.6622 - out_T_loss: 0.0556 - out_S_loss: 1.0004 - tf.math.abs_185_loss: 0.0202 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.5654 - val_out_T_loss: 0.2895 - val_out_S_loss: 0.7894 - val_tf.math.abs_185_loss: 0.0167 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1119/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3436 - out_T_loss: 0.0589 - out_S_loss: 0.9708 - tf.math.abs_185_loss: 0.0192 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.5386 - val_out_T_loss: 0.2794 - val_out_S_loss: 0.7875 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1120/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 11.0007 - out_T_loss: 0.0554 - out_S_loss: 1.0365 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0105 - val_loss: 8.5632 - val_out_T_loss: 0.3097 - val_out_S_loss: 0.7873 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1121/2000\n",
      "24/24 [==============================] - 1s 58ms/step - loss: 10.3617 - out_T_loss: 0.0632 - out_S_loss: 0.9729 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.5762 - val_out_T_loss: 0.2738 - val_out_S_loss: 0.7860 - val_tf.math.abs_185_loss: 0.0198 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1122/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 11.1149 - out_T_loss: 0.0613 - out_S_loss: 1.0467 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.6102 - val_out_T_loss: 0.2771 - val_out_S_loss: 0.7920 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1123/2000\n",
      "24/24 [==============================] - 1s 48ms/step - loss: 10.9186 - out_T_loss: 0.0651 - out_S_loss: 1.0233 - tf.math.abs_185_loss: 0.0209 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.5617 - val_out_T_loss: 0.2787 - val_out_S_loss: 0.7905 - val_tf.math.abs_185_loss: 0.0165 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1124/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.5883 - out_T_loss: 0.0538 - out_S_loss: 0.9936 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.6249 - val_out_T_loss: 0.2843 - val_out_S_loss: 0.7900 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1125/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.9517 - out_T_loss: 0.0568 - out_S_loss: 1.0287 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.5583 - val_out_T_loss: 0.2647 - val_out_S_loss: 0.7914 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1126/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.8359 - out_T_loss: 0.0529 - out_S_loss: 1.0165 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.6039 - val_out_T_loss: 0.2965 - val_out_S_loss: 0.7890 - val_tf.math.abs_185_loss: 0.0184 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1127/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4239 - out_T_loss: 0.0608 - out_S_loss: 0.9794 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.6067 - val_out_T_loss: 0.2791 - val_out_S_loss: 0.7915 - val_tf.math.abs_185_loss: 0.0182 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1128/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.5556 - out_T_loss: 0.0554 - out_S_loss: 0.9890 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.4923 - val_out_T_loss: 0.2722 - val_out_S_loss: 0.7837 - val_tf.math.abs_185_loss: 0.0167 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1129/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.6905 - out_T_loss: 0.0587 - out_S_loss: 1.0042 - tf.math.abs_185_loss: 0.0192 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.5015 - val_out_T_loss: 0.2597 - val_out_S_loss: 0.7858 - val_tf.math.abs_185_loss: 0.0167 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1130/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2742 - out_T_loss: 0.0472 - out_S_loss: 0.9659 - tf.math.abs_185_loss: 0.0184 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.4993 - val_out_T_loss: 0.2666 - val_out_S_loss: 0.7863 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1131/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4593 - out_T_loss: 0.0555 - out_S_loss: 0.9799 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.4992 - val_out_T_loss: 0.2678 - val_out_S_loss: 0.7847 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1132/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.6778 - out_T_loss: 0.0562 - out_S_loss: 1.0041 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.5050 - val_out_T_loss: 0.2710 - val_out_S_loss: 0.7855 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1133/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.8389 - out_T_loss: 0.0490 - out_S_loss: 1.0206 - tf.math.abs_185_loss: 0.0192 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.4628 - val_out_T_loss: 0.2711 - val_out_S_loss: 0.7826 - val_tf.math.abs_185_loss: 0.0159 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1134/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.2769 - out_T_loss: 0.0545 - out_S_loss: 0.9637 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.5178 - val_out_T_loss: 0.2904 - val_out_S_loss: 0.7854 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1135/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.1968 - out_T_loss: 0.0641 - out_S_loss: 0.9508 - tf.math.abs_185_loss: 0.0214 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.4717 - val_out_T_loss: 0.2883 - val_out_S_loss: 0.7800 - val_tf.math.abs_185_loss: 0.0167 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1136/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 9.6505 - out_T_loss: 0.0506 - out_S_loss: 0.9022 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0090 - val_loss: 8.4669 - val_out_T_loss: 0.2634 - val_out_S_loss: 0.7793 - val_tf.math.abs_185_loss: 0.0182 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1137/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3706 - out_T_loss: 0.0497 - out_S_loss: 0.9747 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0094 - val_loss: 8.4344 - val_out_T_loss: 0.2560 - val_out_S_loss: 0.7807 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1138/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.5384 - out_T_loss: 0.0557 - out_S_loss: 0.9880 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.4316 - val_out_T_loss: 0.2594 - val_out_S_loss: 0.7773 - val_tf.math.abs_185_loss: 0.0177 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1139/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3204 - out_T_loss: 0.0490 - out_S_loss: 0.9680 - tf.math.abs_185_loss: 0.0195 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.4118 - val_out_T_loss: 0.2624 - val_out_S_loss: 0.7771 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1140/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.0353 - out_T_loss: 0.0458 - out_S_loss: 0.9411 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.4508 - val_out_T_loss: 0.2685 - val_out_S_loss: 0.7798 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1141/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2085 - out_T_loss: 0.0547 - out_S_loss: 0.9572 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.4346 - val_out_T_loss: 0.2598 - val_out_S_loss: 0.7809 - val_tf.math.abs_185_loss: 0.0160 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1142/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2952 - out_T_loss: 0.0423 - out_S_loss: 0.9700 - tf.math.abs_185_loss: 0.0182 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.3943 - val_out_T_loss: 0.2669 - val_out_S_loss: 0.7772 - val_tf.math.abs_185_loss: 0.0155 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1143/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.5960 - out_T_loss: 0.0602 - out_S_loss: 0.9938 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.4777 - val_out_T_loss: 0.2825 - val_out_S_loss: 0.7798 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1144/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.3382 - out_T_loss: 0.0431 - out_S_loss: 0.9720 - tf.math.abs_185_loss: 0.0190 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.3965 - val_out_T_loss: 0.2589 - val_out_S_loss: 0.7779 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1145/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.1119 - out_T_loss: 0.0484 - out_S_loss: 0.9489 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.4230 - val_out_T_loss: 0.2731 - val_out_S_loss: 0.7778 - val_tf.math.abs_185_loss: 0.0163 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1146/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3313 - out_T_loss: 0.0464 - out_S_loss: 0.9705 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.4513 - val_out_T_loss: 0.2701 - val_out_S_loss: 0.7790 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1147/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2809 - out_T_loss: 0.0499 - out_S_loss: 0.9642 - tf.math.abs_185_loss: 0.0196 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.4428 - val_out_T_loss: 0.2708 - val_out_S_loss: 0.7802 - val_tf.math.abs_185_loss: 0.0163 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1148/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3244 - out_T_loss: 0.0541 - out_S_loss: 0.9669 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.4342 - val_out_T_loss: 0.2745 - val_out_S_loss: 0.7766 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1149/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.4450 - out_T_loss: 0.0453 - out_S_loss: 0.9827 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.4705 - val_out_T_loss: 0.2724 - val_out_S_loss: 0.7814 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1150/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 11.2219 - out_T_loss: 0.0565 - out_S_loss: 1.0547 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.4815 - val_out_T_loss: 0.2733 - val_out_S_loss: 0.7796 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1151/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.2045 - out_T_loss: 0.0403 - out_S_loss: 0.9592 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.4362 - val_out_T_loss: 0.2809 - val_out_S_loss: 0.7769 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1152/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 11.2659 - out_T_loss: 0.0542 - out_S_loss: 1.0627 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.4873 - val_out_T_loss: 0.2672 - val_out_S_loss: 0.7819 - val_tf.math.abs_185_loss: 0.0177 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1153/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.5425 - out_T_loss: 0.0553 - out_S_loss: 0.9908 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.4394 - val_out_T_loss: 0.2820 - val_out_S_loss: 0.7789 - val_tf.math.abs_185_loss: 0.0159 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1154/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.0248 - out_T_loss: 0.0468 - out_S_loss: 0.9390 - tf.math.abs_185_loss: 0.0198 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.3990 - val_out_T_loss: 0.2638 - val_out_S_loss: 0.7768 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1155/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.2821 - out_T_loss: 0.0451 - out_S_loss: 0.9637 - tf.math.abs_185_loss: 0.0202 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.3906 - val_out_T_loss: 0.2653 - val_out_S_loss: 0.7765 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1156/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.3737 - out_T_loss: 0.0426 - out_S_loss: 0.9762 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.4038 - val_out_T_loss: 0.2818 - val_out_S_loss: 0.7761 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1157/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.6982 - out_T_loss: 0.0551 - out_S_loss: 1.0039 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.4073 - val_out_T_loss: 0.2751 - val_out_S_loss: 0.7747 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0021\n",
      "Epoch 1158/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.1191 - out_T_loss: 0.0455 - out_S_loss: 0.9463 - tf.math.abs_185_loss: 0.0206 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.4631 - val_out_T_loss: 0.2810 - val_out_S_loss: 0.7764 - val_tf.math.abs_185_loss: 0.0186 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1159/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4127 - out_T_loss: 0.0526 - out_S_loss: 0.9753 - tf.math.abs_185_loss: 0.0200 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.4366 - val_out_T_loss: 0.2779 - val_out_S_loss: 0.7773 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1160/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.7020 - out_T_loss: 0.0400 - out_S_loss: 1.0070 - tf.math.abs_185_loss: 0.0195 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.4010 - val_out_T_loss: 0.2633 - val_out_S_loss: 0.7738 - val_tf.math.abs_185_loss: 0.0177 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1161/2000\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 10.4660 - out_T_loss: 0.0559 - out_S_loss: 0.9840 - tf.math.abs_185_loss: 0.0185 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.4033 - val_out_T_loss: 0.2728 - val_out_S_loss: 0.7769 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1162/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.3754 - out_T_loss: 0.0578 - out_S_loss: 0.9757 - tf.math.abs_185_loss: 0.0183 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.4437 - val_out_T_loss: 0.2762 - val_out_S_loss: 0.7779 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1163/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9163 - out_T_loss: 0.0464 - out_S_loss: 0.9303 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.3749 - val_out_T_loss: 0.2795 - val_out_S_loss: 0.7725 - val_tf.math.abs_185_loss: 0.0163 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1164/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.8871 - out_T_loss: 0.0537 - out_S_loss: 1.0233 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.3820 - val_out_T_loss: 0.2967 - val_out_S_loss: 0.7718 - val_tf.math.abs_185_loss: 0.0160 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1165/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.1455 - out_T_loss: 0.0469 - out_S_loss: 0.9524 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.3198 - val_out_T_loss: 0.2667 - val_out_S_loss: 0.7685 - val_tf.math.abs_185_loss: 0.0161 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1166/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.4503 - out_T_loss: 0.0499 - out_S_loss: 0.9787 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.3765 - val_out_T_loss: 0.2769 - val_out_S_loss: 0.7696 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1167/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.7204 - out_T_loss: 0.0465 - out_S_loss: 1.0089 - tf.math.abs_185_loss: 0.0198 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.3829 - val_out_T_loss: 0.2755 - val_out_S_loss: 0.7695 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1168/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.5364 - out_T_loss: 0.0613 - out_S_loss: 0.9908 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.3437 - val_out_T_loss: 0.2739 - val_out_S_loss: 0.7677 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1169/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2978 - out_T_loss: 0.0508 - out_S_loss: 0.9660 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.3508 - val_out_T_loss: 0.2777 - val_out_S_loss: 0.7679 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1170/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3206 - out_T_loss: 0.0539 - out_S_loss: 0.9680 - tf.math.abs_185_loss: 0.0198 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.3672 - val_out_T_loss: 0.2721 - val_out_S_loss: 0.7694 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1171/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.5571 - out_T_loss: 0.0447 - out_S_loss: 0.9910 - tf.math.abs_185_loss: 0.0201 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.4214 - val_out_T_loss: 0.2892 - val_out_S_loss: 0.7704 - val_tf.math.abs_185_loss: 0.0191 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1172/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.4707 - out_T_loss: 0.0479 - out_S_loss: 0.9807 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.4545 - val_out_T_loss: 0.2917 - val_out_S_loss: 0.7753 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0027\n",
      "Epoch 1173/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 10.5238 - out_T_loss: 0.0539 - out_S_loss: 0.9863 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.3461 - val_out_T_loss: 0.2784 - val_out_S_loss: 0.7685 - val_tf.math.abs_185_loss: 0.0169 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1174/2000\n",
      "24/24 [==============================] - 1s 52ms/step - loss: 9.9437 - out_T_loss: 0.0468 - out_S_loss: 0.9316 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0094 - val_loss: 8.3777 - val_out_T_loss: 0.2573 - val_out_S_loss: 0.7707 - val_tf.math.abs_185_loss: 0.0184 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1175/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.4412 - out_T_loss: 0.0485 - out_S_loss: 0.9792 - tf.math.abs_185_loss: 0.0201 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.3363 - val_out_T_loss: 0.2652 - val_out_S_loss: 0.7711 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1176/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.2572 - out_T_loss: 0.0445 - out_S_loss: 0.9640 - tf.math.abs_185_loss: 0.0185 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.3304 - val_out_T_loss: 0.2814 - val_out_S_loss: 0.7691 - val_tf.math.abs_185_loss: 0.0156 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1177/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9244 - out_T_loss: 0.0465 - out_S_loss: 0.9322 - tf.math.abs_185_loss: 0.0183 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.3033 - val_out_T_loss: 0.2627 - val_out_S_loss: 0.7670 - val_tf.math.abs_185_loss: 0.0163 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1178/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 10.1345 - out_T_loss: 0.0505 - out_S_loss: 0.9514 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.2933 - val_out_T_loss: 0.2667 - val_out_S_loss: 0.7645 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1179/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.7439 - out_T_loss: 0.0456 - out_S_loss: 1.0111 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.3277 - val_out_T_loss: 0.2817 - val_out_S_loss: 0.7658 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1180/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9983 - out_T_loss: 0.0481 - out_S_loss: 0.9391 - tf.math.abs_185_loss: 0.0184 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.2914 - val_out_T_loss: 0.2647 - val_out_S_loss: 0.7652 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1181/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4671 - out_T_loss: 0.0555 - out_S_loss: 0.9816 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.2773 - val_out_T_loss: 0.2685 - val_out_S_loss: 0.7638 - val_tf.math.abs_185_loss: 0.0161 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1182/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9917 - out_T_loss: 0.0539 - out_S_loss: 0.9360 - tf.math.abs_185_loss: 0.0195 - tf.math.abs_186_loss: 0.0094 - val_loss: 8.3240 - val_out_T_loss: 0.2844 - val_out_S_loss: 0.7637 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1183/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.0678 - out_T_loss: 0.0523 - out_S_loss: 0.9419 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.2662 - val_out_T_loss: 0.2721 - val_out_S_loss: 0.7640 - val_tf.math.abs_185_loss: 0.0153 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1184/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.2788 - out_T_loss: 0.0564 - out_S_loss: 0.9630 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.2979 - val_out_T_loss: 0.2841 - val_out_S_loss: 0.7621 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1185/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.9863 - out_T_loss: 0.0476 - out_S_loss: 0.9351 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.2612 - val_out_T_loss: 0.2664 - val_out_S_loss: 0.7636 - val_tf.math.abs_185_loss: 0.0155 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1186/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.1990 - out_T_loss: 0.0523 - out_S_loss: 0.9566 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.2720 - val_out_T_loss: 0.2615 - val_out_S_loss: 0.7649 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1187/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9359 - out_T_loss: 0.0528 - out_S_loss: 0.9297 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.2882 - val_out_T_loss: 0.2747 - val_out_S_loss: 0.7637 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1188/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4729 - out_T_loss: 0.0502 - out_S_loss: 0.9808 - tf.math.abs_185_loss: 0.0204 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.2771 - val_out_T_loss: 0.2759 - val_out_S_loss: 0.7637 - val_tf.math.abs_185_loss: 0.0159 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1189/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3030 - out_T_loss: 0.0480 - out_S_loss: 0.9676 - tf.math.abs_185_loss: 0.0195 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.3403 - val_out_T_loss: 0.2657 - val_out_S_loss: 0.7696 - val_tf.math.abs_185_loss: 0.0165 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1190/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.3702 - out_T_loss: 0.0446 - out_S_loss: 0.9724 - tf.math.abs_185_loss: 0.0202 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.2482 - val_out_T_loss: 0.2580 - val_out_S_loss: 0.7614 - val_tf.math.abs_185_loss: 0.0165 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1191/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.8361 - out_T_loss: 0.0518 - out_S_loss: 1.0189 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.3300 - val_out_T_loss: 0.2873 - val_out_S_loss: 0.7674 - val_tf.math.abs_185_loss: 0.0161 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1192/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.2422 - out_T_loss: 0.0473 - out_S_loss: 0.9572 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0106 - val_loss: 8.2908 - val_out_T_loss: 0.2714 - val_out_S_loss: 0.7638 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1193/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.4899 - out_T_loss: 0.0466 - out_S_loss: 0.9858 - tf.math.abs_185_loss: 0.0196 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.2490 - val_out_T_loss: 0.2705 - val_out_S_loss: 0.7604 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1194/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.3655 - out_T_loss: 0.0395 - out_S_loss: 0.9750 - tf.math.abs_185_loss: 0.0190 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.2580 - val_out_T_loss: 0.2820 - val_out_S_loss: 0.7623 - val_tf.math.abs_185_loss: 0.0154 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1195/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.2499 - out_T_loss: 0.0462 - out_S_loss: 0.9636 - tf.math.abs_185_loss: 0.0188 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.2450 - val_out_T_loss: 0.2732 - val_out_S_loss: 0.7594 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1196/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.1516 - out_T_loss: 0.0458 - out_S_loss: 0.9500 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.2984 - val_out_T_loss: 0.2700 - val_out_S_loss: 0.7624 - val_tf.math.abs_185_loss: 0.0178 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1197/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3099 - out_T_loss: 0.0551 - out_S_loss: 0.9658 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.2411 - val_out_T_loss: 0.2587 - val_out_S_loss: 0.7595 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1198/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.0907 - out_T_loss: 0.0431 - out_S_loss: 0.9467 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.2106 - val_out_T_loss: 0.2618 - val_out_S_loss: 0.7566 - val_tf.math.abs_185_loss: 0.0169 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1199/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9848 - out_T_loss: 0.0444 - out_S_loss: 0.9340 - tf.math.abs_185_loss: 0.0203 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.1743 - val_out_T_loss: 0.2611 - val_out_S_loss: 0.7553 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1200/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3617 - out_T_loss: 0.0461 - out_S_loss: 0.9741 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.1976 - val_out_T_loss: 0.2635 - val_out_S_loss: 0.7562 - val_tf.math.abs_185_loss: 0.0163 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1201/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.1096 - out_T_loss: 0.0419 - out_S_loss: 0.9482 - tf.math.abs_185_loss: 0.0196 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.2148 - val_out_T_loss: 0.2601 - val_out_S_loss: 0.7583 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1202/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3417 - out_T_loss: 0.0434 - out_S_loss: 0.9729 - tf.math.abs_185_loss: 0.0184 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.2127 - val_out_T_loss: 0.2627 - val_out_S_loss: 0.7582 - val_tf.math.abs_185_loss: 0.0160 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1203/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.5960 - out_T_loss: 0.0479 - out_S_loss: 0.9963 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.2326 - val_out_T_loss: 0.2760 - val_out_S_loss: 0.7601 - val_tf.math.abs_185_loss: 0.0155 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1204/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2297 - out_T_loss: 0.0462 - out_S_loss: 0.9631 - tf.math.abs_185_loss: 0.0181 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.2405 - val_out_T_loss: 0.2721 - val_out_S_loss: 0.7579 - val_tf.math.abs_185_loss: 0.0170 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1205/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.3948 - out_T_loss: 0.0429 - out_S_loss: 0.9745 - tf.math.abs_185_loss: 0.0205 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.2391 - val_out_T_loss: 0.2678 - val_out_S_loss: 0.7609 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1206/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.2017 - out_T_loss: 0.0426 - out_S_loss: 0.9569 - tf.math.abs_185_loss: 0.0202 - tf.math.abs_186_loss: 0.0093 - val_loss: 8.2535 - val_out_T_loss: 0.2723 - val_out_S_loss: 0.7584 - val_tf.math.abs_185_loss: 0.0176 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1207/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.9729 - out_T_loss: 0.0405 - out_S_loss: 0.9369 - tf.math.abs_185_loss: 0.0185 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.2466 - val_out_T_loss: 0.2896 - val_out_S_loss: 0.7586 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1208/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.1066 - out_T_loss: 0.0399 - out_S_loss: 0.9498 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.2388 - val_out_T_loss: 0.2752 - val_out_S_loss: 0.7570 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1209/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2917 - out_T_loss: 0.0409 - out_S_loss: 0.9653 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.2110 - val_out_T_loss: 0.2790 - val_out_S_loss: 0.7577 - val_tf.math.abs_185_loss: 0.0155 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1210/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.2963 - out_T_loss: 0.0463 - out_S_loss: 0.9691 - tf.math.abs_185_loss: 0.0184 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.2086 - val_out_T_loss: 0.2873 - val_out_S_loss: 0.7550 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1211/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.2036 - out_T_loss: 0.0410 - out_S_loss: 0.9591 - tf.math.abs_185_loss: 0.0188 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.2088 - val_out_T_loss: 0.2801 - val_out_S_loss: 0.7579 - val_tf.math.abs_185_loss: 0.0152 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1212/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.1545 - out_T_loss: 0.0418 - out_S_loss: 0.9557 - tf.math.abs_185_loss: 0.0179 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.1991 - val_out_T_loss: 0.2748 - val_out_S_loss: 0.7556 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1213/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.9950 - out_T_loss: 0.0391 - out_S_loss: 0.9378 - tf.math.abs_185_loss: 0.0195 - tf.math.abs_186_loss: 0.0094 - val_loss: 8.1603 - val_out_T_loss: 0.2653 - val_out_S_loss: 0.7519 - val_tf.math.abs_185_loss: 0.0165 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1214/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9394 - out_T_loss: 0.0458 - out_S_loss: 0.9325 - tf.math.abs_185_loss: 0.0185 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.1369 - val_out_T_loss: 0.2583 - val_out_S_loss: 0.7520 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1215/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.7749 - out_T_loss: 0.0501 - out_S_loss: 0.9162 - tf.math.abs_185_loss: 0.0188 - tf.math.abs_186_loss: 0.0094 - val_loss: 8.1768 - val_out_T_loss: 0.2658 - val_out_S_loss: 0.7528 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1216/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.8786 - out_T_loss: 0.0421 - out_S_loss: 0.9261 - tf.math.abs_185_loss: 0.0190 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.2208 - val_out_T_loss: 0.2849 - val_out_S_loss: 0.7539 - val_tf.math.abs_185_loss: 0.0175 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1217/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.6374 - out_T_loss: 0.0452 - out_S_loss: 1.0016 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.2116 - val_out_T_loss: 0.2848 - val_out_S_loss: 0.7551 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1218/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9372 - out_T_loss: 0.0371 - out_S_loss: 0.9341 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0090 - val_loss: 8.1335 - val_out_T_loss: 0.2658 - val_out_S_loss: 0.7513 - val_tf.math.abs_185_loss: 0.0156 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1219/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.5329 - out_T_loss: 0.0441 - out_S_loss: 0.9926 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0094 - val_loss: 8.1455 - val_out_T_loss: 0.2711 - val_out_S_loss: 0.7495 - val_tf.math.abs_185_loss: 0.0167 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1220/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4097 - out_T_loss: 0.0472 - out_S_loss: 0.9798 - tf.math.abs_185_loss: 0.0179 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.1145 - val_out_T_loss: 0.2742 - val_out_S_loss: 0.7486 - val_tf.math.abs_185_loss: 0.0154 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1221/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.8764 - out_T_loss: 0.0378 - out_S_loss: 0.9281 - tf.math.abs_185_loss: 0.0183 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.1653 - val_out_T_loss: 0.2752 - val_out_S_loss: 0.7501 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1222/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.8892 - out_T_loss: 0.0386 - out_S_loss: 0.9300 - tf.math.abs_185_loss: 0.0182 - tf.math.abs_186_loss: 0.0094 - val_loss: 8.1282 - val_out_T_loss: 0.2674 - val_out_S_loss: 0.7510 - val_tf.math.abs_185_loss: 0.0153 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1223/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.4413 - out_T_loss: 0.0361 - out_S_loss: 0.8875 - tf.math.abs_185_loss: 0.0172 - tf.math.abs_186_loss: 0.0093 - val_loss: 8.1211 - val_out_T_loss: 0.2697 - val_out_S_loss: 0.7501 - val_tf.math.abs_185_loss: 0.0153 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1224/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9708 - out_T_loss: 0.0367 - out_S_loss: 0.9399 - tf.math.abs_185_loss: 0.0171 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.1051 - val_out_T_loss: 0.2787 - val_out_S_loss: 0.7480 - val_tf.math.abs_185_loss: 0.0151 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1225/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.0802 - out_T_loss: 0.0355 - out_S_loss: 0.9479 - tf.math.abs_185_loss: 0.0185 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.0940 - val_out_T_loss: 0.2720 - val_out_S_loss: 0.7487 - val_tf.math.abs_185_loss: 0.0144 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1226/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.1894 - out_T_loss: 0.0416 - out_S_loss: 0.9580 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.1138 - val_out_T_loss: 0.2691 - val_out_S_loss: 0.7441 - val_tf.math.abs_185_loss: 0.0180 - val_tf.math.abs_186_loss: 0.0021\n",
      "Epoch 1227/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.5761 - out_T_loss: 0.0392 - out_S_loss: 0.9007 - tf.math.abs_185_loss: 0.0173 - tf.math.abs_186_loss: 0.0092 - val_loss: 8.1211 - val_out_T_loss: 0.2798 - val_out_S_loss: 0.7473 - val_tf.math.abs_185_loss: 0.0160 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1228/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.5876 - out_T_loss: 0.0331 - out_S_loss: 0.8971 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.1261 - val_out_T_loss: 0.2837 - val_out_S_loss: 0.7475 - val_tf.math.abs_185_loss: 0.0161 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1229/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4308 - out_T_loss: 0.0426 - out_S_loss: 0.9794 - tf.math.abs_185_loss: 0.0197 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.0947 - val_out_T_loss: 0.2687 - val_out_S_loss: 0.7455 - val_tf.math.abs_185_loss: 0.0161 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1230/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 9.6203 - out_T_loss: 0.0405 - out_S_loss: 0.9010 - tf.math.abs_185_loss: 0.0188 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.0796 - val_out_T_loss: 0.2811 - val_out_S_loss: 0.7444 - val_tf.math.abs_185_loss: 0.0154 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1231/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 10.3055 - out_T_loss: 0.0500 - out_S_loss: 0.9675 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0104 - val_loss: 8.0648 - val_out_T_loss: 0.2667 - val_out_S_loss: 0.7442 - val_tf.math.abs_185_loss: 0.0156 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1232/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9320 - out_T_loss: 0.0396 - out_S_loss: 0.9341 - tf.math.abs_185_loss: 0.0178 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.1188 - val_out_T_loss: 0.2767 - val_out_S_loss: 0.7471 - val_tf.math.abs_185_loss: 0.0163 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1233/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2152 - out_T_loss: 0.0370 - out_S_loss: 0.9608 - tf.math.abs_185_loss: 0.0188 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.1564 - val_out_T_loss: 0.2833 - val_out_S_loss: 0.7511 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1234/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.0473 - out_T_loss: 0.0397 - out_S_loss: 0.9439 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0098 - val_loss: 8.1405 - val_out_T_loss: 0.2918 - val_out_S_loss: 0.7459 - val_tf.math.abs_185_loss: 0.0171 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1235/2000\n",
      "24/24 [==============================] - 1s 48ms/step - loss: 9.8884 - out_T_loss: 0.0425 - out_S_loss: 0.9293 - tf.math.abs_185_loss: 0.0180 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.0779 - val_out_T_loss: 0.2797 - val_out_S_loss: 0.7444 - val_tf.math.abs_185_loss: 0.0155 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1236/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 9.6733 - out_T_loss: 0.0361 - out_S_loss: 0.9102 - tf.math.abs_185_loss: 0.0174 - tf.math.abs_186_loss: 0.0094 - val_loss: 8.1155 - val_out_T_loss: 0.2795 - val_out_S_loss: 0.7477 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1237/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.6395 - out_T_loss: 0.0355 - out_S_loss: 0.9051 - tf.math.abs_185_loss: 0.0184 - tf.math.abs_186_loss: 0.0092 - val_loss: 8.1030 - val_out_T_loss: 0.2902 - val_out_S_loss: 0.7445 - val_tf.math.abs_185_loss: 0.0161 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1238/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.2770 - out_T_loss: 0.0451 - out_S_loss: 0.9642 - tf.math.abs_185_loss: 0.0198 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.1576 - val_out_T_loss: 0.2851 - val_out_S_loss: 0.7476 - val_tf.math.abs_185_loss: 0.0176 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1239/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 9.7988 - out_T_loss: 0.0408 - out_S_loss: 0.9188 - tf.math.abs_185_loss: 0.0188 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.0959 - val_out_T_loss: 0.2913 - val_out_S_loss: 0.7430 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1240/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.8178 - out_T_loss: 0.0329 - out_S_loss: 0.9229 - tf.math.abs_185_loss: 0.0181 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.0666 - val_out_T_loss: 0.2717 - val_out_S_loss: 0.7431 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1241/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9230 - out_T_loss: 0.0444 - out_S_loss: 0.9315 - tf.math.abs_185_loss: 0.0194 - tf.math.abs_186_loss: 0.0087 - val_loss: 8.0464 - val_out_T_loss: 0.2694 - val_out_S_loss: 0.7415 - val_tf.math.abs_185_loss: 0.0159 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1242/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.1371 - out_T_loss: 0.0340 - out_S_loss: 0.9535 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0095 - val_loss: 7.9952 - val_out_T_loss: 0.2592 - val_out_S_loss: 0.7381 - val_tf.math.abs_185_loss: 0.0155 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1243/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.3556 - out_T_loss: 0.0403 - out_S_loss: 0.9727 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0101 - val_loss: 8.0068 - val_out_T_loss: 0.2767 - val_out_S_loss: 0.7375 - val_tf.math.abs_185_loss: 0.0155 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1244/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.7823 - out_T_loss: 0.0445 - out_S_loss: 0.9173 - tf.math.abs_185_loss: 0.0190 - tf.math.abs_186_loss: 0.0093 - val_loss: 8.0004 - val_out_T_loss: 0.2634 - val_out_S_loss: 0.7375 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1245/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.6753 - out_T_loss: 0.0438 - out_S_loss: 0.9045 - tf.math.abs_185_loss: 0.0196 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.0565 - val_out_T_loss: 0.2651 - val_out_S_loss: 0.7393 - val_tf.math.abs_185_loss: 0.0176 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1246/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.6552 - out_T_loss: 0.0416 - out_S_loss: 0.9048 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0092 - val_loss: 8.0038 - val_out_T_loss: 0.2708 - val_out_S_loss: 0.7387 - val_tf.math.abs_185_loss: 0.0151 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1247/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9266 - out_T_loss: 0.0407 - out_S_loss: 0.9342 - tf.math.abs_185_loss: 0.0179 - tf.math.abs_186_loss: 0.0092 - val_loss: 8.0067 - val_out_T_loss: 0.2667 - val_out_S_loss: 0.7396 - val_tf.math.abs_185_loss: 0.0149 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1248/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.3878 - out_T_loss: 0.0325 - out_S_loss: 0.8832 - tf.math.abs_185_loss: 0.0170 - tf.math.abs_186_loss: 0.0091 - val_loss: 8.0296 - val_out_T_loss: 0.2639 - val_out_S_loss: 0.7419 - val_tf.math.abs_185_loss: 0.0151 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1249/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9977 - out_T_loss: 0.0457 - out_S_loss: 0.9365 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.0458 - val_out_T_loss: 0.2775 - val_out_S_loss: 0.7405 - val_tf.math.abs_185_loss: 0.0159 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1250/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.7814 - out_T_loss: 0.0387 - out_S_loss: 0.9212 - tf.math.abs_185_loss: 0.0171 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.0290 - val_out_T_loss: 0.2697 - val_out_S_loss: 0.7406 - val_tf.math.abs_185_loss: 0.0154 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1251/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.8416 - out_T_loss: 0.0420 - out_S_loss: 0.9239 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.0456 - val_out_T_loss: 0.2878 - val_out_S_loss: 0.7397 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1252/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.4538 - out_T_loss: 0.0432 - out_S_loss: 0.8867 - tf.math.abs_185_loss: 0.0181 - tf.math.abs_186_loss: 0.0091 - val_loss: 8.0027 - val_out_T_loss: 0.2715 - val_out_S_loss: 0.7362 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1253/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.6775 - out_T_loss: 0.0449 - out_S_loss: 1.0016 - tf.math.abs_185_loss: 0.0208 - tf.math.abs_186_loss: 0.0100 - val_loss: 8.0599 - val_out_T_loss: 0.2729 - val_out_S_loss: 0.7370 - val_tf.math.abs_185_loss: 0.0183 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1254/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.4744 - out_T_loss: 0.0384 - out_S_loss: 0.8866 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0096 - val_loss: 8.0364 - val_out_T_loss: 0.2626 - val_out_S_loss: 0.7369 - val_tf.math.abs_185_loss: 0.0179 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1255/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.8971 - out_T_loss: 0.0373 - out_S_loss: 0.9300 - tf.math.abs_185_loss: 0.0185 - tf.math.abs_186_loss: 0.0095 - val_loss: 8.0003 - val_out_T_loss: 0.2885 - val_out_S_loss: 0.7363 - val_tf.math.abs_185_loss: 0.0151 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1256/2000\n",
      "24/24 [==============================] - 1s 48ms/step - loss: 9.8483 - out_T_loss: 0.0433 - out_S_loss: 0.9239 - tf.math.abs_185_loss: 0.0184 - tf.math.abs_186_loss: 0.0099 - val_loss: 8.0192 - val_out_T_loss: 0.2974 - val_out_S_loss: 0.7348 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0025\n",
      "Epoch 1257/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.6139 - out_T_loss: 0.0298 - out_S_loss: 0.9043 - tf.math.abs_185_loss: 0.0175 - tf.math.abs_186_loss: 0.0096 - val_loss: 7.9619 - val_out_T_loss: 0.2648 - val_out_S_loss: 0.7321 - val_tf.math.abs_185_loss: 0.0165 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1258/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.4301 - out_T_loss: 0.0453 - out_S_loss: 0.9808 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0102 - val_loss: 8.0111 - val_out_T_loss: 0.2893 - val_out_S_loss: 0.7337 - val_tf.math.abs_185_loss: 0.0169 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1259/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.2443 - out_T_loss: 0.0453 - out_S_loss: 0.9617 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0098 - val_loss: 7.9487 - val_out_T_loss: 0.2745 - val_out_S_loss: 0.7322 - val_tf.math.abs_185_loss: 0.0153 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1260/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9362 - out_T_loss: 0.0413 - out_S_loss: 0.9318 - tf.math.abs_185_loss: 0.0190 - tf.math.abs_186_loss: 0.0099 - val_loss: 7.9872 - val_out_T_loss: 0.2719 - val_out_S_loss: 0.7318 - val_tf.math.abs_185_loss: 0.0176 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1261/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.8523 - out_T_loss: 0.0424 - out_S_loss: 0.9254 - tf.math.abs_185_loss: 0.0183 - tf.math.abs_186_loss: 0.0095 - val_loss: 7.9858 - val_out_T_loss: 0.2684 - val_out_S_loss: 0.7325 - val_tf.math.abs_185_loss: 0.0174 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1262/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.7780 - out_T_loss: 0.0452 - out_S_loss: 0.9157 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0097 - val_loss: 8.0245 - val_out_T_loss: 0.2882 - val_out_S_loss: 0.7365 - val_tf.math.abs_185_loss: 0.0163 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1263/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.4769 - out_T_loss: 0.0471 - out_S_loss: 0.9806 - tf.math.abs_185_loss: 0.0212 - tf.math.abs_186_loss: 0.0100 - val_loss: 7.9517 - val_out_T_loss: 0.2736 - val_out_S_loss: 0.7329 - val_tf.math.abs_185_loss: 0.0153 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1264/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9665 - out_T_loss: 0.0380 - out_S_loss: 0.9382 - tf.math.abs_185_loss: 0.0175 - tf.math.abs_186_loss: 0.0099 - val_loss: 7.9228 - val_out_T_loss: 0.2541 - val_out_S_loss: 0.7319 - val_tf.math.abs_185_loss: 0.0152 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1265/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 9.4478 - out_T_loss: 0.0356 - out_S_loss: 0.8868 - tf.math.abs_185_loss: 0.0178 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.9204 - val_out_T_loss: 0.2536 - val_out_S_loss: 0.7328 - val_tf.math.abs_185_loss: 0.0146 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1266/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.4203 - out_T_loss: 0.0395 - out_S_loss: 0.9821 - tf.math.abs_185_loss: 0.0177 - tf.math.abs_186_loss: 0.0103 - val_loss: 8.0299 - val_out_T_loss: 0.2830 - val_out_S_loss: 0.7379 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0026\n",
      "Epoch 1267/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 9.7286 - out_T_loss: 0.0464 - out_S_loss: 0.9108 - tf.math.abs_185_loss: 0.0192 - tf.math.abs_186_loss: 0.0095 - val_loss: 7.9898 - val_out_T_loss: 0.2670 - val_out_S_loss: 0.7341 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1268/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9202 - out_T_loss: 0.0387 - out_S_loss: 0.9328 - tf.math.abs_185_loss: 0.0183 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.9987 - val_out_T_loss: 0.2703 - val_out_S_loss: 0.7360 - val_tf.math.abs_185_loss: 0.0160 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1269/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.6523 - out_T_loss: 0.0340 - out_S_loss: 0.9081 - tf.math.abs_185_loss: 0.0176 - tf.math.abs_186_loss: 0.0093 - val_loss: 7.9231 - val_out_T_loss: 0.2642 - val_out_S_loss: 0.7322 - val_tf.math.abs_185_loss: 0.0146 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1270/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.5205 - out_T_loss: 0.0336 - out_S_loss: 0.8941 - tf.math.abs_185_loss: 0.0176 - tf.math.abs_186_loss: 0.0096 - val_loss: 7.9459 - val_out_T_loss: 0.2723 - val_out_S_loss: 0.7315 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1271/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9489 - out_T_loss: 0.0331 - out_S_loss: 0.9376 - tf.math.abs_185_loss: 0.0176 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.9203 - val_out_T_loss: 0.2612 - val_out_S_loss: 0.7314 - val_tf.math.abs_185_loss: 0.0150 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1272/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9959 - out_T_loss: 0.0316 - out_S_loss: 0.9430 - tf.math.abs_185_loss: 0.0174 - tf.math.abs_186_loss: 0.0093 - val_loss: 7.9232 - val_out_T_loss: 0.2738 - val_out_S_loss: 0.7291 - val_tf.math.abs_185_loss: 0.0156 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1273/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.8126 - out_T_loss: 0.0310 - out_S_loss: 0.9214 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0091 - val_loss: 7.9264 - val_out_T_loss: 0.2956 - val_out_S_loss: 0.7290 - val_tf.math.abs_185_loss: 0.0148 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1274/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 9.6798 - out_T_loss: 0.0364 - out_S_loss: 0.9083 - tf.math.abs_185_loss: 0.0183 - tf.math.abs_186_loss: 0.0097 - val_loss: 7.9980 - val_out_T_loss: 0.2741 - val_out_S_loss: 0.7286 - val_tf.math.abs_185_loss: 0.0197 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1275/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.3626 - out_T_loss: 0.0324 - out_S_loss: 0.8777 - tf.math.abs_185_loss: 0.0180 - tf.math.abs_186_loss: 0.0096 - val_loss: 7.8942 - val_out_T_loss: 0.2725 - val_out_S_loss: 0.7249 - val_tf.math.abs_185_loss: 0.0163 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1276/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.6270 - out_T_loss: 0.0339 - out_S_loss: 0.9044 - tf.math.abs_185_loss: 0.0179 - tf.math.abs_186_loss: 0.0096 - val_loss: 7.8813 - val_out_T_loss: 0.2813 - val_out_S_loss: 0.7264 - val_tf.math.abs_185_loss: 0.0146 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1277/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.7492 - out_T_loss: 0.0372 - out_S_loss: 0.9160 - tf.math.abs_185_loss: 0.0183 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.9269 - val_out_T_loss: 0.2770 - val_out_S_loss: 0.7297 - val_tf.math.abs_185_loss: 0.0154 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1278/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.7755 - out_T_loss: 0.0328 - out_S_loss: 0.9176 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0096 - val_loss: 7.9004 - val_out_T_loss: 0.2726 - val_out_S_loss: 0.7297 - val_tf.math.abs_185_loss: 0.0143 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1279/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.6538 - out_T_loss: 0.0418 - out_S_loss: 0.9043 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0096 - val_loss: 7.8953 - val_out_T_loss: 0.2597 - val_out_S_loss: 0.7284 - val_tf.math.abs_185_loss: 0.0154 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1280/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.6128 - out_T_loss: 0.0353 - out_S_loss: 0.9050 - tf.math.abs_185_loss: 0.0171 - tf.math.abs_186_loss: 0.0093 - val_loss: 7.8851 - val_out_T_loss: 0.2601 - val_out_S_loss: 0.7291 - val_tf.math.abs_185_loss: 0.0144 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1281/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.5467 - out_T_loss: 0.0364 - out_S_loss: 0.8954 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0092 - val_loss: 7.9086 - val_out_T_loss: 0.2663 - val_out_S_loss: 0.7284 - val_tf.math.abs_185_loss: 0.0156 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1282/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 9.6272 - out_T_loss: 0.0311 - out_S_loss: 0.9065 - tf.math.abs_185_loss: 0.0172 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.8889 - val_out_T_loss: 0.2791 - val_out_S_loss: 0.7282 - val_tf.math.abs_185_loss: 0.0141 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1283/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.1994 - out_T_loss: 0.0424 - out_S_loss: 0.9591 - tf.math.abs_185_loss: 0.0184 - tf.math.abs_186_loss: 0.0099 - val_loss: 7.9114 - val_out_T_loss: 0.2803 - val_out_S_loss: 0.7267 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1284/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.6437 - out_T_loss: 0.0408 - out_S_loss: 0.9038 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0096 - val_loss: 7.8699 - val_out_T_loss: 0.2803 - val_out_S_loss: 0.7244 - val_tf.math.abs_185_loss: 0.0150 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1285/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.2832 - out_T_loss: 0.0360 - out_S_loss: 0.8687 - tf.math.abs_185_loss: 0.0187 - tf.math.abs_186_loss: 0.0093 - val_loss: 7.8830 - val_out_T_loss: 0.2658 - val_out_S_loss: 0.7240 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1286/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9736 - out_T_loss: 0.0346 - out_S_loss: 0.9372 - tf.math.abs_185_loss: 0.0185 - tf.math.abs_186_loss: 0.0098 - val_loss: 7.8987 - val_out_T_loss: 0.2710 - val_out_S_loss: 0.7267 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1287/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 10.0126 - out_T_loss: 0.0372 - out_S_loss: 0.9410 - tf.math.abs_185_loss: 0.0183 - tf.math.abs_186_loss: 0.0099 - val_loss: 7.8727 - val_out_T_loss: 0.2601 - val_out_S_loss: 0.7260 - val_tf.math.abs_185_loss: 0.0153 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1288/2000\n",
      "24/24 [==============================] - 1s 47ms/step - loss: 9.5869 - out_T_loss: 0.0329 - out_S_loss: 0.8991 - tf.math.abs_185_loss: 0.0189 - tf.math.abs_186_loss: 0.0092 - val_loss: 7.8978 - val_out_T_loss: 0.2660 - val_out_S_loss: 0.7286 - val_tf.math.abs_185_loss: 0.0149 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1289/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 10.3460 - out_T_loss: 0.0418 - out_S_loss: 0.9738 - tf.math.abs_185_loss: 0.0184 - tf.math.abs_186_loss: 0.0099 - val_loss: 7.9229 - val_out_T_loss: 0.2667 - val_out_S_loss: 0.7285 - val_tf.math.abs_185_loss: 0.0161 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1290/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 9.7947 - out_T_loss: 0.0315 - out_S_loss: 0.9202 - tf.math.abs_185_loss: 0.0186 - tf.math.abs_186_loss: 0.0095 - val_loss: 7.8776 - val_out_T_loss: 0.2621 - val_out_S_loss: 0.7252 - val_tf.math.abs_185_loss: 0.0158 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1291/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.9801 - out_T_loss: 0.0297 - out_S_loss: 0.9381 - tf.math.abs_185_loss: 0.0190 - tf.math.abs_186_loss: 0.0095 - val_loss: 7.8690 - val_out_T_loss: 0.2676 - val_out_S_loss: 0.7236 - val_tf.math.abs_185_loss: 0.0160 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1292/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.7435 - out_T_loss: 0.0354 - out_S_loss: 0.9150 - tf.math.abs_185_loss: 0.0185 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.8429 - val_out_T_loss: 0.2802 - val_out_S_loss: 0.7224 - val_tf.math.abs_185_loss: 0.0145 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1293/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9914 - out_T_loss: 0.0356 - out_S_loss: 0.9398 - tf.math.abs_185_loss: 0.0181 - tf.math.abs_186_loss: 0.0097 - val_loss: 7.8921 - val_out_T_loss: 0.2745 - val_out_S_loss: 0.7238 - val_tf.math.abs_185_loss: 0.0166 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1294/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.6989 - out_T_loss: 0.0345 - out_S_loss: 0.9118 - tf.math.abs_185_loss: 0.0180 - tf.math.abs_186_loss: 0.0093 - val_loss: 7.8558 - val_out_T_loss: 0.2878 - val_out_S_loss: 0.7238 - val_tf.math.abs_185_loss: 0.0143 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1295/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9118 - out_T_loss: 0.0330 - out_S_loss: 0.9307 - tf.math.abs_185_loss: 0.0193 - tf.math.abs_186_loss: 0.0093 - val_loss: 7.8541 - val_out_T_loss: 0.2733 - val_out_S_loss: 0.7235 - val_tf.math.abs_185_loss: 0.0151 - val_tf.math.abs_186_loss: 0.0021\n",
      "Epoch 1296/2000\n",
      "24/24 [==============================] - 1s 48ms/step - loss: 9.4611 - out_T_loss: 0.0294 - out_S_loss: 0.8899 - tf.math.abs_185_loss: 0.0172 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.8191 - val_out_T_loss: 0.2675 - val_out_S_loss: 0.7217 - val_tf.math.abs_185_loss: 0.0146 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1297/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 9.5316 - out_T_loss: 0.0291 - out_S_loss: 0.8968 - tf.math.abs_185_loss: 0.0174 - tf.math.abs_186_loss: 0.0093 - val_loss: 7.8624 - val_out_T_loss: 0.2698 - val_out_S_loss: 0.7255 - val_tf.math.abs_185_loss: 0.0147 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1298/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 10.1226 - out_T_loss: 0.0437 - out_S_loss: 0.9490 - tf.math.abs_185_loss: 0.0199 - tf.math.abs_186_loss: 0.0096 - val_loss: 7.9044 - val_out_T_loss: 0.2634 - val_out_S_loss: 0.7266 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0024\n",
      "Epoch 1299/2000\n",
      "24/24 [==============================] - 1s 45ms/step - loss: 9.4639 - out_T_loss: 0.0268 - out_S_loss: 0.8895 - tf.math.abs_185_loss: 0.0178 - tf.math.abs_186_loss: 0.0093 - val_loss: 7.9127 - val_out_T_loss: 0.2664 - val_out_S_loss: 0.7286 - val_tf.math.abs_185_loss: 0.0157 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1300/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.3231 - out_T_loss: 0.0270 - out_S_loss: 0.8763 - tf.math.abs_185_loss: 0.0171 - tf.math.abs_186_loss: 0.0095 - val_loss: 7.9434 - val_out_T_loss: 0.2831 - val_out_S_loss: 0.7278 - val_tf.math.abs_185_loss: 0.0168 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1301/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.3669 - out_T_loss: 0.0342 - out_S_loss: 0.8804 - tf.math.abs_185_loss: 0.0173 - tf.math.abs_186_loss: 0.0092 - val_loss: 7.8740 - val_out_T_loss: 0.2812 - val_out_S_loss: 0.7247 - val_tf.math.abs_185_loss: 0.0150 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1302/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.5245 - out_T_loss: 0.0320 - out_S_loss: 0.8943 - tf.math.abs_185_loss: 0.0179 - tf.math.abs_186_loss: 0.0095 - val_loss: 7.9309 - val_out_T_loss: 0.2929 - val_out_S_loss: 0.7268 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1303/2000\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 9.0289 - out_T_loss: 0.0293 - out_S_loss: 0.8459 - tf.math.abs_185_loss: 0.0179 - tf.math.abs_186_loss: 0.0091 - val_loss: 7.9010 - val_out_T_loss: 0.2766 - val_out_S_loss: 0.7274 - val_tf.math.abs_185_loss: 0.0153 - val_tf.math.abs_186_loss: 0.0022\n",
      "Epoch 1304/2000\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 9.0951 - out_T_loss: 0.0369 - out_S_loss: 0.8489 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.9044 - val_out_T_loss: 0.2802 - val_out_S_loss: 0.7225 - val_tf.math.abs_185_loss: 0.0177 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1305/2000\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 10.1338 - out_T_loss: 0.0364 - out_S_loss: 0.9532 - tf.math.abs_185_loss: 0.0191 - tf.math.abs_186_loss: 0.0092 - val_loss: 7.8491 - val_out_T_loss: 0.2856 - val_out_S_loss: 0.7220 - val_tf.math.abs_185_loss: 0.0149 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1306/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.8436 - out_T_loss: 0.0335 - out_S_loss: 0.9267 - tf.math.abs_185_loss: 0.0180 - tf.math.abs_186_loss: 0.0092 - val_loss: 7.8846 - val_out_T_loss: 0.2753 - val_out_S_loss: 0.7234 - val_tf.math.abs_185_loss: 0.0165 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1307/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9131 - out_T_loss: 0.0406 - out_S_loss: 0.9282 - tf.math.abs_185_loss: 0.0201 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.8959 - val_out_T_loss: 0.2837 - val_out_S_loss: 0.7243 - val_tf.math.abs_185_loss: 0.0162 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1308/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.4324 - out_T_loss: 0.0351 - out_S_loss: 0.8862 - tf.math.abs_185_loss: 0.0174 - tf.math.abs_186_loss: 0.0094 - val_loss: 7.9488 - val_out_T_loss: 0.2940 - val_out_S_loss: 0.7265 - val_tf.math.abs_185_loss: 0.0172 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1309/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.6195 - out_T_loss: 0.0332 - out_S_loss: 0.9058 - tf.math.abs_185_loss: 0.0172 - tf.math.abs_186_loss: 0.0092 - val_loss: 7.8448 - val_out_T_loss: 0.2730 - val_out_S_loss: 0.7225 - val_tf.math.abs_185_loss: 0.0150 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1310/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.9184 - out_T_loss: 0.0324 - out_S_loss: 0.9345 - tf.math.abs_185_loss: 0.0176 - tf.math.abs_186_loss: 0.0095 - val_loss: 7.8754 - val_out_T_loss: 0.2914 - val_out_S_loss: 0.7210 - val_tf.math.abs_185_loss: 0.0164 - val_tf.math.abs_186_loss: 0.0023\n",
      "Epoch 1311/2000\n",
      "24/24 [==============================] - 1s 44ms/step - loss: 9.1875 - out_T_loss: 0.0291 - out_S_loss: 0.8619 - tf.math.abs_185_loss: 0.0179 - tf.math.abs_186_loss: 0.0091 - val_loss: 7.8487 - val_out_T_loss: 0.2899 - val_out_S_loss: 0.7212 - val_tf.math.abs_185_loss: 0.0152 - val_tf.math.abs_186_loss: 0.0022\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_a.fit(\n",
    "    x_train, [y_train , y_train, layer_zero_train, out_zero_train],  #pairTrain[:, 1]\n",
    "  validation_data = ( x_val, [y_val , y_val, layer_zero_val, out_zero_val] ),  #pairVal[:, 1]\n",
    "    epochs=2000\n",
    "    ,batch_size = 64    #8192\n",
    "  , callbacks=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "id": "83d81720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_a.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "id": "fcf1e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_T = []\n",
    "for i in range(760):\n",
    "    pred_T.append(np.argmax(y_pred[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "id": "ccd7ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_S = []\n",
    "for i in range(760):\n",
    "    pred_S.append(np.argmax(y_pred[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "id": "6091a9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9302631578947368"
      ]
     },
     "execution_count": 1412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_T = accuracy_score(y_test, pred_T)\n",
    "accuracy_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "id": "568b25d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.843421052631579"
      ]
     },
     "execution_count": 1413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_S = accuracy_score(y_test, pred_S)\n",
    "accuracy_S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec6e47",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "2668b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "835d979c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 200, 3)"
      ]
     },
     "execution_count": 1018,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "a2693f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276,)"
      ]
     },
     "execution_count": 1019,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0][122].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "71cee2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 200, 3)"
      ]
     },
     "execution_count": 1020,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "55e0ec0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "index = np.argmax(y_pred[1][12])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bf163c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([500, 1])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "330e6938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_130 (MaxPool  (None, 97, 32)               0         ['batch_normalization_62[0][0]\n",
      " ing1D)                                                             ']                            \n",
      "                                                                                                  \n",
      " conv1d_203 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_130[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_131 (MaxPool  (None, 46, 64)               0         ['conv1d_203[0][0]']          \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 46, 64)               256       ['max_pooling1d_131[0][0]']   \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_204 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['conv1d_204[0][0]']          \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_132 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_205 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_132[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_133 (MaxPool  (None, 48, 8)                0         ['batch_normalization_64[0][0]\n",
      " ing1D)                                                             ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 512)                  0         ['conv1d_205[0][0]']          \n",
      " 8 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_43 (G  (None, 512)                  0         ['conv1d_205[0][0]']          \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_133[0][0]']   \n",
      "                                                                                                  \n",
      " add_36 (Add)                (None, 512)                  0         ['global_average_pooling1d_38[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_43[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 9 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_44 (G  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " dense_71 (Dense)            (None, 300)                  153900    ['add_36[0][0]']              \n",
      "                                                                                                  \n",
      " add_37 (Add)                (None, 32)                   0         ['global_average_pooling1d_39[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_44[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)        (None, 300)                  0         ['dense_71[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 32)                   0         ['add_37[0][0]']              \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_36[0][0]']          \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 383188 (1.46 MB)\n",
      "Trainable params: 382980 (1.46 MB)\n",
      "Non-trainable params: 208 (832.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_TS_1 = Model( inputs=dataInp, outputs=[out_T, out_S], name='distillationFramework' )\n",
    "model_TS_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b20d86ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] building and compilation complete!\n",
      "\n",
      " Model summary:\n",
      "Model: \"distillationFramework\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_130 (MaxPool  (None, 97, 32)               0         ['batch_normalization_62[0][0]\n",
      " ing1D)                                                             ']                            \n",
      "                                                                                                  \n",
      " conv1d_203 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_130[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_131 (MaxPool  (None, 46, 64)               0         ['conv1d_203[0][0]']          \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 46, 64)               256       ['max_pooling1d_131[0][0]']   \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_204 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['conv1d_204[0][0]']          \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_132 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_205 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_132[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_133 (MaxPool  (None, 48, 8)                0         ['batch_normalization_64[0][0]\n",
      " ing1D)                                                             ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 512)                  0         ['conv1d_205[0][0]']          \n",
      " 8 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_43 (G  (None, 512)                  0         ['conv1d_205[0][0]']          \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_133[0][0]']   \n",
      "                                                                                                  \n",
      " add_36 (Add)                (None, 512)                  0         ['global_average_pooling1d_38[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_43[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 9 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_44 (G  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " dense_71 (Dense)            (None, 300)                  153900    ['add_36[0][0]']              \n",
      "                                                                                                  \n",
      " add_37 (Add)                (None, 32)                   0         ['global_average_pooling1d_39[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_44[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)        (None, 300)                  0         ['dense_71[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 32)                   0         ['add_37[0][0]']              \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_36[0][0]']          \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 383188 (1.46 MB)\n",
      "Trainable params: 382980 (1.46 MB)\n",
      "Non-trainable params: 208 (832.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate = 0.00001) # 0.00001\n",
    "print(\"[INFO] compiling model...\")\n",
    "model_TS_1.compile(loss = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\", \"mse\", \"mse\"], loss_weights = [1.,1], optimizer= opt)\n",
    "print('[INFO] building and compilation complete!')\n",
    "print('\\n Model summary:')\n",
    "model_TS_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2d4a9eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 2s 30ms/step - loss: 9.2610 - out_T_loss: 3.5406 - out_S_loss: 5.7204 - val_loss: 9.3943 - val_out_T_loss: 3.5978 - val_out_S_loss: 5.7966\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 9.2489 - out_T_loss: 3.5451 - out_S_loss: 5.7037 - val_loss: 9.3796 - val_out_T_loss: 3.5845 - val_out_S_loss: 5.7952\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 9.2149 - out_T_loss: 3.5061 - out_S_loss: 5.7088 - val_loss: 9.3629 - val_out_T_loss: 3.5695 - val_out_S_loss: 5.7934\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 9.1952 - out_T_loss: 3.5073 - out_S_loss: 5.6879 - val_loss: 9.3479 - val_out_T_loss: 3.5554 - val_out_S_loss: 5.7924\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 9.1641 - out_T_loss: 3.4502 - out_S_loss: 5.7139 - val_loss: 9.3283 - val_out_T_loss: 3.5384 - val_out_S_loss: 5.7899\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 9.1415 - out_T_loss: 3.4561 - out_S_loss: 5.6854 - val_loss: 9.3123 - val_out_T_loss: 3.5239 - val_out_S_loss: 5.7884\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 9.1003 - out_T_loss: 3.4117 - out_S_loss: 5.6887 - val_loss: 9.2956 - val_out_T_loss: 3.5095 - val_out_S_loss: 5.7861\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 9.1962 - out_T_loss: 3.4778 - out_S_loss: 5.7184 - val_loss: 9.2795 - val_out_T_loss: 3.4962 - val_out_S_loss: 5.7832\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 9.1489 - out_T_loss: 3.4664 - out_S_loss: 5.6825 - val_loss: 9.2609 - val_out_T_loss: 3.4840 - val_out_S_loss: 5.7769\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 9.1106 - out_T_loss: 3.4290 - out_S_loss: 5.6816 - val_loss: 9.2437 - val_out_T_loss: 3.4686 - val_out_S_loss: 5.7751\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 9.1019 - out_T_loss: 3.4178 - out_S_loss: 5.6841 - val_loss: 9.2294 - val_out_T_loss: 3.4564 - val_out_S_loss: 5.7730\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 9.0462 - out_T_loss: 3.3818 - out_S_loss: 5.6644 - val_loss: 9.2141 - val_out_T_loss: 3.4426 - val_out_S_loss: 5.7715\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 9.0630 - out_T_loss: 3.3746 - out_S_loss: 5.6884 - val_loss: 9.1985 - val_out_T_loss: 3.4293 - val_out_S_loss: 5.7692\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 9.0203 - out_T_loss: 3.3432 - out_S_loss: 5.6771 - val_loss: 9.1838 - val_out_T_loss: 3.4172 - val_out_S_loss: 5.7666\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 9.0081 - out_T_loss: 3.3549 - out_S_loss: 5.6532 - val_loss: 9.1700 - val_out_T_loss: 3.4055 - val_out_S_loss: 5.7644\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 9.0135 - out_T_loss: 3.3398 - out_S_loss: 5.6737 - val_loss: 9.1544 - val_out_T_loss: 3.3926 - val_out_S_loss: 5.7617\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 9.0494 - out_T_loss: 3.3669 - out_S_loss: 5.6825 - val_loss: 9.1375 - val_out_T_loss: 3.3777 - val_out_S_loss: 5.7599\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.9528 - out_T_loss: 3.2834 - out_S_loss: 5.6694 - val_loss: 9.1156 - val_out_T_loss: 3.3616 - val_out_S_loss: 5.7541\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.9804 - out_T_loss: 3.3037 - out_S_loss: 5.6767 - val_loss: 9.1018 - val_out_T_loss: 3.3491 - val_out_S_loss: 5.7527\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.9668 - out_T_loss: 3.2916 - out_S_loss: 5.6752 - val_loss: 9.0880 - val_out_T_loss: 3.3365 - val_out_S_loss: 5.7515\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.9802 - out_T_loss: 3.3145 - out_S_loss: 5.6657 - val_loss: 9.0731 - val_out_T_loss: 3.3230 - val_out_S_loss: 5.7501\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.9284 - out_T_loss: 3.2618 - out_S_loss: 5.6667 - val_loss: 9.0591 - val_out_T_loss: 3.3112 - val_out_S_loss: 5.7479\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 8.9136 - out_T_loss: 3.2417 - out_S_loss: 5.6718 - val_loss: 9.0402 - val_out_T_loss: 3.2978 - val_out_S_loss: 5.7424\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 8.9277 - out_T_loss: 3.2654 - out_S_loss: 5.6624 - val_loss: 9.0231 - val_out_T_loss: 3.2822 - val_out_S_loss: 5.7409\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.8681 - out_T_loss: 3.2131 - out_S_loss: 5.6550 - val_loss: 9.0085 - val_out_T_loss: 3.2700 - val_out_S_loss: 5.7384\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.9305 - out_T_loss: 3.2617 - out_S_loss: 5.6688 - val_loss: 8.9943 - val_out_T_loss: 3.2571 - val_out_S_loss: 5.7372\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.9189 - out_T_loss: 3.2556 - out_S_loss: 5.6633 - val_loss: 8.9803 - val_out_T_loss: 3.2447 - val_out_S_loss: 5.7357\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.9113 - out_T_loss: 3.2482 - out_S_loss: 5.6631 - val_loss: 8.9657 - val_out_T_loss: 3.2322 - val_out_S_loss: 5.7335\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.8378 - out_T_loss: 3.1911 - out_S_loss: 5.6467 - val_loss: 8.9508 - val_out_T_loss: 3.2226 - val_out_S_loss: 5.7282\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 1s 28ms/step - loss: 8.8079 - out_T_loss: 3.1633 - out_S_loss: 5.6445 - val_loss: 8.9368 - val_out_T_loss: 3.2100 - val_out_S_loss: 5.7268\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.7923 - out_T_loss: 3.1428 - out_S_loss: 5.6495 - val_loss: 8.9232 - val_out_T_loss: 3.1976 - val_out_S_loss: 5.7256\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.7829 - out_T_loss: 3.1566 - out_S_loss: 5.6263 - val_loss: 8.9090 - val_out_T_loss: 3.1850 - val_out_S_loss: 5.7241\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.8262 - out_T_loss: 3.1779 - out_S_loss: 5.6483 - val_loss: 8.8962 - val_out_T_loss: 3.1735 - val_out_S_loss: 5.7228\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.7814 - out_T_loss: 3.1388 - out_S_loss: 5.6426 - val_loss: 8.8818 - val_out_T_loss: 3.1611 - val_out_S_loss: 5.7206\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.7794 - out_T_loss: 3.1395 - out_S_loss: 5.6398 - val_loss: 8.8684 - val_out_T_loss: 3.1507 - val_out_S_loss: 5.7177\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 8.7299 - out_T_loss: 3.1012 - out_S_loss: 5.6288 - val_loss: 8.8545 - val_out_T_loss: 3.1383 - val_out_S_loss: 5.7161\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.7585 - out_T_loss: 3.1287 - out_S_loss: 5.6298 - val_loss: 8.8418 - val_out_T_loss: 3.1268 - val_out_S_loss: 5.7150\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.7192 - out_T_loss: 3.0793 - out_S_loss: 5.6399 - val_loss: 8.8244 - val_out_T_loss: 3.1113 - val_out_S_loss: 5.7131\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.7500 - out_T_loss: 3.1011 - out_S_loss: 5.6489 - val_loss: 8.8146 - val_out_T_loss: 3.1034 - val_out_S_loss: 5.7111\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.7207 - out_T_loss: 3.0962 - out_S_loss: 5.6246 - val_loss: 8.8034 - val_out_T_loss: 3.0939 - val_out_S_loss: 5.7095\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.7014 - out_T_loss: 3.0703 - out_S_loss: 5.6311 - val_loss: 8.7867 - val_out_T_loss: 3.0793 - val_out_S_loss: 5.7074\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.6995 - out_T_loss: 3.0863 - out_S_loss: 5.6132 - val_loss: 8.7720 - val_out_T_loss: 3.0674 - val_out_S_loss: 5.7046\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.6824 - out_T_loss: 3.0550 - out_S_loss: 5.6274 - val_loss: 8.7629 - val_out_T_loss: 3.0599 - val_out_S_loss: 5.7031\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.6799 - out_T_loss: 3.0639 - out_S_loss: 5.6161 - val_loss: 8.7518 - val_out_T_loss: 3.0505 - val_out_S_loss: 5.7013\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 8.6648 - out_T_loss: 3.0406 - out_S_loss: 5.6242 - val_loss: 8.7377 - val_out_T_loss: 3.0381 - val_out_S_loss: 5.6996\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 8.6337 - out_T_loss: 3.0081 - out_S_loss: 5.6257 - val_loss: 8.7236 - val_out_T_loss: 3.0274 - val_out_S_loss: 5.6963\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 8.6252 - out_T_loss: 3.0115 - out_S_loss: 5.6137 - val_loss: 8.7106 - val_out_T_loss: 3.0158 - val_out_S_loss: 5.6947\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.5991 - out_T_loss: 2.9727 - out_S_loss: 5.6264 - val_loss: 8.7001 - val_out_T_loss: 3.0068 - val_out_S_loss: 5.6934\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.5633 - out_T_loss: 2.9561 - out_S_loss: 5.6072 - val_loss: 8.6855 - val_out_T_loss: 2.9932 - val_out_S_loss: 5.6922\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.6060 - out_T_loss: 2.9854 - out_S_loss: 5.6206 - val_loss: 8.6687 - val_out_T_loss: 2.9788 - val_out_S_loss: 5.6899\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 8.5992 - out_T_loss: 2.9822 - out_S_loss: 5.6171 - val_loss: 8.6562 - val_out_T_loss: 2.9691 - val_out_S_loss: 5.6872\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 1s 25ms/step - loss: 8.5861 - out_T_loss: 2.9756 - out_S_loss: 5.6106 - val_loss: 8.6453 - val_out_T_loss: 2.9600 - val_out_S_loss: 5.6853\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 8.5154 - out_T_loss: 2.9158 - out_S_loss: 5.5996 - val_loss: 8.6326 - val_out_T_loss: 2.9494 - val_out_S_loss: 5.6832\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 1s 32ms/step - loss: 8.5515 - out_T_loss: 2.9382 - out_S_loss: 5.6132 - val_loss: 8.6205 - val_out_T_loss: 2.9386 - val_out_S_loss: 5.6818\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 1s 33ms/step - loss: 8.5354 - out_T_loss: 2.9513 - out_S_loss: 5.5840 - val_loss: 8.6070 - val_out_T_loss: 2.9268 - val_out_S_loss: 5.6803\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 8.4886 - out_T_loss: 2.8945 - out_S_loss: 5.5942 - val_loss: 8.5966 - val_out_T_loss: 2.9180 - val_out_S_loss: 5.6786\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 1s 31ms/step - loss: 8.5310 - out_T_loss: 2.9269 - out_S_loss: 5.6041 - val_loss: 8.5848 - val_out_T_loss: 2.9080 - val_out_S_loss: 5.6768\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 1s 26ms/step - loss: 8.4949 - out_T_loss: 2.9005 - out_S_loss: 5.5944 - val_loss: 8.5694 - val_out_T_loss: 2.8946 - val_out_S_loss: 5.6748\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.4864 - out_T_loss: 2.8740 - out_S_loss: 5.6125 - val_loss: 8.5575 - val_out_T_loss: 2.8846 - val_out_S_loss: 5.6729\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.5533 - out_T_loss: 2.9412 - out_S_loss: 5.6121 - val_loss: 8.5432 - val_out_T_loss: 2.8746 - val_out_S_loss: 5.6685\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.4857 - out_T_loss: 2.8907 - out_S_loss: 5.5950 - val_loss: 8.5337 - val_out_T_loss: 2.8666 - val_out_S_loss: 5.6671\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.4754 - out_T_loss: 2.8710 - out_S_loss: 5.6043 - val_loss: 8.5204 - val_out_T_loss: 2.8551 - val_out_S_loss: 5.6653\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 0s 21ms/step - loss: 8.4459 - out_T_loss: 2.8551 - out_S_loss: 5.5908 - val_loss: 8.5109 - val_out_T_loss: 2.8475 - val_out_S_loss: 5.6634\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.4440 - out_T_loss: 2.8382 - out_S_loss: 5.6058 - val_loss: 8.4996 - val_out_T_loss: 2.8382 - val_out_S_loss: 5.6614\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.4564 - out_T_loss: 2.8536 - out_S_loss: 5.6028 - val_loss: 8.4880 - val_out_T_loss: 2.8280 - val_out_S_loss: 5.6600\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.3723 - out_T_loss: 2.7900 - out_S_loss: 5.5824 - val_loss: 8.4735 - val_out_T_loss: 2.8148 - val_out_S_loss: 5.6587\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 8.4689 - out_T_loss: 2.8804 - out_S_loss: 5.5885 - val_loss: 8.4644 - val_out_T_loss: 2.8076 - val_out_S_loss: 5.6568\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.3903 - out_T_loss: 2.8209 - out_S_loss: 5.5695 - val_loss: 8.4517 - val_out_T_loss: 2.7988 - val_out_S_loss: 5.6529\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 1s 21ms/step - loss: 8.4087 - out_T_loss: 2.8118 - out_S_loss: 5.5969 - val_loss: 8.4383 - val_out_T_loss: 2.7867 - val_out_S_loss: 5.6516\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.3910 - out_T_loss: 2.8243 - out_S_loss: 5.5667 - val_loss: 8.4279 - val_out_T_loss: 2.7776 - val_out_S_loss: 5.6502\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.3962 - out_T_loss: 2.8037 - out_S_loss: 5.5925 - val_loss: 8.4161 - val_out_T_loss: 2.7696 - val_out_S_loss: 5.6465\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 8.3961 - out_T_loss: 2.8284 - out_S_loss: 5.5677 - val_loss: 8.4061 - val_out_T_loss: 2.7633 - val_out_S_loss: 5.6428\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 8.3207 - out_T_loss: 2.7493 - out_S_loss: 5.5714 - val_loss: 8.3949 - val_out_T_loss: 2.7528 - val_out_S_loss: 5.6421\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 1s 27ms/step - loss: 8.3274 - out_T_loss: 2.7578 - out_S_loss: 5.5696 - val_loss: 8.3881 - val_out_T_loss: 2.7467 - val_out_S_loss: 5.6414\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 8.3184 - out_T_loss: 2.7506 - out_S_loss: 5.5678 - val_loss: 8.3773 - val_out_T_loss: 2.7372 - val_out_S_loss: 5.6401\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.3593 - out_T_loss: 2.7817 - out_S_loss: 5.5776 - val_loss: 8.3657 - val_out_T_loss: 2.7273 - val_out_S_loss: 5.6385\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.3662 - out_T_loss: 2.7939 - out_S_loss: 5.5723 - val_loss: 8.3575 - val_out_T_loss: 2.7204 - val_out_S_loss: 5.6371\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.3460 - out_T_loss: 2.7714 - out_S_loss: 5.5746 - val_loss: 8.3444 - val_out_T_loss: 2.7086 - val_out_S_loss: 5.6358\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2906 - out_T_loss: 2.7300 - out_S_loss: 5.5605 - val_loss: 8.3315 - val_out_T_loss: 2.6973 - val_out_S_loss: 5.6342\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2397 - out_T_loss: 2.6718 - out_S_loss: 5.5679 - val_loss: 8.3176 - val_out_T_loss: 2.6850 - val_out_S_loss: 5.6327\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2512 - out_T_loss: 2.6905 - out_S_loss: 5.5607 - val_loss: 8.3069 - val_out_T_loss: 2.6760 - val_out_S_loss: 5.6309\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2898 - out_T_loss: 2.7322 - out_S_loss: 5.5576 - val_loss: 8.2990 - val_out_T_loss: 2.6703 - val_out_S_loss: 5.6287\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 1s 23ms/step - loss: 8.2231 - out_T_loss: 2.6504 - out_S_loss: 5.5727 - val_loss: 8.2911 - val_out_T_loss: 2.6646 - val_out_S_loss: 5.6265\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 8.2579 - out_T_loss: 2.6994 - out_S_loss: 5.5585 - val_loss: 8.2825 - val_out_T_loss: 2.6581 - val_out_S_loss: 5.6244\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2288 - out_T_loss: 2.6605 - out_S_loss: 5.5683 - val_loss: 8.2709 - val_out_T_loss: 2.6487 - val_out_S_loss: 5.6222\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2428 - out_T_loss: 2.6951 - out_S_loss: 5.5477 - val_loss: 8.2587 - val_out_T_loss: 2.6384 - val_out_S_loss: 5.6202\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2045 - out_T_loss: 2.6513 - out_S_loss: 5.5532 - val_loss: 8.2438 - val_out_T_loss: 2.6274 - val_out_S_loss: 5.6164\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2178 - out_T_loss: 2.6730 - out_S_loss: 5.5448 - val_loss: 8.2349 - val_out_T_loss: 2.6198 - val_out_S_loss: 5.6150\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.2052 - out_T_loss: 2.6605 - out_S_loss: 5.5447 - val_loss: 8.2220 - val_out_T_loss: 2.6119 - val_out_S_loss: 5.6100\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1931 - out_T_loss: 2.6411 - out_S_loss: 5.5521 - val_loss: 8.2132 - val_out_T_loss: 2.6059 - val_out_S_loss: 5.6073\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1393 - out_T_loss: 2.5741 - out_S_loss: 5.5652 - val_loss: 8.2021 - val_out_T_loss: 2.5959 - val_out_S_loss: 5.6062\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1842 - out_T_loss: 2.6189 - out_S_loss: 5.5653 - val_loss: 8.1911 - val_out_T_loss: 2.5859 - val_out_S_loss: 5.6052\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1689 - out_T_loss: 2.6209 - out_S_loss: 5.5480 - val_loss: 8.1839 - val_out_T_loss: 2.5798 - val_out_S_loss: 5.6042\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1569 - out_T_loss: 2.6057 - out_S_loss: 5.5512 - val_loss: 8.1734 - val_out_T_loss: 2.5706 - val_out_S_loss: 5.6028\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1562 - out_T_loss: 2.6167 - out_S_loss: 5.5394 - val_loss: 8.1629 - val_out_T_loss: 2.5617 - val_out_S_loss: 5.6012\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1717 - out_T_loss: 2.6180 - out_S_loss: 5.5537 - val_loss: 8.1556 - val_out_T_loss: 2.5559 - val_out_S_loss: 5.5997\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1484 - out_T_loss: 2.6183 - out_S_loss: 5.5301 - val_loss: 8.1445 - val_out_T_loss: 2.5463 - val_out_S_loss: 5.5982\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 1s 24ms/step - loss: 8.0973 - out_T_loss: 2.5616 - out_S_loss: 5.5357 - val_loss: 8.1329 - val_out_T_loss: 2.5361 - val_out_S_loss: 5.5968\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1153 - out_T_loss: 2.5810 - out_S_loss: 5.5343 - val_loss: 8.1219 - val_out_T_loss: 2.5267 - val_out_S_loss: 5.5952\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 8.1042 - out_T_loss: 2.5657 - out_S_loss: 5.5385 - val_loss: 8.1156 - val_out_T_loss: 2.5220 - val_out_S_loss: 5.5936\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_TS_1.fit(\n",
    "    x_train, [y_train , y_train],  #pairTrain[:, 1]\n",
    "  validation_data = ( x_val, [y_val , y_val ] ),  #pairVal[:, 1]\n",
    "    epochs=100\n",
    "    ,batch_size = 64    #8192\n",
    "  , callbacks=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e926edf",
   "metadata": {},
   "source": [
    "# weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "id": "9785ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_input_shape': (None, 200, 3), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'in'} []\n",
      "{'name': 'l1_T', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': (7,), 'strides': (1,), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[[ 1.08670376e-01,  5.24691306e-02,  1.02911768e-02,\n",
      "         -1.49503797e-01, -9.88483708e-03,  6.44231364e-02,\n",
      "          2.23147012e-02, -8.27949047e-02, -1.07302763e-01,\n",
      "          2.20311917e-02, -7.81056508e-02, -9.14114863e-02,\n",
      "          4.18657511e-02,  1.15119867e-01,  8.09835494e-02,\n",
      "          1.17010877e-01,  2.87218373e-02,  1.27126336e-01,\n",
      "         -7.18637928e-02,  5.44643328e-02, -7.27443919e-02,\n",
      "         -1.85566351e-01,  1.04126453e-01, -1.07911259e-01,\n",
      "         -4.33984026e-02,  7.57686570e-02,  7.17881173e-02,\n",
      "         -1.14868395e-01, -5.28170355e-02, -7.69707263e-02,\n",
      "         -9.86963958e-02, -6.69372156e-02],\n",
      "        [ 3.54651928e-01,  4.63407710e-02, -5.34745492e-03,\n",
      "         -1.36874974e-01,  5.28873727e-02, -1.10084057e-01,\n",
      "          7.56969210e-03, -1.33218840e-01,  6.90868497e-02,\n",
      "         -6.74432740e-02, -9.62952152e-02, -2.04782709e-01,\n",
      "         -1.83034658e-01,  2.60541644e-02,  3.94001752e-02,\n",
      "          4.96800765e-02,  1.02893922e-04,  6.87987953e-02,\n",
      "         -1.83573619e-01, -6.84586447e-03,  7.45167434e-02,\n",
      "          2.21315622e-01, -1.99199766e-01,  1.02004465e-02,\n",
      "         -7.20407367e-02, -1.00665761e-03, -5.92757612e-02,\n",
      "          2.12000236e-01, -8.11501779e-03,  8.36373121e-02,\n",
      "         -1.53372988e-01, -8.74616727e-02],\n",
      "        [-1.24224704e-02, -2.97609586e-02,  8.07088912e-02,\n",
      "         -9.75693315e-02,  1.80537924e-01,  8.30220431e-02,\n",
      "          8.07363074e-03,  5.78578636e-02,  1.24578245e-01,\n",
      "         -1.55233415e-02,  3.15787531e-02, -2.08360687e-01,\n",
      "         -5.53149246e-02, -1.19363680e-01,  1.60013974e-01,\n",
      "         -3.77902947e-02, -2.88183633e-02, -4.94483113e-02,\n",
      "         -4.42896411e-02,  4.25325185e-02, -7.79609615e-03,\n",
      "         -4.81951199e-02,  4.91741970e-02,  5.45855835e-02,\n",
      "         -9.74542648e-02,  9.05619413e-02,  9.19057950e-02,\n",
      "         -5.25628328e-02, -1.96753833e-02, -1.70130618e-02,\n",
      "         -5.89401014e-02, -2.99979523e-02]],\n",
      "\n",
      "       [[ 9.62945148e-02, -6.05150238e-02, -7.68681318e-02,\n",
      "          2.61502117e-02, -1.26512587e-01, -2.42210776e-02,\n",
      "          1.68891232e-02,  1.96939167e-02,  8.91584754e-02,\n",
      "         -6.90408573e-02, -1.48073405e-01,  1.63264588e-01,\n",
      "          1.77011207e-01,  2.12270260e-01,  3.40092219e-02,\n",
      "          1.47817791e-01,  1.39737099e-01,  1.27129093e-01,\n",
      "         -5.90467965e-03,  3.30716036e-02, -2.26776232e-03,\n",
      "         -2.33427975e-02, -4.42765504e-02, -1.61216825e-01,\n",
      "          1.87435374e-01,  1.34195447e-01, -1.14830233e-01,\n",
      "          6.89472482e-02, -1.17679760e-01, -2.30294950e-02,\n",
      "          1.00756913e-01, -1.37737036e-01],\n",
      "        [ 3.09719257e-02,  1.46455571e-01,  1.35285094e-01,\n",
      "         -5.24692237e-03,  4.54437323e-02, -1.18525065e-01,\n",
      "         -4.17852728e-03,  4.37325798e-02,  5.42079993e-02,\n",
      "          9.80870053e-03, -5.46500161e-02, -5.18689938e-02,\n",
      "         -1.46937579e-01,  1.97789460e-01,  9.26159546e-02,\n",
      "          5.80724701e-02,  2.20241733e-02,  1.34076387e-01,\n",
      "         -9.76237357e-02, -1.00946557e-02, -5.78311570e-02,\n",
      "         -7.16564357e-02,  6.48540929e-02,  3.57612991e-03,\n",
      "         -1.91157460e-01, -7.05635697e-02,  1.73215389e-01,\n",
      "         -6.77480549e-02,  1.25456750e-01,  1.26973867e-01,\n",
      "         -2.81097349e-02,  1.05650641e-01],\n",
      "        [ 1.44865746e-02, -3.80927660e-02,  1.03185683e-01,\n",
      "          7.25287795e-02,  1.51901692e-02,  6.72902167e-02,\n",
      "         -1.87812857e-02,  5.58600388e-02,  1.69810519e-01,\n",
      "         -1.07071165e-03,  1.03621483e-01,  1.35594055e-01,\n",
      "          1.39202833e-01, -8.09117220e-03,  2.45377347e-01,\n",
      "         -1.19662635e-01,  7.55467685e-03, -1.04767464e-01,\n",
      "          1.56110302e-01, -1.99717395e-02,  7.21815275e-03,\n",
      "         -6.32249713e-02,  1.47111088e-01, -3.02838646e-02,\n",
      "         -4.23394777e-02,  1.06494397e-01,  8.93185064e-02,\n",
      "         -8.28565285e-02,  6.27805740e-02,  8.52462277e-02,\n",
      "         -6.65046573e-02,  5.17316163e-02]],\n",
      "\n",
      "       [[-1.44412294e-01,  6.20171502e-02, -1.73179489e-02,\n",
      "          1.34023532e-01, -1.38078243e-01, -6.98952600e-02,\n",
      "         -7.65310898e-02,  2.82676965e-02, -8.32087547e-02,\n",
      "         -2.10611094e-02,  8.92017484e-02,  4.02944796e-02,\n",
      "          1.51171302e-02,  1.73847541e-01, -1.30597219e-01,\n",
      "         -1.29864812e-01, -1.65165812e-01, -8.86813328e-02,\n",
      "         -6.59461021e-02,  5.82406372e-02, -1.53743625e-01,\n",
      "         -4.81736809e-02, -1.39625780e-02,  3.37380469e-02,\n",
      "          8.00685734e-02,  1.48095265e-01, -1.28952209e-02,\n",
      "          3.03454157e-02, -5.09325378e-02, -5.85273132e-02,\n",
      "         -8.03956389e-02,  7.68614858e-02],\n",
      "        [ 1.55389383e-01,  1.11785337e-01,  1.72668695e-02,\n",
      "         -6.43749163e-02,  6.35914579e-02, -1.41289100e-01,\n",
      "          2.57702798e-01, -3.65947150e-02,  5.42781651e-02,\n",
      "         -1.04465351e-01, -2.60882750e-02, -7.93173164e-02,\n",
      "         -7.55206048e-02,  1.45068124e-01,  5.75525425e-02,\n",
      "          1.29041057e-02,  1.71222419e-01,  5.87142855e-02,\n",
      "         -1.03161611e-01, -1.70557961e-01, -1.96238324e-01,\n",
      "          8.35748836e-02, -3.96893471e-02,  2.77085253e-03,\n",
      "         -3.23688947e-02,  9.33941603e-02,  2.54041087e-02,\n",
      "          5.88807277e-03,  1.21976018e-01,  1.77616537e-01,\n",
      "         -5.25837727e-02, -1.22614138e-01],\n",
      "        [ 3.51065844e-02,  6.03398345e-02,  1.24528572e-01,\n",
      "         -1.02402925e-01,  8.47139955e-02,  4.40233983e-02,\n",
      "         -9.38865468e-02,  1.11432530e-01,  8.94717425e-02,\n",
      "         -3.16540189e-02, -6.60390407e-02, -1.00336261e-01,\n",
      "         -9.47820991e-02, -1.83837197e-04,  1.29062086e-01,\n",
      "         -6.77364320e-02, -1.39628395e-01, -9.81498882e-02,\n",
      "          2.46566013e-02,  3.32639888e-02, -7.37587288e-02,\n",
      "          1.52150184e-01,  8.30562040e-03, -3.21043395e-02,\n",
      "          6.35455772e-02,  5.00141792e-02,  8.83390307e-02,\n",
      "          8.47135484e-02, -5.90275899e-02, -1.03263281e-01,\n",
      "         -1.16602749e-01, -1.99817624e-02]],\n",
      "\n",
      "       [[-2.53984351e-02,  6.34963904e-03, -1.03907853e-01,\n",
      "         -1.20199949e-01, -3.04731186e-02,  1.38596177e-01,\n",
      "          1.08746961e-01, -1.25865310e-01, -7.20429197e-02,\n",
      "         -9.32070240e-02,  1.06739618e-01, -8.69415759e-04,\n",
      "         -1.06326059e-01,  4.78138290e-02, -9.83547941e-02,\n",
      "          2.27561593e-01,  8.31889808e-02, -2.25978773e-02,\n",
      "          9.81547162e-02,  1.08910665e-01, -6.23533241e-02,\n",
      "         -8.82048681e-02,  1.21937878e-01, -6.01227693e-02,\n",
      "         -9.62435380e-02,  4.74103279e-02, -8.59454572e-02,\n",
      "         -1.32200480e-01,  7.46643096e-02, -2.95111798e-02,\n",
      "         -7.59866983e-02,  1.23801082e-01],\n",
      "        [-6.44510463e-02,  1.23740435e-01,  4.32070121e-02,\n",
      "         -1.45681620e-01,  9.94697139e-02,  9.17135098e-04,\n",
      "         -9.17649567e-02,  8.08620900e-02, -8.43867660e-02,\n",
      "          6.70198118e-03,  3.71519253e-02, -4.92304750e-02,\n",
      "          1.55288298e-02, -6.04800321e-02, -8.12976658e-02,\n",
      "         -5.02609229e-03,  1.38274785e-02,  1.40959784e-01,\n",
      "         -1.28109708e-01, -2.37594917e-02, -3.95470439e-03,\n",
      "          1.27416715e-01, -3.51930484e-02,  6.45937026e-02,\n",
      "         -9.31456033e-03,  1.09144889e-01, -4.39028703e-02,\n",
      "          7.22368285e-02, -6.25984445e-02,  1.34554014e-01,\n",
      "          1.05966935e-02,  1.05013950e-02],\n",
      "        [-1.26303494e-01,  6.47409931e-02,  1.22397160e-02,\n",
      "         -4.71747890e-02, -3.04030124e-02,  7.72436112e-02,\n",
      "         -1.18768588e-01,  5.23918271e-02, -8.11473131e-02,\n",
      "          2.33213808e-02, -1.76018864e-01,  7.90093932e-03,\n",
      "         -1.08263180e-01, -2.71203779e-02,  4.78825681e-02,\n",
      "          3.34981419e-02, -4.84283455e-02, -3.12515981e-02,\n",
      "          4.80943248e-02, -7.44344220e-02, -1.59147177e-02,\n",
      "          1.33887082e-01, -8.45579505e-02,  3.00109740e-02,\n",
      "          8.75766948e-02, -5.61491400e-02, -3.40478607e-02,\n",
      "          9.49944034e-02, -7.13852495e-02, -1.09004499e-02,\n",
      "         -1.20714277e-01,  2.36710976e-03]],\n",
      "\n",
      "       [[-4.84651923e-02,  6.82770461e-03, -7.43678883e-02,\n",
      "         -1.23567052e-01, -2.10046507e-02,  2.10653886e-01,\n",
      "          9.78221372e-02, -2.85405349e-02, -1.06221132e-01,\n",
      "         -9.09799263e-02,  1.13307513e-01,  1.20946534e-01,\n",
      "          1.69733450e-01,  5.63943610e-02, -1.50978193e-01,\n",
      "         -7.29004443e-02,  2.95083672e-02,  3.94226275e-02,\n",
      "          8.08753446e-03, -1.92660298e-02, -6.57052994e-02,\n",
      "          2.63473224e-02, -1.31391697e-02, -1.04127324e-03,\n",
      "          1.71433941e-01, -9.92620587e-02, -8.31321552e-02,\n",
      "          3.30498405e-02,  1.44692108e-01,  1.50620699e-01,\n",
      "          4.60339449e-02,  1.48211583e-01],\n",
      "        [-2.60197911e-02,  2.25969851e-02, -4.90478091e-02,\n",
      "          3.31122577e-02,  6.65331557e-02,  8.93337652e-02,\n",
      "         -3.98502201e-02,  8.78444985e-02, -8.28042477e-02,\n",
      "          2.41377637e-01,  1.15253992e-01,  3.43761011e-03,\n",
      "          2.19267625e-02, -5.65493219e-02,  3.84542160e-02,\n",
      "          1.12136230e-02,  4.79303673e-02,  1.38950393e-01,\n",
      "          1.04121566e-01,  7.41107389e-02,  3.54417488e-02,\n",
      "          1.61709730e-02,  7.32741356e-02,  4.05334346e-02,\n",
      "         -9.77044702e-02,  1.87712535e-01,  3.27973068e-02,\n",
      "         -5.97716235e-02, -1.38269126e-01,  2.02057272e-01,\n",
      "         -1.15554824e-01, -5.10836542e-02],\n",
      "        [ 9.09133554e-02,  1.25138581e-01, -1.35972932e-01,\n",
      "         -1.95267051e-02, -9.18090865e-02, -3.05957086e-02,\n",
      "          8.18930641e-02, -8.70528370e-02, -4.61262949e-02,\n",
      "         -3.14261653e-02, -1.11430302e-01,  9.16890427e-02,\n",
      "          7.90888518e-02,  1.46191970e-01,  1.54529305e-04,\n",
      "         -1.54428497e-01,  7.58234784e-02, -1.57306179e-01,\n",
      "          5.63569218e-02, -1.35227859e-01,  3.67263444e-02,\n",
      "         -4.23619822e-02, -9.24636796e-02,  7.19740987e-02,\n",
      "         -3.76747847e-02, -5.62036112e-02,  2.65713669e-02,\n",
      "         -1.91233493e-03,  4.89124544e-02,  1.06804550e-01,\n",
      "          3.54195908e-02, -7.47124702e-02]],\n",
      "\n",
      "       [[-2.03984492e-02,  3.13667289e-04, -1.09711278e-03,\n",
      "         -1.71974003e-02, -1.44639418e-01,  2.54552990e-01,\n",
      "         -9.96607319e-02, -1.40914097e-01,  6.75931275e-02,\n",
      "          5.88936061e-02,  8.69973823e-02, -9.57504660e-02,\n",
      "         -4.93376963e-02, -9.64974090e-02, -9.18075070e-02,\n",
      "         -1.58026934e-01, -2.06727445e-01,  8.74280110e-02,\n",
      "         -2.29188744e-02,  9.93569866e-02, -9.00457576e-02,\n",
      "         -2.99501959e-02, -3.42950830e-03, -8.55092108e-02,\n",
      "          4.29668240e-02,  8.30434337e-02, -1.59560308e-01,\n",
      "          1.75517742e-02, -1.46857515e-01, -1.79151632e-02,\n",
      "          1.22147597e-01,  1.17121615e-01],\n",
      "        [ 3.94933000e-02,  1.39207944e-01, -3.41410190e-02,\n",
      "         -1.47036642e-01, -9.89529490e-03,  7.94913992e-02,\n",
      "          9.00328755e-02,  4.58713882e-02,  4.70564179e-02,\n",
      "          1.11870125e-01,  4.30207141e-03,  4.17084992e-02,\n",
      "         -1.44642564e-02,  5.93091957e-02,  3.66727933e-02,\n",
      "         -1.37345549e-02,  3.08864433e-02,  3.15443315e-02,\n",
      "          6.42574131e-02, -1.23187438e-01,  3.64617561e-03,\n",
      "         -6.72837114e-03,  6.16696756e-03, -1.41996101e-01,\n",
      "          1.10822693e-02,  1.33336902e-01,  3.71341705e-02,\n",
      "         -6.33237809e-02,  8.98348540e-02,  1.70784548e-01,\n",
      "         -1.32608607e-01, -2.32935157e-02],\n",
      "        [-8.72432962e-02, -8.80005360e-02,  7.93591887e-03,\n",
      "          4.92054969e-02,  3.39539722e-02, -5.81795387e-02,\n",
      "          1.16890654e-01,  1.18261680e-01,  1.14845581e-01,\n",
      "         -9.14371908e-02,  9.34666246e-02, -3.33077163e-02,\n",
      "         -5.69881201e-02, -6.62805066e-02,  9.16460156e-02,\n",
      "          6.33193925e-02,  4.36408706e-02, -1.31440014e-01,\n",
      "          8.50842446e-02,  1.35737270e-01, -1.07101038e-01,\n",
      "         -1.82232186e-02,  1.56056997e-03, -3.86147648e-02,\n",
      "          2.67884694e-02, -2.52302196e-02,  1.11699337e-02,\n",
      "         -1.73483416e-02,  4.75476719e-02, -1.06936753e-01,\n",
      "          2.88069788e-02,  4.74063829e-02]],\n",
      "\n",
      "       [[ 8.02008510e-02,  7.49103352e-02,  1.16168879e-01,\n",
      "         -1.26641259e-01,  7.91413411e-02,  1.06122121e-01,\n",
      "          9.71312970e-02,  1.14823626e-02, -8.41923580e-02,\n",
      "          1.24228023e-01, -1.02149777e-01,  1.60125434e-01,\n",
      "          1.78763866e-01, -1.29316552e-02, -7.75003359e-02,\n",
      "         -7.63197541e-02, -2.50633284e-02, -1.24836512e-01,\n",
      "          1.06083170e-01, -1.07556790e-01,  1.28262043e-01,\n",
      "         -3.41486894e-02, -1.04228646e-01, -1.01493575e-01,\n",
      "          2.04516381e-01, -9.18563828e-02, -9.28166229e-03,\n",
      "          5.07130772e-02,  8.32756534e-02,  3.10987327e-02,\n",
      "         -6.11278415e-02, -3.79342469e-03],\n",
      "        [-7.83228874e-02,  7.85257742e-02,  1.03598595e-01,\n",
      "         -4.61384133e-02,  4.19243947e-02,  7.59484153e-03,\n",
      "         -3.56534310e-02,  1.99783728e-01,  1.31337151e-01,\n",
      "          9.73518416e-02, -6.45299628e-02,  1.45091629e-02,\n",
      "         -6.83127791e-02,  2.84602996e-02, -4.87382300e-02,\n",
      "          5.02021164e-02, -4.61366028e-02,  6.78964108e-02,\n",
      "         -1.39763132e-01,  5.53727299e-02,  4.09732424e-02,\n",
      "          7.95940757e-02,  1.04297839e-01, -5.96248638e-03,\n",
      "         -1.02897078e-01,  1.41810790e-01,  1.59041762e-01,\n",
      "         -7.94980079e-02, -1.10069647e-01,  4.91151847e-02,\n",
      "         -3.77123505e-02,  7.13772401e-02],\n",
      "        [-8.23709071e-02, -3.42572555e-02,  4.94375266e-02,\n",
      "          2.93663144e-02,  1.09052926e-01, -2.22731829e-02,\n",
      "          4.63367626e-02,  1.59156516e-01, -7.81775359e-03,\n",
      "          1.68122366e-01,  1.27176106e-01,  9.19648856e-02,\n",
      "          8.42285827e-02, -1.65759660e-02,  1.92853764e-01,\n",
      "          5.73876016e-02,  2.43144892e-02,  2.55391560e-02,\n",
      "          1.07962511e-01,  9.60439667e-02,  1.41349554e-01,\n",
      "          3.35869798e-03,  1.23823240e-01, -1.35623768e-01,\n",
      "         -2.23978069e-02, -1.03380516e-01,  1.08279094e-01,\n",
      "         -7.34158903e-02, -4.83225212e-02,  5.02967648e-02,\n",
      "         -5.37343919e-02,  5.65176383e-02]]], dtype=float32), array([ 0.2955881 ,  0.02824108,  0.34430644,  0.        , -0.01868392,\n",
      "        0.03670085,  0.17207964,  0.03369514,  0.05405991,  0.11672204,\n",
      "       -0.00099767,  0.1278618 ,  0.02217193, -0.03826191,  0.01721352,\n",
      "       -0.09117839,  0.20321357,  0.06681237,  0.11866291,  0.07818561,\n",
      "        0.04341324,  0.1516516 ,  0.18208148, -0.03065044,  0.03591179,\n",
      "       -0.10017577,  0.04154496,  0.10502244,  0.1089766 ,  0.17895155,\n",
      "       -0.0114985 ,  0.12718658], dtype=float32)]\n",
      "{'name': 'batch_normalization_391', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([2]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None} [array([0.8356824 , 1.1805756 , 1.0111536 , 1.        , 0.9929876 ,\n",
      "       1.0517428 , 1.0515556 , 1.0112666 , 0.90927184, 0.90721184,\n",
      "       0.85645473, 1.037179  , 1.0099574 , 1.0076485 , 1.0283281 ,\n",
      "       0.9463984 , 0.9597882 , 1.0662591 , 0.90567356, 0.9319931 ,\n",
      "       0.7046269 , 0.91089886, 0.9479628 , 0.6073734 , 0.9857032 ,\n",
      "       1.0802296 , 1.0279928 , 1.1550803 , 0.82027894, 1.0017494 ,\n",
      "       0.9336781 , 0.9688051 ], dtype=float32), array([ 0.04478945, -0.07133865,  0.19550475,  0.4508555 ,  0.11239882,\n",
      "       -0.02766854, -0.03613327,  0.13279596,  0.12198889,  0.1486995 ,\n",
      "        0.15382083,  0.11556479,  0.11920065,  0.20325194, -0.00101633,\n",
      "        0.05553182,  0.14382477,  0.09526484,  0.15002094,  0.10480837,\n",
      "        0.0643163 ,  0.02576193,  0.17645074, -0.24149422,  0.02703511,\n",
      "        0.14343342,  0.12656407,  0.1661304 ,  0.00554741,  0.10199593,\n",
      "        0.28751943,  0.11219258], dtype=float32), array([1.44907656e+01, 4.12767639e+01, 1.38707476e+01, 0.00000000e+00,\n",
      "       1.13439846e+01, 3.49523773e+01, 1.85269165e+01, 1.88680706e+01,\n",
      "       1.10360241e+01, 1.14134035e+01, 2.32374763e+00, 4.59664679e+00,\n",
      "       6.85998821e+00, 3.76511269e+01, 2.46454716e+01, 3.60381722e+00,\n",
      "       4.62340164e+00, 1.59695530e+01, 7.34641933e+00, 8.09460735e+00,\n",
      "       1.45338094e-02, 8.68448925e+00, 9.06136894e+00, 1.95355900e-03,\n",
      "       1.06130199e+01, 4.32544861e+01, 1.31621618e+01, 6.34693563e-01,\n",
      "       5.82082629e-01, 4.24424477e+01, 1.16516824e-36, 1.14333887e+01],\n",
      "      dtype=float32), array([9.02561951e+01, 1.81844971e+02, 6.59203720e+01, 1.17198604e-36,\n",
      "       1.03670448e+02, 5.47920837e+02, 7.02419281e+01, 1.63153519e+02,\n",
      "       9.26743622e+01, 5.31635704e+01, 2.58105793e+01, 4.14659271e+01,\n",
      "       7.30966263e+01, 2.68467041e+02, 4.69501495e+02, 2.21880207e+01,\n",
      "       2.84227428e+01, 1.68611450e+02, 1.25934792e+02, 6.17167282e+01,\n",
      "       1.26277104e-01, 9.52621613e+01, 5.36227875e+01, 1.50099155e-02,\n",
      "       1.22152725e+02, 2.45336731e+02, 1.19126465e+02, 5.73957443e+00,\n",
      "       6.98919916e+00, 2.82026459e+02, 1.17230764e-36, 7.51684799e+01],\n",
      "      dtype=float32)]\n",
      "{'name': 'max_pooling1d_453', 'trainable': True, 'dtype': 'float32', 'strides': (2,), 'pool_size': (2,), 'padding': 'valid', 'data_format': 'channels_last'} []\n",
      "{'name': 'conv1d_518', 'trainable': True, 'dtype': 'float32', 'filters': 64, 'kernel_size': (5,), 'strides': (1,), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[[-0.01855079, -0.04037785, -0.02565697, ..., -0.05655057,\n",
      "          0.02019777,  0.07977983],\n",
      "        [-0.02440913, -0.01415364, -0.09006138, ...,  0.05718258,\n",
      "          0.02924409, -0.06558678],\n",
      "        [ 0.04131094, -0.04565715, -0.02872225, ..., -0.1008238 ,\n",
      "          0.03090962, -0.02436236],\n",
      "        ...,\n",
      "        [ 0.00626349,  0.00155419,  0.01423407, ..., -0.05184599,\n",
      "         -0.09778269,  0.08601355],\n",
      "        [-0.00985772,  0.11921699,  0.03443837, ...,  0.02939399,\n",
      "          0.02562601,  0.01359652],\n",
      "        [-0.00281251,  0.05300294,  0.06017064, ...,  0.14961585,\n",
      "         -0.10358099,  0.0357955 ]],\n",
      "\n",
      "       [[-0.00547556, -0.0151336 ,  0.10444448, ..., -0.05079393,\n",
      "          0.12278338,  0.04040269],\n",
      "        [ 0.00484588,  0.05018164, -0.0143884 , ...,  0.07845143,\n",
      "         -0.05578473, -0.09679917],\n",
      "        [ 0.1072487 ,  0.07128047, -0.10742266, ...,  0.01100314,\n",
      "          0.02255839,  0.07400559],\n",
      "        ...,\n",
      "        [-0.01262359, -0.04946912, -0.05749591, ..., -0.11390612,\n",
      "          0.04185596, -0.02502278],\n",
      "        [ 0.1317273 , -0.00085786,  0.11017139, ..., -0.01098337,\n",
      "          0.1602444 , -0.05334472],\n",
      "        [ 0.03213376,  0.03419071,  0.11501013, ..., -0.0580622 ,\n",
      "         -0.02891073,  0.10676498]],\n",
      "\n",
      "       [[-0.03460052, -0.04954382,  0.07740369, ...,  0.10369724,\n",
      "          0.03289762,  0.0246305 ],\n",
      "        [ 0.06622468, -0.08277415, -0.06315096, ...,  0.22219217,\n",
      "          0.06609806, -0.08423649],\n",
      "        [-0.00525804, -0.00570703,  0.0660108 , ..., -0.04702007,\n",
      "         -0.03334986,  0.06283184],\n",
      "        ...,\n",
      "        [ 0.00459041,  0.08547247, -0.08852879, ..., -0.08051179,\n",
      "          0.07456564,  0.08317344],\n",
      "        [ 0.10974261,  0.02414506, -0.01252724, ...,  0.08464091,\n",
      "          0.02417457,  0.05863312],\n",
      "        [ 0.07811701, -0.059431  ,  0.03237943, ...,  0.01052486,\n",
      "         -0.07279985, -0.10888004]],\n",
      "\n",
      "       [[-0.08714568,  0.12330279,  0.03885292, ...,  0.15492424,\n",
      "          0.01345585, -0.03859054],\n",
      "        [ 0.15289438, -0.12590417, -0.06250386, ...,  0.081677  ,\n",
      "          0.01913679,  0.03509355],\n",
      "        [ 0.0409396 , -0.05353144,  0.01192261, ..., -0.00967084,\n",
      "         -0.0492778 ,  0.07947915],\n",
      "        ...,\n",
      "        [ 0.01726826, -0.10342491,  0.09226006, ..., -0.02272821,\n",
      "          0.11507758, -0.02147271],\n",
      "        [ 0.09446399, -0.04614988,  0.04185044, ...,  0.13412873,\n",
      "          0.11748865,  0.07581146],\n",
      "        [-0.01210161, -0.04287155,  0.01768609, ..., -0.02187097,\n",
      "         -0.02787556, -0.03304848]],\n",
      "\n",
      "       [[-0.00951471,  0.02770025,  0.01141497, ...,  0.0330972 ,\n",
      "         -0.0979958 ,  0.01544489],\n",
      "        [ 0.80647224,  0.01321561, -0.00175066, ..., -0.05580929,\n",
      "         -0.06273391, -0.02817114],\n",
      "        [ 0.00572908,  0.04935895,  0.03732976, ...,  0.03918431,\n",
      "         -0.05577007,  0.05035198],\n",
      "        ...,\n",
      "        [-0.03702272,  0.04132362, -0.02875305, ..., -0.0528977 ,\n",
      "         -0.06271557,  0.02229448],\n",
      "        [ 0.01642391,  0.09491082,  0.06889918, ...,  0.05782772,\n",
      "          0.15653545,  0.04347377],\n",
      "        [ 0.05521435,  0.05412208,  0.02192368, ...,  0.03275684,\n",
      "         -0.05059047,  0.07180974]]], dtype=float32), array([ 0.12655272,  0.06617951, -0.01112872,  0.06030976,  0.11066491,\n",
      "        0.34332317, -0.05063171,  0.30787927,  0.13714509,  0.01780204,\n",
      "        0.07747298,  0.20234169,  0.33803198,  0.06446464,  0.07039786,\n",
      "        0.07653151,  0.27953154,  0.1910534 ,  0.08976243,  0.09264962,\n",
      "        0.31376386,  0.1556811 ,  0.05333346,  0.12663615,  0.13089314,\n",
      "        0.11057597,  0.09148221,  0.03787264, -0.035535  ,  0.36383918,\n",
      "        0.09427895,  0.00577203, -0.03442803,  0.10937094,  0.05344512,\n",
      "        0.20442834,  0.377012  ,  0.21945935,  0.15614596,  0.09003323,\n",
      "        0.18547925,  0.04669034,  0.24157251,  0.09428454,  0.06679963,\n",
      "        0.09356658,  0.09079453,  0.10808235,  0.25874496,  0.077344  ,\n",
      "       -0.05872137,  0.03944413,  0.16048522,  0.08169309,  0.3068016 ,\n",
      "        0.2996479 ,  0.12439591, -0.01156993, -0.01044081, -0.04783749,\n",
      "        0.15619044,  0.11456971,  0.13794842,  0.0599308 ], dtype=float32)]\n",
      "{'name': 'batch_normalization_392', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([2]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None} [array([0.9279655 , 0.9477722 , 0.9797676 , 0.9686125 , 0.90986496,\n",
      "       0.9844956 , 0.8873477 , 1.0024147 , 0.8731424 , 0.9836934 ,\n",
      "       0.9393853 , 1.045706  , 0.9081361 , 1.005699  , 0.9351587 ,\n",
      "       0.9843162 , 0.95420396, 0.9502454 , 0.82399213, 0.9728108 ,\n",
      "       0.826804  , 1.0168324 , 0.99923456, 0.93724716, 0.96650535,\n",
      "       1.058554  , 0.9915964 , 0.90050834, 0.9507617 , 1.059239  ,\n",
      "       0.9367062 , 0.98764664, 0.8803099 , 0.9482798 , 0.90048957,\n",
      "       0.96767884, 1.0022342 , 0.97428256, 0.9325206 , 0.93451625,\n",
      "       0.93809485, 0.9981725 , 1.0077758 , 0.92513305, 0.87804204,\n",
      "       0.8620404 , 0.9405253 , 0.9782011 , 1.0542672 , 0.935099  ,\n",
      "       0.86060363, 0.86374736, 0.9110772 , 0.9692344 , 0.97096133,\n",
      "       1.0099411 , 0.6931743 , 1.1041985 , 1.037146  , 0.9641088 ,\n",
      "       1.0026485 , 0.968666  , 0.93655777, 0.88553864], dtype=float32), array([-0.01235315,  0.09526253, -0.04985489,  0.17673482, -0.04313096,\n",
      "        0.05638419, -0.08090746,  0.04821699,  0.05287578,  0.12235831,\n",
      "        0.07124659,  0.09861842, -0.0999094 ,  0.14577611, -0.04861897,\n",
      "        0.0641743 , -0.04551197, -0.09687065, -0.27769953, -0.01906763,\n",
      "       -0.13856979,  0.00717347, -0.05892923,  0.06036866, -0.11855578,\n",
      "        0.04618007, -0.11773198, -0.1049624 , -0.14127249,  0.17950696,\n",
      "       -0.01445785,  0.12024246,  0.07492346, -0.12194213, -0.147829  ,\n",
      "        0.05430335,  0.05299929,  0.04882909, -0.00141629,  0.03926437,\n",
      "        0.15211733,  0.17069824,  0.09195908, -0.01582802, -0.02779923,\n",
      "       -0.08869738,  0.13696234,  0.17851397,  0.12246504,  0.00453655,\n",
      "       -0.08541269, -0.07593074, -0.00429877, -0.01471179, -0.03649427,\n",
      "        0.11879326, -0.16554022, -0.14006734,  0.03544653, -0.13368002,\n",
      "        0.11118987,  0.01204083, -0.02160715,  0.07348223], dtype=float32), array([1.162804  , 0.53468317, 0.36149806, 0.5722647 , 0.9742859 ,\n",
      "       1.8382401 , 0.6768348 , 1.8401561 , 0.9691241 , 0.3198369 ,\n",
      "       0.7393801 , 1.3907644 , 2.0656257 , 1.181429  , 0.99614924,\n",
      "       0.83048564, 1.6353993 , 1.3526121 , 0.8703023 , 0.9007278 ,\n",
      "       1.472156  , 1.2191149 , 0.91616464, 1.0276526 , 1.3050404 ,\n",
      "       0.9571563 , 0.52216685, 0.9736549 , 0.18842845, 1.8303397 ,\n",
      "       0.93616235, 0.61930203, 0.28907493, 0.7500021 , 0.93265605,\n",
      "       1.5461321 , 2.1153836 , 1.1870813 , 1.0470724 , 0.5799149 ,\n",
      "       1.152628  , 0.72878766, 1.5612301 , 1.515942  , 0.49602482,\n",
      "       0.5840348 , 0.9593199 , 0.8695162 , 1.5348264 , 0.4490562 ,\n",
      "       0.32251364, 0.6339863 , 0.99363095, 0.9045116 , 1.6308413 ,\n",
      "       1.9413899 , 0.84475714, 0.5716447 , 0.7867904 , 0.575749  ,\n",
      "       1.0684458 , 0.74967253, 0.97920346, 0.63446146], dtype=float32), array([1.6975999 , 0.44923255, 0.24495977, 0.4156414 , 0.7721661 ,\n",
      "       0.92655647, 1.0014986 , 0.9945377 , 0.58265567, 0.31525213,\n",
      "       0.7044977 , 1.6494621 , 0.7524641 , 2.5019975 , 1.4947042 ,\n",
      "       0.73529726, 0.94419616, 1.2610815 , 0.8322779 , 0.8476469 ,\n",
      "       0.42721978, 1.0319786 , 1.1268015 , 1.0834981 , 1.233226  ,\n",
      "       0.7420622 , 0.55485004, 1.2357796 , 0.09627719, 0.521971  ,\n",
      "       1.0077502 , 0.5272459 , 0.8310415 , 0.6239919 , 1.0988092 ,\n",
      "       0.87710625, 0.624399  , 0.49135444, 1.0157387 , 0.47357515,\n",
      "       0.49464938, 0.7842659 , 1.2463868 , 1.8520188 , 0.29777586,\n",
      "       0.8981474 , 1.0661147 , 0.86578226, 0.84587324, 0.33003944,\n",
      "       0.9809148 , 0.7192362 , 0.905561  , 0.88661814, 0.43926877,\n",
      "       1.6659324 , 0.62292856, 0.5260288 , 1.161099  , 0.9822947 ,\n",
      "       0.5828125 , 0.6038841 , 0.6537144 , 0.5145446 ], dtype=float32)]\n",
      "{'name': 'model_46', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 93, 64), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_image'}, 'registered_name': None, 'name': 'input_image', 'inbound_nodes': []}, {'module': 'keras.src.layers.core.tf_op_layer', 'class_name': 'TFOpLambda', 'config': {'name': 'tf.reshape_79', 'trainable': True, 'dtype': 'float32', 'function': 'reshape'}, 'registered_name': 'TFOpLambda', 'build_config': {'input_shape': (None, 93, 64)}, 'name': 'tf.reshape_79', 'inbound_nodes': [['input_image', 0, 0, {'shape': [-1, 64]}]]}, {'module': 'keras', 'class_name': 'Sequential', 'config': {'name': 'sequential_45', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 64), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'dense_231_input'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_231', 'trainable': True, 'dtype': 'float32', 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_218', 'trainable': True, 'dtype': 'float32', 'rate': 0.4, 'noise_shape': None, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_232', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}]}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}, 'name': 'sequential_45', 'inbound_nodes': [[['tf.reshape_79', 0, 0, {}]]]}, {'module': 'keras.src.layers.core.tf_op_layer', 'class_name': 'TFOpLambda', 'config': {'name': 'tf.reshape_80', 'trainable': True, 'dtype': 'float32', 'function': 'reshape'}, 'registered_name': 'TFOpLambda', 'build_config': {'input_shape': (None, 1)}, 'name': 'tf.reshape_80', 'inbound_nodes': [['sequential_45', 1, 0, {'shape': (-1, 93, 1)}]]}, {'module': 'keras.layers', 'class_name': 'Conv1D', 'config': {'name': 'conv1d_519', 'trainable': True, 'dtype': 'float32', 'filters': 1, 'kernel_size': (5,), 'strides': (1,), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 93, 1)}, 'name': 'conv1d_519', 'inbound_nodes': [[['tf.reshape_80', 0, 0, {}]]]}], 'input_layers': [['input_image', 0, 0]], 'output_layers': [['conv1d_519', 0, 0]]} [array([[-0.11844242,  0.17305699,  0.0134321 , ...,  0.22541924,\n",
      "        -0.02160056, -0.11287681],\n",
      "       [ 0.08484074, -0.01589071, -0.13495909, ..., -0.08801479,\n",
      "        -0.2326292 ,  0.02330384],\n",
      "       [ 0.18058859,  0.04028029, -0.10804764, ..., -0.0583253 ,\n",
      "         0.1188338 ,  0.22694185],\n",
      "       ...,\n",
      "       [-0.18840255, -0.08380531,  0.10143707, ..., -0.20416686,\n",
      "         0.09477329,  0.13351054],\n",
      "       [-0.23401259,  0.05254428, -0.01422748, ...,  0.17783272,\n",
      "        -0.22403619, -0.02497441],\n",
      "       [ 0.22725324,  0.02155459,  0.11543909, ...,  0.01688807,\n",
      "        -0.17883454, -0.03129565]], dtype=float32), array([-0.05407576, -0.10418728, -0.06754314, -0.03853288, -0.1181252 ,\n",
      "       -0.12077343, -0.04427566, -0.05879429, -0.03991108, -0.08742208,\n",
      "       -0.04382745, -0.05902887, -0.04170403, -0.03040615,  0.02642795,\n",
      "       -0.04314122, -0.12091322, -0.00422436, -0.03042609, -0.1156708 ,\n",
      "       -0.0810341 , -0.15339418, -0.04792759, -0.14404944, -0.03921381,\n",
      "       -0.04433259, -0.1027863 , -0.08756398, -0.01081544, -0.07179186,\n",
      "       -0.1480878 , -0.05934915], dtype=float32), array([[-0.20116943],\n",
      "       [-0.36018896],\n",
      "       [-0.06298845],\n",
      "       [-0.1261789 ],\n",
      "       [-0.29794276],\n",
      "       [-0.28822252],\n",
      "       [-0.13584341],\n",
      "       [ 0.26101276],\n",
      "       [-0.11347222],\n",
      "       [-0.13964692],\n",
      "       [ 0.16083157],\n",
      "       [-0.1025131 ],\n",
      "       [ 0.14765146],\n",
      "       [-0.08476053],\n",
      "       [ 0.16403863],\n",
      "       [ 0.09182314],\n",
      "       [ 0.25905886],\n",
      "       [-0.04767899],\n",
      "       [-0.05939303],\n",
      "       [ 0.10262708],\n",
      "       [-0.2011824 ],\n",
      "       [-0.27499348],\n",
      "       [ 0.12100748],\n",
      "       [-0.34184277],\n",
      "       [-0.0460134 ],\n",
      "       [-0.12186176],\n",
      "       [-0.20191881],\n",
      "       [ 0.34066787],\n",
      "       [ 0.10919994],\n",
      "       [ 0.33998206],\n",
      "       [-0.08896609],\n",
      "       [ 0.14111514]], dtype=float32), array([0.02967415], dtype=float32), array([[[ 0.5806287 ]],\n",
      "\n",
      "       [[-0.61205435]],\n",
      "\n",
      "       [[-0.5965926 ]],\n",
      "\n",
      "       [[ 0.16290933]],\n",
      "\n",
      "       [[-0.3662213 ]]], dtype=float32), array([-0.15162784], dtype=float32)]\n",
      "{'name': 'multiply_53', 'trainable': True, 'dtype': 'float32'} []\n",
      "{'name': 'add_234', 'trainable': True, 'dtype': 'float32'} []\n",
      "{'name': 'max_pooling1d_454', 'trainable': True, 'dtype': 'float32', 'strides': (2,), 'pool_size': (2,), 'padding': 'valid', 'data_format': 'channels_last'} []\n",
      "{'name': 'batch_normalization_393', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([2]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None} [array([1.0309199 , 0.9157576 , 0.9664451 , 0.98175764, 1.039259  ,\n",
      "       1.0425402 , 1.0109481 , 1.0949442 , 1.0123514 , 0.9831196 ,\n",
      "       0.9889488 , 1.0111519 , 1.0350865 , 1.0209541 , 0.9949868 ,\n",
      "       1.0171503 , 1.1409277 , 1.0768545 , 1.0248559 , 1.0351034 ,\n",
      "       1.0281949 , 1.1178154 , 1.0139707 , 1.0223649 , 1.0164967 ,\n",
      "       1.1300106 , 0.9628401 , 1.0281779 , 1.00272   , 1.0343099 ,\n",
      "       1.0413485 , 0.9756983 , 1.1755784 , 1.0107045 , 1.007583  ,\n",
      "       1.042546  , 1.0226582 , 1.0312079 , 1.0618858 , 0.96132255,\n",
      "       1.0353993 , 0.9745883 , 1.0621727 , 1.0273185 , 0.9863822 ,\n",
      "       1.0966883 , 1.0039386 , 1.0379573 , 1.0910367 , 1.0078404 ,\n",
      "       1.2565957 , 1.0104823 , 0.991289  , 0.97816354, 1.0222245 ,\n",
      "       1.0232042 , 1.021352  , 0.970429  , 1.0014262 , 1.0086938 ,\n",
      "       1.0371    , 1.0176588 , 1.0337958 , 0.9923914 ], dtype=float32), array([ 0.06576094, -0.06314609, -0.03564319, -0.05106659,  0.00556807,\n",
      "       -0.03252726, -0.0692221 ,  0.0493582 ,  0.03318132, -0.09650207,\n",
      "       -0.01734233,  0.06637087,  0.04450373,  0.0503768 ,  0.13376954,\n",
      "       -0.01802979,  0.06405772,  0.09867229,  0.07497276,  0.06948743,\n",
      "        0.00587052,  0.04685597,  0.0477627 ,  0.01437371,  0.0186485 ,\n",
      "        0.05371736, -0.00099935,  0.09926361, -0.07411718,  0.04693438,\n",
      "        0.01098526,  0.03709233,  0.0351098 ,  0.0093426 ,  0.0819405 ,\n",
      "        0.02218225,  0.04122135,  0.04969124,  0.05523603, -0.05126799,\n",
      "        0.04964085, -0.0134684 ,  0.04153325,  0.04260231,  0.02629678,\n",
      "        0.09267828,  0.01179643,  0.06165317,  0.05129127, -0.01850051,\n",
      "        0.04941442, -0.0184738 , -0.00463568, -0.01952494,  0.09059629,\n",
      "        0.01130745,  0.03157476, -0.05451946,  0.07302258, -0.01023687,\n",
      "        0.04369015,  0.08789593,  0.04898896, -0.11087418], dtype=float32), array([1.3094645 , 0.629541  , 0.43528944, 0.6453352 , 1.1059824 ,\n",
      "       2.095152  , 0.72346616, 2.0396507 , 1.1591538 , 0.3527743 ,\n",
      "       0.9230052 , 1.5867819 , 2.3021638 , 1.3635229 , 1.1488789 ,\n",
      "       0.9730955 , 1.8305418 , 1.5626922 , 1.0098393 , 1.0545179 ,\n",
      "       1.6200613 , 1.3914568 , 1.0732791 , 1.1397694 , 1.4635813 ,\n",
      "       1.1711258 , 0.61904824, 1.091208  , 0.2090854 , 2.0200624 ,\n",
      "       1.1370687 , 0.7150501 , 0.49421716, 0.92009526, 1.0573841 ,\n",
      "       1.6935526 , 2.3130572 , 1.3498443 , 1.206107  , 0.71184254,\n",
      "       1.3549591 , 0.7967846 , 1.7517171 , 1.6559601 , 0.60854656,\n",
      "       0.84851885, 1.112075  , 1.0214106 , 1.7265179 , 0.6098988 ,\n",
      "       0.5474464 , 0.710694  , 1.119129  , 1.0041469 , 1.7775763 ,\n",
      "       2.1218593 , 0.98259485, 0.6289364 , 0.8674763 , 0.6359273 ,\n",
      "       1.2424289 , 0.9340006 , 1.165701  , 0.7426631 ], dtype=float32), array([1.9206506 , 0.50984776, 0.3041193 , 0.46371156, 0.88431406,\n",
      "       1.0757079 , 1.0654737 , 1.1456958 , 0.7261491 , 0.35480422,\n",
      "       0.9150128 , 1.874333  , 0.8732154 , 3.0422482 , 1.7231648 ,\n",
      "       0.8580607 , 0.9855584 , 1.4110618 , 0.9706029 , 1.0323844 ,\n",
      "       0.47661963, 1.2051629 , 1.3065907 , 1.1872847 , 1.3761274 ,\n",
      "       0.94155973, 0.64234424, 1.4244812 , 0.10703298, 0.72761965,\n",
      "       1.2351896 , 0.6121529 , 1.407314  , 0.76868135, 1.279769  ,\n",
      "       0.9539143 , 0.7031795 , 0.5244286 , 1.1016405 , 0.5929803 ,\n",
      "       0.542029  , 0.8153021 , 1.4006449 , 2.0633492 , 0.36555165,\n",
      "       1.4436336 , 1.2497187 , 1.0500067 , 0.9406209 , 0.47217575,\n",
      "       1.649351  , 0.78082776, 1.0151551 , 0.9880106 , 0.44455615,\n",
      "       1.8249393 , 0.73509717, 0.5724237 , 1.3153727 , 1.0766593 ,\n",
      "       0.709063  , 0.76976573, 0.8199823 , 0.5796257 ], dtype=float32)]\n",
      "{'name': 'conv1d_520', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': (3,), 'strides': (1,), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[[ 0.12874766,  0.05202235,  0.05912601, ..., -0.04123299,\n",
      "          0.0141925 ,  0.03756504],\n",
      "        [ 0.00048126, -0.02667985,  0.04841806, ..., -0.07745706,\n",
      "         -0.01326559, -0.0469925 ],\n",
      "        [ 0.05115171, -0.01525575,  0.0069789 , ..., -0.0210908 ,\n",
      "          0.03115042,  0.04033935],\n",
      "        ...,\n",
      "        [ 0.08779584,  0.04384052,  0.04590427, ..., -0.03664699,\n",
      "         -0.06494163, -0.00732795],\n",
      "        [-0.01960638, -0.10410128,  0.07232674, ..., -0.08505887,\n",
      "          0.01520828,  0.06747711],\n",
      "        [ 0.0888998 ,  0.02350123, -0.0068557 , ..., -0.06525245,\n",
      "          0.03296863,  0.00527432]],\n",
      "\n",
      "       [[ 0.06771862, -0.00897428, -0.03122083, ...,  0.0586452 ,\n",
      "         -0.03234896,  0.07115644],\n",
      "        [ 0.04320615, -0.05472122,  0.003865  , ...,  0.05975581,\n",
      "          0.06220149, -0.01463409],\n",
      "        [-0.04498369,  0.09955051, -0.10758916, ..., -0.03231325,\n",
      "         -0.06639846, -0.01887099],\n",
      "        ...,\n",
      "        [-0.03151428, -0.09757804, -0.02760343, ...,  0.01288598,\n",
      "          0.05238884,  0.05584878],\n",
      "        [ 0.0708789 ,  0.0504843 , -0.04009986, ..., -0.02160254,\n",
      "         -0.03470811, -0.0041862 ],\n",
      "        [ 0.01815875,  0.02058539,  0.04712499, ...,  0.06153235,\n",
      "          0.08109951, -0.07902947]],\n",
      "\n",
      "       [[ 0.10231258,  0.01561777,  0.04325312, ...,  0.03142035,\n",
      "         -0.03748764,  0.03638274],\n",
      "        [ 0.01125192, -0.06299175,  0.04910824, ..., -0.03749211,\n",
      "          0.03644147,  0.04454495],\n",
      "        [ 0.07642974, -0.02618798, -0.02259011, ..., -0.01844909,\n",
      "          0.01989432, -0.09627067],\n",
      "        ...,\n",
      "        [ 0.05068266, -0.03212893, -0.00628752, ..., -0.02898439,\n",
      "          0.01488484, -0.08886106],\n",
      "        [-0.03746303,  0.01089529, -0.06221921, ..., -0.02125054,\n",
      "          0.02051143,  0.08791191],\n",
      "        [ 0.06997751, -0.00381758,  0.02008281, ...,  0.0100722 ,\n",
      "          0.02620664, -0.01172418]]], dtype=float32), array([ 0.071993  ,  0.05547029,  0.08249956,  0.17997517,  0.13532215,\n",
      "        0.10489925,  0.04744453, -0.01021135,  0.08339653,  0.01335774,\n",
      "        0.02526851,  0.05619755,  0.05081545,  0.08591258,  0.07156075,\n",
      "        0.05080002,  0.01333003, -0.01048924,  0.08403935, -0.03353329,\n",
      "        0.10826109,  0.03640677, -0.02837991,  0.17021318,  0.04832002,\n",
      "        0.08019233,  0.11176395,  0.12421771,  0.01519436,  0.0204307 ,\n",
      "        0.06742344, -0.00046131,  0.12025863,  0.08721087, -0.00214375,\n",
      "        0.10296278,  0.06704315,  0.00460761,  0.00567848,  0.11861353,\n",
      "        0.11155533,  0.1652268 ,  0.01872547,  0.01265291, -0.02753665,\n",
      "        0.08572477,  0.10919494,  0.02274298,  0.1436171 ,  0.01220428,\n",
      "        0.17693247,  0.03278548,  0.00968285,  0.02219671,  0.12297934,\n",
      "        0.0130121 ,  0.0438535 ,  0.00925449,  0.04714032,  0.0452045 ,\n",
      "        0.03654701,  0.08672718,  0.09707849,  0.11117852,  0.11505225,\n",
      "       -0.0046441 ,  0.10995839,  0.160432  ,  0.07134678,  0.00460973,\n",
      "        0.00450248,  0.02226253,  0.02632171,  0.04472967,  0.11805263,\n",
      "        0.05167645,  0.09014408, -0.0072859 ,  0.03129118,  0.00632496,\n",
      "        0.10040807,  0.09452137,  0.07582624,  0.04840796,  0.03803011,\n",
      "        0.00872562,  0.05465612,  0.10054671,  0.08851897,  0.18476301,\n",
      "        0.0869333 ,  0.00094656,  0.06497145,  0.04195658,  0.0712256 ,\n",
      "        0.10302497,  0.07805914,  0.037246  ,  0.08907553,  0.0387647 ,\n",
      "        0.03687873,  0.019837  ,  0.03338484,  0.10538355,  0.05751434,\n",
      "        0.06794196,  0.12991425,  0.0317088 ,  0.01602535, -0.00204407,\n",
      "        0.00111897,  0.03344731,  0.0381276 , -0.01115849,  0.10769422,\n",
      "        0.07016236,  0.01101782, -0.03289141, -0.02453848, -0.01177078,\n",
      "        0.1081717 ,  0.00194021,  0.04464617,  0.09103886,  0.0761364 ,\n",
      "        0.05055732,  0.02273987,  0.01065576,  0.04488124,  0.10403045,\n",
      "        0.0071921 ,  0.07066423,  0.01323419,  0.0012965 ,  0.05620954,\n",
      "        0.04673254,  0.00677673,  0.14076026,  0.11394025,  0.08728018,\n",
      "       -0.01297267,  0.04967749,  0.02089218,  0.09975366,  0.03171997,\n",
      "        0.06841765,  0.05482335,  0.07132839,  0.00285171,  0.02668641,\n",
      "        0.10809821,  0.02746359,  0.08529916,  0.09663867,  0.07092889,\n",
      "        0.00643874, -0.00413814,  0.07490854, -0.0116893 ,  0.03728112,\n",
      "        0.04090111,  0.05733157,  0.11123876,  0.06554485, -0.00505188,\n",
      "        0.07404917,  0.06863629,  0.06881589,  0.02334138,  0.08225672,\n",
      "        0.04128124,  0.0633158 , -0.02428235,  0.06365959,  0.0480711 ,\n",
      "        0.0865631 ,  0.05837855,  0.08449285,  0.02228337,  0.07073474,\n",
      "        0.08554075,  0.07379768,  0.01715509,  0.1361793 , -0.00687906,\n",
      "        0.05426775,  0.12771608,  0.08074864,  0.03525732,  0.06322928,\n",
      "        0.01440056,  0.02047614,  0.09743806, -0.00283325,  0.06459679,\n",
      "        0.10343485,  0.02835126,  0.10945928,  0.04859333,  0.02534985,\n",
      "        0.05337039,  0.00930425,  0.04080399,  0.06244919, -0.0098651 ,\n",
      "        0.04461696,  0.01898675,  0.12736902,  0.05407386, -0.02495304,\n",
      "        0.10832129, -0.00707922,  0.06231321,  0.0481972 , -0.01501795,\n",
      "        0.03421117,  0.00177342,  0.01309957,  0.0543272 ,  0.0901662 ,\n",
      "        0.01465957,  0.19498007,  0.05897201,  0.06707106,  0.00294132,\n",
      "        0.05694761,  0.11159856,  0.00758904,  0.10396808,  0.02345031,\n",
      "        0.02353307,  0.04256389,  0.18407759,  0.0690908 ,  0.01929592,\n",
      "        0.05229294,  0.12342253,  0.043848  , -0.01410657,  0.05496528,\n",
      "        0.06007306,  0.03584928,  0.02830277, -0.0266595 ,  0.06244162,\n",
      "        0.08931139,  0.09723385,  0.01860783,  0.05982286,  0.01716265,\n",
      "        0.04681326, -0.00062289,  0.0019633 ,  0.09823641,  0.12390865,\n",
      "        0.06448591], dtype=float32)]\n",
      "{'name': 'batch_normalization_394', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([2]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None} [array([1.0068649 , 0.9452234 , 0.97933125, 0.98774904, 0.9866539 ,\n",
      "       0.94658065, 1.0093259 , 1.0135454 , 0.9773786 , 1.005749  ,\n",
      "       1.0185537 , 0.9945918 , 0.9883469 , 0.9878739 , 0.9804129 ,\n",
      "       0.985854  , 0.9878599 , 1.0025138 , 0.9893022 , 1.0024637 ,\n",
      "       1.0037087 , 1.0139874 , 0.9893312 , 0.99091035, 1.030635  ,\n",
      "       1.0153859 , 1.0188419 , 0.9775188 , 1.0174127 , 1.0024893 ,\n",
      "       0.97261363, 1.0006394 , 0.9834114 , 0.9894561 , 0.9449634 ,\n",
      "       0.9797806 , 0.9753464 , 0.9880063 , 1.0067179 , 1.0193472 ,\n",
      "       0.9910995 , 0.9867132 , 1.0039024 , 0.964233  , 0.9746301 ,\n",
      "       0.9781169 , 0.99762106, 0.96831185, 0.97101146, 1.008309  ,\n",
      "       0.9418996 , 1.014753  , 0.9852863 , 0.99720144, 0.98351574,\n",
      "       1.0077529 , 1.0156904 , 1.0015326 , 0.93868273, 0.9806539 ,\n",
      "       0.9736029 , 1.0113541 , 1.0023857 , 0.94356835, 0.9829124 ,\n",
      "       0.99305123, 0.989448  , 0.97069806, 1.0243607 , 0.9587297 ,\n",
      "       1.0181196 , 0.98529   , 0.99719185, 0.97119176, 0.9880995 ,\n",
      "       0.9611025 , 0.9800529 , 0.9787869 , 0.9655056 , 0.98967206,\n",
      "       1.0023863 , 1.029446  , 1.0011573 , 0.99833703, 0.9860801 ,\n",
      "       0.99052554, 1.0006905 , 0.9922243 , 0.9925148 , 1.011615  ,\n",
      "       0.97135985, 1.0124962 , 0.98454523, 1.0097094 , 0.9955612 ,\n",
      "       0.9990123 , 0.9644742 , 0.9660089 , 0.9906906 , 0.9739449 ,\n",
      "       0.9858495 , 0.9710641 , 0.9739897 , 0.98105174, 1.0107648 ,\n",
      "       0.9523476 , 0.97165257, 0.9366632 , 0.9693638 , 0.97910607,\n",
      "       0.97906506, 0.9891502 , 0.9927427 , 1.0310858 , 0.99268085,\n",
      "       0.9783996 , 1.0007164 , 0.9946131 , 0.95153785, 0.9833991 ,\n",
      "       0.9522763 , 1.0079027 , 0.97592354, 0.9451045 , 1.0088919 ,\n",
      "       0.94694614, 1.0101796 , 0.959619  , 0.992124  , 0.97399074,\n",
      "       0.9689886 , 1.0013301 , 1.0036331 , 0.9843418 , 0.9451158 ,\n",
      "       0.97051   , 0.996367  , 1.0029972 , 0.95525545, 0.99344283,\n",
      "       0.98874617, 1.0168797 , 0.98471683, 0.9522762 , 0.9793877 ,\n",
      "       0.9690004 , 0.96740204, 1.0160768 , 0.9895141 , 1.0070999 ,\n",
      "       0.9986606 , 0.998539  , 1.0081733 , 1.0210428 , 0.9629006 ,\n",
      "       0.98996633, 0.998318  , 1.0008452 , 0.99316216, 0.95160955,\n",
      "       0.9976496 , 0.9551321 , 0.998226  , 0.96224684, 1.0101783 ,\n",
      "       0.99354005, 0.9815203 , 0.97533584, 0.9771438 , 0.9884024 ,\n",
      "       0.9992499 , 1.0170805 , 1.0092735 , 0.9833509 , 0.97142816,\n",
      "       0.9980287 , 0.99317133, 1.0007489 , 1.0199821 , 1.0218711 ,\n",
      "       0.93323076, 0.97262263, 0.97138363, 1.005752  , 0.98905945,\n",
      "       1.0121791 , 0.9722054 , 0.9809445 , 1.0006099 , 0.9905024 ,\n",
      "       0.97555715, 1.0037239 , 0.9847746 , 0.9601775 , 0.996691  ,\n",
      "       0.9861445 , 1.0439545 , 0.99611205, 0.9543006 , 0.9957361 ,\n",
      "       1.0243202 , 0.977075  , 1.0124973 , 0.96932465, 1.0257422 ,\n",
      "       1.0062562 , 0.98230743, 0.96472174, 0.9810795 , 1.0068891 ,\n",
      "       0.99842554, 0.99763495, 0.9811507 , 1.0030591 , 1.0080442 ,\n",
      "       0.9840531 , 0.9966173 , 1.0030562 , 0.98084   , 0.98583597,\n",
      "       0.9931404 , 0.9346013 , 1.0055003 , 0.97374356, 0.97218823,\n",
      "       0.9845146 , 0.97563535, 0.99242526, 1.0049249 , 0.97223616,\n",
      "       1.0082895 , 0.9545391 , 0.96935445, 0.95275134, 0.99473435,\n",
      "       0.99952614, 0.99049085, 1.0185117 , 1.0647999 , 0.9996666 ,\n",
      "       0.97058827, 1.0353763 , 0.9893751 , 0.9446018 , 0.9800259 ,\n",
      "       0.989444  , 0.9850584 , 1.0130185 , 0.9981432 , 0.9938066 ,\n",
      "       1.008501  , 1.0089914 , 0.9771409 , 0.98252857, 0.99102044,\n",
      "       0.9746501 ], dtype=float32), array([ 0.10451613,  0.05990618, -0.04874105,  0.1115667 ,  0.05864647,\n",
      "        0.05356668,  0.09096672,  0.04125038,  0.00025736, -0.05397517,\n",
      "       -0.10245468, -0.09552069,  0.01390739,  0.0370488 , -0.0572978 ,\n",
      "       -0.00726621,  0.09299903,  0.0286041 , -0.08215626, -0.0353655 ,\n",
      "        0.09961256, -0.06970122,  0.0981418 , -0.10560429, -0.10187607,\n",
      "        0.02065625,  0.03284562,  0.09323756,  0.06951667, -0.05345983,\n",
      "        0.05776637,  0.03066501,  0.10805619,  0.07862167,  0.01041001,\n",
      "        0.01110529, -0.08635787,  0.11338054,  0.01441708,  0.10525396,\n",
      "       -0.09360372,  0.06542375, -0.06947566,  0.01729073, -0.0784895 ,\n",
      "       -0.08492268, -0.05901299,  0.10615797,  0.02847475, -0.05630334,\n",
      "       -0.04250313, -0.04794328,  0.0518101 ,  0.01841968, -0.08278264,\n",
      "       -0.07229183, -0.05712381, -0.10215534,  0.10079221,  0.10003789,\n",
      "        0.04886256, -0.09229337, -0.08371811,  0.06998373, -0.10279609,\n",
      "        0.05569623, -0.06359836, -0.07297152,  0.02595419, -0.09534262,\n",
      "       -0.0987343 , -0.09073752, -0.04204012,  0.09974394,  0.04195419,\n",
      "        0.06596171, -0.02740424, -0.05343652,  0.06198665, -0.08395882,\n",
      "        0.09320555,  0.03641945,  0.09601732, -0.11077956, -0.09124018,\n",
      "       -0.09894573,  0.07841601,  0.06150477, -0.03887163, -0.03172604,\n",
      "        0.10910036,  0.06717233,  0.06985668, -0.05424034, -0.06323942,\n",
      "        0.03204792,  0.10566045, -0.07318594,  0.01712999, -0.09819284,\n",
      "       -0.04694097,  0.06027626,  0.08597555, -0.07516295, -0.13778135,\n",
      "       -0.05817113, -0.04017513, -0.01527517, -0.08482219,  0.04683755,\n",
      "        0.07678647, -0.01674593, -0.0685805 ,  0.02791188, -0.11323871,\n",
      "        0.06099748, -0.03774545, -0.07451545, -0.07103433,  0.06276115,\n",
      "        0.05384564,  0.08121465,  0.02300517,  0.02595615,  0.03676852,\n",
      "       -0.06617296, -0.09403305, -0.01312434,  0.0395263 , -0.10506591,\n",
      "        0.03164918,  0.08039619,  0.02753872,  0.00290953, -0.05060511,\n",
      "       -0.08793195,  0.08894203,  0.01807929, -0.04546689,  0.09104588,\n",
      "        0.0826662 , -0.08200022, -0.01643765, -0.08676366,  0.07263683,\n",
      "       -0.06191044,  0.0551554 , -0.01754764, -0.04523496, -0.03932809,\n",
      "       -0.06376457,  0.08016231, -0.09594803, -0.04632591, -0.09438206,\n",
      "       -0.12063136,  0.16153339,  0.07193087, -0.05739881, -0.046762  ,\n",
      "        0.07611958, -0.03612715,  0.09129949,  0.05373004, -0.09906088,\n",
      "       -0.07466894, -0.00468938,  0.10304489,  0.05124965, -0.06247283,\n",
      "       -0.11021116,  0.03730574, -0.0628503 , -0.09922359,  0.05156716,\n",
      "       -0.10174687, -0.10342849,  0.03043018, -0.10136989, -0.1221176 ,\n",
      "        0.04755412, -0.06927797,  0.10452805,  0.06161509,  0.10016994,\n",
      "       -0.0808937 , -0.08326411, -0.12840272,  0.06069186, -0.03516751,\n",
      "       -0.02372126, -0.06621669, -0.0684572 ,  0.11220114,  0.06172235,\n",
      "        0.08885811, -0.11116039,  0.09275316, -0.0440312 ,  0.09819878,\n",
      "        0.05687293,  0.05769186, -0.14966471,  0.09285545,  0.03758028,\n",
      "        0.07381366, -0.00071674, -0.00161864,  0.05155583, -0.03406389,\n",
      "        0.1187112 , -0.07837163, -0.0415939 , -0.09769353, -0.1032161 ,\n",
      "       -0.08478001, -0.05510267, -0.08253294,  0.04131272,  0.10930187,\n",
      "       -0.07533218,  0.06068653,  0.06504642,  0.05152637,  0.08446238,\n",
      "        0.08883625,  0.06482427,  0.07732119, -0.02269909,  0.07657995,\n",
      "        0.05204808,  0.07535677, -0.10535406,  0.09119529,  0.05494934,\n",
      "       -0.06377111, -0.08895539, -0.02911508, -0.15698288,  0.1272636 ,\n",
      "        0.08498559, -0.09699535, -0.14279144,  0.04051232,  0.04537811,\n",
      "        0.09362091,  0.03942963,  0.04213064,  0.04839582, -0.07061411,\n",
      "        0.04043076, -0.07378067,  0.08192091, -0.0272449 ,  0.06772171,\n",
      "        0.02645168], dtype=float32), array([0.8239122 , 0.6428337 , 0.32180837, 0.44057602, 0.42043206,\n",
      "       0.39830026, 0.434169  , 0.47958115, 0.5217316 , 0.5074964 ,\n",
      "       0.45336163, 1.1177865 , 0.43629935, 1.0760112 , 0.5571784 ,\n",
      "       0.70728105, 0.6295543 , 0.66290164, 0.3885109 , 0.47329286,\n",
      "       0.37205803, 0.55494255, 0.36797425, 0.42130828, 0.57735586,\n",
      "       0.3242345 , 0.646929  , 0.7604826 , 0.32410216, 0.43420514,\n",
      "       0.4407966 , 0.622902  , 0.38348228, 0.72012377, 0.69442695,\n",
      "       0.4105442 , 0.7421963 , 0.55659974, 0.4552138 , 0.5058242 ,\n",
      "       0.81215537, 0.4616231 , 0.38635755, 0.43968683, 0.28820828,\n",
      "       0.3964402 , 0.41962895, 0.7196645 , 0.4762631 , 0.2524641 ,\n",
      "       0.48526037, 0.7533686 , 0.54354644, 0.46904644, 0.6299237 ,\n",
      "       0.8370553 , 0.3305158 , 0.5769803 , 1.0290458 , 0.93048674,\n",
      "       0.5735362 , 0.66213584, 0.39668772, 0.7153828 , 0.5094044 ,\n",
      "       0.51236737, 0.58189005, 0.4897763 , 0.7281479 , 0.5301393 ,\n",
      "       0.68561685, 0.60104775, 0.29439217, 0.5586603 , 0.42882884,\n",
      "       0.52960604, 0.43369868, 0.37123632, 0.7050088 , 0.33646828,\n",
      "       0.53634566, 0.40124008, 0.4095715 , 0.31794626, 0.68000907,\n",
      "       0.30694747, 0.7188403 , 0.8389387 , 0.7049166 , 0.5147344 ,\n",
      "       0.7599492 , 0.5572571 , 0.31160846, 0.8145078 , 0.3841528 ,\n",
      "       0.5184072 , 0.6330002 , 0.47962084, 0.60245806, 0.6363142 ,\n",
      "       0.49687633, 0.43307117, 0.266675  , 0.78169537, 0.35386539,\n",
      "       0.42884597, 0.30944315, 0.48709866, 0.67804176, 0.46180373,\n",
      "       0.48319155, 0.5786094 , 0.6196133 , 0.6975485 , 0.3777787 ,\n",
      "       0.76622766, 0.17162776, 0.47673807, 0.4106151 , 0.7118346 ,\n",
      "       0.3620875 , 0.23772934, 0.4145629 , 0.4578782 , 0.67614686,\n",
      "       0.7078902 , 0.40372545, 0.6017691 , 0.48114884, 0.63509077,\n",
      "       0.28254944, 0.5096181 , 0.3854145 , 0.76221114, 0.9307145 ,\n",
      "       0.50463116, 0.47395474, 0.34523574, 0.38238186, 0.8660577 ,\n",
      "       0.5024352 , 0.73560286, 0.8107925 , 0.39341962, 0.48927647,\n",
      "       0.8256633 , 0.83940315, 0.44500136, 0.56219155, 0.49756515,\n",
      "       0.56029034, 0.7238644 , 0.35953817, 0.29098555, 0.7249007 ,\n",
      "       0.4788967 , 0.62101936, 0.4621247 , 0.5136396 , 0.30623496,\n",
      "       0.2935483 , 0.39360583, 0.51094574, 0.22659056, 0.44877955,\n",
      "       0.7730758 , 0.6231629 , 0.5095237 , 0.5545749 , 0.79249907,\n",
      "       0.41374385, 1.0522449 , 0.536633  , 0.64154255, 0.3290839 ,\n",
      "       0.421059  , 0.4739556 , 0.48777294, 0.38049132, 0.8227791 ,\n",
      "       0.68633693, 0.43875104, 0.70688546, 0.32305074, 0.22935851,\n",
      "       0.32825884, 0.6598327 , 0.75907123, 0.91610134, 0.5116414 ,\n",
      "       0.45254466, 0.53729206, 0.3700487 , 0.6797826 , 0.33163598,\n",
      "       0.62147474, 0.60281205, 0.6621785 , 0.5662756 , 0.5372529 ,\n",
      "       0.2384663 , 0.5722827 , 0.5531279 , 0.8714155 , 0.46981978,\n",
      "       0.47594637, 0.43911812, 0.43338934, 0.40854874, 0.46820518,\n",
      "       0.34713867, 0.3765472 , 0.46978727, 0.6000119 , 0.4023341 ,\n",
      "       0.21534593, 0.2594528 , 0.18356965, 0.80168927, 0.45755896,\n",
      "       0.6514481 , 0.3683103 , 0.2878821 , 0.5091837 , 0.6746661 ,\n",
      "       0.4092221 , 0.311292  , 0.60033035, 0.6212242 , 0.57145566,\n",
      "       0.68608516, 0.3501546 , 0.46715504, 0.63925326, 0.4607274 ,\n",
      "       0.6474366 , 0.50307983, 0.33115062, 0.6760165 , 0.4038502 ,\n",
      "       0.9557267 , 0.38513005, 0.66390073, 0.43042547, 0.39285192,\n",
      "       0.33511823, 0.81672317, 0.56022626, 0.5102681 , 0.8279091 ,\n",
      "       0.29836318, 0.6953376 , 0.80865204, 0.5692525 , 0.23609899,\n",
      "       0.3713209 ], dtype=float32), array([2.2289379 , 0.9700319 , 0.17383441, 0.36966547, 0.69952166,\n",
      "       0.31058112, 0.6148883 , 0.36834425, 0.51039594, 0.38568857,\n",
      "       0.32710734, 2.268333  , 0.47026476, 2.673717  , 0.6541056 ,\n",
      "       0.71617776, 0.7275024 , 0.8698243 , 0.69733787, 0.4323337 ,\n",
      "       0.4147341 , 0.55192554, 0.27424955, 0.6079345 , 0.765403  ,\n",
      "       0.19087988, 0.96449035, 1.4641348 , 0.24760592, 0.35028845,\n",
      "       0.4361441 , 0.6958824 , 0.70974505, 0.9255026 , 0.6392442 ,\n",
      "       0.42991295, 1.0185208 , 1.1049702 , 0.31292027, 0.61199284,\n",
      "       1.5829525 , 1.2243176 , 0.24345227, 0.41939482, 0.37621853,\n",
      "       0.25865096, 0.9830924 , 1.1072106 , 0.6276544 , 0.08532369,\n",
      "       0.54957855, 1.0958508 , 0.36634588, 0.24651718, 0.85399544,\n",
      "       1.583082  , 0.14381135, 0.43952596, 1.5869924 , 1.458045  ,\n",
      "       0.6618861 , 0.5978104 , 0.49695984, 0.6461187 , 0.5527542 ,\n",
      "       0.502819  , 0.6206142 , 0.5261744 , 0.6640487 , 0.72679776,\n",
      "       0.800797  , 0.37122667, 0.21455206, 0.52501875, 1.0807608 ,\n",
      "       0.42689845, 0.5152239 , 0.44072536, 1.151775  , 0.15454131,\n",
      "       0.80510056, 0.24129358, 0.33075854, 0.2589522 , 0.82039744,\n",
      "       0.13661292, 1.4159199 , 1.9134161 , 1.1911069 , 1.2988024 ,\n",
      "       1.0083727 , 0.7346385 , 0.1379439 , 1.0945386 , 0.15015753,\n",
      "       0.52647614, 1.202636  , 0.33586   , 0.5530352 , 0.7786706 ,\n",
      "       0.41856223, 0.5629989 , 0.13479286, 1.2014898 , 0.18471943,\n",
      "       0.42522055, 0.20186378, 0.25363562, 0.8778932 , 0.31926626,\n",
      "       0.30755392, 0.7859813 , 0.39283374, 1.0015497 , 0.6620003 ,\n",
      "       1.3754462 , 0.05364432, 0.5706788 , 0.18235774, 1.1920952 ,\n",
      "       0.20527375, 0.25279093, 0.20024702, 0.38598368, 0.7662511 ,\n",
      "       0.9740612 , 0.19917417, 0.96781427, 0.29113987, 0.78905547,\n",
      "       0.22128437, 0.53476983, 0.5379655 , 1.0668267 , 1.8105898 ,\n",
      "       0.5758166 , 0.5389502 , 0.39112812, 0.47751966, 1.5647498 ,\n",
      "       0.4006132 , 0.93874896, 1.4534167 , 0.37281793, 0.8814108 ,\n",
      "       2.226226  , 1.4926121 , 0.7886117 , 0.46053407, 0.34809864,\n",
      "       0.64789206, 1.0894644 , 0.43082362, 0.1940264 , 1.2072732 ,\n",
      "       0.37968644, 0.6503421 , 0.44418424, 0.88633317, 0.15015532,\n",
      "       0.18779202, 0.4005907 , 1.0490865 , 0.13562335, 0.4357039 ,\n",
      "       1.5044527 , 0.83298856, 1.4605978 , 0.6918606 , 1.1359799 ,\n",
      "       0.342183  , 2.290862  , 0.66011804, 0.8369742 , 0.30443797,\n",
      "       0.38607407, 0.64784026, 0.524859  , 0.14531007, 1.0816437 ,\n",
      "       0.8163736 , 0.4058236 , 0.94347656, 0.26748165, 0.08174313,\n",
      "       0.16269559, 0.9273963 , 0.58733475, 2.029232  , 0.71909696,\n",
      "       0.71378237, 0.6629469 , 0.8074515 , 1.0487529 , 0.25605398,\n",
      "       0.9838569 , 0.49198863, 0.9196588 , 0.46769196, 0.6665175 ,\n",
      "       0.12899162, 0.6371371 , 0.6677111 , 1.8055183 , 0.7550263 ,\n",
      "       0.7894902 , 0.5746877 , 0.31412572, 0.3035438 , 0.37189153,\n",
      "       0.36686102, 0.2963928 , 0.5913563 , 0.87049717, 0.30188516,\n",
      "       0.09190308, 0.07120619, 0.10607103, 1.5455662 , 1.0632727 ,\n",
      "       0.54042655, 0.3920045 , 0.16640553, 0.45860276, 0.90423065,\n",
      "       0.71666545, 0.28394714, 0.9545082 , 1.1324111 , 0.6421634 ,\n",
      "       0.7409941 , 0.20711444, 0.4900784 , 0.9088158 , 0.34055832,\n",
      "       0.9917384 , 0.54914254, 0.15381142, 1.1387006 , 0.2076022 ,\n",
      "       1.1850605 , 0.3439809 , 0.9506367 , 0.45963985, 0.53140795,\n",
      "       0.34979248, 1.5427465 , 0.47325477, 0.58426243, 2.3321862 ,\n",
      "       0.20153752, 1.0167979 , 1.2519829 , 0.652319  , 0.13944453,\n",
      "       0.16376044], dtype=float32)]\n",
      "{'name': 'model_47', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 44, 256), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_image'}, 'registered_name': None, 'name': 'input_image', 'inbound_nodes': []}, {'module': 'keras.src.layers.core.tf_op_layer', 'class_name': 'TFOpLambda', 'config': {'name': 'tf.reshape_81', 'trainable': True, 'dtype': 'float32', 'function': 'reshape'}, 'registered_name': 'TFOpLambda', 'build_config': {'input_shape': (None, 44, 256)}, 'name': 'tf.reshape_81', 'inbound_nodes': [['input_image', 0, 0, {'shape': [-1, 256]}]]}, {'module': 'keras', 'class_name': 'Sequential', 'config': {'name': 'sequential_46', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 256), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'dense_233_input'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_233', 'trainable': True, 'dtype': 'float32', 'units': 32, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_219', 'trainable': True, 'dtype': 'float32', 'rate': 0.4, 'noise_shape': None, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_234', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}]}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}, 'name': 'sequential_46', 'inbound_nodes': [[['tf.reshape_81', 0, 0, {}]]]}, {'module': 'keras.src.layers.core.tf_op_layer', 'class_name': 'TFOpLambda', 'config': {'name': 'tf.reshape_82', 'trainable': True, 'dtype': 'float32', 'function': 'reshape'}, 'registered_name': 'TFOpLambda', 'build_config': {'input_shape': (None, 1)}, 'name': 'tf.reshape_82', 'inbound_nodes': [['sequential_46', 1, 0, {'shape': (-1, 44, 1)}]]}, {'module': 'keras.layers', 'class_name': 'Conv1D', 'config': {'name': 'conv1d_521', 'trainable': True, 'dtype': 'float32', 'filters': 1, 'kernel_size': (3,), 'strides': (1,), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 44, 1)}, 'name': 'conv1d_521', 'inbound_nodes': [[['tf.reshape_82', 0, 0, {}]]]}], 'input_layers': [['input_image', 0, 0]], 'output_layers': [['conv1d_521', 0, 0]]} [array([[ 0.11144047, -0.09411728, -0.04972828, ...,  0.15272625,\n",
      "         0.02868014,  0.01576245],\n",
      "       [ 0.10633209,  0.11390102, -0.12634753, ..., -0.0841371 ,\n",
      "        -0.16790767,  0.00784803],\n",
      "       [-0.09860937, -0.08831532, -0.090932  , ...,  0.01611121,\n",
      "         0.04260547, -0.01731941],\n",
      "       ...,\n",
      "       [ 0.10554305, -0.01176756, -0.06553659, ...,  0.07532586,\n",
      "         0.11077349, -0.01054461],\n",
      "       [-0.09574259, -0.05929311, -0.06751132, ...,  0.06639034,\n",
      "        -0.19931601, -0.10789498],\n",
      "       [ 0.05408255, -0.09871928, -0.1017007 , ...,  0.05545424,\n",
      "        -0.26944938, -0.01709074]], dtype=float32), array([ 0.04406154,  0.09630805,  0.05789749,  0.05191354,  0.05674946,\n",
      "        0.07182723,  0.06409385, -0.09559954,  0.06838714, -0.0988021 ,\n",
      "       -0.08686018,  0.06123244,  0.04740462,  0.05767887,  0.03871591,\n",
      "       -0.08145045,  0.01743491,  0.06551635,  0.06607556,  0.13479537,\n",
      "       -0.09821326,  0.08412348,  0.06738587, -0.09824936,  0.06645405,\n",
      "        0.0604144 , -0.1038223 ,  0.07135698, -0.09268929,  0.05193931,\n",
      "        0.18491761, -0.09611952], dtype=float32), array([[-0.45515507],\n",
      "       [-0.27910027],\n",
      "       [-0.2735502 ],\n",
      "       [-0.38952118],\n",
      "       [-0.4097106 ],\n",
      "       [-0.19235352],\n",
      "       [-0.17678986],\n",
      "       [ 0.07912573],\n",
      "       [-0.19164886],\n",
      "       [ 0.02776948],\n",
      "       [ 0.04266888],\n",
      "       [-0.3968285 ],\n",
      "       [-0.28900892],\n",
      "       [-0.3553842 ],\n",
      "       [-0.35560063],\n",
      "       [ 0.01027307],\n",
      "       [-0.05820532],\n",
      "       [-0.39989266],\n",
      "       [-0.28645405],\n",
      "       [-0.1159225 ],\n",
      "       [ 0.00927236],\n",
      "       [-0.28247264],\n",
      "       [-0.21152805],\n",
      "       [ 0.05202342],\n",
      "       [-0.27901435],\n",
      "       [-0.36768284],\n",
      "       [ 0.01794879],\n",
      "       [-0.18581891],\n",
      "       [ 0.35416985],\n",
      "       [-0.26284873],\n",
      "       [-0.12184353],\n",
      "       [ 0.19128199]], dtype=float32), array([-0.1970761], dtype=float32), array([[[ 0.36956912]],\n",
      "\n",
      "       [[-0.06928778]],\n",
      "\n",
      "       [[ 0.2801395 ]]], dtype=float32), array([-0.19663139], dtype=float32)]\n",
      "{'name': 'multiply_54', 'trainable': True, 'dtype': 'float32'} []\n",
      "{'name': 'add_235', 'trainable': True, 'dtype': 'float32'} []\n",
      "{'name': 'bottle_T', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': (3,), 'strides': (1,), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[[ 0.05152109, -0.10477523,  0.01635024, ...,  0.01072318,\n",
      "          0.00096658, -0.05246234],\n",
      "        [-0.05727495,  0.01350414, -0.08248571, ..., -0.06795812,\n",
      "          0.02673225,  0.06423215],\n",
      "        [ 0.00170941, -0.15021358, -0.10116754, ...,  0.08743715,\n",
      "          0.04547298, -0.03167857],\n",
      "        ...,\n",
      "        [ 0.01026148,  0.02535225,  0.04083842, ...,  0.01554945,\n",
      "          0.04563442, -0.03183221],\n",
      "        [ 0.05946637, -0.04759927,  0.01328284, ..., -0.00517829,\n",
      "         -0.06325468, -0.09228571],\n",
      "        [-0.00336499, -0.10937902,  0.07408874, ...,  0.0003239 ,\n",
      "         -0.03031653,  0.03022139]],\n",
      "\n",
      "       [[ 0.07647803,  0.11393913, -0.01571595, ..., -0.01919517,\n",
      "         -0.00062837,  0.03459153],\n",
      "        [-0.04513754, -0.07616397, -0.03245027, ..., -0.07154215,\n",
      "         -0.05659455,  0.10216393],\n",
      "        [-0.02943612, -0.11243284,  0.04238349, ...,  0.00032679,\n",
      "          0.05125347, -0.02636227],\n",
      "        ...,\n",
      "        [-0.06897079, -0.06115743,  0.02421532, ..., -0.04059663,\n",
      "          0.10741598, -0.03497637],\n",
      "        [ 0.02314291,  0.00761426, -0.09173536, ..., -0.013324  ,\n",
      "         -0.02752047,  0.00706926],\n",
      "        [ 0.02648669, -0.1158794 ,  0.03708625, ..., -0.02436367,\n",
      "         -0.02093317, -0.0306885 ]],\n",
      "\n",
      "       [[-0.03132139, -0.10324741,  0.03398814, ..., -0.03739069,\n",
      "         -0.07085485, -0.01443616],\n",
      "        [-0.03361996, -0.03001026, -0.04718792, ..., -0.12328356,\n",
      "         -0.01872016, -0.06225583],\n",
      "        [ 0.0355007 , -0.14992848,  0.022706  , ...,  0.08517727,\n",
      "          0.03072576, -0.06848363],\n",
      "        ...,\n",
      "        [-0.02312668,  0.01589233,  0.02412382, ...,  0.03811551,\n",
      "          0.00091212, -0.06586408],\n",
      "        [-0.06449837, -0.1803221 ,  0.06348692, ..., -0.03817513,\n",
      "         -0.05732414, -0.06121437],\n",
      "        [ 0.02294176,  0.00320701, -0.0039532 , ...,  0.10085499,\n",
      "         -0.01717913,  0.04264155]]], dtype=float32), array([-0.02110811, -0.01942418,  0.02169759, -0.01585455, -0.00916072,\n",
      "        0.03223524, -0.01607306, -0.01662206, -0.00063581,  0.01724794,\n",
      "       -0.00169835, -0.04710537,  0.06572163, -0.00810482, -0.034126  ,\n",
      "        0.00617301,  0.04085502, -0.00881352,  0.0340814 ,  0.01624708,\n",
      "        0.03086427, -0.02999683,  0.00433682, -0.00310513, -0.00209094,\n",
      "        0.01927389,  0.0223749 , -0.02687603,  0.01318996,  0.0230816 ,\n",
      "        0.00373034,  0.00852244], dtype=float32)]\n",
      "{'name': 'l1_S', 'trainable': True, 'dtype': 'float32', 'filters': 8, 'kernel_size': (7,), 'strides': (1,), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[[ 0.14765337, -0.08292597,  0.10625795,  0.24139154,\n",
      "         -0.20766129, -0.00056791,  0.06568924, -0.09035908],\n",
      "        [-0.12159061, -0.24244615,  0.15541461,  0.13909677,\n",
      "          0.3618828 , -0.0113461 ,  0.31381074, -0.3155456 ],\n",
      "        [ 0.21159609, -0.0071369 , -0.07507134, -0.07253537,\n",
      "         -0.10315753,  0.24132304, -0.09963208, -0.28495878]],\n",
      "\n",
      "       [[-0.36745164, -0.07675503, -0.11254194,  0.30038252,\n",
      "          0.11296042, -0.02178727,  0.09675463,  0.26843774],\n",
      "        [ 0.2158609 , -0.01145265,  0.35322022,  0.15617582,\n",
      "         -0.11666256,  0.20505486,  0.10092632, -0.14656295],\n",
      "        [ 0.11905869,  0.14347465, -0.07422975, -0.22828585,\n",
      "         -0.14378615,  0.23917228, -0.13649386,  0.19216394]],\n",
      "\n",
      "       [[-0.19618295,  0.13279659,  0.15960383, -0.18427758,\n",
      "          0.05137173, -0.03980571,  0.22664489,  0.06990117],\n",
      "        [-0.00396833, -0.19108425,  0.35960224,  0.10979548,\n",
      "          0.00510428,  0.02256588, -0.05759259, -0.10692135],\n",
      "        [ 0.05893592, -0.11142304,  0.1554098 , -0.21475138,\n",
      "          0.14033894,  0.2531634 ,  0.11563978, -0.10983754]],\n",
      "\n",
      "       [[-0.1630623 ,  0.2615558 , -0.00177851,  0.2908213 ,\n",
      "         -0.23566182, -0.2667666 ,  0.22685911, -0.06465278],\n",
      "        [ 0.16280116,  0.00757315,  0.33348596,  0.11254057,\n",
      "          0.11630479, -0.0360464 , -0.08800711, -0.07028592],\n",
      "        [ 0.20956928, -0.19108452,  0.12600935, -0.02253895,\n",
      "          0.16075009, -0.10637938,  0.07116279, -0.03880453]],\n",
      "\n",
      "       [[-0.2577899 ,  0.19768967,  0.03026363, -0.10961069,\n",
      "          0.06525066, -0.15065901, -0.05756284,  0.22635019],\n",
      "        [ 0.23170613,  0.15526964,  0.15640675,  0.09959774,\n",
      "         -0.10690554, -0.01108155,  0.05389866,  0.00577045],\n",
      "        [-0.03480951, -0.14228311,  0.28807724, -0.23358653,\n",
      "          0.00668804, -0.1908592 , -0.09522178,  0.13491376]],\n",
      "\n",
      "       [[-0.15488961,  0.10799395,  0.02023466, -0.24890132,\n",
      "          0.02995871, -0.21685815,  0.13488455, -0.11799848],\n",
      "        [ 0.14472133, -0.07485539,  0.31962323, -0.02025984,\n",
      "         -0.10084821, -0.07320361, -0.07156421,  0.06246147],\n",
      "        [-0.07472447,  0.1691799 , -0.14838745,  0.05556198,\n",
      "         -0.03301637,  0.05480283,  0.09794755, -0.06479345]],\n",
      "\n",
      "       [[ 0.05586221, -0.16797799,  0.12174619, -0.14682266,\n",
      "          0.08310273,  0.18553972, -0.09399532,  0.2538381 ],\n",
      "        [ 0.25139073, -0.02488118,  0.19804494,  0.04462163,\n",
      "         -0.12583518,  0.21137342,  0.12596443, -0.02268817],\n",
      "        [ 0.21322271,  0.24715108, -0.07150431,  0.09338052,\n",
      "         -0.11315175,  0.18481426, -0.03581841,  0.13600692]]],\n",
      "      dtype=float32), array([-0.8107882 ,  0.04639888, -0.410697  , -0.5706873 , -0.00873439,\n",
      "       -0.10987294, -0.57292575, -0.05072244], dtype=float32)]\n",
      "{'name': 'max_pooling1d_455', 'trainable': True, 'dtype': 'float32', 'strides': (2,), 'pool_size': (2,), 'padding': 'valid', 'data_format': 'channels_last'} []\n",
      "{'name': 'batch_normalization_395', 'trainable': True, 'dtype': 'float32', 'axis': ListWrapper([2]), 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'gamma_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'moving_mean_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'moving_variance_initializer': {'module': 'keras.initializers', 'class_name': 'Ones', 'config': {}, 'registered_name': None}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None} [array([1.7854017, 1.5587968, 1.8421512, 1.6984049, 1.52802  , 1.7320176,\n",
      "       1.6077427, 1.65818  ], dtype=float32), array([-0.09232387,  0.10331097, -0.15299971, -0.04890814, -0.46391156,\n",
      "       -0.05480796, -0.12692145,  0.01486081], dtype=float32), array([ 26.651814 ,  11.467041 , 112.69227  ,  14.753295 ,   0.9036137,\n",
      "        20.368513 ,  44.748604 ,   7.9684515], dtype=float32), array([ 575.04224 ,  164.71045 , 1336.0592  ,  187.2387  ,   14.222394,\n",
      "        323.49036 ,  395.58054 ,  118.67197 ], dtype=float32)]\n",
      "{'name': 'conv1d_522', 'trainable': True, 'dtype': 'float32', 'filters': 512, 'kernel_size': (3,), 'strides': (1,), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[[ 0.09394458,  0.08152083,  0.01542217, ...,  0.03758632,\n",
      "         -0.19269946, -0.22089225],\n",
      "        [-0.04724629, -0.03597053,  0.0974143 , ..., -0.0014147 ,\n",
      "         -0.01030464,  0.01739207],\n",
      "        [-0.10158315,  0.05580694, -0.01708832, ..., -0.00494363,\n",
      "         -0.15025063, -0.00313785],\n",
      "        ...,\n",
      "        [ 0.1248934 ,  0.02334268, -0.03770491, ..., -0.00963473,\n",
      "         -0.01968123, -0.01378798],\n",
      "        [-0.03607341,  0.05568781,  0.07132176, ..., -0.02834613,\n",
      "         -0.0521124 ,  0.02895547],\n",
      "        [ 0.00351461,  0.05217123,  0.09560947, ...,  0.00429533,\n",
      "         -0.00867741, -0.04361982]],\n",
      "\n",
      "       [[-0.05687049,  0.12609784, -0.00251141, ..., -0.06269151,\n",
      "         -0.15284021,  0.04752016],\n",
      "        [-0.08167018, -0.03381309,  0.0359681 , ..., -0.01549061,\n",
      "         -0.03976954,  0.01517636],\n",
      "        [-0.03194018,  0.01014027, -0.00782381, ..., -0.00351308,\n",
      "         -0.26273927,  0.02558138],\n",
      "        ...,\n",
      "        [-0.02546553, -0.0332332 ,  0.10963413, ..., -0.03243675,\n",
      "         -0.04554414, -0.0474251 ],\n",
      "        [ 0.01952091, -0.00558759,  0.07727037, ...,  0.01984388,\n",
      "          0.10627335, -0.05975727],\n",
      "        [ 0.04789524, -0.02886722,  0.04300654, ...,  0.06075798,\n",
      "          0.09479455,  0.03639213]],\n",
      "\n",
      "       [[ 0.07647456,  0.11818645,  0.01713712, ...,  0.01007735,\n",
      "         -0.03566232, -0.01979117],\n",
      "        [ 0.03842632,  0.02540471,  0.01896638, ...,  0.05602804,\n",
      "         -0.03690946, -0.02824841],\n",
      "        [ 0.0996521 ,  0.08900896,  0.0425877 , ..., -0.00291462,\n",
      "         -0.09136491,  0.03485426],\n",
      "        ...,\n",
      "        [-0.11783229,  0.08574761,  0.04981523, ..., -0.08207082,\n",
      "         -0.02948347, -0.2037078 ],\n",
      "        [-0.03023367, -0.08219869, -0.00632164, ...,  0.06284715,\n",
      "          0.09872342, -0.10898452],\n",
      "        [-0.05905979, -0.04650955, -0.00027231, ..., -0.03529518,\n",
      "          0.0694956 ,  0.04824177]]], dtype=float32), array([ 2.74737813e-02,  2.21620705e-02, -1.02254450e-02, -5.47261871e-02,\n",
      "        5.51650301e-02,  1.01888599e-02, -2.58455817e-02, -2.15304922e-02,\n",
      "        2.06418778e-03,  1.62730217e-02,  8.63929614e-02, -5.10010542e-03,\n",
      "        2.60948222e-02, -2.04144213e-02,  1.72066689e-02,  1.52332382e-02,\n",
      "       -6.39819447e-03,  2.97550354e-02, -1.52709736e-02,  5.92334978e-02,\n",
      "       -1.13508357e-02,  2.74733789e-02,  3.00919637e-03, -1.32082580e-02,\n",
      "       -7.36459903e-03,  1.30294897e-02, -3.64411213e-02,  6.31678291e-03,\n",
      "        1.09596793e-02, -2.15308536e-02, -6.59414474e-03,  2.57532392e-02,\n",
      "        7.90313557e-02,  3.22582829e-03, -7.09530106e-03,  4.13905941e-02,\n",
      "       -5.67195704e-03, -4.88158278e-02,  3.81566323e-02,  1.30279129e-02,\n",
      "       -1.04642501e-02,  2.06431411e-02,  4.60088290e-02, -3.09062656e-02,\n",
      "        3.44214849e-02,  8.09277175e-04,  3.18575650e-02, -7.89524894e-03,\n",
      "        1.00449109e-02,  3.37803438e-02, -1.40852546e-02, -1.07406741e-02,\n",
      "        7.17476457e-02,  1.91100575e-02,  3.50269265e-02,  3.15775722e-02,\n",
      "        1.56809185e-02, -1.68509036e-02,  1.10553868e-01, -1.58407986e-02,\n",
      "        5.43371728e-03, -5.84032945e-03, -3.55619155e-02,  9.48457569e-02,\n",
      "       -2.13959161e-02,  1.65127888e-02,  5.25834598e-03, -1.34225599e-02,\n",
      "       -2.18367726e-02, -3.95062864e-02,  2.17476208e-02,  1.06741423e-02,\n",
      "        6.30916143e-03,  2.66669714e-03,  9.01181716e-03, -1.39035853e-02,\n",
      "       -5.73353730e-02, -5.55024073e-02,  2.78008971e-02,  1.98887363e-02,\n",
      "        5.68992719e-02,  3.17309983e-02, -1.37817534e-02,  1.20880725e-02,\n",
      "        1.71331838e-02,  4.55931090e-02, -4.12652120e-02,  6.35913461e-02,\n",
      "        1.99466515e-02,  3.69546227e-02,  1.31437951e-03, -4.41346131e-03,\n",
      "        5.32069383e-03,  4.38110717e-02,  6.28115162e-02, -4.79099117e-02,\n",
      "       -2.56803874e-02,  1.99609473e-02, -1.58917625e-02,  2.92962114e-03,\n",
      "       -3.83499861e-02, -3.45338471e-02,  3.72855626e-02, -2.86444481e-02,\n",
      "        2.11962201e-02, -3.81340971e-03,  4.27398942e-02,  4.23896573e-02,\n",
      "        2.25062147e-02,  1.80377712e-04, -2.27849884e-03,  2.88770627e-02,\n",
      "        2.73064524e-02,  7.91153056e-04,  6.01022504e-02,  9.50829312e-03,\n",
      "        3.08004469e-02,  2.02790946e-02, -2.83371238e-03,  1.00744534e-02,\n",
      "        3.47915366e-02,  4.64103296e-02,  4.59127091e-02,  2.58677620e-02,\n",
      "        4.54541445e-02,  3.28410640e-02,  5.13072172e-03, -6.00659475e-02,\n",
      "        1.00207083e-01,  1.48959495e-02,  2.48012673e-02, -2.55013704e-02,\n",
      "       -3.40106674e-02,  1.39863994e-02,  9.38378647e-03,  2.99790576e-02,\n",
      "       -3.19772027e-03, -4.83563126e-05,  4.60422002e-02,  1.26321847e-02,\n",
      "        1.66809205e-02,  1.04647260e-02,  7.62500092e-02,  2.60379501e-02,\n",
      "       -5.39092999e-03, -4.29574326e-02, -2.08004750e-02, -2.36162706e-03,\n",
      "        8.65728874e-03,  3.24034840e-02,  4.82818186e-02,  2.83887200e-02,\n",
      "        1.39805460e-02, -2.99595147e-02,  3.65675353e-02,  4.60060947e-02,\n",
      "        1.97900105e-02,  4.07496095e-02, -1.42666223e-02,  1.85020138e-02,\n",
      "        7.45609170e-03, -1.85439773e-02,  5.45231067e-02, -1.56779177e-02,\n",
      "        7.70474086e-03, -8.55482300e-04,  2.10922044e-02,  5.11875516e-03,\n",
      "       -1.50732817e-02, -1.20936111e-02, -9.34398361e-03,  8.83014873e-03,\n",
      "       -2.58118026e-02,  3.02718468e-02,  2.90734340e-02,  1.04310298e-02,\n",
      "       -7.99161755e-03,  4.01673354e-02, -4.56021028e-03,  1.94217917e-02,\n",
      "       -5.47105186e-02,  2.89831497e-02,  1.06037118e-01,  3.44582945e-02,\n",
      "        2.83957664e-02,  1.47237722e-02,  4.08955924e-02,  2.51426900e-05,\n",
      "       -1.55714322e-02, -4.67462931e-03,  4.35458906e-02, -3.87738273e-02,\n",
      "        3.23208645e-02, -3.62742064e-03,  1.70046948e-02,  5.92618249e-03,\n",
      "        2.60928478e-02,  5.44630587e-02, -8.96777399e-03,  8.79178382e-03,\n",
      "        1.38033722e-02, -3.36661376e-02, -3.41704711e-02,  5.20631555e-04,\n",
      "        3.08411345e-02, -3.23517900e-03, -1.29748750e-02,  3.84407267e-02,\n",
      "        2.09405050e-02, -8.13847873e-03, -7.44574424e-03,  4.10126224e-02,\n",
      "        1.94703378e-02, -1.64529905e-02,  1.35646276e-02, -5.47894128e-02,\n",
      "        6.91473335e-02, -5.44198696e-03,  4.36268188e-02, -4.36187908e-02,\n",
      "        1.78746339e-02, -2.85386723e-02,  1.49875665e-02,  7.94988126e-02,\n",
      "       -1.06822411e-02,  2.91334409e-02,  1.23476365e-03,  1.96792036e-02,\n",
      "        2.26829890e-02,  6.49444535e-02,  4.27662954e-02, -2.41749007e-02,\n",
      "       -1.18897604e-02,  1.58225739e-04, -6.66941563e-03, -2.67903158e-03,\n",
      "        2.15858221e-02,  5.04024811e-02, -1.41789112e-02,  1.42452456e-02,\n",
      "        5.32877557e-02,  6.64891559e-05,  1.84820089e-02,  5.70743605e-02,\n",
      "        5.10395728e-02, -5.55032752e-02, -1.76328439e-02, -5.31866588e-02,\n",
      "       -2.39937883e-02, -5.09064784e-03,  3.10559273e-02,  1.07597150e-01,\n",
      "        2.56082509e-02, -8.95048864e-03,  3.42112198e-03, -1.23471636e-02,\n",
      "        4.27595116e-02,  3.87345217e-02,  1.37718752e-01, -2.24844869e-02,\n",
      "        1.97815592e-03, -3.49248340e-03,  1.70084946e-02,  3.50868003e-03,\n",
      "        2.20345594e-02,  4.65919599e-02, -1.55624831e-02,  6.97619617e-02,\n",
      "       -1.97704621e-02,  3.95635813e-02, -1.61343198e-02,  1.83097292e-02,\n",
      "        4.03307844e-03, -7.22190272e-03,  3.13234963e-02, -4.91592363e-02,\n",
      "        2.73401104e-03,  1.75820831e-02, -3.94959413e-02, -3.01256906e-02,\n",
      "        3.13631408e-02, -1.60524733e-02,  2.40968517e-03,  5.57791069e-02,\n",
      "        2.85635074e-03,  4.33321744e-02,  9.78120416e-03,  2.07307260e-03,\n",
      "        2.92973057e-03, -3.54168043e-02,  6.36954792e-03,  1.17437486e-02,\n",
      "       -1.14894868e-03, -1.76411495e-03,  3.86583246e-02, -1.44107718e-04,\n",
      "        5.95561089e-03, -1.96412187e-02,  7.33028650e-02, -1.38258552e-02,\n",
      "        2.66012698e-02,  1.86565405e-04, -5.92512637e-02, -2.33352780e-02,\n",
      "       -1.84067804e-03,  5.59579767e-02, -2.02398859e-02,  1.85661018e-02,\n",
      "        5.13052009e-02, -4.48787287e-02,  4.52043163e-03,  2.02312525e-02,\n",
      "       -1.28536327e-02,  1.46842385e-02, -2.25055106e-02, -2.69768033e-02,\n",
      "        3.70346941e-02,  7.78427068e-03,  1.47349783e-03, -3.71118798e-03,\n",
      "       -1.77916884e-02, -2.13244222e-02,  3.67311686e-02, -7.66476616e-03,\n",
      "        3.06256283e-02,  5.91496564e-02, -5.92805538e-03, -2.64752358e-02,\n",
      "        2.19287220e-02,  4.87190895e-02, -1.48093635e-02, -3.17129598e-04,\n",
      "        4.13852371e-02,  3.92967537e-02, -2.39909738e-02,  4.38336022e-02,\n",
      "       -4.83847149e-02,  2.10334770e-02,  4.65500280e-02,  4.80187833e-02,\n",
      "       -3.29694971e-02,  6.56306650e-03,  7.81425927e-03,  2.18067430e-02,\n",
      "       -3.08927726e-02, -2.69140583e-02, -5.67830130e-02, -4.98668402e-02,\n",
      "       -3.06235589e-02,  2.51691248e-02,  8.08131136e-03, -1.23561192e-02,\n",
      "        4.60669287e-02, -1.86835155e-02,  8.60371217e-02,  1.40336854e-02,\n",
      "        3.44843417e-02,  7.90678430e-03,  2.54746731e-02, -1.68045331e-02,\n",
      "        1.24759376e-02, -2.65717953e-02,  8.10211711e-03, -1.04522863e-02,\n",
      "        2.42658462e-02,  1.86783709e-02, -3.86235979e-03,  1.60079338e-02,\n",
      "       -1.59883332e-02,  8.54855999e-02,  3.58376317e-02,  1.46115571e-03,\n",
      "        5.33095654e-03,  7.52994232e-03, -3.21434587e-02,  3.80953662e-02,\n",
      "       -3.35694067e-02, -3.48169692e-02,  5.98762818e-02, -2.51710471e-02,\n",
      "       -8.99085589e-03,  5.63564785e-02,  2.92215636e-03,  1.09808659e-02,\n",
      "        3.67414556e-03, -3.09334937e-02,  3.64484964e-03, -3.75627466e-02,\n",
      "       -5.95486257e-03, -9.31006297e-03,  5.39067686e-02,  2.62309369e-02,\n",
      "       -1.08523658e-02,  2.98279114e-02, -4.41522337e-02,  2.27577258e-02,\n",
      "       -2.04758309e-02,  7.38889584e-03, -1.19538363e-02,  4.76888604e-02,\n",
      "        5.96201327e-03,  2.72468086e-02,  1.24656584e-03,  3.27937081e-02,\n",
      "        2.92975865e-02,  4.18748967e-02,  2.31574420e-02,  1.00211956e-01,\n",
      "        1.67296920e-02,  1.97460167e-02,  2.13698018e-02,  1.56855993e-02,\n",
      "       -5.49120922e-03,  2.56488379e-02, -9.67651792e-03,  3.02065276e-02,\n",
      "        5.69701493e-02,  3.37070189e-02, -1.21623650e-02,  3.24920677e-02,\n",
      "       -5.59713915e-02, -6.46748096e-02, -2.50914302e-02,  8.02827475e-04,\n",
      "        2.50080377e-02, -2.30611730e-02, -2.17393562e-02,  3.42074297e-02,\n",
      "        5.73353693e-02,  4.13723066e-02, -1.09518692e-02, -2.61342227e-02,\n",
      "       -1.72166023e-02,  2.44566742e-02, -4.87700626e-02,  1.14880083e-02,\n",
      "        3.51687185e-02,  5.55737950e-02,  1.66507345e-02,  8.74041487e-03,\n",
      "       -4.88891825e-03,  2.91889217e-02, -1.77916046e-02,  8.95426050e-03,\n",
      "        4.67616655e-02,  2.57861037e-02, -1.70710254e-02, -5.65199368e-03,\n",
      "       -5.58444336e-02, -3.95955425e-03,  2.52569392e-02,  1.68017186e-02,\n",
      "       -1.42204892e-02,  5.50057292e-02,  3.70033272e-02,  2.82767639e-02,\n",
      "        1.74925532e-02,  7.49670714e-02, -4.79961298e-02, -2.66612507e-02,\n",
      "        3.44531722e-02,  1.85661577e-02,  6.00388944e-02,  4.67800871e-02,\n",
      "        2.19471287e-02,  9.76307224e-03, -5.76766692e-02, -3.90413813e-02,\n",
      "        1.56568661e-02, -3.45225260e-03,  2.16829553e-02,  8.80064443e-02,\n",
      "        3.26697007e-02,  2.70748213e-02, -2.22032089e-02, -2.52035204e-02,\n",
      "        2.43374445e-02, -2.53867228e-02, -2.07998883e-02,  3.48538831e-02,\n",
      "        3.90583687e-02,  1.96096045e-03,  2.53849849e-03,  2.94757448e-03,\n",
      "        2.17872374e-02,  6.13285005e-02, -8.47501587e-03, -2.31182529e-03,\n",
      "        1.65488701e-02, -1.76778063e-02,  6.39591692e-03,  2.04807613e-03,\n",
      "        1.66120585e-02, -2.95257550e-02,  1.67818251e-03,  2.94838939e-02,\n",
      "        8.27490985e-02, -6.43839652e-04, -3.43308295e-03, -7.82049028e-04,\n",
      "        8.72664712e-03, -1.23081999e-02,  3.11309006e-02,  8.81970953e-03,\n",
      "       -7.43515091e-04,  3.80877815e-02, -6.03985833e-03, -2.70393714e-02,\n",
      "        4.88110334e-02,  7.73192104e-03,  2.02386938e-02, -3.61612812e-02],\n",
      "      dtype=float32)]\n",
      "{'name': 'max_pooling1d_456', 'trainable': True, 'dtype': 'float32', 'strides': (4,), 'pool_size': (4,), 'padding': 'valid', 'data_format': 'channels_last'} []\n",
      "{'name': 'global_average_pooling1d_181', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'keepdims': False} []\n",
      "{'name': 'global_max_pooling1d_187', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'keepdims': False} []\n",
      "{'name': 'bottle_S', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': (7,), 'strides': (1,), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1,), 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[[-0.1264946 , -0.2239544 ,  0.09374572, ..., -0.23373885,\n",
      "         -0.17060275, -0.11999331],\n",
      "        [-0.17520322, -0.01750687, -0.28420475, ...,  0.04514569,\n",
      "         -0.19858608,  0.0859481 ],\n",
      "        [-0.12951405, -0.15170641,  0.10081014, ...,  0.30013084,\n",
      "          0.04706093, -0.24397713],\n",
      "        ...,\n",
      "        [-0.09212123, -0.18253753,  0.13369015, ...,  0.0107442 ,\n",
      "         -0.11863066,  0.06480904],\n",
      "        [-0.1142372 , -0.15183236, -0.2603792 , ...,  0.13492788,\n",
      "         -0.1496151 , -0.05147745],\n",
      "        [-0.08787974,  0.08427624, -0.1316299 , ...,  0.10579832,\n",
      "         -0.3753681 ,  0.16809794]],\n",
      "\n",
      "       [[ 0.00430068, -0.31439137,  0.02191586, ...,  0.18460584,\n",
      "         -0.2769317 , -0.13469817],\n",
      "        [-0.01850672, -0.06296957, -0.22244665, ..., -0.02189458,\n",
      "         -0.11581514,  0.05008728],\n",
      "        [-0.04873078, -0.24434964, -0.20060061, ...,  0.16183531,\n",
      "          0.01136617, -0.2998538 ],\n",
      "        ...,\n",
      "        [-0.00489396, -0.11487071,  0.12519193, ...,  0.21309912,\n",
      "         -0.24780966,  0.09364734],\n",
      "        [-0.06719881,  0.0645251 , -0.1909377 , ...,  0.30040535,\n",
      "         -0.05889871, -0.04583279],\n",
      "        [-0.04095453, -0.06690637,  0.28441104, ...,  0.33967182,\n",
      "         -0.23234771,  0.07559891]],\n",
      "\n",
      "       [[ 0.0277082 , -0.46855664, -0.43802825, ..., -0.21040526,\n",
      "         -0.3218957 ,  0.04128357],\n",
      "        [ 0.08953698,  0.02092601, -0.13900188, ..., -0.00575822,\n",
      "         -0.21911648,  0.04986886],\n",
      "        [ 0.01793822, -0.29027387,  0.09912962, ...,  0.23324506,\n",
      "         -0.07867899, -0.15408915],\n",
      "        ...,\n",
      "        [-0.00343782, -0.24582613, -0.537978  , ..., -0.13350068,\n",
      "         -0.37543142,  0.06585084],\n",
      "        [-0.1217785 , -0.12697046,  0.0324024 , ...,  0.24515368,\n",
      "         -0.04910183, -0.01756372],\n",
      "        [ 0.06211802, -0.04887127,  0.23069024, ...,  0.01966312,\n",
      "         -0.1787117 ,  0.11958222]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.08505529, -0.498169  , -0.39581138, ..., -0.15000694,\n",
      "         -0.416311  , -0.03927151],\n",
      "        [ 0.00090403, -0.04597738,  0.09097336, ..., -0.2508244 ,\n",
      "         -0.20684901,  0.03574159],\n",
      "        [ 0.02000159, -0.45930055,  0.07375673, ...,  0.10997446,\n",
      "          0.09084399, -0.02145674],\n",
      "        ...,\n",
      "        [ 0.02187704, -0.17552854, -0.36867806, ..., -0.39338604,\n",
      "         -0.47698084,  0.04644823],\n",
      "        [-0.02235793, -0.01310702, -0.05584175, ...,  0.00290374,\n",
      "          0.1334    ,  0.22192647],\n",
      "        [ 0.15400626,  0.08139254,  0.24729322, ..., -0.2467869 ,\n",
      "         -0.04629466,  0.00691004]],\n",
      "\n",
      "       [[-0.21043031, -0.40029916, -0.349075  , ..., -0.3184295 ,\n",
      "         -0.23895828,  0.12121717],\n",
      "        [ 0.02273945,  0.05359209, -0.22894251, ..., -0.30468056,\n",
      "         -0.17323238,  0.01831704],\n",
      "        [ 0.10008784, -0.26823568,  0.09126344, ...,  0.13167615,\n",
      "         -0.07466693, -0.27440053],\n",
      "        ...,\n",
      "        [-0.06439336, -0.223257  , -0.34203252, ..., -0.3887431 ,\n",
      "         -0.2929941 ,  0.24489716],\n",
      "        [ 0.25163278, -0.18690556,  0.15500285, ...,  0.04204599,\n",
      "         -0.17665924, -0.22135817],\n",
      "        [ 0.23842388,  0.11076572,  0.05839526, ..., -0.13147587,\n",
      "         -0.1288787 , -0.05423791]],\n",
      "\n",
      "       [[-0.03896061, -0.34564734, -0.13501559, ..., -0.19081652,\n",
      "         -0.11731047,  0.01999456],\n",
      "        [ 0.03554128, -0.07603148, -0.01225471, ..., -0.16127142,\n",
      "         -0.13527073,  0.04136105],\n",
      "        [ 0.19849989, -0.3564951 ,  0.24250254, ..., -0.00975962,\n",
      "         -0.22596566, -0.32795206],\n",
      "        ...,\n",
      "        [-0.0840333 , -0.09465282, -0.11290993, ..., -0.15930131,\n",
      "         -0.02844395,  0.11688837],\n",
      "        [ 0.10235044, -0.05984295,  0.24550967, ...,  0.05372829,\n",
      "         -0.06056667, -0.21067452],\n",
      "        [ 0.11347708, -0.01676435,  0.07697488, ..., -0.16241118,\n",
      "         -0.21693872,  0.08944804]]], dtype=float32), array([-0.69506   ,  0.07924619,  0.24520215, -0.02026864, -0.4181484 ,\n",
      "        0.02192415, -0.41328743, -0.32588217,  0.07445362, -0.4148137 ,\n",
      "       -0.13296711, -0.29955822, -0.03906946,  0.23945422, -0.3621    ,\n",
      "        0.36787432,  0.05373161, -0.17109132,  0.02208646,  0.00123621,\n",
      "       -0.02824673, -0.1754622 , -0.01122063, -0.01203838,  0.13455278,\n",
      "       -0.01885127,  0.29382265, -0.50578874,  0.28456008,  0.0607131 ,\n",
      "       -0.08933365, -0.2207737 ], dtype=float32)]\n",
      "{'name': 'add_236', 'trainable': True, 'dtype': 'float32'} []\n",
      "{'name': 'global_average_pooling1d_182', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'keepdims': False} []\n",
      "{'name': 'global_max_pooling1d_188', 'trainable': True, 'dtype': 'float32', 'data_format': 'channels_last', 'keepdims': False} []\n",
      "{'name': 'dense_235', 'trainable': True, 'dtype': 'float32', 'units': 300, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 1.8147832e-02, -1.9758977e-02,  1.7465664e-03, ...,\n",
      "         1.1073198e-01, -1.0027133e-01, -2.6277896e-02],\n",
      "       [ 8.7257057e-02,  1.6651176e-02, -1.8840084e-02, ...,\n",
      "        -8.4388457e-02, -6.8516545e-02, -8.9051068e-02],\n",
      "       [ 6.3029669e-02, -1.7804776e-03, -2.9856730e-03, ...,\n",
      "         2.1963805e-02, -8.2858555e-02,  7.8510299e-02],\n",
      "       ...,\n",
      "       [-5.8278430e-02,  1.1363346e-02, -3.2299090e-02, ...,\n",
      "        -1.6039493e-02,  1.9448643e-03, -2.9900970e-02],\n",
      "       [-3.6593653e-02, -3.7689544e-02,  1.1169759e-06, ...,\n",
      "         8.8842295e-02, -4.8326369e-02,  5.7956558e-02],\n",
      "       [-8.9180611e-02, -4.7156032e-02,  1.6923070e-02, ...,\n",
      "         4.5446050e-03, -1.1483741e-02, -4.2945299e-02]], dtype=float32), array([ 0.0097468 , -0.0395252 , -0.00541665, -0.0323916 , -0.03374229,\n",
      "       -0.04494806, -0.05289877, -0.00524181, -0.00497299, -0.05400339,\n",
      "       -0.01848065, -0.00695705,  0.07643038,  0.01252861, -0.01217874,\n",
      "        0.02109975,  0.01599766, -0.00827175, -0.04556876,  0.0131681 ,\n",
      "       -0.03071524, -0.00327565, -0.00301304,  0.05364599,  0.02266452,\n",
      "        0.01645421,  0.01502079,  0.0012141 , -0.00781479,  0.00892039,\n",
      "        0.01355326,  0.0346075 ,  0.02194235, -0.02454523, -0.00720087,\n",
      "       -0.0027343 , -0.03072308, -0.00650385,  0.03199897, -0.02134687,\n",
      "       -0.00346714, -0.00199805,  0.00250839,  0.03809828,  0.00867093,\n",
      "       -0.00332497,  0.01180206, -0.0355414 ,  0.03760144, -0.02547607,\n",
      "       -0.03251873, -0.04846681,  0.01315915,  0.01261118, -0.00714787,\n",
      "        0.00805249,  0.04160383, -0.00303379, -0.01803004,  0.00221757,\n",
      "        0.01936136, -0.05263188, -0.00198119, -0.02593109, -0.0035622 ,\n",
      "       -0.02367645,  0.02875306,  0.        , -0.01631946,  0.01267125,\n",
      "       -0.02756624, -0.01436985, -0.0352324 , -0.00880322,  0.03553382,\n",
      "        0.01861347, -0.03270154,  0.02404489, -0.0005298 ,  0.02940613,\n",
      "       -0.00693962, -0.0090785 ,  0.00136519, -0.01004609, -0.03028274,\n",
      "       -0.00349786, -0.00775347, -0.01551293, -0.00210099,  0.05870578,\n",
      "        0.00305286,  0.006535  ,  0.00579722,  0.02154062,  0.00215698,\n",
      "       -0.00553806, -0.00385418,  0.00861209, -0.0245432 , -0.00242655,\n",
      "       -0.04699102, -0.00256897, -0.02015069,  0.02927877, -0.00385256,\n",
      "       -0.0036579 ,  0.02886182,  0.03893942,  0.00889879,  0.03236022,\n",
      "        0.06048014,  0.02057139,  0.02591409,  0.00291352,  0.05359463,\n",
      "       -0.00219573,  0.02814259, -0.0029601 , -0.02753953, -0.00473018,\n",
      "       -0.00233928,  0.06290237,  0.02505203,  0.00181034, -0.00022159,\n",
      "       -0.00799465,  0.        ,  0.00840492,  0.02771223,  0.00142999,\n",
      "       -0.01531952, -0.00799701, -0.00303369,  0.01410116,  0.0235645 ,\n",
      "        0.0074089 ,  0.01662546,  0.0424322 , -0.00464124, -0.00566362,\n",
      "       -0.02802299, -0.00967623, -0.00346094,  0.00898337, -0.00714453,\n",
      "       -0.02803644,  0.05636451, -0.00587323,  0.01106162, -0.00762956,\n",
      "       -0.0617918 ,  0.02850864, -0.03406262,  0.02578935, -0.06113281,\n",
      "        0.03060022, -0.03028232, -0.02772291,  0.00330183,  0.01514481,\n",
      "       -0.01608003,  0.02805514,  0.03138617, -0.02322195,  0.04166707,\n",
      "       -0.01231772, -0.04124566,  0.04729686, -0.06893048, -0.00920575,\n",
      "       -0.04390343,  0.00564378, -0.0234101 ,  0.01874333, -0.01330971,\n",
      "       -0.04329189, -0.03172283, -0.00134503, -0.01290668,  0.00159335,\n",
      "       -0.00825334, -0.00757071, -0.01335127, -0.00596956,  0.02686041,\n",
      "        0.08911902, -0.00411775, -0.02762266,  0.01737018,  0.00067366,\n",
      "        0.03608375, -0.00489392, -0.01239529, -0.00673743,  0.0320743 ,\n",
      "        0.04276169,  0.02747091, -0.00746464,  0.037838  ,  0.02744771,\n",
      "       -0.01878646, -0.01136068, -0.0069664 , -0.05094265, -0.02629093,\n",
      "       -0.04629725,  0.00716889, -0.00425897,  0.01333068, -0.0183454 ,\n",
      "       -0.01768617,  0.03239377, -0.00538955,  0.01962754,  0.03900833,\n",
      "       -0.00372943,  0.01116401,  0.00944483,  0.026289  , -0.03219832,\n",
      "        0.08029602,  0.00210006,  0.01914059,  0.01558948,  0.05470055,\n",
      "       -0.0055296 ,  0.00402892,  0.        ,  0.05860635, -0.00881135,\n",
      "        0.02330236,  0.01799304, -0.03215097, -0.0051634 ,  0.02641471,\n",
      "       -0.00560201,  0.00602775, -0.03817725, -0.02164124,  0.01151723,\n",
      "        0.04137931, -0.03316312, -0.04033505,  0.02113402,  0.00290418,\n",
      "       -0.00069343, -0.04824205,  0.01224365, -0.00742789,  0.01584809,\n",
      "        0.00094239,  0.00488474, -0.02782237,  0.02081956, -0.00022535,\n",
      "        0.01379387,  0.        , -0.01384509, -0.00100187, -0.02071396,\n",
      "        0.00983183,  0.04833454, -0.04550449,  0.01389496,  0.04147832,\n",
      "        0.04050268, -0.06115501, -0.04338062, -0.00637402, -0.01253566,\n",
      "       -0.01261856, -0.01038502, -0.0139239 , -0.00716859, -0.02252515,\n",
      "        0.01389479, -0.00354666, -0.00409323, -0.03948744,  0.04301916,\n",
      "       -0.05585048, -0.00389176,  0.02732253,  0.03042886,  0.02777045,\n",
      "       -0.01190718,  0.03441666,  0.01872369, -0.0310746 ,  0.01816238,\n",
      "        0.05163175,  0.010426  , -0.00378443, -0.06824672, -0.00430151,\n",
      "       -0.02510104, -0.00732209, -0.00810977, -0.04226742, -0.00044187],\n",
      "      dtype=float32)]\n",
      "{'name': 'add_237', 'trainable': True, 'dtype': 'float32'} []\n",
      "{'name': 'dropout_220', 'trainable': True, 'dtype': 'float32', 'rate': 0.3, 'noise_shape': None, 'seed': None} []\n",
      "{'name': 'dropout_221', 'trainable': True, 'dtype': 'float32', 'rate': 0.3, 'noise_shape': None, 'seed': None} []\n",
      "{'name': 'out_T', 'trainable': True, 'dtype': 'float32', 'units': 276, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 0.03400328, -0.15705846,  0.01237887, ..., -0.16602904,\n",
      "        -0.27984992, -0.10729799],\n",
      "       [-0.17756157, -0.02486868, -0.0127778 , ..., -0.02227222,\n",
      "         0.05247915, -0.10379962],\n",
      "       [-0.06444596, -0.19763938, -0.09646782, ..., -0.02873577,\n",
      "         0.043626  ,  0.00429646],\n",
      "       ...,\n",
      "       [ 0.03290372,  0.04534919, -0.06708684, ..., -0.02357281,\n",
      "        -0.06139743, -0.02751578],\n",
      "       [-0.06521974, -0.08751845,  0.01389001, ..., -0.11734048,\n",
      "        -0.24477981,  0.01412953],\n",
      "       [-0.07322837, -0.01426524, -0.06511548, ..., -0.12129653,\n",
      "        -0.11022855, -0.1191631 ]], dtype=float32), array([-0.03274775, -0.00850416,  0.01111347, -0.07596781,  0.00097496,\n",
      "       -0.04855945, -0.0086636 , -0.01262024, -0.01295512, -0.07402755,\n",
      "       -0.00507644, -0.08670074, -0.00895014, -0.03977846, -0.00997659,\n",
      "       -0.11702657,  0.03040383, -0.00716211,  0.02556661, -0.03044111,\n",
      "        0.02150881, -0.01712202,  0.01591338, -0.11712896, -0.0030291 ,\n",
      "        0.00154316,  0.0155977 , -0.02524435, -0.01789191,  0.02767823,\n",
      "        0.02763758, -0.00667364,  0.03280035, -0.00501555,  0.07411137,\n",
      "        0.01700135, -0.01946608, -0.01103398,  0.02283114, -0.03323514,\n",
      "        0.07332476, -0.0184289 ,  0.01079586,  0.03446343,  0.01739011,\n",
      "        0.0483921 ,  0.07538152,  0.0617241 ,  0.0618938 ,  0.00974351,\n",
      "       -0.02970978,  0.02105349, -0.01836175,  0.01767493,  0.05895824,\n",
      "        0.09524829, -0.00498055,  0.03524349,  0.02396201,  0.03217178,\n",
      "       -0.038799  ,  0.10645489,  0.03186186,  0.03979712,  0.06222361,\n",
      "       -0.0221121 ,  0.04457448,  0.08365823,  0.02753119,  0.02285331,\n",
      "        0.01018118,  0.07594278,  0.06277494,  0.08730633,  0.02224521,\n",
      "        0.01658185,  0.05332291,  0.05555118,  0.04257917,  0.12915613,\n",
      "        0.00684121,  0.04047331,  0.01616979,  0.07386421,  0.0581765 ,\n",
      "        0.07041851, -0.00941569,  0.0143141 , -0.00138662, -0.01774216,\n",
      "        0.03322726, -0.06869368,  0.08323808, -0.00783222, -0.04458898,\n",
      "        0.00952658, -0.01358475, -0.0356355 , -0.03855074, -0.03192506,\n",
      "       -0.04100116, -0.02766735, -0.02346326, -0.04505764,  0.01535168,\n",
      "       -0.03506076, -0.04580071, -0.08266943,  0.00346614, -0.02415288,\n",
      "       -0.01742058, -0.03097675, -0.05247224, -0.0432827 , -0.02843591,\n",
      "       -0.04239998, -0.04188883, -0.04411666, -0.0527728 , -0.00838177,\n",
      "       -0.07718923, -0.06756977, -0.0423571 , -0.09391414,  0.00196767,\n",
      "       -0.05916841,  0.00631092, -0.05535574, -0.05813677, -0.04468283,\n",
      "       -0.02290075, -0.01797038, -0.0283572 ,  0.07103445,  0.00348843,\n",
      "        0.06893472,  0.01469419, -0.03398325,  0.00799581,  0.00073209,\n",
      "        0.01462263,  0.01801205, -0.01460506, -0.02929203, -0.02358112,\n",
      "        0.04070968,  0.04320422,  0.03627835, -0.01478342,  0.03227578,\n",
      "       -0.01537526, -0.00996726,  0.00033031,  0.02339974, -0.01177014,\n",
      "        0.01171908,  0.08599287,  0.0576704 ,  0.08228981,  0.07959761,\n",
      "        0.03203421,  0.07557626,  0.00563915,  0.02622249,  0.02131419,\n",
      "        0.07995711, -0.03021172, -0.0036497 ,  0.01049076,  0.03538476,\n",
      "       -0.02754395,  0.04485234, -0.04689609,  0.00915018, -0.06160124,\n",
      "       -0.0496721 ,  0.01652629, -0.03370525, -0.01164303, -0.01149678,\n",
      "        0.08531035,  0.06827558,  0.08042543,  0.03577818,  0.05722518,\n",
      "       -0.02144226, -0.11474495,  0.00393668, -0.0132513 , -0.05362493,\n",
      "        0.0418978 ,  0.03000971,  0.04723224, -0.01289645, -0.04473526,\n",
      "       -0.06490959, -0.05815687, -0.05706189, -0.01934963,  0.02001565,\n",
      "       -0.00255191,  0.0293684 ,  0.05475282,  0.04706153, -0.04442659,\n",
      "        0.05776624,  0.04066489,  0.04788478,  0.04053374,  0.02127893,\n",
      "        0.06852371, -0.05789969, -0.03242344, -0.04721587, -0.0391973 ,\n",
      "        0.0537838 ,  0.0210134 ,  0.06709724,  0.05515069,  0.04497683,\n",
      "        0.04735206,  0.03845911, -0.02711171, -0.03273656, -0.022491  ,\n",
      "       -0.05015821,  0.07605431,  0.0669513 , -0.0125435 ,  0.04151872,\n",
      "        0.05681925,  0.05383594,  0.01816866,  0.08476628,  0.03806452,\n",
      "        0.04999288,  0.06455233,  0.07575239,  0.06862224,  0.05033988,\n",
      "        0.03030581,  0.05930287,  0.05526083, -0.00186268, -0.08266254,\n",
      "        0.06218579,  0.02818802,  0.06132694,  0.07177468,  0.05329249,\n",
      "        0.06289151,  0.05687966,  0.03349208,  0.06119812, -0.03314421,\n",
      "        0.01318566, -0.05756159, -0.01649919,  0.00705664,  0.00816325,\n",
      "       -0.06508427,  0.05744977, -0.00875371, -0.01073761, -0.00053224,\n",
      "        0.00943322, -0.04938245,  0.00041825,  0.00744027, -0.01833692,\n",
      "        0.01330227,  0.01059616, -0.04452015, -0.05284471,  0.02376151,\n",
      "       -0.05464253], dtype=float32)]\n",
      "{'name': 'out_S', 'trainable': True, 'dtype': 'float32', 'units': 276, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[-0.10074607, -0.46497267,  0.00785198, ..., -0.12027611,\n",
      "        -1.1399786 ,  0.05818688],\n",
      "       [ 0.02873061,  0.0881805 , -0.8456242 , ..., -0.18025069,\n",
      "        -0.63911724, -0.46308884],\n",
      "       [-0.54935867, -0.6901644 , -0.974464  , ..., -0.0672408 ,\n",
      "        -0.03161118, -0.08470228],\n",
      "       ...,\n",
      "       [-0.6659477 , -0.50127876, -0.10349794, ..., -0.01982859,\n",
      "         0.09015565, -0.02288052],\n",
      "       [-0.4368276 , -0.699749  , -0.40118173, ...,  0.05298676,\n",
      "         0.08938362,  0.03600172],\n",
      "       [ 0.07619406,  0.07571735, -0.05807097, ..., -0.39113504,\n",
      "        -0.35538143, -0.15823177]], dtype=float32), array([ 1.96898803e-02, -5.83067946e-02,  1.29970154e-02, -1.87123194e-01,\n",
      "       -1.52721941e-01, -1.57771498e-01, -2.81382650e-01,  4.63111810e-02,\n",
      "       -1.55269459e-01, -2.77191371e-01, -4.56808135e-02, -4.89519894e-01,\n",
      "       -6.29044399e-02, -2.36571163e-01, -3.14382404e-01, -4.23081040e-01,\n",
      "        1.14648439e-01,  9.28309709e-02, -9.94553417e-03, -1.80947855e-01,\n",
      "       -4.83066775e-02, -4.05774899e-02, -3.87015045e-02, -2.11368650e-01,\n",
      "       -5.27128438e-03, -8.96498002e-03, -5.38622886e-02, -1.78823724e-01,\n",
      "       -5.82564361e-02, -5.71161062e-02,  2.14605313e-02, -1.47881746e-01,\n",
      "        3.70446742e-02,  8.03445280e-02,  5.96456602e-02,  3.32510546e-02,\n",
      "        6.59871921e-02,  3.15714657e-01, -1.98129602e-02, -2.72285938e-01,\n",
      "        9.33352485e-02, -6.95591271e-02, -7.76111707e-02,  5.70260100e-02,\n",
      "        6.92787617e-02,  2.98682284e-02,  1.23545252e-01,  1.57793015e-01,\n",
      "        1.73695222e-01, -3.55251580e-02, -1.32245719e-01,  4.67777364e-02,\n",
      "       -1.30123377e-01,  7.68527240e-02,  4.33873478e-03,  3.44328105e-01,\n",
      "       -1.07016981e-01, -1.67640448e-02, -2.03797631e-02,  1.40117332e-01,\n",
      "       -1.02644667e-01,  1.68085247e-01, -1.34193208e-02,  1.50169641e-01,\n",
      "        1.20534390e-01, -1.47619382e-01,  1.09235302e-01,  1.92967683e-01,\n",
      "        6.54020011e-02,  7.10009336e-02, -1.00277729e-01, -2.62429495e-03,\n",
      "        4.62381653e-02,  1.67652428e-01,  1.13175772e-01,  1.52552733e-02,\n",
      "        5.05256653e-02,  1.59195755e-02,  1.00233391e-01,  1.47214413e-01,\n",
      "       -3.08248084e-02, -3.33171748e-02, -3.38903666e-02,  4.30784114e-02,\n",
      "        1.38343573e-02,  1.79089114e-01, -2.58914754e-02,  4.46367450e-03,\n",
      "        5.70645416e-03, -1.91616621e-02,  3.16630453e-02, -3.32075149e-01,\n",
      "        2.29646713e-01,  6.35480061e-02, -1.26105964e-01, -2.62156278e-02,\n",
      "       -8.92319456e-02, -1.92013219e-01, -1.60899952e-01, -2.03887299e-01,\n",
      "       -3.71806592e-01, -7.46955164e-04, -3.30488868e-02, -2.30113477e-01,\n",
      "        1.18783293e-02, -2.87945718e-02, -1.84120074e-01, -2.26697281e-01,\n",
      "        1.15291074e-01, -1.83317512e-01, -7.53270909e-02, -1.48486674e-01,\n",
      "       -2.49033108e-01, -1.30082265e-01, -3.63816880e-02, -1.03353612e-01,\n",
      "       -3.43451768e-01, -1.93360284e-01, -1.52746707e-01, -1.68780535e-01,\n",
      "       -2.75569111e-01, -1.73870429e-01, -1.01887181e-01, -1.69609383e-01,\n",
      "        1.12134509e-01, -4.11999077e-01,  8.61563236e-02, -1.49490148e-01,\n",
      "       -3.35375130e-01, -2.38341093e-01, -2.87245214e-01,  6.16067164e-02,\n",
      "        1.46465106e-02,  9.56222937e-02, -5.14522716e-02,  1.60541177e-01,\n",
      "       -1.92255415e-02, -7.51266330e-02, -1.45622730e-01, -1.22346699e-01,\n",
      "        1.01357317e-02,  6.68913797e-02,  9.72969383e-02, -1.66222990e-01,\n",
      "        4.52397770e-04,  1.61259659e-02,  1.31256670e-01,  4.26013097e-02,\n",
      "        3.49253369e-03,  9.91293862e-02, -1.58769011e-01,  6.53303489e-02,\n",
      "       -8.02358985e-03, -1.15674334e-02,  1.26428947e-01, -1.79603081e-02,\n",
      "        2.14388937e-01,  2.68042266e-01,  1.82082087e-01,  1.88690931e-01,\n",
      "        1.83495536e-01,  2.43985862e-01,  1.54692769e-01,  1.27090678e-01,\n",
      "        9.74626541e-02,  2.51507044e-01, -4.34404872e-02,  5.55408485e-02,\n",
      "       -4.00385857e-02, -1.89251676e-02, -8.92763026e-03,  8.96679834e-02,\n",
      "       -7.84999505e-03,  7.56269693e-02, -2.06073388e-01, -3.15169036e-01,\n",
      "        1.89690478e-03, -4.60027792e-02, -1.13513045e-01, -8.51332769e-02,\n",
      "        6.97912872e-02,  3.93079966e-02, -1.21388948e-02,  4.98964451e-02,\n",
      "        2.16775715e-01, -3.63015421e-02, -2.50143737e-01,  6.77320808e-02,\n",
      "       -1.34744033e-01, -4.03602123e-02,  3.57856788e-02,  1.44049630e-01,\n",
      "        4.05600369e-02, -1.12298004e-01, -8.03613663e-02, -3.64992470e-01,\n",
      "       -1.42503247e-01, -3.64504829e-02, -1.16296075e-01, -3.03621367e-02,\n",
      "       -9.77851674e-02, -9.90720838e-03,  3.64677794e-02,  4.95414063e-02,\n",
      "       -4.79829349e-02,  1.06670856e-01,  3.60832773e-02,  1.08200453e-01,\n",
      "       -4.78463173e-02, -4.31670845e-02, -1.00645237e-02, -8.39725435e-02,\n",
      "       -1.54792622e-01, -2.64895588e-01, -2.83529669e-01, -1.53905153e-02,\n",
      "        5.21384925e-03,  2.39913072e-02,  1.18614495e-01,  4.31136861e-02,\n",
      "        7.95828700e-02,  6.50574267e-02, -1.29688635e-01, -1.57012537e-01,\n",
      "       -6.49547949e-02, -3.47358435e-01,  1.70163736e-01,  1.21442921e-01,\n",
      "       -2.42223367e-02,  1.12362780e-01,  9.65599492e-02,  8.35244060e-02,\n",
      "        1.13198265e-01,  8.66834968e-02,  4.10797186e-02,  1.02592140e-01,\n",
      "        1.18326485e-01,  1.01780958e-01,  1.67116448e-01,  1.38031662e-01,\n",
      "        6.98097125e-02,  6.94010556e-02,  1.10542387e-01, -1.04536548e-01,\n",
      "       -1.69422314e-01,  7.85363019e-02,  7.28963166e-02,  2.92456131e-02,\n",
      "        8.99900422e-02,  7.28770122e-02,  8.96017179e-02,  1.24184430e-01,\n",
      "        1.09232105e-01,  1.58536270e-01, -3.80505323e-02, -3.44686434e-02,\n",
      "       -3.84387851e-01,  1.06077738e-01,  2.53535122e-01,  1.35150760e-01,\n",
      "       -2.31881738e-01,  7.43297189e-02, -3.95891331e-02,  7.64298812e-02,\n",
      "       -1.36256143e-01,  8.56854394e-03, -7.64723495e-02, -1.22911766e-01,\n",
      "       -6.33871928e-02, -1.72845945e-01, -4.34893407e-02, -1.05480254e-01,\n",
      "       -2.02619657e-01, -3.25824529e-01,  8.33844393e-03, -2.59027243e-01],\n",
      "      dtype=float32)]\n",
      "{'name': 'subtract_138', 'trainable': True, 'dtype': 'float32'} []\n",
      "{'name': 'subtract_139', 'trainable': True, 'dtype': 'float32'} []\n",
      "{'name': 'tf.math.abs_143', 'trainable': True, 'dtype': 'float32', 'function': 'math.abs'} []\n",
      "{'name': 'tf.math.abs_144', 'trainable': True, 'dtype': 'float32', 'function': 'math.abs'} []\n"
     ]
    }
   ],
   "source": [
    "for layer in model_a.layers : print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "id": "8a51ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_a = []\n",
    "for layer in model_a.layers:\n",
    "    weights_a.append(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "id": "c548a0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 32)"
      ]
     },
     "execution_count": 1063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(weights_a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "id": "baad362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All weights have been obtained.\n"
     ]
    }
   ],
   "source": [
    "layer_weights = {}\n",
    "for layer in model_a.layers:\n",
    "    if hasattr(layer, 'weights'):\n",
    "        weights = layer.get_weights()\n",
    "        layer_weights[layer.name] = weights\n",
    "\n",
    "# Notify when all weights have been obtained\n",
    "if len(layer_weights) == len(model_a.layers):\n",
    "    print(\"All weights have been obtained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "id": "1f77c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: in\n",
      "Layer: l1_T\n",
      "  Weight 1 shape: (7, 3, 32)\n",
      "  Weight 2 shape: (32,)\n",
      "Layer: batch_normalization_391\n",
      "  Weight 1 shape: (32,)\n",
      "  Weight 2 shape: (32,)\n",
      "  Weight 3 shape: (32,)\n",
      "  Weight 4 shape: (32,)\n",
      "Layer: max_pooling1d_453\n",
      "Layer: conv1d_518\n",
      "  Weight 1 shape: (5, 32, 64)\n",
      "  Weight 2 shape: (64,)\n",
      "Layer: batch_normalization_392\n",
      "  Weight 1 shape: (64,)\n",
      "  Weight 2 shape: (64,)\n",
      "  Weight 3 shape: (64,)\n",
      "  Weight 4 shape: (64,)\n",
      "Layer: model_46\n",
      "  Weight 1 shape: (64, 32)\n",
      "  Weight 2 shape: (32,)\n",
      "  Weight 3 shape: (32, 1)\n",
      "  Weight 4 shape: (1,)\n",
      "  Weight 5 shape: (5, 1, 1)\n",
      "  Weight 6 shape: (1,)\n",
      "Layer: multiply_53\n",
      "Layer: add_234\n",
      "Layer: max_pooling1d_454\n",
      "Layer: batch_normalization_393\n",
      "  Weight 1 shape: (64,)\n",
      "  Weight 2 shape: (64,)\n",
      "  Weight 3 shape: (64,)\n",
      "  Weight 4 shape: (64,)\n",
      "Layer: conv1d_520\n",
      "  Weight 1 shape: (3, 64, 256)\n",
      "  Weight 2 shape: (256,)\n",
      "Layer: batch_normalization_394\n",
      "  Weight 1 shape: (256,)\n",
      "  Weight 2 shape: (256,)\n",
      "  Weight 3 shape: (256,)\n",
      "  Weight 4 shape: (256,)\n",
      "Layer: model_47\n",
      "  Weight 1 shape: (256, 32)\n",
      "  Weight 2 shape: (32,)\n",
      "  Weight 3 shape: (32, 1)\n",
      "  Weight 4 shape: (1,)\n",
      "  Weight 5 shape: (3, 1, 1)\n",
      "  Weight 6 shape: (1,)\n",
      "Layer: multiply_54\n",
      "Layer: add_235\n",
      "Layer: bottle_T\n",
      "  Weight 1 shape: (3, 256, 32)\n",
      "  Weight 2 shape: (32,)\n",
      "Layer: l1_S\n",
      "  Weight 1 shape: (7, 3, 8)\n",
      "  Weight 2 shape: (8,)\n",
      "Layer: max_pooling1d_455\n",
      "Layer: batch_normalization_395\n",
      "  Weight 1 shape: (8,)\n",
      "  Weight 2 shape: (8,)\n",
      "  Weight 3 shape: (8,)\n",
      "  Weight 4 shape: (8,)\n",
      "Layer: conv1d_522\n",
      "  Weight 1 shape: (3, 32, 512)\n",
      "  Weight 2 shape: (512,)\n",
      "Layer: max_pooling1d_456\n",
      "Layer: global_average_pooling1d_181\n",
      "Layer: global_max_pooling1d_187\n",
      "Layer: bottle_S\n",
      "  Weight 1 shape: (7, 8, 32)\n",
      "  Weight 2 shape: (32,)\n",
      "Layer: add_236\n",
      "Layer: global_average_pooling1d_182\n",
      "Layer: global_max_pooling1d_188\n",
      "Layer: dense_235\n",
      "  Weight 1 shape: (512, 300)\n",
      "  Weight 2 shape: (300,)\n",
      "Layer: add_237\n",
      "Layer: dropout_220\n",
      "Layer: dropout_221\n",
      "Layer: out_T\n",
      "  Weight 1 shape: (300, 276)\n",
      "  Weight 2 shape: (276,)\n",
      "Layer: out_S\n",
      "  Weight 1 shape: (32, 276)\n",
      "  Weight 2 shape: (276,)\n",
      "Layer: subtract_138\n",
      "Layer: subtract_139\n",
      "Layer: tf.math.abs_143\n",
      "Layer: tf.math.abs_144\n"
     ]
    }
   ],
   "source": [
    "# Print shapes of weights\n",
    "for layer_name, weights in layer_weights.items():\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    for i, w in enumerate(weights):\n",
    "        print(f\"  Weight {i+1} shape: {w.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "id": "75176b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: l1_S\n",
      "  Weight 1 shape: (7, 3, 8)\n",
      "  Weight 2 shape: (8,)\n",
      "Layer: batch_normalization_395\n",
      "  Weight 1 shape: (8,)\n",
      "  Weight 2 shape: (8,)\n",
      "  Weight 3 shape: (8,)\n",
      "  Weight 4 shape: (8,)\n",
      "Layer: conv1d_522\n",
      "  Weight 1 shape: (3, 32, 512)\n",
      "  Weight 2 shape: (512,)\n",
      "Layer: bottle_S\n",
      "  Weight 1 shape: (7, 8, 32)\n",
      "  Weight 2 shape: (32,)\n",
      "Layer: dense_235\n",
      "  Weight 1 shape: (512, 300)\n",
      "  Weight 2 shape: (300,)\n",
      "Layer: out_S\n",
      "  Weight 1 shape: (32, 276)\n",
      "  Weight 2 shape: (276,)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes of weights\n",
    "for layer_name, weights in layer_weights_dict.items():\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    for i, w in enumerate(weights):\n",
    "        print(f\"  Weight {i+1} shape: {w.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "id": "879bfc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: l1_S\n",
      "Weights: [array([[[ 0.14765337, -0.08292597,  0.10625795,  0.24139154,\n",
      "         -0.20766129, -0.00056791,  0.06568924, -0.09035908],\n",
      "        [-0.12159061, -0.24244615,  0.15541461,  0.13909677,\n",
      "          0.3618828 , -0.0113461 ,  0.31381074, -0.3155456 ],\n",
      "        [ 0.21159609, -0.0071369 , -0.07507134, -0.07253537,\n",
      "         -0.10315753,  0.24132304, -0.09963208, -0.28495878]],\n",
      "\n",
      "       [[-0.36745164, -0.07675503, -0.11254194,  0.30038252,\n",
      "          0.11296042, -0.02178727,  0.09675463,  0.26843774],\n",
      "        [ 0.2158609 , -0.01145265,  0.35322022,  0.15617582,\n",
      "         -0.11666256,  0.20505486,  0.10092632, -0.14656295],\n",
      "        [ 0.11905869,  0.14347465, -0.07422975, -0.22828585,\n",
      "         -0.14378615,  0.23917228, -0.13649386,  0.19216394]],\n",
      "\n",
      "       [[-0.19618295,  0.13279659,  0.15960383, -0.18427758,\n",
      "          0.05137173, -0.03980571,  0.22664489,  0.06990117],\n",
      "        [-0.00396833, -0.19108425,  0.35960224,  0.10979548,\n",
      "          0.00510428,  0.02256588, -0.05759259, -0.10692135],\n",
      "        [ 0.05893592, -0.11142304,  0.1554098 , -0.21475138,\n",
      "          0.14033894,  0.2531634 ,  0.11563978, -0.10983754]],\n",
      "\n",
      "       [[-0.1630623 ,  0.2615558 , -0.00177851,  0.2908213 ,\n",
      "         -0.23566182, -0.2667666 ,  0.22685911, -0.06465278],\n",
      "        [ 0.16280116,  0.00757315,  0.33348596,  0.11254057,\n",
      "          0.11630479, -0.0360464 , -0.08800711, -0.07028592],\n",
      "        [ 0.20956928, -0.19108452,  0.12600935, -0.02253895,\n",
      "          0.16075009, -0.10637938,  0.07116279, -0.03880453]],\n",
      "\n",
      "       [[-0.2577899 ,  0.19768967,  0.03026363, -0.10961069,\n",
      "          0.06525066, -0.15065901, -0.05756284,  0.22635019],\n",
      "        [ 0.23170613,  0.15526964,  0.15640675,  0.09959774,\n",
      "         -0.10690554, -0.01108155,  0.05389866,  0.00577045],\n",
      "        [-0.03480951, -0.14228311,  0.28807724, -0.23358653,\n",
      "          0.00668804, -0.1908592 , -0.09522178,  0.13491376]],\n",
      "\n",
      "       [[-0.15488961,  0.10799395,  0.02023466, -0.24890132,\n",
      "          0.02995871, -0.21685815,  0.13488455, -0.11799848],\n",
      "        [ 0.14472133, -0.07485539,  0.31962323, -0.02025984,\n",
      "         -0.10084821, -0.07320361, -0.07156421,  0.06246147],\n",
      "        [-0.07472447,  0.1691799 , -0.14838745,  0.05556198,\n",
      "         -0.03301637,  0.05480283,  0.09794755, -0.06479345]],\n",
      "\n",
      "       [[ 0.05586221, -0.16797799,  0.12174619, -0.14682266,\n",
      "          0.08310273,  0.18553972, -0.09399532,  0.2538381 ],\n",
      "        [ 0.25139073, -0.02488118,  0.19804494,  0.04462163,\n",
      "         -0.12583518,  0.21137342,  0.12596443, -0.02268817],\n",
      "        [ 0.21322271,  0.24715108, -0.07150431,  0.09338052,\n",
      "         -0.11315175,  0.18481426, -0.03581841,  0.13600692]]],\n",
      "      dtype=float32), array([-0.8107882 ,  0.04639888, -0.410697  , -0.5706873 , -0.00873439,\n",
      "       -0.10987294, -0.57292575, -0.05072244], dtype=float32)]\n",
      "Layer: batch_normalization_395\n",
      "Weights: [array([1.7854017, 1.5587968, 1.8421512, 1.6984049, 1.52802  , 1.7320176,\n",
      "       1.6077427, 1.65818  ], dtype=float32), array([-0.09232387,  0.10331097, -0.15299971, -0.04890814, -0.46391156,\n",
      "       -0.05480796, -0.12692145,  0.01486081], dtype=float32), array([ 26.651814 ,  11.467041 , 112.69227  ,  14.753295 ,   0.9036137,\n",
      "        20.368513 ,  44.748604 ,   7.9684515], dtype=float32), array([ 575.04224 ,  164.71045 , 1336.0592  ,  187.2387  ,   14.222394,\n",
      "        323.49036 ,  395.58054 ,  118.67197 ], dtype=float32)]\n",
      "Layer: conv1d_522\n",
      "Weights: [array([[[ 0.09394458,  0.08152083,  0.01542217, ...,  0.03758632,\n",
      "         -0.19269946, -0.22089225],\n",
      "        [-0.04724629, -0.03597053,  0.0974143 , ..., -0.0014147 ,\n",
      "         -0.01030464,  0.01739207],\n",
      "        [-0.10158315,  0.05580694, -0.01708832, ..., -0.00494363,\n",
      "         -0.15025063, -0.00313785],\n",
      "        ...,\n",
      "        [ 0.1248934 ,  0.02334268, -0.03770491, ..., -0.00963473,\n",
      "         -0.01968123, -0.01378798],\n",
      "        [-0.03607341,  0.05568781,  0.07132176, ..., -0.02834613,\n",
      "         -0.0521124 ,  0.02895547],\n",
      "        [ 0.00351461,  0.05217123,  0.09560947, ...,  0.00429533,\n",
      "         -0.00867741, -0.04361982]],\n",
      "\n",
      "       [[-0.05687049,  0.12609784, -0.00251141, ..., -0.06269151,\n",
      "         -0.15284021,  0.04752016],\n",
      "        [-0.08167018, -0.03381309,  0.0359681 , ..., -0.01549061,\n",
      "         -0.03976954,  0.01517636],\n",
      "        [-0.03194018,  0.01014027, -0.00782381, ..., -0.00351308,\n",
      "         -0.26273927,  0.02558138],\n",
      "        ...,\n",
      "        [-0.02546553, -0.0332332 ,  0.10963413, ..., -0.03243675,\n",
      "         -0.04554414, -0.0474251 ],\n",
      "        [ 0.01952091, -0.00558759,  0.07727037, ...,  0.01984388,\n",
      "          0.10627335, -0.05975727],\n",
      "        [ 0.04789524, -0.02886722,  0.04300654, ...,  0.06075798,\n",
      "          0.09479455,  0.03639213]],\n",
      "\n",
      "       [[ 0.07647456,  0.11818645,  0.01713712, ...,  0.01007735,\n",
      "         -0.03566232, -0.01979117],\n",
      "        [ 0.03842632,  0.02540471,  0.01896638, ...,  0.05602804,\n",
      "         -0.03690946, -0.02824841],\n",
      "        [ 0.0996521 ,  0.08900896,  0.0425877 , ..., -0.00291462,\n",
      "         -0.09136491,  0.03485426],\n",
      "        ...,\n",
      "        [-0.11783229,  0.08574761,  0.04981523, ..., -0.08207082,\n",
      "         -0.02948347, -0.2037078 ],\n",
      "        [-0.03023367, -0.08219869, -0.00632164, ...,  0.06284715,\n",
      "          0.09872342, -0.10898452],\n",
      "        [-0.05905979, -0.04650955, -0.00027231, ..., -0.03529518,\n",
      "          0.0694956 ,  0.04824177]]], dtype=float32), array([ 2.74737813e-02,  2.21620705e-02, -1.02254450e-02, -5.47261871e-02,\n",
      "        5.51650301e-02,  1.01888599e-02, -2.58455817e-02, -2.15304922e-02,\n",
      "        2.06418778e-03,  1.62730217e-02,  8.63929614e-02, -5.10010542e-03,\n",
      "        2.60948222e-02, -2.04144213e-02,  1.72066689e-02,  1.52332382e-02,\n",
      "       -6.39819447e-03,  2.97550354e-02, -1.52709736e-02,  5.92334978e-02,\n",
      "       -1.13508357e-02,  2.74733789e-02,  3.00919637e-03, -1.32082580e-02,\n",
      "       -7.36459903e-03,  1.30294897e-02, -3.64411213e-02,  6.31678291e-03,\n",
      "        1.09596793e-02, -2.15308536e-02, -6.59414474e-03,  2.57532392e-02,\n",
      "        7.90313557e-02,  3.22582829e-03, -7.09530106e-03,  4.13905941e-02,\n",
      "       -5.67195704e-03, -4.88158278e-02,  3.81566323e-02,  1.30279129e-02,\n",
      "       -1.04642501e-02,  2.06431411e-02,  4.60088290e-02, -3.09062656e-02,\n",
      "        3.44214849e-02,  8.09277175e-04,  3.18575650e-02, -7.89524894e-03,\n",
      "        1.00449109e-02,  3.37803438e-02, -1.40852546e-02, -1.07406741e-02,\n",
      "        7.17476457e-02,  1.91100575e-02,  3.50269265e-02,  3.15775722e-02,\n",
      "        1.56809185e-02, -1.68509036e-02,  1.10553868e-01, -1.58407986e-02,\n",
      "        5.43371728e-03, -5.84032945e-03, -3.55619155e-02,  9.48457569e-02,\n",
      "       -2.13959161e-02,  1.65127888e-02,  5.25834598e-03, -1.34225599e-02,\n",
      "       -2.18367726e-02, -3.95062864e-02,  2.17476208e-02,  1.06741423e-02,\n",
      "        6.30916143e-03,  2.66669714e-03,  9.01181716e-03, -1.39035853e-02,\n",
      "       -5.73353730e-02, -5.55024073e-02,  2.78008971e-02,  1.98887363e-02,\n",
      "        5.68992719e-02,  3.17309983e-02, -1.37817534e-02,  1.20880725e-02,\n",
      "        1.71331838e-02,  4.55931090e-02, -4.12652120e-02,  6.35913461e-02,\n",
      "        1.99466515e-02,  3.69546227e-02,  1.31437951e-03, -4.41346131e-03,\n",
      "        5.32069383e-03,  4.38110717e-02,  6.28115162e-02, -4.79099117e-02,\n",
      "       -2.56803874e-02,  1.99609473e-02, -1.58917625e-02,  2.92962114e-03,\n",
      "       -3.83499861e-02, -3.45338471e-02,  3.72855626e-02, -2.86444481e-02,\n",
      "        2.11962201e-02, -3.81340971e-03,  4.27398942e-02,  4.23896573e-02,\n",
      "        2.25062147e-02,  1.80377712e-04, -2.27849884e-03,  2.88770627e-02,\n",
      "        2.73064524e-02,  7.91153056e-04,  6.01022504e-02,  9.50829312e-03,\n",
      "        3.08004469e-02,  2.02790946e-02, -2.83371238e-03,  1.00744534e-02,\n",
      "        3.47915366e-02,  4.64103296e-02,  4.59127091e-02,  2.58677620e-02,\n",
      "        4.54541445e-02,  3.28410640e-02,  5.13072172e-03, -6.00659475e-02,\n",
      "        1.00207083e-01,  1.48959495e-02,  2.48012673e-02, -2.55013704e-02,\n",
      "       -3.40106674e-02,  1.39863994e-02,  9.38378647e-03,  2.99790576e-02,\n",
      "       -3.19772027e-03, -4.83563126e-05,  4.60422002e-02,  1.26321847e-02,\n",
      "        1.66809205e-02,  1.04647260e-02,  7.62500092e-02,  2.60379501e-02,\n",
      "       -5.39092999e-03, -4.29574326e-02, -2.08004750e-02, -2.36162706e-03,\n",
      "        8.65728874e-03,  3.24034840e-02,  4.82818186e-02,  2.83887200e-02,\n",
      "        1.39805460e-02, -2.99595147e-02,  3.65675353e-02,  4.60060947e-02,\n",
      "        1.97900105e-02,  4.07496095e-02, -1.42666223e-02,  1.85020138e-02,\n",
      "        7.45609170e-03, -1.85439773e-02,  5.45231067e-02, -1.56779177e-02,\n",
      "        7.70474086e-03, -8.55482300e-04,  2.10922044e-02,  5.11875516e-03,\n",
      "       -1.50732817e-02, -1.20936111e-02, -9.34398361e-03,  8.83014873e-03,\n",
      "       -2.58118026e-02,  3.02718468e-02,  2.90734340e-02,  1.04310298e-02,\n",
      "       -7.99161755e-03,  4.01673354e-02, -4.56021028e-03,  1.94217917e-02,\n",
      "       -5.47105186e-02,  2.89831497e-02,  1.06037118e-01,  3.44582945e-02,\n",
      "        2.83957664e-02,  1.47237722e-02,  4.08955924e-02,  2.51426900e-05,\n",
      "       -1.55714322e-02, -4.67462931e-03,  4.35458906e-02, -3.87738273e-02,\n",
      "        3.23208645e-02, -3.62742064e-03,  1.70046948e-02,  5.92618249e-03,\n",
      "        2.60928478e-02,  5.44630587e-02, -8.96777399e-03,  8.79178382e-03,\n",
      "        1.38033722e-02, -3.36661376e-02, -3.41704711e-02,  5.20631555e-04,\n",
      "        3.08411345e-02, -3.23517900e-03, -1.29748750e-02,  3.84407267e-02,\n",
      "        2.09405050e-02, -8.13847873e-03, -7.44574424e-03,  4.10126224e-02,\n",
      "        1.94703378e-02, -1.64529905e-02,  1.35646276e-02, -5.47894128e-02,\n",
      "        6.91473335e-02, -5.44198696e-03,  4.36268188e-02, -4.36187908e-02,\n",
      "        1.78746339e-02, -2.85386723e-02,  1.49875665e-02,  7.94988126e-02,\n",
      "       -1.06822411e-02,  2.91334409e-02,  1.23476365e-03,  1.96792036e-02,\n",
      "        2.26829890e-02,  6.49444535e-02,  4.27662954e-02, -2.41749007e-02,\n",
      "       -1.18897604e-02,  1.58225739e-04, -6.66941563e-03, -2.67903158e-03,\n",
      "        2.15858221e-02,  5.04024811e-02, -1.41789112e-02,  1.42452456e-02,\n",
      "        5.32877557e-02,  6.64891559e-05,  1.84820089e-02,  5.70743605e-02,\n",
      "        5.10395728e-02, -5.55032752e-02, -1.76328439e-02, -5.31866588e-02,\n",
      "       -2.39937883e-02, -5.09064784e-03,  3.10559273e-02,  1.07597150e-01,\n",
      "        2.56082509e-02, -8.95048864e-03,  3.42112198e-03, -1.23471636e-02,\n",
      "        4.27595116e-02,  3.87345217e-02,  1.37718752e-01, -2.24844869e-02,\n",
      "        1.97815592e-03, -3.49248340e-03,  1.70084946e-02,  3.50868003e-03,\n",
      "        2.20345594e-02,  4.65919599e-02, -1.55624831e-02,  6.97619617e-02,\n",
      "       -1.97704621e-02,  3.95635813e-02, -1.61343198e-02,  1.83097292e-02,\n",
      "        4.03307844e-03, -7.22190272e-03,  3.13234963e-02, -4.91592363e-02,\n",
      "        2.73401104e-03,  1.75820831e-02, -3.94959413e-02, -3.01256906e-02,\n",
      "        3.13631408e-02, -1.60524733e-02,  2.40968517e-03,  5.57791069e-02,\n",
      "        2.85635074e-03,  4.33321744e-02,  9.78120416e-03,  2.07307260e-03,\n",
      "        2.92973057e-03, -3.54168043e-02,  6.36954792e-03,  1.17437486e-02,\n",
      "       -1.14894868e-03, -1.76411495e-03,  3.86583246e-02, -1.44107718e-04,\n",
      "        5.95561089e-03, -1.96412187e-02,  7.33028650e-02, -1.38258552e-02,\n",
      "        2.66012698e-02,  1.86565405e-04, -5.92512637e-02, -2.33352780e-02,\n",
      "       -1.84067804e-03,  5.59579767e-02, -2.02398859e-02,  1.85661018e-02,\n",
      "        5.13052009e-02, -4.48787287e-02,  4.52043163e-03,  2.02312525e-02,\n",
      "       -1.28536327e-02,  1.46842385e-02, -2.25055106e-02, -2.69768033e-02,\n",
      "        3.70346941e-02,  7.78427068e-03,  1.47349783e-03, -3.71118798e-03,\n",
      "       -1.77916884e-02, -2.13244222e-02,  3.67311686e-02, -7.66476616e-03,\n",
      "        3.06256283e-02,  5.91496564e-02, -5.92805538e-03, -2.64752358e-02,\n",
      "        2.19287220e-02,  4.87190895e-02, -1.48093635e-02, -3.17129598e-04,\n",
      "        4.13852371e-02,  3.92967537e-02, -2.39909738e-02,  4.38336022e-02,\n",
      "       -4.83847149e-02,  2.10334770e-02,  4.65500280e-02,  4.80187833e-02,\n",
      "       -3.29694971e-02,  6.56306650e-03,  7.81425927e-03,  2.18067430e-02,\n",
      "       -3.08927726e-02, -2.69140583e-02, -5.67830130e-02, -4.98668402e-02,\n",
      "       -3.06235589e-02,  2.51691248e-02,  8.08131136e-03, -1.23561192e-02,\n",
      "        4.60669287e-02, -1.86835155e-02,  8.60371217e-02,  1.40336854e-02,\n",
      "        3.44843417e-02,  7.90678430e-03,  2.54746731e-02, -1.68045331e-02,\n",
      "        1.24759376e-02, -2.65717953e-02,  8.10211711e-03, -1.04522863e-02,\n",
      "        2.42658462e-02,  1.86783709e-02, -3.86235979e-03,  1.60079338e-02,\n",
      "       -1.59883332e-02,  8.54855999e-02,  3.58376317e-02,  1.46115571e-03,\n",
      "        5.33095654e-03,  7.52994232e-03, -3.21434587e-02,  3.80953662e-02,\n",
      "       -3.35694067e-02, -3.48169692e-02,  5.98762818e-02, -2.51710471e-02,\n",
      "       -8.99085589e-03,  5.63564785e-02,  2.92215636e-03,  1.09808659e-02,\n",
      "        3.67414556e-03, -3.09334937e-02,  3.64484964e-03, -3.75627466e-02,\n",
      "       -5.95486257e-03, -9.31006297e-03,  5.39067686e-02,  2.62309369e-02,\n",
      "       -1.08523658e-02,  2.98279114e-02, -4.41522337e-02,  2.27577258e-02,\n",
      "       -2.04758309e-02,  7.38889584e-03, -1.19538363e-02,  4.76888604e-02,\n",
      "        5.96201327e-03,  2.72468086e-02,  1.24656584e-03,  3.27937081e-02,\n",
      "        2.92975865e-02,  4.18748967e-02,  2.31574420e-02,  1.00211956e-01,\n",
      "        1.67296920e-02,  1.97460167e-02,  2.13698018e-02,  1.56855993e-02,\n",
      "       -5.49120922e-03,  2.56488379e-02, -9.67651792e-03,  3.02065276e-02,\n",
      "        5.69701493e-02,  3.37070189e-02, -1.21623650e-02,  3.24920677e-02,\n",
      "       -5.59713915e-02, -6.46748096e-02, -2.50914302e-02,  8.02827475e-04,\n",
      "        2.50080377e-02, -2.30611730e-02, -2.17393562e-02,  3.42074297e-02,\n",
      "        5.73353693e-02,  4.13723066e-02, -1.09518692e-02, -2.61342227e-02,\n",
      "       -1.72166023e-02,  2.44566742e-02, -4.87700626e-02,  1.14880083e-02,\n",
      "        3.51687185e-02,  5.55737950e-02,  1.66507345e-02,  8.74041487e-03,\n",
      "       -4.88891825e-03,  2.91889217e-02, -1.77916046e-02,  8.95426050e-03,\n",
      "        4.67616655e-02,  2.57861037e-02, -1.70710254e-02, -5.65199368e-03,\n",
      "       -5.58444336e-02, -3.95955425e-03,  2.52569392e-02,  1.68017186e-02,\n",
      "       -1.42204892e-02,  5.50057292e-02,  3.70033272e-02,  2.82767639e-02,\n",
      "        1.74925532e-02,  7.49670714e-02, -4.79961298e-02, -2.66612507e-02,\n",
      "        3.44531722e-02,  1.85661577e-02,  6.00388944e-02,  4.67800871e-02,\n",
      "        2.19471287e-02,  9.76307224e-03, -5.76766692e-02, -3.90413813e-02,\n",
      "        1.56568661e-02, -3.45225260e-03,  2.16829553e-02,  8.80064443e-02,\n",
      "        3.26697007e-02,  2.70748213e-02, -2.22032089e-02, -2.52035204e-02,\n",
      "        2.43374445e-02, -2.53867228e-02, -2.07998883e-02,  3.48538831e-02,\n",
      "        3.90583687e-02,  1.96096045e-03,  2.53849849e-03,  2.94757448e-03,\n",
      "        2.17872374e-02,  6.13285005e-02, -8.47501587e-03, -2.31182529e-03,\n",
      "        1.65488701e-02, -1.76778063e-02,  6.39591692e-03,  2.04807613e-03,\n",
      "        1.66120585e-02, -2.95257550e-02,  1.67818251e-03,  2.94838939e-02,\n",
      "        8.27490985e-02, -6.43839652e-04, -3.43308295e-03, -7.82049028e-04,\n",
      "        8.72664712e-03, -1.23081999e-02,  3.11309006e-02,  8.81970953e-03,\n",
      "       -7.43515091e-04,  3.80877815e-02, -6.03985833e-03, -2.70393714e-02,\n",
      "        4.88110334e-02,  7.73192104e-03,  2.02386938e-02, -3.61612812e-02],\n",
      "      dtype=float32)]\n",
      "Layer: bottle_S\n",
      "Weights: [array([[[-0.1264946 , -0.2239544 ,  0.09374572, ..., -0.23373885,\n",
      "         -0.17060275, -0.11999331],\n",
      "        [-0.17520322, -0.01750687, -0.28420475, ...,  0.04514569,\n",
      "         -0.19858608,  0.0859481 ],\n",
      "        [-0.12951405, -0.15170641,  0.10081014, ...,  0.30013084,\n",
      "          0.04706093, -0.24397713],\n",
      "        ...,\n",
      "        [-0.09212123, -0.18253753,  0.13369015, ...,  0.0107442 ,\n",
      "         -0.11863066,  0.06480904],\n",
      "        [-0.1142372 , -0.15183236, -0.2603792 , ...,  0.13492788,\n",
      "         -0.1496151 , -0.05147745],\n",
      "        [-0.08787974,  0.08427624, -0.1316299 , ...,  0.10579832,\n",
      "         -0.3753681 ,  0.16809794]],\n",
      "\n",
      "       [[ 0.00430068, -0.31439137,  0.02191586, ...,  0.18460584,\n",
      "         -0.2769317 , -0.13469817],\n",
      "        [-0.01850672, -0.06296957, -0.22244665, ..., -0.02189458,\n",
      "         -0.11581514,  0.05008728],\n",
      "        [-0.04873078, -0.24434964, -0.20060061, ...,  0.16183531,\n",
      "          0.01136617, -0.2998538 ],\n",
      "        ...,\n",
      "        [-0.00489396, -0.11487071,  0.12519193, ...,  0.21309912,\n",
      "         -0.24780966,  0.09364734],\n",
      "        [-0.06719881,  0.0645251 , -0.1909377 , ...,  0.30040535,\n",
      "         -0.05889871, -0.04583279],\n",
      "        [-0.04095453, -0.06690637,  0.28441104, ...,  0.33967182,\n",
      "         -0.23234771,  0.07559891]],\n",
      "\n",
      "       [[ 0.0277082 , -0.46855664, -0.43802825, ..., -0.21040526,\n",
      "         -0.3218957 ,  0.04128357],\n",
      "        [ 0.08953698,  0.02092601, -0.13900188, ..., -0.00575822,\n",
      "         -0.21911648,  0.04986886],\n",
      "        [ 0.01793822, -0.29027387,  0.09912962, ...,  0.23324506,\n",
      "         -0.07867899, -0.15408915],\n",
      "        ...,\n",
      "        [-0.00343782, -0.24582613, -0.537978  , ..., -0.13350068,\n",
      "         -0.37543142,  0.06585084],\n",
      "        [-0.1217785 , -0.12697046,  0.0324024 , ...,  0.24515368,\n",
      "         -0.04910183, -0.01756372],\n",
      "        [ 0.06211802, -0.04887127,  0.23069024, ...,  0.01966312,\n",
      "         -0.1787117 ,  0.11958222]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.08505529, -0.498169  , -0.39581138, ..., -0.15000694,\n",
      "         -0.416311  , -0.03927151],\n",
      "        [ 0.00090403, -0.04597738,  0.09097336, ..., -0.2508244 ,\n",
      "         -0.20684901,  0.03574159],\n",
      "        [ 0.02000159, -0.45930055,  0.07375673, ...,  0.10997446,\n",
      "          0.09084399, -0.02145674],\n",
      "        ...,\n",
      "        [ 0.02187704, -0.17552854, -0.36867806, ..., -0.39338604,\n",
      "         -0.47698084,  0.04644823],\n",
      "        [-0.02235793, -0.01310702, -0.05584175, ...,  0.00290374,\n",
      "          0.1334    ,  0.22192647],\n",
      "        [ 0.15400626,  0.08139254,  0.24729322, ..., -0.2467869 ,\n",
      "         -0.04629466,  0.00691004]],\n",
      "\n",
      "       [[-0.21043031, -0.40029916, -0.349075  , ..., -0.3184295 ,\n",
      "         -0.23895828,  0.12121717],\n",
      "        [ 0.02273945,  0.05359209, -0.22894251, ..., -0.30468056,\n",
      "         -0.17323238,  0.01831704],\n",
      "        [ 0.10008784, -0.26823568,  0.09126344, ...,  0.13167615,\n",
      "         -0.07466693, -0.27440053],\n",
      "        ...,\n",
      "        [-0.06439336, -0.223257  , -0.34203252, ..., -0.3887431 ,\n",
      "         -0.2929941 ,  0.24489716],\n",
      "        [ 0.25163278, -0.18690556,  0.15500285, ...,  0.04204599,\n",
      "         -0.17665924, -0.22135817],\n",
      "        [ 0.23842388,  0.11076572,  0.05839526, ..., -0.13147587,\n",
      "         -0.1288787 , -0.05423791]],\n",
      "\n",
      "       [[-0.03896061, -0.34564734, -0.13501559, ..., -0.19081652,\n",
      "         -0.11731047,  0.01999456],\n",
      "        [ 0.03554128, -0.07603148, -0.01225471, ..., -0.16127142,\n",
      "         -0.13527073,  0.04136105],\n",
      "        [ 0.19849989, -0.3564951 ,  0.24250254, ..., -0.00975962,\n",
      "         -0.22596566, -0.32795206],\n",
      "        ...,\n",
      "        [-0.0840333 , -0.09465282, -0.11290993, ..., -0.15930131,\n",
      "         -0.02844395,  0.11688837],\n",
      "        [ 0.10235044, -0.05984295,  0.24550967, ...,  0.05372829,\n",
      "         -0.06056667, -0.21067452],\n",
      "        [ 0.11347708, -0.01676435,  0.07697488, ..., -0.16241118,\n",
      "         -0.21693872,  0.08944804]]], dtype=float32), array([-0.69506   ,  0.07924619,  0.24520215, -0.02026864, -0.4181484 ,\n",
      "        0.02192415, -0.41328743, -0.32588217,  0.07445362, -0.4148137 ,\n",
      "       -0.13296711, -0.29955822, -0.03906946,  0.23945422, -0.3621    ,\n",
      "        0.36787432,  0.05373161, -0.17109132,  0.02208646,  0.00123621,\n",
      "       -0.02824673, -0.1754622 , -0.01122063, -0.01203838,  0.13455278,\n",
      "       -0.01885127,  0.29382265, -0.50578874,  0.28456008,  0.0607131 ,\n",
      "       -0.08933365, -0.2207737 ], dtype=float32)]\n",
      "Layer: dense_235\n",
      "Weights: [array([[ 1.8147832e-02, -1.9758977e-02,  1.7465664e-03, ...,\n",
      "         1.1073198e-01, -1.0027133e-01, -2.6277896e-02],\n",
      "       [ 8.7257057e-02,  1.6651176e-02, -1.8840084e-02, ...,\n",
      "        -8.4388457e-02, -6.8516545e-02, -8.9051068e-02],\n",
      "       [ 6.3029669e-02, -1.7804776e-03, -2.9856730e-03, ...,\n",
      "         2.1963805e-02, -8.2858555e-02,  7.8510299e-02],\n",
      "       ...,\n",
      "       [-5.8278430e-02,  1.1363346e-02, -3.2299090e-02, ...,\n",
      "        -1.6039493e-02,  1.9448643e-03, -2.9900970e-02],\n",
      "       [-3.6593653e-02, -3.7689544e-02,  1.1169759e-06, ...,\n",
      "         8.8842295e-02, -4.8326369e-02,  5.7956558e-02],\n",
      "       [-8.9180611e-02, -4.7156032e-02,  1.6923070e-02, ...,\n",
      "         4.5446050e-03, -1.1483741e-02, -4.2945299e-02]], dtype=float32), array([ 0.0097468 , -0.0395252 , -0.00541665, -0.0323916 , -0.03374229,\n",
      "       -0.04494806, -0.05289877, -0.00524181, -0.00497299, -0.05400339,\n",
      "       -0.01848065, -0.00695705,  0.07643038,  0.01252861, -0.01217874,\n",
      "        0.02109975,  0.01599766, -0.00827175, -0.04556876,  0.0131681 ,\n",
      "       -0.03071524, -0.00327565, -0.00301304,  0.05364599,  0.02266452,\n",
      "        0.01645421,  0.01502079,  0.0012141 , -0.00781479,  0.00892039,\n",
      "        0.01355326,  0.0346075 ,  0.02194235, -0.02454523, -0.00720087,\n",
      "       -0.0027343 , -0.03072308, -0.00650385,  0.03199897, -0.02134687,\n",
      "       -0.00346714, -0.00199805,  0.00250839,  0.03809828,  0.00867093,\n",
      "       -0.00332497,  0.01180206, -0.0355414 ,  0.03760144, -0.02547607,\n",
      "       -0.03251873, -0.04846681,  0.01315915,  0.01261118, -0.00714787,\n",
      "        0.00805249,  0.04160383, -0.00303379, -0.01803004,  0.00221757,\n",
      "        0.01936136, -0.05263188, -0.00198119, -0.02593109, -0.0035622 ,\n",
      "       -0.02367645,  0.02875306,  0.        , -0.01631946,  0.01267125,\n",
      "       -0.02756624, -0.01436985, -0.0352324 , -0.00880322,  0.03553382,\n",
      "        0.01861347, -0.03270154,  0.02404489, -0.0005298 ,  0.02940613,\n",
      "       -0.00693962, -0.0090785 ,  0.00136519, -0.01004609, -0.03028274,\n",
      "       -0.00349786, -0.00775347, -0.01551293, -0.00210099,  0.05870578,\n",
      "        0.00305286,  0.006535  ,  0.00579722,  0.02154062,  0.00215698,\n",
      "       -0.00553806, -0.00385418,  0.00861209, -0.0245432 , -0.00242655,\n",
      "       -0.04699102, -0.00256897, -0.02015069,  0.02927877, -0.00385256,\n",
      "       -0.0036579 ,  0.02886182,  0.03893942,  0.00889879,  0.03236022,\n",
      "        0.06048014,  0.02057139,  0.02591409,  0.00291352,  0.05359463,\n",
      "       -0.00219573,  0.02814259, -0.0029601 , -0.02753953, -0.00473018,\n",
      "       -0.00233928,  0.06290237,  0.02505203,  0.00181034, -0.00022159,\n",
      "       -0.00799465,  0.        ,  0.00840492,  0.02771223,  0.00142999,\n",
      "       -0.01531952, -0.00799701, -0.00303369,  0.01410116,  0.0235645 ,\n",
      "        0.0074089 ,  0.01662546,  0.0424322 , -0.00464124, -0.00566362,\n",
      "       -0.02802299, -0.00967623, -0.00346094,  0.00898337, -0.00714453,\n",
      "       -0.02803644,  0.05636451, -0.00587323,  0.01106162, -0.00762956,\n",
      "       -0.0617918 ,  0.02850864, -0.03406262,  0.02578935, -0.06113281,\n",
      "        0.03060022, -0.03028232, -0.02772291,  0.00330183,  0.01514481,\n",
      "       -0.01608003,  0.02805514,  0.03138617, -0.02322195,  0.04166707,\n",
      "       -0.01231772, -0.04124566,  0.04729686, -0.06893048, -0.00920575,\n",
      "       -0.04390343,  0.00564378, -0.0234101 ,  0.01874333, -0.01330971,\n",
      "       -0.04329189, -0.03172283, -0.00134503, -0.01290668,  0.00159335,\n",
      "       -0.00825334, -0.00757071, -0.01335127, -0.00596956,  0.02686041,\n",
      "        0.08911902, -0.00411775, -0.02762266,  0.01737018,  0.00067366,\n",
      "        0.03608375, -0.00489392, -0.01239529, -0.00673743,  0.0320743 ,\n",
      "        0.04276169,  0.02747091, -0.00746464,  0.037838  ,  0.02744771,\n",
      "       -0.01878646, -0.01136068, -0.0069664 , -0.05094265, -0.02629093,\n",
      "       -0.04629725,  0.00716889, -0.00425897,  0.01333068, -0.0183454 ,\n",
      "       -0.01768617,  0.03239377, -0.00538955,  0.01962754,  0.03900833,\n",
      "       -0.00372943,  0.01116401,  0.00944483,  0.026289  , -0.03219832,\n",
      "        0.08029602,  0.00210006,  0.01914059,  0.01558948,  0.05470055,\n",
      "       -0.0055296 ,  0.00402892,  0.        ,  0.05860635, -0.00881135,\n",
      "        0.02330236,  0.01799304, -0.03215097, -0.0051634 ,  0.02641471,\n",
      "       -0.00560201,  0.00602775, -0.03817725, -0.02164124,  0.01151723,\n",
      "        0.04137931, -0.03316312, -0.04033505,  0.02113402,  0.00290418,\n",
      "       -0.00069343, -0.04824205,  0.01224365, -0.00742789,  0.01584809,\n",
      "        0.00094239,  0.00488474, -0.02782237,  0.02081956, -0.00022535,\n",
      "        0.01379387,  0.        , -0.01384509, -0.00100187, -0.02071396,\n",
      "        0.00983183,  0.04833454, -0.04550449,  0.01389496,  0.04147832,\n",
      "        0.04050268, -0.06115501, -0.04338062, -0.00637402, -0.01253566,\n",
      "       -0.01261856, -0.01038502, -0.0139239 , -0.00716859, -0.02252515,\n",
      "        0.01389479, -0.00354666, -0.00409323, -0.03948744,  0.04301916,\n",
      "       -0.05585048, -0.00389176,  0.02732253,  0.03042886,  0.02777045,\n",
      "       -0.01190718,  0.03441666,  0.01872369, -0.0310746 ,  0.01816238,\n",
      "        0.05163175,  0.010426  , -0.00378443, -0.06824672, -0.00430151,\n",
      "       -0.02510104, -0.00732209, -0.00810977, -0.04226742, -0.00044187],\n",
      "      dtype=float32)]\n",
      "Layer: out_S\n",
      "Weights: [array([[-0.10074607, -0.46497267,  0.00785198, ..., -0.12027611,\n",
      "        -1.1399786 ,  0.05818688],\n",
      "       [ 0.02873061,  0.0881805 , -0.8456242 , ..., -0.18025069,\n",
      "        -0.63911724, -0.46308884],\n",
      "       [-0.54935867, -0.6901644 , -0.974464  , ..., -0.0672408 ,\n",
      "        -0.03161118, -0.08470228],\n",
      "       ...,\n",
      "       [-0.6659477 , -0.50127876, -0.10349794, ..., -0.01982859,\n",
      "         0.09015565, -0.02288052],\n",
      "       [-0.4368276 , -0.699749  , -0.40118173, ...,  0.05298676,\n",
      "         0.08938362,  0.03600172],\n",
      "       [ 0.07619406,  0.07571735, -0.05807097, ..., -0.39113504,\n",
      "        -0.35538143, -0.15823177]], dtype=float32), array([ 1.96898803e-02, -5.83067946e-02,  1.29970154e-02, -1.87123194e-01,\n",
      "       -1.52721941e-01, -1.57771498e-01, -2.81382650e-01,  4.63111810e-02,\n",
      "       -1.55269459e-01, -2.77191371e-01, -4.56808135e-02, -4.89519894e-01,\n",
      "       -6.29044399e-02, -2.36571163e-01, -3.14382404e-01, -4.23081040e-01,\n",
      "        1.14648439e-01,  9.28309709e-02, -9.94553417e-03, -1.80947855e-01,\n",
      "       -4.83066775e-02, -4.05774899e-02, -3.87015045e-02, -2.11368650e-01,\n",
      "       -5.27128438e-03, -8.96498002e-03, -5.38622886e-02, -1.78823724e-01,\n",
      "       -5.82564361e-02, -5.71161062e-02,  2.14605313e-02, -1.47881746e-01,\n",
      "        3.70446742e-02,  8.03445280e-02,  5.96456602e-02,  3.32510546e-02,\n",
      "        6.59871921e-02,  3.15714657e-01, -1.98129602e-02, -2.72285938e-01,\n",
      "        9.33352485e-02, -6.95591271e-02, -7.76111707e-02,  5.70260100e-02,\n",
      "        6.92787617e-02,  2.98682284e-02,  1.23545252e-01,  1.57793015e-01,\n",
      "        1.73695222e-01, -3.55251580e-02, -1.32245719e-01,  4.67777364e-02,\n",
      "       -1.30123377e-01,  7.68527240e-02,  4.33873478e-03,  3.44328105e-01,\n",
      "       -1.07016981e-01, -1.67640448e-02, -2.03797631e-02,  1.40117332e-01,\n",
      "       -1.02644667e-01,  1.68085247e-01, -1.34193208e-02,  1.50169641e-01,\n",
      "        1.20534390e-01, -1.47619382e-01,  1.09235302e-01,  1.92967683e-01,\n",
      "        6.54020011e-02,  7.10009336e-02, -1.00277729e-01, -2.62429495e-03,\n",
      "        4.62381653e-02,  1.67652428e-01,  1.13175772e-01,  1.52552733e-02,\n",
      "        5.05256653e-02,  1.59195755e-02,  1.00233391e-01,  1.47214413e-01,\n",
      "       -3.08248084e-02, -3.33171748e-02, -3.38903666e-02,  4.30784114e-02,\n",
      "        1.38343573e-02,  1.79089114e-01, -2.58914754e-02,  4.46367450e-03,\n",
      "        5.70645416e-03, -1.91616621e-02,  3.16630453e-02, -3.32075149e-01,\n",
      "        2.29646713e-01,  6.35480061e-02, -1.26105964e-01, -2.62156278e-02,\n",
      "       -8.92319456e-02, -1.92013219e-01, -1.60899952e-01, -2.03887299e-01,\n",
      "       -3.71806592e-01, -7.46955164e-04, -3.30488868e-02, -2.30113477e-01,\n",
      "        1.18783293e-02, -2.87945718e-02, -1.84120074e-01, -2.26697281e-01,\n",
      "        1.15291074e-01, -1.83317512e-01, -7.53270909e-02, -1.48486674e-01,\n",
      "       -2.49033108e-01, -1.30082265e-01, -3.63816880e-02, -1.03353612e-01,\n",
      "       -3.43451768e-01, -1.93360284e-01, -1.52746707e-01, -1.68780535e-01,\n",
      "       -2.75569111e-01, -1.73870429e-01, -1.01887181e-01, -1.69609383e-01,\n",
      "        1.12134509e-01, -4.11999077e-01,  8.61563236e-02, -1.49490148e-01,\n",
      "       -3.35375130e-01, -2.38341093e-01, -2.87245214e-01,  6.16067164e-02,\n",
      "        1.46465106e-02,  9.56222937e-02, -5.14522716e-02,  1.60541177e-01,\n",
      "       -1.92255415e-02, -7.51266330e-02, -1.45622730e-01, -1.22346699e-01,\n",
      "        1.01357317e-02,  6.68913797e-02,  9.72969383e-02, -1.66222990e-01,\n",
      "        4.52397770e-04,  1.61259659e-02,  1.31256670e-01,  4.26013097e-02,\n",
      "        3.49253369e-03,  9.91293862e-02, -1.58769011e-01,  6.53303489e-02,\n",
      "       -8.02358985e-03, -1.15674334e-02,  1.26428947e-01, -1.79603081e-02,\n",
      "        2.14388937e-01,  2.68042266e-01,  1.82082087e-01,  1.88690931e-01,\n",
      "        1.83495536e-01,  2.43985862e-01,  1.54692769e-01,  1.27090678e-01,\n",
      "        9.74626541e-02,  2.51507044e-01, -4.34404872e-02,  5.55408485e-02,\n",
      "       -4.00385857e-02, -1.89251676e-02, -8.92763026e-03,  8.96679834e-02,\n",
      "       -7.84999505e-03,  7.56269693e-02, -2.06073388e-01, -3.15169036e-01,\n",
      "        1.89690478e-03, -4.60027792e-02, -1.13513045e-01, -8.51332769e-02,\n",
      "        6.97912872e-02,  3.93079966e-02, -1.21388948e-02,  4.98964451e-02,\n",
      "        2.16775715e-01, -3.63015421e-02, -2.50143737e-01,  6.77320808e-02,\n",
      "       -1.34744033e-01, -4.03602123e-02,  3.57856788e-02,  1.44049630e-01,\n",
      "        4.05600369e-02, -1.12298004e-01, -8.03613663e-02, -3.64992470e-01,\n",
      "       -1.42503247e-01, -3.64504829e-02, -1.16296075e-01, -3.03621367e-02,\n",
      "       -9.77851674e-02, -9.90720838e-03,  3.64677794e-02,  4.95414063e-02,\n",
      "       -4.79829349e-02,  1.06670856e-01,  3.60832773e-02,  1.08200453e-01,\n",
      "       -4.78463173e-02, -4.31670845e-02, -1.00645237e-02, -8.39725435e-02,\n",
      "       -1.54792622e-01, -2.64895588e-01, -2.83529669e-01, -1.53905153e-02,\n",
      "        5.21384925e-03,  2.39913072e-02,  1.18614495e-01,  4.31136861e-02,\n",
      "        7.95828700e-02,  6.50574267e-02, -1.29688635e-01, -1.57012537e-01,\n",
      "       -6.49547949e-02, -3.47358435e-01,  1.70163736e-01,  1.21442921e-01,\n",
      "       -2.42223367e-02,  1.12362780e-01,  9.65599492e-02,  8.35244060e-02,\n",
      "        1.13198265e-01,  8.66834968e-02,  4.10797186e-02,  1.02592140e-01,\n",
      "        1.18326485e-01,  1.01780958e-01,  1.67116448e-01,  1.38031662e-01,\n",
      "        6.98097125e-02,  6.94010556e-02,  1.10542387e-01, -1.04536548e-01,\n",
      "       -1.69422314e-01,  7.85363019e-02,  7.28963166e-02,  2.92456131e-02,\n",
      "        8.99900422e-02,  7.28770122e-02,  8.96017179e-02,  1.24184430e-01,\n",
      "        1.09232105e-01,  1.58536270e-01, -3.80505323e-02, -3.44686434e-02,\n",
      "       -3.84387851e-01,  1.06077738e-01,  2.53535122e-01,  1.35150760e-01,\n",
      "       -2.31881738e-01,  7.43297189e-02, -3.95891331e-02,  7.64298812e-02,\n",
      "       -1.36256143e-01,  8.56854394e-03, -7.64723495e-02, -1.22911766e-01,\n",
      "       -6.33871928e-02, -1.72845945e-01, -4.34893407e-02, -1.05480254e-01,\n",
      "       -2.02619657e-01, -3.25824529e-01,  8.33844393e-03, -2.59027243e-01],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Get the layer by name\n",
    "layer_names = ['l1_S', 'batch_normalization_395', 'conv1d_522', 'bottle_S', 'dense_235','out_S']  # List of layer names you want to retrieve weights for\n",
    "\n",
    "# Dictionary to store layer weights\n",
    "layer_weights_dict = {}\n",
    "\n",
    "# Iterate over each layer name\n",
    "for layer_name in layer_names:\n",
    "    # Get the layer by name\n",
    "    layer = model_a.get_layer(layer_name)\n",
    "    \n",
    "    # Get weights of the layer\n",
    "    layer_weights = layer.get_weights()\n",
    "    \n",
    "    # Store the weights in the dictionary\n",
    "    layer_weights_dict[layer_name] = layer_weights\n",
    "\n",
    "# Print or use the weights as needed\n",
    "for layer_name, weights in layer_weights_dict.items():\n",
    "    print(\"Layer:\", layer_name)\n",
    "    print(\"Weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "id": "e75eac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_array = np.array(layer_weights_dict['out_S'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "a39b0b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 8, 32)"
      ]
     },
     "execution_count": 1233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "c4846522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "id": "5403c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00746073e-01 -4.64972675e-01  7.85198249e-03 -1.85585339e-02\n",
      "  -6.61128089e-02 -1.40395224e-01 -2.74681449e-01 -1.75292224e-01\n",
      "  -7.91863084e-01  5.78738786e-02 -1.35943934e-01 -3.23789264e-03\n",
      "  -1.74470618e-01  7.05430692e-04  2.48870477e-02  1.02696232e-01\n",
      "   1.09968618e-01 -7.60317221e-02 -1.81955293e-01 -1.39673188e-01\n",
      "   8.34751292e-04 -1.15224056e-01 -1.49845436e-01  7.56091401e-02\n",
      "   7.01213628e-02 -8.05567726e-02 -3.81551906e-02  1.25626341e-01\n",
      "  -1.66384339e-01 -1.08450532e-01 -1.33571565e-01 -1.50404379e-01\n",
      "   3.74866277e-02 -6.30280674e-02 -3.17687541e-02  6.71149343e-02\n",
      "   5.42667359e-02 -8.70054886e-02  1.23408876e-01 -1.13709114e-01\n",
      "   4.79083359e-02 -1.26898959e-01  5.24042472e-02  1.45312682e-01\n",
      "   2.54191943e-02 -4.21062298e-02  3.05355396e-02 -2.67235581e-02\n",
      "  -5.83199412e-02 -1.88212857e-01 -4.78703864e-02  4.25186940e-02\n",
      "  -1.38704062e-01 -5.78756770e-03 -3.47634070e-02 -2.33316980e-02\n",
      "   5.84480399e-03 -2.06113130e-01 -6.68795733e-03 -2.06847042e-02\n",
      "   1.64166931e-02 -1.97903328e-02 -1.51406646e-01  2.44460776e-02\n",
      "  -1.64615491e-03 -1.94771007e-01  9.69057679e-02 -2.23248169e-01\n",
      "  -3.98501046e-02 -9.57758129e-02 -9.03107226e-02 -2.36141514e-02\n",
      "   1.04081146e-02 -2.30908338e-02 -1.26568452e-01 -2.26737522e-02\n",
      "   7.46663883e-02  9.67350509e-03 -6.06235564e-02 -2.56497804e-02\n",
      "   2.06389767e-03 -8.82485434e-02  1.32873347e-02 -6.49962053e-02\n",
      "   8.25236067e-02  1.94392279e-02  1.45241069e-02  1.24747595e-02\n",
      "   4.35325541e-02  8.66342932e-02  7.36880004e-02 -1.57641545e-01\n",
      "   1.22410581e-01  4.20882031e-02 -1.83310872e-03  1.08851008e-01\n",
      "  -7.63226906e-03  4.52837683e-02 -7.80716166e-02 -2.75567155e-02\n",
      "   5.04709445e-02 -2.37991184e-01 -2.29839217e-02 -5.48833683e-02\n",
      "  -7.75907993e-01 -2.25979760e-01  6.80089742e-02  3.40817198e-02\n",
      "   3.22243050e-02 -3.20841730e-01  4.60800529e-02 -8.61471370e-02\n",
      "  -1.40805403e-02  1.56511337e-01  6.23016432e-02  6.95603713e-02\n",
      "   2.85967644e-02  5.97973391e-02  3.92957442e-02  3.97362150e-02\n",
      "   4.27514203e-02  1.53318793e-01  4.29952554e-02  1.20996155e-01\n",
      "   8.55729058e-02 -5.19354194e-02  5.54130934e-02  1.35297835e-01\n",
      "   2.32020579e-03 -1.18683605e-02 -5.67844845e-02 -1.04247697e-01\n",
      "   1.63171440e-01  7.89648816e-02 -1.18625276e-01  1.21627890e-01\n",
      "  -1.21927969e-01  1.36869743e-01  8.22252408e-02  2.48854179e-02\n",
      "   1.26992464e-01 -1.51628880e-02 -8.50820318e-02  2.68045980e-02\n",
      "  -9.35293436e-02  1.92294996e-02  8.51746202e-02  8.93435925e-02\n",
      "  -1.44287214e-01 -1.87325999e-02 -3.05579901e-01 -3.10955256e-01\n",
      "   1.20789856e-01 -6.85115308e-02 -5.82629144e-01 -1.61357746e-01\n",
      "  -1.81625858e-02 -3.50339375e-02 -1.12448908e-01 -2.60336757e-01\n",
      "  -8.41321275e-02 -9.91200283e-02 -7.89926425e-02 -8.41606185e-02\n",
      "  -6.08783327e-02  2.55625229e-02 -4.32510104e-04  3.83206666e-03\n",
      "   2.99365427e-02  2.11372413e-02  6.70610145e-02  1.12011828e-01\n",
      "   1.09079942e-01  4.32932749e-02 -3.62522155e-03 -3.23740654e-02\n",
      "   5.91434166e-02  5.70526831e-02 -1.03499092e-01  3.41841169e-02\n",
      "   8.59525576e-02  3.27668786e-02  2.12544315e-02  1.73625164e-02\n",
      "   9.90233794e-02  5.96151799e-02  9.62022468e-02 -1.15504060e-02\n",
      "   9.96740162e-02  1.69487759e-01  7.36228824e-02 -6.47929776e-03\n",
      "  -2.63747722e-02  2.40729526e-02  4.82800864e-02 -4.29631621e-02\n",
      "  -2.20032036e-02  8.13946724e-02  4.55638431e-02  6.18824027e-02\n",
      "  -5.07117152e-01 -6.85826063e-01 -3.87581885e-01 -4.75187570e-01\n",
      "  -1.98154703e-01 -1.94810942e-01 -5.77291965e-01 -8.23589385e-01\n",
      "  -5.59588492e-01 -8.06362271e-01 -5.95960915e-01  1.03767946e-01\n",
      "   1.51684703e-02  6.34636432e-02  5.00241108e-02 -3.77407163e-01\n",
      "  -4.00472313e-01 -5.23195565e-01 -4.43009645e-01 -1.03423023e+00\n",
      "  -4.10713971e-01 -5.10006130e-01 -4.33602035e-02  4.70320545e-02\n",
      "  -5.02962014e-03 -1.40924394e-01 -4.13421988e-01 -4.77555782e-01\n",
      "  -2.57628977e-01 -7.64256835e-01 -2.26543471e-01 -6.50986135e-01\n",
      "  -7.08597302e-01 -7.50622928e-01 -2.23852947e-01 -3.55328858e-01\n",
      "  -3.25508952e-01 -4.84572738e-01 -5.84433496e-01 -5.99620938e-01\n",
      "  -9.39139962e-01 -6.54440045e-01 -2.96725780e-01 -7.43006542e-02\n",
      "   9.97620597e-02 -8.40497255e-01 -3.82489413e-01 -1.24554820e-01\n",
      "  -1.96195304e-01 -3.84908348e-01 -3.53437454e-01 -2.86201566e-01\n",
      "  -4.08939004e-01 -5.60366988e-01 -8.04789782e-01 -7.15968668e-01\n",
      "   9.38152373e-02 -9.22137201e-01 -4.09984618e-01 -8.81992340e-01\n",
      "   3.07834446e-02 -6.31636381e-01 -4.14508551e-01 -8.68358240e-02\n",
      "  -6.68348134e-01 -1.92001879e-01 -3.10422927e-01 -4.49375987e-01\n",
      "  -1.04410601e+00 -2.16699600e-01 -1.31612909e+00 -1.94086269e-01\n",
      "  -5.89827061e-01 -1.20276108e-01 -1.13997865e+00  5.81868775e-02]\n",
      " [ 2.87306141e-02  8.81804973e-02 -8.45624208e-01 -5.29089332e-01\n",
      "  -6.65301323e-01 -9.23722163e-02  4.45729587e-03  9.85032767e-02\n",
      "   5.93728870e-02 -9.51919138e-01 -8.75064265e-03 -9.68172550e-01\n",
      "   1.76898763e-02 -4.20309752e-02 -2.55125239e-02 -7.93930113e-01\n",
      "   1.09712802e-01  8.36924613e-02  8.94774124e-02  8.42654854e-02\n",
      "   7.39259645e-02  1.13306567e-01  9.16516930e-02  7.69504830e-02\n",
      "   6.90444857e-02  8.62825513e-02  8.98965821e-02  8.28206837e-02\n",
      "   1.05888493e-01  9.29164067e-02  1.00857571e-01  8.14774856e-02\n",
      "   2.86416560e-02 -3.13904554e-01 -2.19881758e-02 -1.73584931e-02\n",
      "   4.85919453e-02 -9.78726029e-01  1.38317589e-02  7.25127235e-02\n",
      "  -1.07876854e-02  1.05023764e-01  2.35825032e-03 -9.62898806e-02\n",
      "  -6.11356378e-01  5.03332168e-02 -2.94961810e-01 -4.06183988e-01\n",
      "  -1.31725192e-01  7.71792084e-02  8.02071765e-02 -1.20403074e-01\n",
      "   8.00796598e-02 -4.94739473e-01 -4.23430949e-02  6.54957071e-02\n",
      "  -1.39919892e-01 -9.16833580e-02 -5.62542021e-01 -2.42121845e-01\n",
      "   8.34699124e-02  3.91081274e-02  9.13582668e-02 -1.27437085e-01\n",
      "  -3.48754436e-01  7.43877366e-02 -1.99733630e-01  3.32869440e-02\n",
      "   4.79248576e-02 -4.90054905e-01  8.03280547e-02  7.53737167e-02\n",
      "   1.30054331e-03 -1.48873240e-01  8.37161615e-02  4.67134789e-02\n",
      "  -2.85898126e-03 -1.22441202e-01 -1.41893312e-01  8.59167334e-03\n",
      "  -5.60914725e-02 -7.88028445e-03 -9.82843041e-02  3.77130881e-02\n",
      "  -9.19205770e-02 -1.00882128e-01 -1.55564323e-01  1.25389434e-02\n",
      "  -1.78075414e-02 -1.39731482e-01  1.72464754e-02  6.19182549e-02\n",
      "   7.92220309e-02  2.06632055e-02 -6.60862103e-02 -1.78686567e-02\n",
      "  -1.81169175e-02  6.38208585e-03  7.21675456e-02  7.92105570e-02\n",
      "   5.42722940e-02  1.06456630e-01 -8.11975002e-01  7.75559396e-02\n",
      "  -1.81079935e-03 -5.67463525e-02 -1.15722992e-01  7.68940523e-02\n",
      "  -6.63130283e-01 -2.40542710e-01 -3.57997566e-01  9.00934488e-02\n",
      "  -1.01415850e-01 -4.00510073e-01 -6.91272914e-01 -6.10426366e-01\n",
      "  -2.36923750e-02  8.35715458e-02 -7.85743356e-01 -6.93917572e-01\n",
      "   2.31241565e-02  3.64475064e-02 -3.76949050e-02 -4.26010489e-01\n",
      "   6.29562363e-02 -3.73608768e-02 -2.86618263e-01  3.69914547e-02\n",
      "  -5.31230152e-01 -1.78402752e-01  2.43600667e-03 -5.87966025e-01\n",
      "  -1.00829113e+00 -2.71050662e-01 -4.90683436e-01 -3.96687001e-01\n",
      "   9.61699635e-02 -8.79743576e-01 -6.40272260e-01 -6.59770787e-01\n",
      "  -4.77637053e-01 -3.80536288e-01 -8.35548759e-01 -3.35103273e-01\n",
      "  -3.93748075e-01 -3.89893264e-01 -8.90663624e-01 -8.91380787e-01\n",
      "  -7.41144598e-01 -7.85426259e-01 -1.39756322e-01 -4.03905958e-01\n",
      "  -5.42546570e-01 -5.62978625e-01 -2.48402312e-01 -5.20235300e-01\n",
      "   1.23256214e-01  9.20574293e-02  7.72960410e-02  1.13552853e-01\n",
      "   1.48216203e-01  1.07370481e-01  2.37833839e-02  1.25399292e-01\n",
      "   9.23761055e-02  1.24118224e-01  3.81100588e-02  4.11298163e-02\n",
      "   7.47371018e-02 -1.77689567e-02 -6.14885926e-01  5.32631390e-03\n",
      "   9.66431051e-02  8.24981555e-02  7.39969239e-02  5.82751371e-02\n",
      "   2.65805442e-02 -1.49181336e-02  1.91855598e-02  3.22012305e-02\n",
      "   2.79582310e-02  5.66895865e-02  6.41036108e-02  5.48831597e-02\n",
      "  -1.19797409e-01  5.01444638e-02 -1.23354530e+00  1.22244246e-01\n",
      "  -5.07206060e-02 -6.78518713e-01 -2.95127422e-01 -2.50703722e-01\n",
      "   4.90962677e-02 -1.43466964e-01 -8.66405785e-01  5.27129285e-02\n",
      "  -4.39231962e-01 -6.69938978e-03 -6.98343441e-02 -5.93694262e-02\n",
      "  -8.35970696e-03 -1.11703919e-02 -1.00391179e-01 -1.73592810e-02\n",
      "  -1.12425327e+00 -1.67269900e-01  1.36699289e-01 -2.73006499e-01\n",
      "   6.82600364e-02 -1.17704337e-02 -1.42856628e-01 -1.07325129e-01\n",
      "  -6.93904519e-01 -8.30762446e-01 -3.08207184e-01 -2.65614629e-01\n",
      "  -1.19277775e-01 -1.22086905e-01 -4.74252924e-02 -2.67088920e-01\n",
      "  -2.68825561e-01 -1.98711798e-01 -5.82060635e-01 -6.97138071e-01\n",
      "  -6.18470609e-01  1.02994561e-01 -1.70407355e-01 -3.07484597e-01\n",
      "  -1.16633587e-01 -3.26594532e-01 -2.35468522e-01 -2.38071278e-01\n",
      "  -3.43803316e-01 -2.22265527e-01  3.96976881e-02 -2.26354972e-01\n",
      "  -1.95798054e-01 -1.49472252e-01 -2.22100541e-01 -2.50501633e-01\n",
      "  -1.83948055e-01 -2.29751438e-01 -2.46794999e-01 -6.24999821e-01\n",
      "  -7.92331100e-01 -2.39516348e-01 -2.99746692e-01 -1.21238343e-01\n",
      "  -1.58839047e-01 -9.82395187e-02 -3.00998896e-01 -2.10709199e-01\n",
      "  -2.30654374e-01 -2.85801232e-01 -2.44783789e-01 -3.64168227e-01\n",
      "  -4.11482044e-02 -1.10146224e+00 -7.65084863e-01 -8.00598264e-01\n",
      "  -6.53597236e-01 -6.39217794e-01 -6.91619694e-01 -1.05943906e+00\n",
      "  -3.55781347e-01 -1.21141565e+00 -7.97373950e-01 -1.21199645e-01\n",
      "  -8.97460282e-01 -2.67058551e-01 -3.57742131e-01 -1.71568140e-01\n",
      "  -2.17659727e-01 -1.80250689e-01 -6.39117241e-01 -4.63088840e-01]\n",
      " [-5.49358666e-01 -6.90164387e-01 -9.74463999e-01 -2.00661749e-01\n",
      "  -9.19234395e-01 -8.44724715e-01 -6.47018731e-01 -6.90542579e-01\n",
      "  -7.82242119e-01 -1.45359319e-02 -6.47044480e-01 -3.27119917e-01\n",
      "  -4.86917645e-01 -6.62728310e-01  5.43446429e-02 -1.76478148e-01\n",
      "   7.19219670e-02  6.54048249e-02  3.57858725e-02  1.91344507e-02\n",
      "   5.11627309e-02  6.53347671e-02  3.29936519e-02  6.78893998e-02\n",
      "   5.39012514e-02  3.70062664e-02  3.72733586e-02  3.00653744e-02\n",
      "   6.21382929e-02  3.88275012e-02  5.62343895e-02  1.67906322e-02\n",
      "   6.11903444e-02 -1.43923122e-03  6.04254082e-02  5.26387282e-02\n",
      "   8.74554515e-02 -5.54303564e-02  2.54612043e-02  1.55616933e-02\n",
      "   8.99104401e-02  4.76246774e-02  5.86965196e-02  8.06951821e-02\n",
      "   3.65591459e-02  7.90129974e-02  5.87706342e-02  6.95500597e-02\n",
      "   4.08174843e-02  1.16448840e-02  2.46563405e-02  5.85113354e-02\n",
      "   1.25722019e-02  6.94261938e-02  2.89639626e-02  4.97112684e-02\n",
      "   4.45630401e-02  4.30507325e-02  4.39346246e-02  3.40863392e-02\n",
      "   2.47037895e-02  5.99211976e-02  3.39423418e-02  6.12840950e-02\n",
      "   4.94539663e-02  1.42746242e-02  8.67078230e-02  5.32366186e-02\n",
      "   4.49352786e-02  4.62944284e-02  1.29260560e-02  5.27556799e-02\n",
      "   6.53585643e-02  7.09159821e-02  4.36980985e-02  7.37532452e-02\n",
      "   8.37341323e-02  6.12712093e-02  6.85240552e-02  6.66116625e-02\n",
      "   2.25003045e-02  3.99605222e-02  7.84226600e-03  6.42863438e-02\n",
      "   3.03369891e-02  7.70698041e-02  5.78625407e-03  3.67233120e-02\n",
      "   8.63102637e-03  5.57731166e-02  5.79575226e-02  1.09113175e-02\n",
      "   8.85410234e-02  3.86381000e-02  2.99002174e-02  6.64230362e-02\n",
      "   5.93401194e-02  3.21542956e-02  9.55981389e-03  2.72324160e-02\n",
      "  -1.23502634e-01 -2.71151606e-02  3.70199303e-03  2.50435788e-02\n",
      "  -5.55554032e-01 -7.68758535e-01 -2.75587440e-01 -2.38140356e-02\n",
      "  -6.35510504e-01 -6.57188952e-01 -5.19055247e-01  6.56037312e-03\n",
      "  -2.12506056e-01  7.37051889e-02 -6.98560104e-02 -2.63588339e-01\n",
      "   1.52422599e-02 -1.82216227e-01 -2.73472220e-01 -1.06653102e-01\n",
      "   6.12130016e-02  1.00235209e-01 -2.53503472e-01  7.33476579e-02\n",
      "  -3.40662509e-01 -1.41646907e-01 -9.06650901e-01 -6.16360962e-01\n",
      "   4.17556474e-03  3.19181122e-02  6.42846972e-02 -1.97846904e-01\n",
      "  -2.93312371e-02 -4.46176916e-01 -1.38310894e-01 -1.71004310e-01\n",
      "  -3.79235566e-01  1.34871513e-01 -4.13026541e-01 -7.88762569e-01\n",
      "  -5.48925042e-01 -6.89768553e-01 -4.19332713e-01 -7.00702071e-01\n",
      "  -2.74741501e-01  7.28721842e-02 -1.05300200e+00 -5.07938504e-01\n",
      "  -8.39377105e-01 -9.79545295e-01 -6.49587870e-01 -5.60381413e-01\n",
      "  -7.63601303e-01 -7.44906127e-01 -5.09006977e-01 -9.68398273e-01\n",
      "   2.21674666e-02  3.89595367e-02 -8.42466112e-03  4.77397330e-02\n",
      "   1.22027025e-01  9.12153795e-02  7.22447187e-02  6.23525307e-02\n",
      "   5.40239587e-02  1.28115222e-01  6.11009188e-02  3.06375176e-02\n",
      "   5.46689294e-02 -1.50364771e-01 -1.20148011e-01 -1.19812526e-02\n",
      "   7.70671368e-02  7.87523687e-02  1.76167656e-02  1.18209459e-02\n",
      "  -9.72719565e-02 -2.18065064e-02 -3.65190506e-02 -1.24023251e-01\n",
      "  -1.52587295e-01  1.25573045e-02 -1.08530261e-01  2.07278086e-03\n",
      "  -2.10184604e-01 -2.37653911e-01 -2.38786321e-02 -7.20878318e-02\n",
      "  -1.93064198e-01 -1.70207828e-01 -4.71894592e-01 -1.30844295e-01\n",
      "  -2.84744710e-01 -3.71172428e-01 -5.15729249e-01  7.85082695e-04\n",
      "  -6.12594187e-02  9.04979780e-02 -3.87360126e-01 -1.07010171e-01\n",
      "  -5.80405712e-01 -6.63398623e-01 -5.84901750e-01 -3.10901791e-01\n",
      "   1.59983747e-02 -6.19556665e-01 -3.35649759e-01 -2.91310735e-02\n",
      "  -9.72755849e-02 -2.61689872e-01 -4.84021455e-02 -2.27773488e-01\n",
      "  -9.73433554e-01 -7.73276985e-01 -1.67724922e-01 -9.94914234e-01\n",
      "  -3.93885821e-01 -7.98754215e-01 -6.27473414e-01 -2.44819388e-01\n",
      "  -6.71744525e-01 -9.65902865e-01 -6.77274883e-01 -7.82750964e-01\n",
      "  -7.73302376e-01 -8.33739713e-03 -3.73732448e-01 -8.25595498e-01\n",
      "   3.14987055e-03 -3.20871502e-01 -6.22314751e-01 -8.10018420e-01\n",
      "  -6.24265194e-01 -2.02807337e-01 -4.90243495e-01 -7.25535989e-01\n",
      "  -5.32439709e-01 -5.71251512e-01 -6.58532083e-01 -9.99610052e-02\n",
      "  -1.88301444e-01 -8.12403381e-01 -8.54057789e-01 -9.20679867e-01\n",
      "  -1.81392476e-01 -1.61677212e-01 -4.20517884e-02 -6.22145534e-01\n",
      "  -5.18592417e-01 -6.08751237e-01 -8.27852666e-01 -6.24408901e-01\n",
      "  -6.64305806e-01 -1.38886213e+00 -7.22923219e-01 -2.30409250e-01\n",
      "  -8.07289034e-02 -1.20295918e+00 -5.18853903e-01 -1.19064307e+00\n",
      "  -8.22064281e-01 -1.05215955e+00 -8.56387996e-05  3.71326171e-02\n",
      "   2.47839019e-02  2.90321298e-02  1.75061729e-02 -1.88296456e-02\n",
      "  -1.56896133e-02 -8.42473149e-01 -2.04990581e-02 -4.57909405e-02\n",
      "  -2.60779947e-01 -6.72407970e-02 -3.16111781e-02 -8.47022831e-02]\n",
      " [ 2.24847868e-02  4.59342897e-02 -9.44338381e-01 -6.33707941e-01\n",
      "  -5.90439737e-01 -2.29297895e-02  3.26040899e-03  5.80480993e-02\n",
      "   2.56384127e-02 -9.26833868e-01 -8.78232252e-03 -7.97890246e-01\n",
      "   5.92209632e-03 -2.64252853e-02 -4.19427119e-02 -3.04375976e-01\n",
      "   7.87650570e-02  5.92234768e-02  6.77632093e-02  6.48867637e-02\n",
      "   5.37440255e-02  8.31954032e-02  6.86676428e-02  5.67435473e-02\n",
      "   4.95963879e-02  6.74704909e-02  6.58315867e-02  6.39076084e-02\n",
      "   7.83612654e-02  6.91071600e-02  7.34452829e-02  6.11280911e-02\n",
      "  -9.38722864e-02 -2.79212952e-01 -2.44177982e-01 -8.90510082e-02\n",
      "  -1.25218108e-01 -8.62783372e-01  1.02145541e-02  5.80648445e-02\n",
      "  -1.70804337e-01  7.97442868e-02 -1.59756884e-01 -4.48949248e-01\n",
      "  -1.14270282e+00 -5.64857386e-02 -8.37746024e-01 -5.78241587e-01\n",
      "  -4.76486944e-02  5.56006916e-02  5.81616573e-02 -1.20274298e-01\n",
      "   6.19952083e-02 -9.83695149e-01 -2.18711779e-01  2.72504445e-02\n",
      "  -6.77348554e-01 -1.81788281e-01 -1.31130278e+00 -6.30761325e-01\n",
      "   6.09455891e-02 -8.40244442e-02  6.63284883e-02 -1.13493845e-01\n",
      "  -9.04168904e-01  5.70125878e-02 -4.47463363e-01  1.52703151e-02\n",
      "  -1.29919574e-02 -3.40674758e-01  6.28864393e-02 -2.07432121e-01\n",
      "  -1.75922215e-01 -3.53074074e-01  6.42356500e-02 -5.87929748e-02\n",
      "  -1.33363932e-01 -6.06404245e-01 -2.31969491e-01 -1.98406115e-01\n",
      "  -5.52261055e-01 -1.81166884e-02 -9.76254344e-02 -1.88577212e-02\n",
      "  -3.19795549e-01 -3.85499924e-01 -1.93695873e-01 -2.46408898e-02\n",
      "  -8.53141621e-02 -4.57103223e-01 -1.32796198e-01 -2.14500558e-02\n",
      "  -5.67475741e-04 -1.54043464e-02 -4.12674427e-01 -2.65559465e-01\n",
      "  -1.21508598e-01 -4.12699953e-02  5.08155376e-02  6.02224804e-02\n",
      "   2.79328674e-02  5.96768521e-02 -4.33133364e-01  3.71075720e-02\n",
      "  -4.14340524e-03 -3.90195288e-02  3.83833866e-03  5.23407310e-02\n",
      "  -1.70045316e-01 -1.08112440e-01 -9.99612585e-02  4.92494702e-02\n",
      "   1.22998217e-02 -5.13164103e-01 -5.92387319e-01 -4.87779737e-01\n",
      "  -3.33245359e-02  3.64746973e-02 -3.33026946e-02 -9.40439925e-02\n",
      "   1.32264784e-02  5.52338585e-02  8.16390440e-02 -6.60824478e-02\n",
      "   1.23081438e-01  3.14679854e-02  6.72844648e-02  1.10167831e-01\n",
      "  -4.69731718e-01  2.12107450e-02  8.67703371e-03 -1.45188645e-01\n",
      "  -2.94282101e-02  2.85445550e-03 -8.44340920e-02 -3.26118059e-02\n",
      "   5.92133813e-02 -2.65911520e-01 -3.53079557e-01 -4.61413264e-01\n",
      "  -8.50962043e-01 -1.01745224e+00 -1.15174741e-01 -1.48361057e-01\n",
      "  -1.09337792e-02 -8.44787061e-03 -3.33154678e-01 -1.81892782e-01\n",
      "  -6.50449395e-01 -1.14714622e-01 -3.86060886e-02 -7.92161882e-01\n",
      "  -1.09332240e+00 -1.05125332e+00 -5.76267958e-01 -5.33912539e-01\n",
      "   1.13211721e-01  9.44272354e-02  9.56389010e-02  9.78450924e-02\n",
      "   1.12087838e-01  9.88881364e-02  3.81703936e-02  9.33306441e-02\n",
      "   9.02536437e-02  1.05989553e-01  2.15034615e-02  1.46644879e-02\n",
      "   4.44524847e-02  1.34942215e-02 -1.85534820e-01  5.19428663e-02\n",
      "   7.87471980e-02  5.86898215e-02  6.50875568e-02  5.22152558e-02\n",
      "   3.82012539e-02  9.86078754e-03  3.38118151e-02  4.79302220e-02\n",
      "   7.97498375e-02  8.44768584e-02  7.06096739e-02  7.43862912e-02\n",
      "   1.18463198e-02  6.13949262e-02 -1.18861151e+00  1.05229601e-01\n",
      "   2.29464546e-02 -1.22489095e-01 -1.31492689e-02  2.56962217e-02\n",
      "   9.20541957e-02 -1.09009678e-02 -1.75955698e-01  3.49551924e-02\n",
      "  -6.85831830e-02 -2.30584964e-02 -2.09205947e-03  4.70374599e-02\n",
      "  -2.08967760e-01 -2.94915318e-01 -4.03221816e-01 -2.30537161e-01\n",
      "  -1.07274401e+00 -3.23887467e-01  2.96042883e-03 -5.91871738e-01\n",
      "  -9.21057910e-02 -2.21716106e-01 -4.89514470e-01 -3.74541968e-01\n",
      "  -6.60169601e-01 -6.67542994e-01 -6.09784007e-01 -4.87815320e-01\n",
      "  -1.91406161e-01 -2.90033489e-01 -2.08790705e-01 -5.88007510e-01\n",
      "  -3.97661865e-01 -3.91264290e-01 -6.19235814e-01 -5.33405840e-01\n",
      "  -4.96489942e-01 -1.77215531e-01 -2.99678385e-01 -5.85753083e-01\n",
      "  -4.04139161e-01 -3.68826985e-01 -3.37059349e-01 -2.80424744e-01\n",
      "  -5.16471803e-01 -4.49399590e-01 -1.29008561e-01 -4.18107718e-01\n",
      "  -3.40792686e-01 -4.18096960e-01 -3.96237284e-01 -5.88182151e-01\n",
      "  -4.71715927e-01 -3.48066866e-01 -4.55176115e-01 -4.64132726e-01\n",
      "  -5.86534798e-01 -4.77519274e-01 -4.68576908e-01 -2.45751902e-01\n",
      "  -2.92434871e-01 -2.43161336e-01 -5.00577211e-01 -3.44526321e-01\n",
      "  -3.67126554e-01 -4.53844458e-01 -3.32303017e-01 -6.97950423e-01\n",
      "  -1.71695221e-02 -4.96343225e-01 -2.61754304e-01 -5.42728364e-01\n",
      "  -6.69012785e-01 -7.31911123e-01 -8.01304579e-01 -9.74983037e-01\n",
      "  -7.55797684e-01 -1.07285285e+00 -9.34460819e-01 -5.35392582e-01\n",
      "  -8.93142581e-01 -4.84062344e-01 -7.86816359e-01 -3.83521885e-01\n",
      "  -4.50385004e-01 -9.74028468e-01 -8.52481604e-01 -5.07204056e-01]\n",
      " [ 1.24462403e-01 -1.08762458e-02  2.67305970e-02 -1.55630484e-01\n",
      "  -2.81577464e-02 -1.44034773e-02 -3.81852761e-02 -1.40599951e-01\n",
      "  -6.68731928e-02  3.87636758e-02 -3.73789929e-02 -4.61856611e-02\n",
      "   8.67459923e-02  5.73458672e-02 -4.10111323e-02 -1.96671318e-02\n",
      "   1.43797725e-01  3.91662419e-02 -1.45735413e-01 -1.75124764e-01\n",
      "  -6.85609430e-02 -1.18199088e-01 -1.69864729e-01 -2.45442409e-02\n",
      "  -1.14560924e-01  1.36825731e-02 -8.51113498e-02 -3.59015986e-02\n",
      "  -9.39337816e-03 -1.30158782e-01  4.44988683e-02 -6.84708059e-02\n",
      "   6.69775829e-02  5.50597683e-02  1.49658248e-01  9.28775594e-02\n",
      "   1.52967006e-01 -1.95809850e-03 -8.53424966e-02 -1.38409138e-02\n",
      "   6.09386228e-02 -3.21303844e-03  1.02901563e-01  7.66528994e-02\n",
      "  -4.42460179e-02  5.83482813e-03 -2.54409462e-02 -5.79944737e-02\n",
      "  -6.16021119e-02  1.33251480e-03 -8.11323076e-02  7.34097362e-02\n",
      "  -1.64374202e-01 -3.92186642e-02  1.14079535e-01 -9.50506628e-02\n",
      "   3.26268263e-02 -3.04438099e-02  8.87814462e-02  6.60566837e-02\n",
      "  -9.67644006e-02 -1.18642211e-01 -1.87854916e-01  2.50951853e-02\n",
      "   1.03535905e-01 -4.63757999e-02  6.71953661e-03 -1.95641182e-02\n",
      "   5.59657104e-02  3.44225653e-02 -1.24111474e-01  8.72614831e-02\n",
      "   6.60431683e-02  3.68045643e-02 -1.63947418e-01  5.96936122e-02\n",
      "  -1.12276219e-01  4.39492650e-02  5.14680743e-02  2.43394505e-02\n",
      "   6.31494522e-02 -5.09112468e-03  7.22657666e-02  6.99078739e-02\n",
      "  -4.99745943e-02  1.81213655e-02  1.35361463e-01  8.23274404e-02\n",
      "   1.09889843e-01  1.31400228e-01  1.22307129e-01 -1.51980132e-01\n",
      "   8.80392119e-02  1.51393592e-01  8.85878950e-02  7.95699134e-02\n",
      "   1.11253805e-01  9.34843644e-02  5.14326133e-02  5.30865677e-02\n",
      "   7.63529865e-03 -4.71248329e-02  1.17979348e-01 -3.92467305e-02\n",
      "   1.05414167e-01  4.52092551e-02  3.20139900e-02  1.25045925e-01\n",
      "   1.68833315e-01  9.61181223e-02  5.96525781e-02 -6.28568009e-02\n",
      "  -4.43409383e-02  1.10334724e-01  7.74304792e-02  6.59502298e-02\n",
      "   4.74705873e-03 -9.70581323e-02  5.46620972e-02  3.88917350e-03\n",
      "   7.20043480e-02 -1.47745684e-01  5.99887408e-02 -8.78411904e-02\n",
      "  -1.00326633e+00  1.08005414e-02 -5.86642206e-01 -7.66517520e-01\n",
      "   1.04102399e-02 -2.99676925e-01 -1.20138481e-01 -3.64243150e-01\n",
      "  -4.69452053e-01 -5.74986696e-01 -5.85892797e-01 -7.78430104e-01\n",
      "  -8.97560120e-01 -8.64937544e-01 -1.56454146e-01 -2.95163274e-01\n",
      "  -3.77592593e-01 -9.94291425e-01 -1.05053377e+00 -8.63569304e-02\n",
      "  -3.07304829e-01 -1.01507652e+00 -2.37363264e-01 -6.76648736e-01\n",
      "  -2.63959527e-01 -1.01220950e-01 -4.89568025e-01 -1.12238256e-02\n",
      "  -1.27600357e-01  1.18698357e-02 -4.07004446e-01 -8.47136617e-01\n",
      "  -9.31307673e-01 -1.04320824e+00 -1.93287909e-01 -8.49722445e-01\n",
      "  -9.72432554e-01 -1.13186443e+00 -2.79914588e-01 -3.22550714e-01\n",
      "  -4.15256649e-01 -1.33279121e+00  1.44690290e-01 -6.11331463e-02\n",
      "  -6.75151274e-02 -3.04437522e-02 -1.06485735e-03  3.79190817e-02\n",
      "   4.54185046e-02 -1.44478112e-01 -1.02235965e-01 -1.20219290e-01\n",
      "  -1.19941697e-01  1.31748974e-01 -2.49320403e-01 -6.66763354e-03\n",
      "  -4.09903675e-01 -2.57470161e-01  1.03245664e-03  1.30904391e-01\n",
      "   5.30141518e-02  7.38545060e-02 -5.72111532e-02 -3.55530620e-01\n",
      "   1.24401756e-01  2.83642132e-02 -4.51292992e-02 -6.51341736e-01\n",
      "  -2.83224553e-01 -1.07819617e-01  2.76204124e-02 -5.06535769e-02\n",
      "  -2.30782479e-01 -1.37756048e-02 -4.72454160e-01 -5.51036835e-01\n",
      "   2.38776460e-01  7.73274675e-02 -1.21816605e-01 -3.58699262e-01\n",
      "   1.23533361e-01 -6.78418934e-01 -1.77250430e-01  1.74318522e-03\n",
      "  -1.09960347e-01  2.24610925e-01  4.39863652e-02 -1.37800500e-02\n",
      "   7.55159259e-02  7.62461498e-02  3.73129174e-02 -2.52620988e-02\n",
      "   3.10100894e-03 -7.34772310e-02 -7.35954821e-01 -1.46930128e-01\n",
      "  -7.08941817e-01 -3.77265364e-02  9.58657265e-02  1.00836053e-01\n",
      "   5.63983396e-02  6.51309043e-02 -2.86744803e-01 -8.92007202e-02\n",
      "   1.43852115e-01 -5.96561790e-01 -5.66158891e-01  8.17703828e-03\n",
      "  -6.86856806e-02 -5.77055871e-01 -3.20271045e-01 -3.85134697e-01\n",
      "  -6.09083354e-01 -4.59947646e-01 -1.35161668e-01 -3.33454430e-01\n",
      "   1.64198689e-02 -4.09144210e-04 -9.46791768e-02  8.70584026e-02\n",
      "   2.58351061e-02 -8.05187523e-02  1.22858748e-01 -1.27748656e+00\n",
      "  -9.40983057e-01 -1.08293080e+00 -1.04458451e-01 -7.45654702e-01\n",
      "  -6.77748799e-01 -6.96814477e-01 -2.53260672e-01 -5.48704118e-02\n",
      "   1.21912166e-01 -4.86056246e-02 -1.02201974e+00  1.54538944e-01\n",
      "   1.12399235e-01  8.26815292e-02  6.30161539e-02  1.54580817e-01\n",
      "  -1.99458838e-01  1.39381334e-01  1.41450530e-02 -1.34589389e-01\n",
      "  -1.41222611e-01 -7.23312944e-02 -3.55216354e-01 -7.63817877e-02\n",
      "   6.47182316e-02  1.64107546e-01 -3.72934908e-01 -3.75611102e-03]\n",
      " [-5.64104021e-01 -5.54250479e-01 -5.52182971e-03  7.26967528e-02\n",
      "   6.65268227e-02  2.16589626e-02  9.86690400e-04 -4.33318287e-01\n",
      "  -5.55610120e-01  3.97160836e-02 -5.87629199e-01  5.77609688e-02\n",
      "  -5.52202463e-01 -2.16252327e-01 -9.21835676e-02  2.10532807e-02\n",
      "  -8.78198743e-01  2.91414675e-03 -1.15265744e-02 -3.53147765e-03\n",
      "   6.29639849e-02 -8.86374712e-01 -7.76470872e-03 -1.73119724e-01\n",
      "   3.64243127e-02  1.16441185e-02 -1.00604873e-02 -1.82385240e-02\n",
      "  -7.54706383e-01 -2.80972272e-02 -2.03987971e-01 -2.81377696e-02\n",
      "   7.04739392e-02  6.09707870e-02  7.60043934e-02  5.15530221e-02\n",
      "   8.83134380e-02  4.46362533e-02  5.27649298e-02  1.97397154e-02\n",
      "   9.43227038e-02 -1.44099459e-01  5.78266904e-02  9.54451114e-02\n",
      "   1.05273798e-01  9.01266783e-02  8.66431519e-02  9.02699307e-02\n",
      "   6.46879748e-02  5.87970344e-03 -1.62256565e-02  7.29479417e-02\n",
      "  -2.71977559e-02  1.01010226e-01  6.45100772e-02  2.30192225e-02\n",
      "   7.65257105e-02  7.75120482e-02  8.30615833e-02  7.81944841e-02\n",
      "  -1.68454535e-02  7.25963488e-02 -4.38483357e-02  8.40574950e-02\n",
      "   7.80859366e-02 -3.53587605e-02  1.05020247e-01  7.28193894e-02\n",
      "   4.14554812e-02  8.11913311e-02 -4.57265601e-02  9.12950039e-02\n",
      "   6.87489659e-02  8.93127099e-02  3.82504761e-02  8.30143765e-02\n",
      "   8.39643255e-02  8.43827203e-02  9.16309580e-02  7.59500191e-02\n",
      "   7.60628060e-02  8.42095688e-02  6.77294135e-02  7.72261918e-02\n",
      "   7.19225332e-02  7.95256868e-02  5.17651252e-02 -5.18945511e-03\n",
      "   4.73734625e-02  5.60638644e-02  6.67915717e-02 -1.43271568e-03\n",
      "   5.01511954e-02 -4.75737453e-03  4.71590571e-02  6.58022016e-02\n",
      "   7.25146160e-02  3.40901501e-02  1.34296184e-02  9.87090077e-03\n",
      "  -5.89863919e-02 -5.20172358e-01 -1.56598732e-01 -4.15663123e-01\n",
      "  -4.21861231e-01 -4.91374254e-01  2.45496398e-03 -8.38074833e-02\n",
      "  -6.38415515e-01 -7.40085006e-01 -2.52576113e-01 -5.29164851e-01\n",
      "  -2.95764133e-02  6.83925077e-02 -6.31495416e-02  2.61366013e-02\n",
      "  -1.80850208e-01 -6.99747026e-01 -1.02943424e-02 -4.62315930e-03\n",
      "  -4.10585135e-01 -1.18934058e-01 -7.30018675e-01  8.17318261e-02\n",
      "  -5.87764204e-01 -9.92492497e-01 -7.74748206e-01 -6.75181627e-01\n",
      "  -6.68738387e-04  5.51906265e-02 -6.46639466e-01 -5.10306239e-01\n",
      "  -8.98786247e-01 -9.11787391e-01 -5.31399176e-02 -3.81942153e-01\n",
      "  -2.93236583e-01 -8.11043441e-01 -3.60049903e-01 -6.36830449e-01\n",
      "  -3.58657509e-01 -1.25162333e-01 -5.16382277e-01 -6.75294340e-01\n",
      "  -5.67511246e-02 -4.10734892e-01 -9.74617660e-01 -4.56790656e-01\n",
      "  -3.42348292e-02 -8.34174991e-01 -6.05102852e-02 -7.27938786e-02\n",
      "  -3.61126095e-01 -1.33339912e-01 -8.66377633e-03 -1.00076899e-01\n",
      "  -7.49059141e-01 -6.56357050e-01 -9.28908587e-01 -8.59886348e-01\n",
      "  -6.99882329e-01 -8.27078879e-01 -1.35886399e-02 -7.36143291e-01\n",
      "  -5.81889153e-01 -6.40330076e-01 -6.32294193e-02 -1.00341976e-01\n",
      "  -5.83827980e-02  2.22151894e-02  7.99861923e-02 -2.49087110e-01\n",
      "  -1.41627669e-01 -1.17775269e-01 -1.79624874e-02  4.43126773e-03\n",
      "   3.41133215e-02  5.33286557e-02  6.36545941e-02  4.85355780e-02\n",
      "  -2.07305536e-01 -1.06995773e+00 -1.16144143e-01 -8.06580305e-01\n",
      "  -5.56324981e-03 -9.09736827e-02  9.52062979e-02 -8.48944485e-01\n",
      "  -4.02297191e-02  2.71502770e-02 -3.50841470e-02 -3.50362360e-02\n",
      "  -1.03099561e+00  5.94854131e-02  7.66173676e-02  4.92300699e-03\n",
      "   7.83522055e-02  3.74384634e-02  1.74329709e-02 -1.01256058e-01\n",
      "  -8.35477114e-01 -1.00644374e+00 -9.89324868e-01 -9.61556137e-01\n",
      "   8.50782916e-02 -8.84476900e-01 -5.01439273e-02 -1.00501120e-01\n",
      "  -2.24262141e-02 -1.16961360e-01 -3.07888612e-02 -6.14596158e-02\n",
      "  -1.28544355e-02 -4.76342104e-02 -7.30657429e-02 -9.35451627e-01\n",
      "  -9.11076903e-01 -9.19838130e-01 -8.62385213e-01 -4.49031107e-02\n",
      "  -6.94484890e-01 -7.23905325e-01 -1.21692719e-03 -2.07235664e-02\n",
      "   3.30792950e-03 -8.06712136e-02 -1.03939283e+00 -9.02844012e-01\n",
      "  -7.29625076e-02 -9.97851789e-03 -7.73870766e-01 -1.00900185e+00\n",
      "  -9.76411283e-01 -4.29273807e-02 -1.04360628e+00 -8.47982228e-01\n",
      "  -9.44966316e-01 -9.52766716e-01 -8.54298353e-01 -1.99607499e-02\n",
      "   1.35308746e-02 -1.01686692e+00 -9.16357279e-01 -1.93281788e-02\n",
      "  -6.83575124e-02 -6.16214331e-03 -3.93455252e-02 -8.01146150e-01\n",
      "  -8.89468312e-01 -8.31772506e-01 -8.14522207e-01 -8.20795596e-01\n",
      "  -7.55077004e-01 -1.74789146e-01 -1.33773267e-01 -7.87681416e-02\n",
      "  -2.57383794e-01 -8.19649640e-03  3.99195850e-02 -7.60891140e-02\n",
      "  -2.87955329e-02 -4.81806695e-01  7.32841343e-02  9.43549648e-02\n",
      "   6.92162067e-02  9.94460359e-02  7.58910105e-02  6.32783026e-02\n",
      "   7.48806670e-02 -1.01550609e-01  7.69317821e-02  6.27661645e-02\n",
      "  -5.81224822e-02 -2.05410365e-02  9.04485881e-02 -6.91120187e-03]\n",
      " [-3.77149954e-02  1.57972336e-01 -1.73152867e-03 -1.25794217e-01\n",
      "  -6.11181781e-02 -2.57558167e-01 -2.40989313e-01 -1.21263452e-01\n",
      "  -4.65226173e-02 -9.03387442e-02 -7.34589696e-01 -1.33496642e-01\n",
      "   1.59161881e-01 -2.64130801e-01  3.96723896e-02  1.10006072e-01\n",
      "   2.66398229e-02  1.69100128e-02 -2.16974039e-02 -1.63970754e-01\n",
      "  -9.15592983e-02  6.89757317e-02  4.34646308e-02  3.51818986e-02\n",
      "   1.38135552e-02 -8.82932320e-02 -3.67324017e-02 -1.02301482e-02\n",
      "   1.04135528e-01  5.31532103e-03  8.11901316e-02  1.17491573e-01\n",
      "   2.04471219e-02 -9.32454914e-02  8.93613622e-02  1.00093246e-01\n",
      "   1.28326505e-01  3.70552875e-02 -7.87947029e-02 -7.79498890e-02\n",
      "   4.64468114e-02  4.13809717e-02  1.24218784e-01  5.97257018e-02\n",
      "  -2.37205490e-01 -1.88094061e-02  5.08603938e-02 -9.19987932e-02\n",
      "  -2.57364046e-02 -1.91506483e-02 -7.50381872e-02 -6.66336417e-02\n",
      "  -2.25600582e-02 -3.51291336e-02 -6.80316687e-02 -5.79610188e-03\n",
      "   3.29563394e-02 -3.70839834e-02 -1.00621752e-01 -4.29607090e-03\n",
      "  -2.49544512e-02 -1.26775878e-03 -1.13737043e-02  2.83337547e-03\n",
      "   8.64661187e-02  6.00317419e-02  1.54550776e-01 -1.37383401e-01\n",
      "   2.04811916e-02  4.04262245e-02  2.20279973e-02 -2.19290964e-02\n",
      "   1.50874421e-01  9.49386228e-03 -1.02684356e-01  9.97981280e-02\n",
      "   4.64302003e-02 -6.99503198e-02 -3.21042798e-02  1.08552925e-01\n",
      "  -1.66134417e-01 -4.84935492e-02 -9.32265148e-02  5.07990457e-02\n",
      "   2.85220873e-02  1.17381886e-01 -1.15682542e-01  1.02170803e-01\n",
      "  -3.32182012e-02  8.95182043e-02  1.95138734e-02  7.69970343e-02\n",
      "   1.77200735e-01  2.91932393e-02  1.00914039e-01  1.24291986e-01\n",
      "   5.48342243e-02  5.91821969e-02 -4.29799519e-02 -6.28061444e-02\n",
      "  -1.35289803e-01 -5.31449139e-01 -1.94078714e-01  1.29718529e-02\n",
      "  -7.95337200e-01 -5.26183546e-01  9.01216492e-02  4.39287908e-02\n",
      "  -5.70771337e-01 -3.71158570e-01  9.47822351e-03 -1.84535787e-01\n",
      "  -1.37643041e-02  5.33011667e-02 -9.63103473e-02  1.40956864e-01\n",
      "  -9.64210182e-02 -1.82155403e-03 -8.59314427e-02 -1.10120595e-01\n",
      "   5.15789054e-02 -6.60003675e-03 -1.09123945e-01 -1.95699651e-02\n",
      "   5.22631370e-02 -3.80292796e-02 -6.38271093e-01 -7.15513408e-01\n",
      "   9.60430950e-02  5.04447632e-02  1.18250519e-01 -9.17961776e-01\n",
      "   1.30689502e-01  4.04824577e-02 -3.11635911e-01 -3.13599586e-01\n",
      "  -6.49004400e-01  1.11821815e-01 -8.38226199e-01 -1.65213346e-01\n",
      "   2.54768860e-02  5.01253977e-02 -5.43576777e-01 -1.21270657e-01\n",
      "  -8.21735412e-02 -7.22827137e-01 -5.52215993e-01 -9.47406709e-01\n",
      "  -4.41821307e-01 -5.60719371e-01 -4.73292381e-01 -4.93927032e-01\n",
      "  -5.56989372e-01 -9.13228393e-01 -2.58705229e-01 -9.67367828e-01\n",
      "  -9.95511174e-01 -7.74662077e-01 -1.08591759e+00 -7.06778467e-01\n",
      "  -6.98269904e-01 -8.29534709e-01  7.60891140e-02 -7.64855146e-01\n",
      "  -4.49679494e-01 -2.42234349e-01  1.09804638e-01 -2.20289472e-02\n",
      "  -1.96529791e-01 -5.24655521e-01 -6.27288878e-01 -4.71972764e-01\n",
      "  -7.53451958e-02  2.52267923e-02 -4.08008285e-02  6.91452473e-02\n",
      "  -4.78074104e-01 -5.75120628e-01 -1.59407973e-01 -1.34887993e+00\n",
      "  -3.02187949e-01 -3.42384785e-01 -1.69162154e-02 -7.04527497e-01\n",
      "  -6.75815225e-01 -4.22405601e-01 -2.89653867e-01 -7.00748801e-01\n",
      "  -7.68912673e-01 -5.21682322e-01 -8.24601829e-01 -8.17017615e-01\n",
      "  -7.74591327e-01 -2.67248482e-01 -6.40197694e-01 -1.01059834e-02\n",
      "  -8.05337846e-01 -9.98077542e-02 -2.58388132e-01 -1.14720309e+00\n",
      "   2.35402994e-02  6.75029829e-02  6.85107037e-02  6.04949631e-02\n",
      "  -9.95888375e-04  7.78667554e-02  4.05630618e-02  2.92469393e-02\n",
      "   5.60174808e-02  5.55294342e-02  4.41293828e-02 -9.18759555e-02\n",
      "  -3.22859585e-01 -2.38484010e-01  6.94407448e-02 -2.50943378e-02\n",
      "   3.35463025e-02  7.72855431e-02  3.08029838e-02 -2.03748904e-02\n",
      "   6.46979362e-02  3.52127925e-02 -1.23698652e-01  2.08658706e-02\n",
      "  -8.49703968e-01  7.68676996e-02  1.25785917e-01  2.14545485e-02\n",
      "   4.04107720e-02  4.18000333e-02  4.84948084e-02  5.91159426e-02\n",
      "   1.53266519e-01 -3.00577898e-02  3.26737463e-02  9.70899314e-02\n",
      "   2.89714988e-02  1.51643351e-01  1.11252002e-01  7.19437897e-02\n",
      "  -2.50736694e-03  7.86145702e-02  8.78074914e-02 -6.95330143e-01\n",
      "   1.98917463e-02  9.04790387e-02  9.21666175e-02 -1.24293901e-02\n",
      "   7.65612200e-02  1.50761870e-03  1.83026474e-02  1.07751578e-01\n",
      "   5.08308597e-02 -6.86364621e-02  6.82798848e-02  7.39474520e-02\n",
      "   4.17524800e-02 -1.62608176e-01 -6.80762008e-02 -1.35468349e-01\n",
      "  -1.05132855e-01  7.57588912e-03  3.71044092e-02  1.02085307e-01\n",
      "   3.83874178e-02  1.16003871e-01  1.60333999e-02 -6.79288805e-03\n",
      "   2.75315885e-02  3.99984680e-02  2.71423487e-03 -2.16798652e-02\n",
      "   1.03809968e-01  3.32658403e-02  3.12512703e-02 -6.58115000e-02]\n",
      " [-8.53647441e-02  1.51167005e-01  1.31193265e-01 -3.61124873e-02\n",
      "   3.94340605e-02 -2.66862288e-02  5.61244115e-02 -3.53390276e-01\n",
      "   4.30712663e-02 -9.56201851e-02 -6.77873611e-01 -3.27784680e-02\n",
      "   8.96826759e-03 -1.66052118e-01 -5.87395206e-02  5.30540869e-02\n",
      "  -2.60755688e-01  3.16624865e-02  1.14827193e-02 -4.32812013e-02\n",
      "  -4.51425370e-03 -8.94855559e-02 -4.79502194e-02  3.89516801e-02\n",
      "   2.04058737e-02  1.15088224e-02  3.55985924e-03 -3.75928581e-02\n",
      "  -7.71631375e-02 -1.35900090e-02  2.24560164e-02 -8.17501098e-02\n",
      "   5.38425110e-02  5.41584156e-02  6.80510029e-02  6.88255206e-02\n",
      "   7.63593465e-02  6.45822436e-02  1.42501118e-02 -5.06612360e-02\n",
      "   8.09698328e-02 -8.43557939e-02  8.43946114e-02  7.16191903e-02\n",
      "   8.65007564e-02  3.97993848e-02  1.16305105e-01  1.08489551e-01\n",
      "   6.63210079e-02 -1.88107491e-02 -1.85177196e-02  4.53098342e-02\n",
      "  -5.16402684e-02  1.44065604e-01  6.26760647e-02 -2.78113801e-02\n",
      "   8.04851875e-02  3.61911021e-02  9.76147801e-02  4.86000068e-02\n",
      "  -1.21968286e-02  3.44068259e-02 -8.64351243e-02  6.27661496e-02\n",
      "   8.85263011e-02 -3.08877490e-02  9.85076800e-02  9.49576274e-02\n",
      "   2.53615733e-02  6.88009933e-02 -4.08027545e-02  6.08819760e-02\n",
      "   7.25528002e-02  7.48139694e-02 -1.05269505e-02  5.99871911e-02\n",
      "   8.09983015e-02  8.49284157e-02  7.67276138e-02  6.30882606e-02\n",
      "   9.40505117e-02  5.40408306e-02  6.08739778e-02  4.29788865e-02\n",
      "   2.49640495e-02  5.51831909e-02  7.97298029e-02  8.86071101e-03\n",
      "   1.14827091e-02  5.07887676e-02  8.49709064e-02 -4.38057743e-02\n",
      "   5.43904975e-02  4.12331745e-02  1.81135889e-02  6.49208948e-02\n",
      "   1.00592576e-01  4.48143147e-02 -4.74621495e-03 -2.01426689e-02\n",
      "  -6.39151871e-01 -9.37324762e-01 -2.08452180e-01 -1.60645600e-02\n",
      "  -3.54568303e-01 -8.73541594e-01 -4.21784312e-01 -1.83073282e-01\n",
      "  -8.16454768e-01  8.50651637e-02 -7.32770339e-02 -3.33246142e-01\n",
      "  -4.22097385e-01 -1.20380369e-03 -7.68080875e-02 -1.50712401e-01\n",
      "  -3.84012073e-01 -1.18754186e-01 -2.14978665e-01 -5.34041375e-02\n",
      "  -4.45444435e-01 -1.04576789e-01 -7.14238107e-01  8.99792165e-02\n",
      "  -9.33495820e-01  6.49371296e-02 -9.61468041e-01 -9.93921101e-01\n",
      "  -7.69456476e-02  2.36576749e-03 -1.16390146e-01 -6.54904306e-01\n",
      "  -9.55981970e-01 -7.46289864e-02 -1.24611706e-01 -3.36559296e-01\n",
      "  -8.23795915e-01 -5.84283292e-01 -5.95601976e-01 -2.14489415e-01\n",
      "  -5.32427847e-01 -5.20563185e-01 -3.88238654e-02 -1.38142735e-01\n",
      "  -1.00975049e+00 -1.94855765e-01 -7.11566567e-01 -3.87914777e-01\n",
      "  -9.25026536e-02 -3.82860839e-01 -1.41886577e-01 -5.93231738e-01\n",
      "  -1.93523198e-01 -6.93933666e-01 -1.43960342e-01 -1.10577142e+00\n",
      "  -1.47646630e+00 -8.00404549e-01  8.57958570e-02  1.45063430e-01\n",
      "  -5.37791967e-01 -1.86139956e-01 -2.54618912e-03 -1.30218649e-02\n",
      "   9.43393186e-02 -3.65426898e-01 -5.66276796e-02 -4.13825596e-03\n",
      "  -1.09112225e-02  1.42981112e-01  3.20654623e-02  1.23685203e-01\n",
      "  -5.19973338e-02 -1.04961999e-01 -2.09341437e-01  2.45204698e-02\n",
      "   4.02497090e-02  3.57592031e-02 -5.40468805e-02  1.08935358e-02\n",
      "   3.40065919e-02  1.50899189e-02  6.79176375e-02  1.03457101e-01\n",
      "   5.82678504e-02  3.22904959e-02  7.47191906e-02 -5.84073782e-01\n",
      "   9.04084295e-02  1.18855283e-01  3.95514853e-02  1.01847947e-01\n",
      "  -1.29375413e-01  8.47297609e-02  5.29858731e-02 -3.73638570e-02\n",
      "   5.32655679e-02  2.36245357e-02  3.84848155e-02  1.41138643e-01\n",
      "  -9.47353095e-02 -6.03876770e-01 -4.03208643e-01 -7.46131480e-01\n",
      "   9.16314423e-02 -7.84097195e-01 -1.61964327e-01 -1.70785174e-01\n",
      "  -1.56693503e-01  1.94410663e-02  6.17218949e-03  6.57370239e-02\n",
      "  -2.13952005e-01 -2.69746065e-01 -2.13689148e-01 -1.56426743e-01\n",
      "  -8.23425829e-01 -5.04717052e-01 -5.65299571e-01  1.56103104e-01\n",
      "  -1.24023296e-01 -3.03406328e-01 -4.01478261e-01 -1.88542441e-01\n",
      "   8.19753408e-02 -1.12311654e-01 -1.51191700e+00 -5.76860718e-02\n",
      "  -2.54899293e-01 -9.89779830e-02 -3.36807549e-01 -9.43870366e-01\n",
      "  -4.00972247e-01 -1.28116101e-01 -8.05149496e-01 -3.09478015e-01\n",
      "  -3.55630249e-01 -9.05383646e-01 -5.44853926e-01 -1.83633983e-01\n",
      "  -1.08319372e-01 -6.59583807e-01 -4.30565327e-01 -1.44663572e-01\n",
      "  -9.32738483e-02 -1.48471736e-03 -2.34062776e-01 -7.72522032e-01\n",
      "  -7.00392902e-01 -8.66290867e-01  1.36778299e-02 -5.52044928e-01\n",
      "  -4.93591100e-01 -5.72737455e-01 -1.28577307e-01 -4.84688252e-01\n",
      "  -7.99996331e-02 -5.36139488e-01 -1.57763809e-01  1.93352178e-01\n",
      "   7.43611753e-02 -4.18825328e-01  3.67388539e-02  8.14623460e-02\n",
      "   3.12253125e-02  9.11003500e-02  5.54328263e-02  1.29936561e-02\n",
      "   9.35511589e-02  1.94449157e-01  4.64113802e-02  3.81061313e-04\n",
      "  -3.52993384e-02 -9.64860711e-03  6.82901144e-02 -1.58402007e-02]\n",
      " [-2.13561468e-02  4.25677262e-02 -1.50097147e-01 -3.63526285e-01\n",
      "  -5.00552021e-02  1.57876592e-02  8.39391141e-04  5.69548532e-02\n",
      "   2.56442577e-02 -1.51799798e-01  4.68345322e-02 -2.11320706e-02\n",
      "   5.50616160e-02  1.49200475e-02 -3.02637219e-02 -1.21607803e-01\n",
      "  -5.02959371e-01 -6.63507059e-02  1.12278736e-03 -6.70267880e-01\n",
      "  -5.38922071e-01 -4.79481220e-01 -1.42117754e-01 -6.61770821e-01\n",
      "   3.88083868e-02 -5.51606655e-01 -1.20095655e-01 -2.96719968e-01\n",
      "  -7.88195953e-02 -2.53282398e-01 -4.47075814e-02  1.04099577e-02\n",
      "  -5.52440524e-01 -2.50453830e-01 -3.59342217e-01 -1.54761478e-01\n",
      "  -8.00729871e-01 -7.38514423e-01 -2.84370959e-01 -6.25771642e-01\n",
      "  -6.13629162e-01 -5.31522989e-01 -4.88641232e-01 -7.51560569e-01\n",
      "  -5.07823110e-01 -5.35095453e-01 -6.54492795e-01 -6.57120466e-01\n",
      "  -9.95430410e-01 -8.02124478e-03 -7.22596884e-01 -8.48218948e-02\n",
      "  -1.22017547e-01 -7.19249725e-01 -5.67540348e-01 -3.10207188e-01\n",
      "  -5.20540655e-01 -2.98253477e-01 -4.31494653e-01 -8.33036825e-02\n",
      "  -2.97709238e-02 -6.34554386e-01 -6.07372262e-02 -3.08079690e-01\n",
      "  -6.69930041e-01 -1.43748656e-01 -3.08480769e-01 -2.93436497e-01\n",
      "  -5.86921394e-01 -4.43047494e-01 -1.33343548e-01 -3.03376853e-01\n",
      "  -3.66681278e-01 -1.16681650e-01 -4.42083716e-01 -6.40795112e-01\n",
      "  -3.78467798e-01 -9.72124755e-01 -3.48830014e-01 -6.82791114e-01\n",
      "  -5.80323040e-01 -7.40926027e-01 -2.03702405e-01 -5.70892930e-01\n",
      "  -6.06501102e-01 -3.99168640e-01 -6.07738018e-01 -2.94571847e-01\n",
      "  -2.89491177e-01 -8.41717601e-01 -8.10505211e-01 -3.22064906e-02\n",
      "  -6.90620482e-01 -6.45561576e-01 -6.19151711e-01 -9.73900616e-01\n",
      "  -8.06246638e-01 -8.51501763e-01 -1.37319759e-01 -1.06369925e+00\n",
      "   2.25190017e-02  5.01806363e-02  2.71759462e-02  2.99718566e-02\n",
      "   7.03226626e-02  6.89622164e-02 -5.32777548e-01 -1.14688158e-01\n",
      "   5.46765104e-02  5.37676625e-02  3.76178548e-02  3.46684232e-02\n",
      "  -5.49373515e-02 -4.91534859e-01 -3.25715938e-03 -3.96191254e-02\n",
      "  -2.01848596e-02  3.13768983e-02 -5.02165556e-02  1.75785627e-02\n",
      "  -1.61620583e-02 -6.42227829e-01 -5.41424990e-01 -6.35327458e-01\n",
      "  -4.87829775e-01 -1.58724844e-01 -4.40592438e-01 -5.00962019e-01\n",
      "  -3.36349249e-01 -2.95640737e-01 -5.58302458e-03  8.07593316e-02\n",
      "  -2.21396759e-01 -7.44589388e-01 -5.08946598e-01 -7.75858104e-01\n",
      "  -4.65715677e-01 -7.88532794e-02  4.81861532e-02 -4.83187623e-02\n",
      "   5.70555851e-02 -1.31465346e-01 -9.63452220e-01  3.49132828e-02\n",
      "  -7.42566288e-02 -6.23866200e-01 -9.20626462e-01 -7.46158659e-02\n",
      "  -2.61126429e-01 -5.77985756e-02  3.75103988e-02 -1.11214191e-01\n",
      "  -8.61346871e-02 -6.43365979e-02  5.48861958e-02  1.81481205e-02\n",
      "  -3.51042688e-01 -5.09244382e-01 -9.67010140e-01 -3.48350048e-01\n",
      "  -3.34433258e-01 -1.87372103e-01 -5.78323066e-01 -5.98037660e-01\n",
      "  -3.86616796e-01 -4.10350472e-01 -7.34616876e-01 -7.89323211e-01\n",
      "  -4.53938514e-01 -6.20942771e-01 -6.29182994e-01 -9.25242484e-01\n",
      "  -9.18666720e-01 -6.28745854e-01 -7.51268506e-01 -1.01270363e-01\n",
      "  -4.47116196e-02 -6.28071964e-01 -9.38731879e-02 -1.05752110e+00\n",
      "  -6.75598621e-01 -4.43587720e-01 -2.86889941e-01 -4.22635883e-01\n",
      "  -4.50101286e-01 -1.24137126e-01 -6.87422156e-01 -4.06210333e-01\n",
      "  -6.00287616e-01 -3.44524711e-01 -9.13312972e-01 -2.94084787e-01\n",
      "  -7.81800747e-01 -3.73263955e-01 -3.85155052e-01 -6.68890715e-01\n",
      "  -7.55237341e-01 -7.70776451e-01 -1.26496226e-01 -4.46808308e-01\n",
      "   5.63409962e-02  7.82178193e-02  9.49065611e-02  8.92549381e-02\n",
      "  -2.04415037e-03  1.01557635e-01  5.62849902e-02  7.55141899e-02\n",
      "   3.85403559e-02  4.35420386e-02  4.37971428e-02 -8.41591507e-03\n",
      "  -7.40445405e-02 -4.33639362e-02 -2.72070095e-02  8.55407417e-02\n",
      "   8.28988701e-02  8.56711715e-02  9.85252038e-02  5.05835749e-02\n",
      "   9.54977423e-02  8.63924176e-02 -2.52709873e-02 -4.12802286e-02\n",
      "   7.39567867e-03 -4.65143695e-02  1.03612475e-01  9.65456963e-02\n",
      "   4.96873558e-02  7.81791955e-02  9.36937183e-02  9.13021863e-02\n",
      "   8.67108107e-02  6.43546283e-02  9.69925225e-02  9.83793437e-02\n",
      "   9.37257782e-02  9.80048180e-02  9.37516913e-02  7.73183778e-02\n",
      "   6.47518411e-02  8.51180702e-02  8.99034888e-02  3.09121143e-02\n",
      "  -2.17660088e-02  5.52273057e-02  4.36465777e-02  9.82362330e-02\n",
      "   1.01595566e-01  1.02360554e-01  9.51932892e-02  1.06147848e-01\n",
      "   9.67614055e-02  7.10634962e-02  5.12222685e-02  4.02099937e-02\n",
      "  -2.80184895e-02  4.08272445e-02  6.76777735e-02  4.23632637e-02\n",
      "  -1.90055311e-01  7.80151263e-02  5.96250314e-03 -2.65437543e-01\n",
      "  -9.17182653e-04 -3.14158887e-01 -2.33319867e-03  1.38694150e-02\n",
      "   9.99487936e-03  2.86741015e-02  7.04375096e-03  2.78581940e-02\n",
      "   3.20468768e-02 -3.93362530e-02  1.27750020e-02 -2.07095183e-02]\n",
      " [ 2.36551464e-02 -4.19965051e-02  5.99842966e-02  5.50335869e-02\n",
      "   6.98334724e-02  5.35332523e-02  6.30550310e-02 -4.40736651e-01\n",
      "  -5.30678704e-02  5.92576116e-02 -1.87996738e-02  4.98295985e-02\n",
      "  -1.07756160e-01  5.73303774e-02 -1.46665983e-02  5.12500145e-02\n",
      "  -7.11629391e-01 -1.06229532e+00 -1.03782845e+00 -2.86270887e-01\n",
      "  -1.39337552e+00 -5.25514007e-01 -1.68505788e+00 -4.52231336e-03\n",
      "  -1.34255791e+00 -1.28451145e+00 -6.50915861e-01 -1.53811261e-01\n",
      "  -3.71350884e-01 -6.86582506e-01 -9.08795893e-01 -2.26337031e-01\n",
      "  -2.74475012e-02 -3.33821401e-02 -9.69289541e-02 -1.43560484e-01\n",
      "  -4.71555173e-01 -3.24188196e-03 -4.00212914e-01 -2.64711410e-01\n",
      "  -2.07146525e-01 -7.53532231e-01 -2.27665287e-02 -2.64500380e-01\n",
      "   2.23471764e-02 -5.74915886e-01 -1.79230526e-01 -1.00739792e-01\n",
      "  -1.47241265e-01 -2.34486535e-01 -4.82355654e-01 -3.30599666e-01\n",
      "  -2.78535008e-01 -5.09633757e-02 -4.62753028e-02 -1.06240261e+00\n",
      "  -1.91587005e-02 -3.32182109e-01 -3.89898270e-02 -2.73308307e-01\n",
      "  -9.89584267e-01 -5.19668981e-02 -9.03277040e-01 -5.44134378e-01\n",
      "  -7.92708099e-02 -2.90790081e-01 -3.36058527e-01 -2.73829967e-01\n",
      "  -9.10319239e-02 -3.39351356e-01 -8.08988035e-01 -1.36734039e-01\n",
      "  -2.60707825e-01 -6.00294098e-02 -2.49267489e-01 -4.48769152e-01\n",
      "  -6.53170288e-01 -8.08254778e-02 -2.64360398e-01 -2.96834767e-01\n",
      "   2.98012164e-03 -2.49378577e-01 -1.07081132e-02 -2.99700767e-01\n",
      "  -2.09751502e-01 -1.87577784e-01  4.03367132e-02 -5.27291745e-02\n",
      "   4.38328534e-02  1.73320784e-03 -2.44180523e-02  2.32053716e-02\n",
      "  -5.82340434e-02  2.09497823e-03 -2.63056960e-02 -6.20574616e-02\n",
      "  -1.94536857e-02 -2.94591356e-02 -1.06351124e-02 -1.87816620e-02\n",
      "   3.11733894e-02 -2.22753897e-01  2.33899243e-02 -1.56049319e-02\n",
      "   4.02213559e-02 -7.54911825e-02  3.66070261e-03  5.01875691e-02\n",
      "   5.15855588e-02  1.82388108e-02 -1.96481738e-02 -2.25173365e-02\n",
      "  -3.21916677e-02  3.71114910e-02  5.04174642e-02  4.15432788e-02\n",
      "   9.11578629e-03 -1.12133771e-01  2.91828681e-02 -3.21819559e-02\n",
      "  -4.38697897e-02 -2.87160248e-01 -4.99707796e-02 -9.94541794e-02\n",
      "  -1.22419584e+00 -2.66345367e-02 -1.44305199e-01 -5.43555260e-01\n",
      "   3.03677469e-02 -3.05694699e-01 -4.80105840e-02 -3.63313943e-01\n",
      "  -5.35222366e-02 -3.72451246e-02 -3.33583727e-02 -6.10239990e-02\n",
      "  -2.37163398e-02  1.77810285e-02 -7.87189603e-02  4.85284366e-02\n",
      "   3.54885426e-03  6.46297336e-02 -1.35751171e-02 -6.59710392e-02\n",
      "  -4.13086116e-02 -1.50839575e-02  6.11935928e-02  8.33458174e-03\n",
      "   5.77084087e-02  8.06740895e-02 -2.84947455e-02  5.33109568e-02\n",
      "   7.59934485e-02  3.11236233e-02  3.93473357e-02  4.68738861e-02\n",
      "  -1.14744413e+00 -7.78301895e-01 -4.21367824e-01 -1.05046892e+00\n",
      "  -8.21173429e-01 -9.43088353e-01 -1.06618512e+00 -9.79923785e-01\n",
      "  -7.18221366e-01 -1.32383740e+00 -1.68973103e-01 -1.68752447e-01\n",
      "  -6.01040386e-02 -3.14480476e-02  4.83324705e-03 -9.70438272e-02\n",
      "  -7.20034659e-01 -1.09711266e+00 -2.43664533e-02 -4.78599034e-02\n",
      "  -5.16475886e-02 -1.02222808e-01 -5.57080060e-02 -2.77675521e-02\n",
      "  -8.33280757e-02 -5.60975194e-01 -6.06126152e-02 -3.53485614e-01\n",
      "  -1.82668380e-02 -7.65029714e-02  3.98872159e-02 -3.02731842e-01\n",
      "  -2.92821899e-02  1.70797147e-02  2.85481084e-02 -8.30149427e-02\n",
      "  -3.72090377e-02  5.32667711e-03 -1.48806525e-02  5.63169539e-04\n",
      "  -5.32409735e-02 -1.82859182e-01  1.32578127e-02 -8.04777592e-02\n",
      "  -1.39673539e-02 -6.28450960e-02 -1.31103635e-01 -3.01117033e-01\n",
      "  -2.18085181e-02 -2.96272784e-01 -1.32780939e-01 -2.07562402e-01\n",
      "  -1.69829860e-01  8.18260293e-03  1.47902891e-02  7.92181939e-02\n",
      "   8.49872157e-02  7.58856535e-02  6.87583089e-02 -4.81331125e-02\n",
      "  -1.35138914e-01 -2.46366728e-02 -2.07854584e-01 -8.45848992e-02\n",
      "  -3.91090959e-02 -9.42528341e-03  8.78750756e-02  9.07301679e-02\n",
      "   9.49939787e-02  5.70270717e-02 -1.82018787e-01 -6.59343302e-02\n",
      "  -1.74612626e-01 -8.01036209e-02 -1.80615067e-01  2.93579157e-02\n",
      "  -4.53663059e-02 -6.41127303e-02 -3.34715635e-01 -1.32118076e-01\n",
      "  -1.46891549e-01 -2.30083734e-01 -1.82714298e-01 -7.50750542e-01\n",
      "  -4.92521152e-02  8.47552158e-03 -5.54488450e-02  8.99991170e-02\n",
      "   7.36181661e-02 -5.31476699e-02 -1.29797667e-01 -1.66825682e-01\n",
      "  -5.31626865e-02 -1.36973411e-01 -5.43308556e-02 -2.45296136e-01\n",
      "  -2.64463335e-01 -1.05038546e-01  6.33420125e-02 -1.56806950e-02\n",
      "   4.14360128e-02 -2.38435455e-02 -9.27004516e-02  2.08146963e-02\n",
      "   8.52504969e-02  1.36477485e-01 -9.26664397e-02  3.86271253e-02\n",
      "  -1.42617908e-03  4.80767339e-02 -1.47143751e-01 -7.30358213e-02\n",
      "  -2.46497663e-03  7.57903010e-02  1.64676197e-02 -1.91478245e-02\n",
      "   7.53409192e-02  4.22846340e-02 -7.75278658e-02  5.42619899e-02]\n",
      " [-5.47437549e-01 -6.99312985e-01 -1.19841002e-01  6.23301640e-02\n",
      "   4.41853702e-02  3.00825164e-02 -6.28561666e-03 -6.25485957e-01\n",
      "  -5.62486351e-02 -2.74732243e-02 -2.90913191e-02  3.96541692e-03\n",
      "  -7.95966983e-01 -7.45319054e-02 -4.30810601e-01 -9.36802477e-04\n",
      "  -7.23041773e-01 -6.45024717e-01 -1.73672363e-01 -2.67643761e-02\n",
      "   3.40205692e-02 -7.04730213e-01 -1.15305454e-01 -8.48839819e-01\n",
      "  -1.10559344e-01  1.53113715e-02 -1.15047820e-01 -1.58584714e-01\n",
      "  -8.17162514e-01 -3.55699748e-01 -1.13151014e+00 -5.64512461e-02\n",
      "  -9.15224105e-03 -3.16781015e-03  9.71641578e-03 -8.06366503e-02\n",
      "  -1.77635960e-02  9.35552083e-03  1.91967227e-02  4.55818586e-02\n",
      "   1.64278913e-02 -5.20447612e-01 -1.34194955e-01  2.95150802e-02\n",
      "   4.68190275e-02  2.23159548e-02  4.29465100e-02  2.50987150e-02\n",
      "  -3.10026221e-02 -6.14983253e-02 -2.61341985e-02 -1.87731197e-03\n",
      "  -1.06354682e-02  3.82246599e-02 -6.19109673e-03 -1.61325857e-01\n",
      "   1.31552322e-02  4.77785841e-02  3.01339682e-02  1.64353382e-02\n",
      "  -2.44827531e-02  7.99139868e-03 -1.22840703e-01  2.26777401e-02\n",
      "  -2.41126996e-02 -8.71064067e-02  7.38607645e-02 -1.18035078e-02\n",
      "  -1.23668008e-01  3.09798550e-02 -7.16181025e-02  8.84107724e-02\n",
      "  -3.65309487e-03 -1.97137762e-02 -3.24720107e-02  3.73509824e-02\n",
      "   7.46266963e-03  3.67525443e-02  6.11129552e-02  1.63304359e-02\n",
      "   3.62403728e-02  6.08946346e-02  2.95685977e-02  4.16802280e-02\n",
      "   2.36650892e-02 -2.86465026e-02 -6.36905655e-02 -3.91420513e-01\n",
      "  -6.35423288e-02 -1.46957919e-01 -7.69793987e-02  6.91104354e-03\n",
      "  -4.71359104e-01 -5.25324881e-01 -1.33586481e-01 -5.90145364e-02\n",
      "  -2.45199092e-02 -8.35103020e-02 -1.28566310e-01 -1.03992537e-01\n",
      "  -3.83304060e-01 -7.31937289e-01 -5.96700788e-01 -7.17074692e-01\n",
      "  -6.40730619e-01 -8.83833051e-01 -6.59621119e-01 -6.05241835e-01\n",
      "  -6.76816761e-01 -3.02218735e-01 -6.66678667e-01 -5.72125435e-01\n",
      "  -5.10036469e-01 -4.88194376e-01 -3.83359522e-01 -4.84371781e-01\n",
      "  -6.79330587e-01 -4.58274513e-01 -5.40192664e-01 -6.36595428e-01\n",
      "  -5.36584318e-01 -9.01355624e-01 -5.59531271e-01 -1.83221236e-01\n",
      "  -4.53323394e-01 -6.32649004e-01 -4.67265397e-01 -2.98325390e-01\n",
      "  -6.61745429e-01 -1.37136206e-01 -5.87825894e-01 -1.71714470e-01\n",
      "  -8.18024337e-01 -1.12331104e+00 -1.77148208e-01 -1.01551259e+00\n",
      "  -6.88359082e-01 -4.93014365e-01 -4.23386544e-01 -2.54625157e-02\n",
      "  -2.02750694e-02 -3.00388169e-02 -8.17828655e-01 -3.30304921e-01\n",
      "  -4.91103865e-02 -9.55441073e-02 -9.43763137e-01 -1.77461430e-01\n",
      "  -1.15893170e-01 -6.09779000e-01 -4.39678729e-02 -6.47819228e-03\n",
      "  -1.00949205e-01 -1.00563345e-02  2.35019084e-02 -8.29396397e-03\n",
      "  -2.79125601e-01 -4.77996945e-01 -4.41533118e-01 -2.92240053e-01\n",
      "  -4.78005111e-01 -4.01666135e-01 -7.66454875e-01 -4.38255101e-01\n",
      "  -6.62588000e-01 -4.44254279e-01 -1.00065029e+00 -1.03150761e+00\n",
      "  -8.35149050e-01 -2.97148734e-01 -1.17206275e-02 -6.46509290e-01\n",
      "  -1.05912244e+00 -8.51352692e-01 -3.31469983e-01 -1.59110501e-01\n",
      "  -3.20685714e-01  4.24659848e-02  4.04982343e-02 -8.96769986e-02\n",
      "  -7.53817677e-01 -5.07144332e-01 -7.63440371e-01 -6.94581866e-01\n",
      "  -4.73536044e-01 -8.15177858e-01  4.77836244e-02 -2.74636120e-01\n",
      "  -1.01817596e+00 -5.46565592e-01 -9.50691342e-01 -8.77757430e-01\n",
      "  -5.94373763e-01 -9.33039114e-02 -2.94930767e-02 -2.16222629e-01\n",
      "   5.21895699e-02 -4.60528105e-01 -2.93890774e-01 -7.55003691e-01\n",
      "   1.66469961e-02  4.82484251e-02  6.85255304e-02  5.92632145e-02\n",
      "   5.01693077e-02  9.00250077e-02  4.61451486e-02  4.44059335e-02\n",
      "   3.37673649e-02  2.59305034e-02  2.80127637e-02  4.79160808e-03\n",
      "   1.69357713e-02  3.56264226e-02  1.22056296e-02  6.08877167e-02\n",
      "   5.59799075e-02  6.56711832e-02  8.23963434e-02  4.35629189e-02\n",
      "   8.91879648e-02  7.71044642e-02  4.37626168e-02 -2.32358854e-02\n",
      "   2.81879120e-02 -2.08632927e-03  9.16266963e-02  7.42822513e-02\n",
      "   2.90996972e-02  6.70115054e-02  7.83312693e-02  7.42879584e-02\n",
      "   6.70614466e-02  6.15244955e-02  7.72601888e-02  8.36334676e-02\n",
      "   8.05825368e-02  8.15442428e-02  7.90278763e-02  6.38533086e-02\n",
      "   6.49969876e-02  7.53991157e-02  6.79704845e-02  2.91080084e-02\n",
      "   2.20146663e-02  5.27935773e-02  4.65886034e-02  8.90420601e-02\n",
      "   8.54158849e-02  9.71274897e-02  7.78863207e-02  8.52094740e-02\n",
      "   8.08809549e-02  7.11740702e-02  5.96785471e-02  3.81958894e-02\n",
      "  -5.31310514e-02  7.15233982e-02  8.37628096e-02  4.81008328e-02\n",
      "  -5.65351844e-02  5.53728007e-02  3.99364345e-02  6.86877817e-02\n",
      "   3.89390737e-02  7.81770498e-02  3.97289656e-02  3.23497541e-02\n",
      "   4.81170006e-02  3.53415497e-02  4.88504283e-02  4.89276052e-02\n",
      "   4.80908118e-02 -1.27738127e-02  4.98182215e-02 -6.54429337e-03]\n",
      " [ 1.61454365e-01  1.18254445e-01  5.56717403e-02 -8.16551924e-01\n",
      "  -1.33401286e-02 -5.57275005e-02  5.38532995e-02  1.14659943e-01\n",
      "   8.11605006e-02  9.03481916e-02 -4.43477742e-02 -1.36058211e-01\n",
      "  -4.17638011e-02  4.30051461e-02 -1.14994524e-02  7.93099310e-03\n",
      "   7.63675049e-02  1.03829294e-01  7.17922673e-02  1.29330698e-02\n",
      "   1.36539899e-02  1.17247701e-01  5.39105125e-02  1.27847582e-01\n",
      "   1.04732662e-02 -1.88125595e-02  9.02466699e-02  9.80647877e-02\n",
      "   1.08446360e-01  6.28416389e-02  5.54611906e-02  5.07704765e-02\n",
      "  -1.87624410e-01  2.26640571e-02  4.65932898e-02  1.64105538e-02\n",
      "   3.46196555e-02  1.83648653e-02 -1.03636198e-02 -5.00009302e-03\n",
      "   3.42720374e-02  1.08683467e-01  7.23133683e-02 -1.62786648e-01\n",
      "  -3.56777459e-01 -5.66325225e-02  1.72353327e-01  1.69860404e-02\n",
      "   1.53756768e-01  7.36851245e-02 -2.08846778e-02  5.32440841e-02\n",
      "   7.82677904e-02 -2.35508844e-01 -3.17018256e-02 -1.00744054e-01\n",
      "  -1.73538234e-02 -5.35430387e-02 -1.02015756e-01 -1.42929420e-01\n",
      "   8.15166011e-02 -4.72369045e-01  7.65930563e-02  1.02217339e-01\n",
      "  -1.22967318e-01  9.61976424e-02 -2.60113865e-01 -4.64447364e-02\n",
      "  -3.69215384e-02 -2.21942775e-02 -7.71533640e-04  3.69049050e-02\n",
      "   9.11960676e-02 -2.98240274e-01 -1.93669528e-01  5.61556481e-02\n",
      "  -4.88863289e-02 -3.29357479e-03 -1.85715586e-01 -1.11184984e-01\n",
      "  -2.99718399e-02 -1.03416756e-01 -1.27601668e-01  3.94721441e-02\n",
      "   1.04326293e-01  4.27976809e-02  6.60614669e-02  3.45282885e-03\n",
      "   3.16387303e-02  6.39289320e-02 -2.42966469e-02 -2.78973073e-01\n",
      "   8.48746765e-03  5.53515255e-02  4.04735319e-02 -1.00471511e-01\n",
      "   4.67196964e-02  1.06709681e-01  6.84348047e-02 -1.44365951e-02\n",
      "   2.40706583e-03 -1.33133546e-01 -6.97373748e-02 -4.98147160e-02\n",
      "  -4.41410542e-01 -8.41826871e-02  2.24501826e-02  1.00321367e-01\n",
      "  -5.59233315e-02 -1.34604841e-01 -1.11676075e-01 -2.17503160e-01\n",
      "  -1.74009755e-01 -1.93093106e-01  2.00915225e-02 -9.92055461e-02\n",
      "  -1.13250643e-01 -1.42387748e-01 -1.25201747e-01 -1.83006182e-01\n",
      "  -2.11246535e-01 -8.69489536e-02  8.33106190e-02 -3.90124351e-01\n",
      "  -1.13096619e+00  1.21939644e-01 -1.31396309e-01 -5.71228266e-01\n",
      "   9.13760215e-02 -1.01332641e+00  3.36690061e-03 -8.05151880e-01\n",
      "  -9.38450575e-01  4.41183932e-02  2.80683674e-02 -6.81555510e-01\n",
      "  -2.27401599e-01 -8.07666838e-01 -4.43827808e-01  7.36560524e-02\n",
      "  -9.17089581e-01 -3.93537700e-01 -1.10594034e+00 -1.81161836e-01\n",
      "  -1.68065339e-01 -3.76981676e-01 -6.56271279e-02 -2.49517277e-01\n",
      "  -6.66214526e-01  5.32064810e-02 -1.18616395e-01 -4.08403546e-01\n",
      "  -1.04564559e-02 -2.42963195e-01 -5.48554420e-01 -9.81466770e-01\n",
      "   5.43202460e-02 -1.27264583e+00 -5.37156537e-02 -8.04231703e-01\n",
      "  -4.03465271e-01 -4.69961822e-01 -2.79190332e-01  3.12252203e-04\n",
      "  -1.87616274e-01 -1.02228200e+00  9.63684544e-02  2.85136048e-02\n",
      "  -1.01123461e-02 -9.57710221e-02 -3.27280574e-02  2.04205140e-01\n",
      "   2.35744826e-02 -4.04903367e-02  1.19061157e-01  1.01143733e-01\n",
      "   8.26665759e-03  1.06160820e-01 -7.23139167e-01  1.03884012e-01\n",
      "  -3.04553919e-02 -8.27613112e-04 -2.31866062e-01  1.00741580e-01\n",
      "   1.83087885e-01 -6.39128014e-02 -4.14443016e-01  2.64462233e-02\n",
      "   2.10902870e-01  5.03126495e-02 -1.45904496e-01 -1.29154921e+00\n",
      "  -9.98909235e-01  3.82955447e-02 -1.74876023e-03  5.55102415e-02\n",
      "  -2.71700054e-01 -1.67045280e-01 -4.08360958e-01 -1.36375397e-01\n",
      "   1.89539135e-01  1.02279142e-01 -2.10886896e-02  1.22971460e-01\n",
      "  -3.05617690e-01 -1.07206655e+00  1.22195438e-01 -5.35181999e-01\n",
      "  -1.33236960e-01  3.44612658e-01  7.19214231e-02  8.19782913e-02\n",
      "   9.13076773e-02  7.84542188e-02  5.59750758e-02  4.44108173e-02\n",
      "   3.69416058e-01 -2.38876402e-01 -5.45993209e-01 -3.47025156e-01\n",
      "  -1.11158562e+00 -2.93693662e-01 -5.14833368e-02  8.73679519e-02\n",
      "   3.75749543e-02  3.21916342e-02 -7.05000401e-01 -1.36199251e-01\n",
      "  -8.63581598e-02 -3.64576548e-01 -9.79346097e-01 -8.35537910e-02\n",
      "  -4.40858565e-02 -6.07897639e-01 -5.25237694e-02 -5.76868057e-01\n",
      "  -6.54712081e-01 -8.64005864e-01 -4.71529782e-01 -7.08577394e-01\n",
      "  -4.55167770e-01 -2.59968877e-01 -4.00988013e-03  4.29638959e-02\n",
      "  -6.94824532e-02 -1.74492359e-01 -1.55031428e-01 -7.90708005e-01\n",
      "  -7.19388843e-01 -6.07824922e-01 -1.76502675e-01 -1.06717968e+00\n",
      "  -1.02494204e+00 -1.31425428e+00 -1.03239134e-01 -4.51183692e-02\n",
      "   1.54728621e-01 -5.21795511e-01 -8.00003350e-01  1.91905379e-01\n",
      "   1.33975983e-01  5.90953603e-03 -4.90768194e-01 -4.64126654e-02\n",
      "  -4.77147132e-01 -5.83876967e-02 -1.93360373e-01 -2.40187913e-01\n",
      "   2.00229645e-01 -1.49984568e-01 -1.99251100e-01 -2.43691411e-02\n",
      "   1.08691908e-01  1.30371168e-01 -7.89092958e-01  3.58162560e-02]\n",
      " [-7.97859952e-02 -2.36555055e-01  3.31833884e-02  4.48970571e-02\n",
      "   1.71311963e-02 -1.72334015e-02 -9.77423787e-03 -6.63978755e-01\n",
      "  -6.92608878e-02  1.37138665e-02 -5.02131321e-02  2.34721676e-02\n",
      "  -1.07934937e-01 -5.78792067e-04 -5.41766472e-02 -1.15232705e-03\n",
      "  -7.72850394e-01 -1.37525427e+00 -1.44085312e+00 -6.17873296e-02\n",
      "  -7.92453229e-01 -5.55295825e-01 -1.57000685e+00 -6.52936220e-01\n",
      "  -9.01854157e-01 -1.26742435e+00 -1.38191342e+00 -7.52017021e-01\n",
      "  -6.06365919e-01 -1.23919964e+00 -1.02261925e+00 -8.74652565e-02\n",
      "   2.73247366e-03 -4.83901538e-02 -6.29409671e-01 -7.43534505e-01\n",
      "  -9.74737942e-01  5.03697386e-03 -7.88507983e-02 -9.67247561e-02\n",
      "  -2.67399162e-01 -5.11163712e-01 -8.44171047e-01 -4.28889811e-01\n",
      "   1.55653004e-02 -3.48515689e-01 -1.89796820e-01 -8.08566138e-02\n",
      "  -3.91698442e-02 -6.50869787e-01 -2.28328958e-01  5.34447143e-03\n",
      "  -2.67785132e-01 -4.27463092e-02 -3.03879708e-01 -2.41164505e-01\n",
      "  -8.75459164e-02 -6.43897578e-02 -8.29779580e-02 -2.51889275e-03\n",
      "  -5.07450700e-01 -3.09935324e-02 -3.27517241e-01 -2.37056673e-01\n",
      "  -8.20241496e-02 -1.87650084e-01 -2.11295813e-01 -2.00659052e-01\n",
      "  -3.27406257e-01 -2.60942429e-02 -3.96814764e-01 -5.40525854e-01\n",
      "  -3.36909711e-01 -1.51458845e-01  3.35551947e-02 -1.59334108e-01\n",
      "  -2.47738242e-01 -1.28604636e-01 -2.59109400e-02 -1.36794493e-01\n",
      "  -2.51475908e-03 -1.73933417e-01 -9.22798589e-02 -6.88299894e-01\n",
      "  -1.08497478e-01 -8.38918462e-02 -6.41437992e-02 -2.69805074e-01\n",
      "  -1.47538036e-02 -2.02914163e-01 -3.48666877e-01  2.27934793e-02\n",
      "  -3.76728863e-01 -3.29903185e-01 -8.35607499e-02 -1.20698720e-01\n",
      "  -2.89747924e-01 -2.72670358e-01 -1.38431206e-01 -4.73856956e-01\n",
      "  -9.73636732e-02 -1.19979464e-01 -3.60311233e-02 -5.00034615e-02\n",
      "  -3.16016749e-03 -8.61930773e-02  2.70341747e-02 -4.23264392e-02\n",
      "   1.30667221e-02 -1.31742787e-02 -7.31115937e-02 -2.98741534e-02\n",
      "   7.47987442e-03 -9.89062861e-02 -5.69625665e-03 -1.37162749e-02\n",
      "  -7.40140444e-03 -7.09345788e-02  6.14862097e-03 -2.16750037e-02\n",
      "  -1.92144103e-02 -1.38551801e-01  3.64712104e-02 -1.42849935e-02\n",
      "   5.84095903e-02  3.66381817e-02  7.63866082e-02  5.10334522e-02\n",
      "   2.70100962e-02  2.98754573e-02  2.07734462e-02 -1.11504449e-02\n",
      "   5.80721386e-02  4.98249047e-02  5.28896190e-02  6.67841658e-02\n",
      "   6.00080229e-02  6.28224015e-02  2.53553595e-02  4.32545058e-02\n",
      "   4.65485118e-02  5.64689040e-02  7.41662681e-02  1.10217296e-02\n",
      "   5.00875562e-02  5.43510355e-02  7.71981552e-02  4.99744304e-02\n",
      "   6.48803562e-02  5.86368889e-02  2.48867068e-02  5.51077239e-02\n",
      "   5.54786809e-02  4.87628765e-02  5.37602194e-02  4.56048958e-02\n",
      "  -2.41986359e-03  3.11581399e-02  1.92687381e-02 -9.78763774e-03\n",
      "  -2.34550700e-01  2.02342775e-02  3.02126054e-02 -2.08944619e-01\n",
      "  -1.52036606e-03 -2.45782938e-02 -4.36974056e-02  8.90040863e-03\n",
      "  -1.88810267e-02  1.60956960e-02  1.85015760e-02  6.77329663e-05\n",
      "  -9.41608727e-01 -7.26918951e-02  5.51573513e-03 -3.91466469e-02\n",
      "  -2.92288698e-03 -1.70463771e-02  6.64978521e-03 -7.04251276e-03\n",
      "   4.16470468e-02  1.54793523e-02  1.78579707e-02 -1.30091654e-02\n",
      "   3.45323160e-02  7.65483174e-03  2.39215214e-02  4.87001939e-03\n",
      "   6.99922675e-03  2.56117340e-02  5.66540770e-02  5.27691469e-02\n",
      "   5.27626276e-02  2.91902907e-02  3.70932333e-02  2.89175250e-02\n",
      "   1.65581405e-02 -3.82580534e-02  3.38252783e-02  4.94236387e-02\n",
      "  -6.12923086e-01 -8.63664269e-01 -8.26266587e-01 -7.90028512e-01\n",
      "  -1.03592849e+00 -5.02562046e-01 -5.40634632e-01 -1.28140354e+00\n",
      "  -7.09625304e-01 -6.22591197e-01 -1.19095230e+00 -2.70223953e-02\n",
      "   4.83897980e-03 -1.43930526e-03 -2.54859030e-02 -8.23155046e-01\n",
      "  -5.41557789e-01 -7.78880656e-01 -8.13219607e-01 -3.87656242e-01\n",
      "  -8.49339545e-01 -9.93910074e-01  1.76144093e-02  3.15787434e-03\n",
      "   1.99904833e-02 -6.44146046e-03 -7.15849042e-01 -7.99548328e-01\n",
      "  -1.07752991e+00 -3.32716793e-01 -9.61516321e-01 -8.59728694e-01\n",
      "  -8.74932647e-01 -3.71936738e-01 -4.32049721e-01 -6.98572457e-01\n",
      "  -6.39343083e-01 -6.78385317e-01 -8.34827423e-01 -1.33355081e+00\n",
      "  -3.46073061e-01 -8.89521778e-01 -8.66134107e-01  9.72310267e-03\n",
      "   7.30573758e-03 -5.56824803e-01 -2.71581501e-01 -6.10117257e-01\n",
      "  -8.10123503e-01 -7.18340576e-01 -9.59307730e-01 -5.77020168e-01\n",
      "  -7.77914882e-01 -7.22232834e-02 -3.08960327e-03 -6.66420609e-02\n",
      "  -2.39206314e-01 -4.12566448e-03 -5.49312606e-02 -7.31288493e-02\n",
      "  -8.79328698e-03 -8.02029908e-01 -9.96941745e-01 -4.07849669e-01\n",
      "  -1.85209766e-01 -2.49611005e-01 -1.12564981e+00 -6.11207783e-01\n",
      "  -3.09313595e-01 -3.93781364e-02 -4.24617290e-01 -3.93973827e-01\n",
      "  -2.10786778e-02 -3.91352661e-02 -5.48121870e-01 -1.11093558e-01]\n",
      " [-3.04550529e-01 -8.25789571e-02 -1.96824178e-01 -1.98441856e-02\n",
      "  -3.17884423e-02 -6.37222081e-02 -5.68263382e-02 -1.86155755e-02\n",
      "  -4.80971187e-02 -1.31701171e-01 -1.03867110e-02 -1.39719382e-01\n",
      "  -1.91736706e-02 -4.20384519e-02 -2.18547314e-01 -4.99424115e-02\n",
      "  -8.43823254e-01 -4.13285941e-01 -8.88904035e-01 -1.00103259e+00\n",
      "  -6.24480188e-01 -6.83483779e-01 -3.13762903e-01 -8.18789840e-01\n",
      "  -8.80891800e-01  1.71587430e-02 -2.95898139e-01 -5.45052998e-02\n",
      "  -1.03682324e-01 -2.41360798e-01 -5.86026073e-01 -7.43477106e-01\n",
      "  -3.02798271e-01 -2.53732651e-01  1.15986308e-02 -1.44179896e-01\n",
      "   3.19859423e-02 -1.09939732e-01 -3.22141230e-01 -1.03474833e-01\n",
      "  -3.69133949e-01 -9.07647729e-01  6.28195051e-03 -7.77159035e-02\n",
      "  -1.77840903e-01 -2.43805200e-01 -1.37634680e-01 -3.26078176e-01\n",
      "  -4.35064346e-01 -9.35215205e-02 -3.47543433e-02 -1.93512812e-01\n",
      "  -6.29456267e-02 -2.30488837e-01  5.85106015e-02 -2.67418087e-01\n",
      "  -1.34685814e-01  1.79518736e-03 -7.41528124e-02 -2.49008849e-01\n",
      "  -4.57567573e-01 -1.37933955e-01 -1.92616671e-01 -5.03164172e-01\n",
      "  -2.66489625e-01 -2.09282234e-01 -1.39051542e-01 -1.79244593e-01\n",
      "  -9.74018946e-02 -1.98548734e-01 -2.59940773e-01  1.24886343e-02\n",
      "  -1.09737203e-01 -2.58741826e-01 -2.00019240e-01 -1.41216412e-01\n",
      "  -2.62374908e-01  1.52694061e-02 -2.13063210e-01 -7.69162253e-02\n",
      "  -9.07791108e-02  7.69714415e-02 -6.81985170e-02 -4.35141735e-02\n",
      "  -2.13689476e-01 -1.63955927e-01 -5.37721105e-02 -1.29250549e-02\n",
      "  -1.19938254e-01 -6.74395338e-02  3.75170782e-02 -4.47800346e-02\n",
      "  -5.05991280e-01 -8.08202010e-03 -1.40334234e-01  1.10894954e-02\n",
      "  -8.47418047e-03 -6.18651547e-02 -1.59574989e-02 -1.67421177e-01\n",
      "  -4.41747606e-02 -5.14770858e-02 -1.22303747e-01 -1.30289719e-01\n",
      "  -2.32968759e-02 -3.39653715e-02 -6.82738602e-01 -6.96538806e-01\n",
      "  -1.30736798e-01  5.49099583e-04 -8.29154551e-02 -8.85852352e-02\n",
      "  -8.54499564e-02 -6.99719071e-01 -5.36157429e-01 -7.11984873e-01\n",
      "  -1.87532395e-01 -4.20842431e-02 -6.82765901e-01 -2.14432329e-01\n",
      "  -3.97601396e-01 -7.47075379e-01 -5.09868979e-01 -1.02182865e+00\n",
      "  -1.27299085e-01 -4.51338857e-01 -4.89745498e-01 -2.40605965e-01\n",
      "  -6.35108888e-01 -6.09206080e-01 -5.35470098e-02  7.37053761e-03\n",
      "  -4.02256995e-01 -7.10430861e-01 -7.17545629e-01 -4.42010790e-01\n",
      "  -4.77296442e-01 -6.81038260e-01 -3.16341706e-02 -8.43201205e-02\n",
      "  -7.96238407e-02 -6.10930443e-01 -5.28001606e-01 -1.03863133e-02\n",
      "  -9.15682077e-01 -5.74602783e-01 -3.58047694e-01 -9.51529741e-01\n",
      "  -9.72565353e-01 -6.94552302e-01 -1.24536529e-02 -3.18092436e-01\n",
      "  -9.19459403e-01 -1.69968799e-01 -2.47644149e-02  1.94479304e-03\n",
      "  -2.12829277e-01 -3.49283516e-01 -4.06278908e-01 -3.90139043e-01\n",
      "  -4.40798312e-01 -2.93932170e-01 -5.99097133e-01 -4.80583876e-01\n",
      "  -4.40117955e-01 -4.30502683e-01 -6.52506530e-01 -7.79821754e-01\n",
      "  -1.02139735e+00 -8.18149149e-01 -9.27626252e-01 -5.61538994e-01\n",
      "  -8.78434420e-01 -5.86417854e-01 -7.02711582e-01 -1.28978252e+00\n",
      "  -9.88418818e-01 -9.46420729e-01 -7.37709224e-01 -9.25211728e-01\n",
      "  -3.31303656e-01 -4.68957752e-01 -3.44978511e-01 -4.02987629e-01\n",
      "  -5.16265631e-01 -6.74906313e-01 -3.35392565e-01 -2.58178234e-01\n",
      "  -5.37297130e-01 -6.16531849e-01 -3.86328310e-01 -4.24645245e-01\n",
      "  -2.05245882e-01 -8.01323175e-01 -6.43535733e-01 -8.14544022e-01\n",
      "  -5.45272589e-01 -1.05071020e+00 -4.39924181e-01 -4.65469956e-01\n",
      "   6.73902258e-02  7.47949705e-02  7.83660561e-02  8.52939114e-02\n",
      "   3.00287269e-02  9.19426009e-02  5.67914136e-02  6.54000640e-02\n",
      "   4.10341993e-02  4.92768735e-02  4.78610285e-02  5.16043790e-03\n",
      "   2.04928406e-03  1.79466524e-03 -5.75008010e-03  8.10696632e-02\n",
      "   7.57517070e-02  8.77206177e-02  9.18583199e-02  5.37072010e-02\n",
      "   9.11656544e-02  8.62317383e-02  8.92555434e-03 -2.33486053e-02\n",
      "   1.93488989e-02  2.63124611e-03  9.72602963e-02  9.01422054e-02\n",
      "   4.68959622e-02  6.99598566e-02  9.34464559e-02  8.49558040e-02\n",
      "   8.32170323e-02  6.32219091e-02  8.70893598e-02  9.18435603e-02\n",
      "   9.46679711e-02  9.19742435e-02  9.43880230e-02  7.18985200e-02\n",
      "   5.82716912e-02  8.76709223e-02  8.89688432e-02  1.29084075e-02\n",
      "  -5.73904114e-03  6.06203973e-02  4.50465642e-02  9.12903324e-02\n",
      "   9.18453410e-02  9.77120325e-02  9.07417610e-02  9.26405117e-02\n",
      "   9.05507728e-02  6.52022287e-02  4.11673896e-02  3.40190791e-02\n",
      "  -1.52581995e-02  4.41104956e-02  6.24052808e-02  4.42427248e-02\n",
      "  -3.88136171e-02  8.73920545e-02  2.55560316e-02  4.80963662e-02\n",
      "   2.55212020e-02  4.26523648e-02  3.13065127e-02  2.22601276e-02\n",
      "   3.74920070e-02  2.46359557e-02  3.30815092e-02  3.38262767e-02\n",
      "   4.47850972e-02 -1.10419309e-02  3.01702600e-02 -1.48977973e-02]\n",
      " [ 9.62677822e-02  4.18606028e-02  5.92242181e-02 -4.02537823e-01\n",
      "   7.45176673e-02  4.52195257e-02  2.44398210e-02  5.48615493e-02\n",
      "   2.18341183e-02  3.80805023e-02 -3.19218240e-03  4.41131555e-02\n",
      "   4.65708561e-02  1.44856684e-02 -8.15744791e-03  4.24988344e-02\n",
      "   1.33631425e-02 -2.90097371e-02 -2.19668113e-02 -7.18594268e-02\n",
      "   2.03224532e-02  3.83993573e-02 -2.94923373e-02 -4.69877422e-02\n",
      "   3.92606743e-02  9.59623884e-03 -4.61202301e-03  2.77559012e-02\n",
      "   9.62855294e-03 -2.89662532e-03 -2.13556755e-02 -3.38463895e-02\n",
      "  -5.91712475e-01 -5.37929237e-02 -2.54177272e-01 -7.83027411e-02\n",
      "  -7.45584786e-01 -1.11584477e-01  1.13881506e-01  1.83010884e-02\n",
      "  -7.32594371e-01  5.76971881e-02 -3.05200350e-02 -1.22234023e+00\n",
      "  -5.98019719e-01 -2.08853688e-02 -1.35410464e+00 -1.13677278e-01\n",
      "  -3.83758783e-01 -1.39930591e-01  5.83179807e-03 -4.60379004e-01\n",
      "   4.40298114e-03 -1.07673609e+00  8.25124532e-02 -5.87041341e-02\n",
      "  -4.93037105e-01 -4.64655012e-01 -5.74258529e-03 -2.05741674e-01\n",
      "   1.77273434e-02 -8.12079430e-01  3.29822488e-02 -4.84326720e-01\n",
      "  -6.52942657e-01 -7.63349310e-02 -9.74006653e-01 -4.54915017e-01\n",
      "  -8.89715180e-02 -5.99112868e-01 -1.84728522e-02 -6.65737212e-01\n",
      "  -4.84919399e-01 -1.71990275e-01 -3.88369381e-01 -7.63577461e-01\n",
      "  -5.66959500e-01 -1.13813385e-01 -1.67941880e+00 -8.21726501e-01\n",
      "  -7.11929440e-01 -6.57046139e-01  1.47617050e-02 -2.75461137e-01\n",
      "  -3.22448350e-02 -4.57021385e-01  2.85237078e-02  1.08981058e-01\n",
      "   6.60154521e-02 -4.16445471e-02 -5.98620176e-02 -4.55076367e-01\n",
      "  -4.59779680e-01  1.48213310e-02 -1.04677022e-01 -3.21042180e-01\n",
      "  -7.23119676e-02 -4.81725857e-02 -1.43668801e-01 -2.65568998e-02\n",
      "  -5.07170148e-02  3.28400321e-02  9.60451588e-02  1.02706533e-02\n",
      "   3.15036811e-02  7.96892345e-02 -8.55912566e-02  3.38421166e-02\n",
      "   1.08859614e-01  3.78449745e-02  4.24225628e-02 -1.05561353e-02\n",
      "  -5.68694286e-02  8.63366947e-02  9.16724578e-02  9.03426260e-02\n",
      "   2.26307334e-03  1.24048814e-02  1.12060132e-02  1.63567998e-02\n",
      "   2.14640750e-03 -2.39256889e-01  6.55981302e-02 -3.92043918e-01\n",
      "   1.23237476e-01 -5.54767065e-02 -2.71979012e-02  1.91276878e-01\n",
      "  -1.01665288e-01 -2.20746905e-01 -6.20640591e-02 -4.43862472e-03\n",
      "  -1.40471548e-01 -6.84298947e-02 -5.42932115e-02 -1.71943188e-01\n",
      "  -1.40499577e-01 -2.65258789e-01  6.79706335e-02 -6.93622380e-02\n",
      "  -8.42670351e-02 -2.58469701e-01 -2.76180923e-01  1.25274435e-02\n",
      "  -5.14485240e-02  1.11773228e-02 -8.41884315e-02 -3.33607793e-01\n",
      "  -1.89944565e-01  1.76337704e-01 -1.40702184e-02 -9.00181711e-01\n",
      "   7.95245096e-02 -1.79154426e-01 -8.06612670e-01 -7.80206621e-02\n",
      "   2.42378116e-02 -6.37467345e-03  3.18757333e-02  3.35336104e-02\n",
      "   1.52896330e-01 -3.98955047e-02 -4.00513709e-02  2.79605445e-02\n",
      "   2.99302731e-02  5.03599979e-02 -3.70934755e-02  1.04989680e-02\n",
      "   5.94731160e-02 -5.39789815e-03  1.94024481e-02  1.04680993e-01\n",
      "  -1.85420867e-02 -3.94033501e-03 -1.30923418e-02  1.54064046e-02\n",
      "  -1.85203422e-02 -1.16043398e-02 -7.30715394e-02 -5.01973070e-02\n",
      "   5.94122671e-02  8.21930245e-02 -3.62305529e-02  9.73479822e-02\n",
      "   5.76787665e-02  4.65837494e-02 -2.71035526e-02  1.14711709e-01\n",
      "   7.90371224e-02  1.08287625e-01  1.34890288e-01 -1.55318864e-02\n",
      "   5.30940816e-02  1.34585975e-02 -3.00050676e-02 -6.47736341e-03\n",
      "  -1.27684936e-01 -3.36870775e-02 -3.70381139e-02  1.12019316e-03\n",
      "  -5.57608344e-02 -1.66755199e-01 -4.04567383e-02 -2.94903815e-01\n",
      "  -1.73715413e+00 -3.36726069e-01 -2.35538527e-01  4.39952835e-02\n",
      "  -6.73997328e-02 -4.36472774e-01 -5.05921319e-02 -5.77662364e-02\n",
      "   1.99535228e-02  2.70360475e-03  5.05893752e-02  3.21186855e-02\n",
      "  -2.52024204e-01 -1.99633032e-01 -2.81167537e-01  1.11563951e-01\n",
      "  -1.33265913e-01  4.75907475e-02  2.26941053e-02 -3.25514004e-02\n",
      "  -4.63038404e-03 -1.56435505e-01 -8.08722854e-01 -1.67617381e-01\n",
      "   1.71204939e-01 -6.38267457e-01 -4.73258853e-01 -3.22439730e-01\n",
      "  -1.86680049e-01 -1.56131655e-01 -4.48064446e-01 -2.75766134e-01\n",
      "  -5.92001453e-02 -2.98439264e-01 -3.59050900e-01 -2.59849906e-01\n",
      "  -3.43082547e-01 -1.51256487e-01 -2.62473583e-01  1.55886719e-02\n",
      "  -8.34289640e-02 -4.12909627e-01 -7.04497471e-02 -6.50509417e-01\n",
      "  -1.27744842e+00 -6.12042665e-01 -2.02281680e-02 -5.64170063e-01\n",
      "   1.51103199e-01 -5.12032285e-02 -4.97729838e-01 -2.22465590e-01\n",
      "  -2.71830782e-02 -6.52797446e-02 -1.59263408e+00 -2.58513749e-01\n",
      "   1.34896478e-02 -2.13537917e-01 -1.98581621e-01 -8.87017250e-01\n",
      "  -4.46120322e-01 -3.00793052e-01 -2.81693250e-01  1.77656993e-01\n",
      "  -3.87536973e-01  3.00598238e-02 -4.01077479e-01 -3.06976914e-01\n",
      "  -5.36333621e-01 -4.13563669e-01 -4.39641446e-01  2.07990631e-02]\n",
      " [-2.57573687e-02  4.58521843e-02 -6.38985157e-01 -6.20119691e-01\n",
      "  -6.71492934e-01 -5.35676479e-02 -4.37776819e-02  5.81578910e-02\n",
      "   2.15688348e-02 -8.91971290e-01  1.81926768e-02 -6.73065305e-01\n",
      "   3.48391011e-02  3.95748939e-05 -4.54400539e-01 -9.36482191e-01\n",
      "   5.08968905e-02  3.14310705e-03  2.54453830e-02  2.20215158e-03\n",
      "  -1.32661816e-02  5.65364324e-02  2.24841814e-02 -5.49129583e-02\n",
      "  -2.63994318e-02  1.55416047e-02  2.44361013e-02  7.44941225e-03\n",
      "   5.93236499e-02  2.37095021e-02  4.57060672e-02  7.38832355e-03\n",
      "   3.30071636e-02 -5.76353550e-01 -1.26487717e-01 -1.68666914e-01\n",
      "  -4.85609770e-02 -8.92819285e-01 -1.42757565e-01 -3.97101016e-04\n",
      "  -6.42895937e-01  4.58986461e-02 -1.40806407e-01 -4.59838867e-01\n",
      "  -1.20840609e+00 -1.75546035e-01 -7.85412669e-01 -8.91763270e-01\n",
      "  -7.55768120e-01  3.78090743e-04  6.22411584e-03 -3.00218850e-01\n",
      "   8.88816547e-03 -1.25052619e+00 -3.05278033e-01 -2.86953580e-02\n",
      "  -7.55947232e-01 -1.56734452e-01 -1.29333460e+00 -1.01037085e+00\n",
      "   1.59795508e-02 -1.07718788e-01  2.65665688e-02 -5.62026620e-01\n",
      "  -1.01890314e+00  6.18683035e-03 -9.50659990e-01 -1.82724163e-01\n",
      "  -3.32389995e-02 -1.18051124e+00  1.47683006e-02 -1.48257047e-01\n",
      "  -1.34294853e-01 -9.54128146e-01 -8.64314437e-02 -2.11687431e-01\n",
      "  -6.75058603e-01 -3.70568991e-01 -7.89438963e-01 -1.77969709e-01\n",
      "  -3.07990104e-01 -3.73473465e-01 -5.20606935e-01 -7.34589994e-02\n",
      "  -5.09506524e-01 -7.12224782e-01 -1.15330672e+00 -1.60181180e-01\n",
      "  -2.85264105e-01 -8.81879866e-01 -1.64716765e-01 -2.27177620e-01\n",
      "  -2.97788680e-01 -1.94791451e-01 -4.02294159e-01 -9.03926909e-01\n",
      "  -2.56906778e-01 -7.80593157e-02 -4.99525666e-03  6.22522226e-03\n",
      "   4.56828391e-04  2.99173389e-02 -1.51052713e-01 -1.45299574e-02\n",
      "   4.87643927e-02  3.61279547e-02 -9.28576410e-01 -8.86576530e-03\n",
      "  -4.70501818e-02 -5.18922243e-05 -2.49580238e-02  1.06383907e-02\n",
      "  -5.69205523e-01 -8.83922279e-01 -5.24669468e-01 -2.53745139e-01\n",
      "  -1.06179975e-01  2.10670382e-02 -3.50918084e-01 -3.53789628e-01\n",
      "  -5.86356759e-01 -8.00610125e-01 -6.19311273e-01 -1.20123172e+00\n",
      "  -9.92194176e-01 -6.59380853e-01 -4.40095097e-01 -7.57154346e-01\n",
      "  -7.80626893e-01 -9.19710457e-01 -2.34286487e-01  2.99977455e-02\n",
      "  -2.80104935e-01 -4.63003635e-01 -5.54165483e-01 -6.08227611e-01\n",
      "  -5.77161670e-01 -3.85847837e-01 -4.70718108e-02 -4.89492923e-01\n",
      "  -5.38136959e-01 -3.63032997e-01 -2.21149519e-01 -2.59032454e-02\n",
      "  -4.85241503e-01 -4.83529955e-01 -3.90789449e-01 -4.67266560e-01\n",
      "  -3.08340400e-01 -5.13875008e-01  5.67468675e-03 -4.59467381e-01\n",
      "  -4.31157738e-01 -4.91766989e-01 -5.22016585e-01 -4.73707557e-01\n",
      "  -6.90543056e-01 -9.30707932e-01 -5.97272158e-01 -1.65742069e-01\n",
      "  -2.09736079e-02 -6.30778432e-01 -4.11681056e-01  2.72162212e-03\n",
      "  -6.01638675e-01 -3.21658880e-01 -1.06206104e-01 -4.74429168e-02\n",
      "   1.42862629e-02 -5.46421111e-01 -7.30807960e-01 -8.18013847e-01\n",
      "  -3.71869677e-03 -5.37246913e-02 -1.16647363e-01 -3.35765556e-02\n",
      "  -2.63119459e-01 -7.21668303e-01 -1.75886184e-01 -4.92012501e-01\n",
      "  -7.15680420e-01 -8.45884740e-01 -4.55035537e-01 -3.01849842e-01\n",
      "  -1.07351756e+00 -4.58518356e-01 -6.09007359e-01 -3.44990939e-01\n",
      "  -6.48898602e-01 -7.80849814e-01 -4.91085261e-01 -4.38997418e-01\n",
      "  -9.58065331e-01 -6.47356153e-01 -5.75934947e-01 -6.59966171e-02\n",
      "  -5.56616664e-01 -6.31989956e-01 -6.84473455e-01 -7.31750011e-01\n",
      "   6.58336431e-02  8.35501477e-02  8.75159130e-02  8.82467479e-02\n",
      "  -4.48560305e-02  9.46188271e-02  5.58210090e-02  6.46328703e-02\n",
      "   4.56400625e-02  5.81713878e-02  4.24543992e-02  6.69364352e-03\n",
      "  -7.44413733e-01 -6.07780278e-01 -4.60485555e-02  7.73030967e-02\n",
      "   8.40283856e-02  8.70865583e-02  9.20992866e-02  4.11616825e-02\n",
      "   8.98903087e-02  8.27401727e-02 -6.39102697e-01 -6.95220530e-01\n",
      "  -7.10408688e-01 -3.17690931e-02  9.77388546e-02  9.05828625e-02\n",
      "   4.79953550e-02  6.46098554e-02  9.09020156e-02  9.09899324e-02\n",
      "   8.25117379e-02  6.14325851e-02  9.10338387e-02  8.89166519e-02\n",
      "   8.95885229e-02  8.53428394e-02  9.15726274e-02  7.75846094e-02\n",
      "   4.91183773e-02  8.54459926e-02  8.61460268e-02 -6.50563776e-01\n",
      "  -1.68055043e-01  4.96936031e-02  3.86384204e-02  9.08278748e-02\n",
      "   9.32509005e-02  1.06712513e-01  8.95191357e-02  9.22377259e-02\n",
      "   8.78948495e-02  5.83077669e-02 -3.97873390e-03  3.87358479e-03\n",
      "  -4.11626026e-02 -2.67660264e-02  1.08876247e-02  5.26831811e-03\n",
      "  -6.71481311e-01  6.04642518e-02 -9.37868841e-03 -3.28584522e-01\n",
      "   6.76197605e-03 -5.77659488e-01 -2.55418103e-02  7.98244588e-03\n",
      "  -1.79208890e-02 -5.17857075e-03  2.06480362e-03  2.33974680e-02\n",
      "  -5.78442030e-03 -4.89278287e-02 -1.02344407e-02 -8.92956182e-02]\n",
      " [ 1.10544385e-02 -5.36090098e-02  4.20581065e-02  3.12008709e-02\n",
      "   7.07606645e-03 -1.16322804e-02 -1.60637852e-02 -7.71142244e-02\n",
      "  -2.03502998e-02  1.17455898e-02 -1.41987605e-02  4.70104569e-04\n",
      "   9.99040343e-03 -3.52632231e-03 -1.06590167e-02 -5.15792461e-04\n",
      "  -7.87380517e-01 -6.12488270e-01 -1.05901325e+00 -1.55740753e-01\n",
      "  -1.36027658e+00 -5.45279026e-01 -6.17701530e-01 -6.57782257e-02\n",
      "  -1.01470959e+00 -1.05111873e+00 -1.47444093e+00 -4.50259328e-01\n",
      "  -4.53053445e-01 -5.62128186e-01 -9.00242150e-01 -2.63255209e-01\n",
      "  -1.09349802e-01 -6.94489256e-02 -6.14198387e-01 -3.43472034e-01\n",
      "  -1.21139038e+00 -1.78236626e-02 -4.82628226e-01 -1.01547301e+00\n",
      "  -1.25377047e+00 -8.58343482e-01 -2.25602910e-01 -8.40854704e-01\n",
      "  -3.85938346e-01 -1.41252232e+00 -6.30739927e-01 -8.82769287e-01\n",
      "  -8.68555233e-02 -8.05464506e-01 -3.83882850e-01 -6.61703348e-01\n",
      "  -3.27782422e-01 -1.02393949e+00 -2.79341638e-01 -3.97759050e-01\n",
      "  -1.17387861e-01 -2.27871224e-01 -5.10514200e-01 -1.53516695e-01\n",
      "  -1.12343109e+00 -2.76641309e-01 -1.00160456e+00 -7.18826652e-01\n",
      "  -1.35329470e-01 -1.41351134e-01 -1.23605466e+00 -1.01325655e+00\n",
      "  -1.57785907e-01 -4.92746621e-01 -8.74940217e-01 -8.76370549e-01\n",
      "  -3.18207711e-01 -1.61046967e-01 -3.32249224e-01 -1.51547372e+00\n",
      "  -1.44268441e+00 -5.29182613e-01 -6.54216707e-01 -1.21970129e+00\n",
      "  -2.35126317e-01 -8.72855842e-01 -1.27050489e-01 -8.28826547e-01\n",
      "  -2.57637739e-01 -2.26980358e-01 -4.69725765e-02 -1.01057529e-01\n",
      "  -1.07109547e-01 -5.69038242e-02 -1.06182024e-01  1.36663215e-02\n",
      "  -3.72870058e-01 -7.59284198e-02 -6.30806247e-03 -1.28559098e-01\n",
      "  -1.59305051e-01 -8.76829922e-02 -1.75099835e-01 -2.21930414e-01\n",
      "  -2.92666722e-02 -7.70968646e-02  3.26232589e-03 -4.72744033e-02\n",
      "  -2.02894900e-02 -2.03301739e-02  3.53312306e-02 -4.51187193e-02\n",
      "   1.42508140e-02  3.61218490e-02 -3.08181960e-02 -3.29887308e-02\n",
      "   3.35150212e-02 -7.32771456e-02 -1.92311462e-02 -1.41252708e-02\n",
      "   2.60649621e-02 -4.63911556e-02  2.94051412e-02 -1.54330907e-02\n",
      "  -3.06739169e-03 -5.47558293e-02  7.05978349e-02 -3.37589309e-02\n",
      "   7.51965642e-02  5.58290221e-02  8.78091231e-02  7.83025324e-02\n",
      "   3.97519507e-02  5.12272455e-02  1.68547276e-02  3.35826874e-02\n",
      "   8.38057622e-02  6.68803826e-02  5.58232144e-02  7.39826337e-02\n",
      "   6.38879091e-02  8.83597955e-02  4.05083150e-02  6.65877908e-02\n",
      "   5.69003411e-02  6.84097186e-02  9.14177001e-02  9.30321927e-04\n",
      "   5.54936118e-02  6.08017817e-02  8.76073912e-02  6.65726364e-02\n",
      "   7.20182955e-02  7.64522105e-02  1.40284626e-02  7.63074011e-02\n",
      "   7.19327927e-02  6.01591393e-02  6.97429925e-02  5.29633537e-02\n",
      "  -8.81707482e-03  2.02795733e-02  8.92348029e-03 -3.62234078e-02\n",
      "  -1.05605207e-01 -2.88413838e-03 -6.04657980e-04 -1.43969327e-01\n",
      "  -1.07412953e-02 -1.22061493e-02 -4.57006134e-02 -6.37246901e-03\n",
      "  -5.48750311e-02 -1.63270179e-02 -2.45958064e-02  4.04335326e-04\n",
      "  -5.65479159e-01 -8.50325227e-02 -4.48307209e-02 -1.16374381e-01\n",
      "  -4.78061959e-02 -4.64440361e-02 -4.13323082e-02 -5.24407104e-02\n",
      "   2.68594138e-02  3.08594368e-02  1.31096467e-02 -1.44303925e-02\n",
      "   2.81797405e-02 -4.17372072e-03 -4.63306010e-02  1.28655070e-02\n",
      "   6.28058612e-03  8.03019758e-03  4.70606051e-02  5.07707894e-02\n",
      "   5.91831654e-02  3.39762378e-03  4.64822957e-03 -2.58755498e-02\n",
      "  -1.76907443e-02 -1.44103663e-02  2.30252054e-02  3.83293070e-02\n",
      "  -1.46627888e-01 -3.68655920e-01 -3.50896984e-01 -9.44874644e-01\n",
      "  -5.21532118e-01 -7.64107466e-01 -3.97191256e-01 -3.00430089e-01\n",
      "  -7.64985085e-02 -2.02092171e-01 -1.94358215e-01  2.66264305e-02\n",
      "   3.48542966e-02  2.25983243e-02 -1.92824192e-03 -2.78325975e-01\n",
      "  -3.47768486e-01 -2.63801545e-01 -9.75063920e-01 -4.49967057e-01\n",
      "  -5.05611956e-01 -2.39095181e-01  4.43776138e-02  4.28562425e-02\n",
      "   5.04835881e-02  1.21691423e-02 -2.63999760e-01 -3.71483058e-01\n",
      "  -2.24998459e-01 -5.50024092e-01 -7.69980848e-01 -1.57355845e-01\n",
      "  -3.21505964e-02 -3.84095132e-01 -1.10524368e+00 -6.21372998e-01\n",
      "  -4.70661759e-01 -1.75684556e-01 -5.24905026e-01 -5.58905125e-01\n",
      "  -3.58183801e-01 -1.98452517e-01 -2.70560354e-01  3.77012715e-02\n",
      "   3.02710887e-02 -8.76665533e-01 -1.35360539e-01 -3.77250165e-01\n",
      "  -5.42866588e-01 -6.64740622e-01 -3.73203605e-01 -5.95840812e-01\n",
      "  -8.28115582e-01 -7.92606100e-02  4.45357822e-02 -1.21255547e-01\n",
      "  -2.76137199e-02 -9.36097652e-03 -1.11384310e-01 -3.68419029e-02\n",
      "   3.35933715e-02 -1.03579544e-01 -2.51704097e-01 -1.73218682e-01\n",
      "  -6.75658166e-01 -6.25054955e-01 -1.74806997e-01 -1.02368462e+00\n",
      "  -7.65770435e-01  7.10413791e-03 -6.51670039e-01 -1.82705268e-01\n",
      "   4.00557704e-02  1.83607303e-02 -5.29512286e-01 -2.60984036e-03]\n",
      " [ 1.23053230e-01  4.94039245e-02 -8.03264678e-02 -7.08722591e-01\n",
      "  -1.92333445e-01  1.15225120e-02  2.02054884e-02  9.39230248e-02\n",
      "   5.78479022e-02 -8.36422294e-02  5.41885234e-02 -4.43512723e-02\n",
      "   8.06093141e-02 -2.03281287e-02  5.41994795e-02  3.65755185e-02\n",
      "   8.54690522e-02 -1.04286131e-02  4.30147350e-02 -2.35113539e-02\n",
      "  -5.95212281e-02  8.45158398e-02  2.65333839e-02  7.28395060e-02\n",
      "  -1.04760624e-01  1.89607050e-02  2.86697913e-02  4.00040485e-02\n",
      "   9.35309976e-02  3.05298418e-02  4.24220189e-02  5.72807994e-03\n",
      "  -3.65245074e-01 -3.65389744e-03 -3.04129958e-01 -5.59144020e-02\n",
      "  -2.08754957e-01 -1.54752424e-02 -2.86453068e-01  2.94704027e-02\n",
      "  -5.19032538e-01  8.86862278e-02  1.07733347e-02 -1.14818168e+00\n",
      "  -9.46421325e-01 -7.00207472e-01 -8.16710413e-01 -1.10394728e+00\n",
      "  -5.90748966e-01 -1.83079764e-02 -1.42488629e-03 -5.72579861e-01\n",
      "  -3.18633422e-04 -1.16701484e+00 -5.46928123e-03 -1.59861013e-01\n",
      "  -9.54437196e-01 -1.23238552e+00 -3.18636179e-01 -1.12021458e+00\n",
      "   1.09928995e-02 -8.58142972e-01  3.21827680e-02 -1.01085401e+00\n",
      "  -3.64764899e-01  5.46927191e-02 -1.28131318e+00 -8.56702387e-01\n",
      "   2.65558492e-02 -1.31920385e+00  4.07949202e-02 -7.39680350e-01\n",
      "  -2.98714936e-01 -2.25106537e-01 -6.27898097e-01 -4.71574694e-01\n",
      "  -1.61432016e+00 -1.50493875e-01 -1.79941893e+00 -1.02367401e-01\n",
      "  -3.75804901e-01 -1.13661313e+00 -2.42047776e-02 -1.80948898e-01\n",
      "  -1.83038846e-01 -9.24000084e-01  3.57998237e-02  1.01798423e-01\n",
      "   5.92887867e-03  5.26971184e-02 -1.72654726e-02 -2.51858830e-01\n",
      "  -2.99066097e-01  1.16629623e-01 -4.05448116e-02 -9.87754017e-02\n",
      "  -7.16973916e-02  3.36069651e-02 -3.24458443e-02  1.73707190e-03\n",
      "   2.73246765e-02  3.89920175e-02  4.70244735e-02  4.47656773e-03\n",
      "   3.91690852e-03  5.85202314e-02 -1.28221124e-01 -6.16860436e-03\n",
      "   2.48719398e-02 -3.31411399e-02  5.19171543e-02  1.81942992e-02\n",
      "  -3.64287533e-02 -3.69034149e-02 -3.43945771e-02  2.22445428e-02\n",
      "   7.23241419e-02  1.75483543e-02 -6.60365000e-02 -2.37945491e-03\n",
      "  -3.55289802e-02 -1.60509422e-01  7.42209554e-02 -9.30012643e-01\n",
      "  -7.65174508e-01  3.93527783e-02  9.18533802e-02 -3.82654577e-01\n",
      "   1.82428546e-02 -5.97922921e-01 -9.16237980e-02  7.31373355e-02\n",
      "  -2.51052156e-02 -8.71832818e-02 -1.85237546e-02 -2.78615236e-01\n",
      "  -3.34435523e-01 -2.06975877e-01 -4.99271415e-02  4.19525169e-02\n",
      "  -5.73369041e-02 -1.09284973e+00 -1.09021115e+00  9.87276994e-03\n",
      "  -9.65483427e-01 -4.03798074e-01 -2.62204170e-01  5.78803010e-02\n",
      "  -4.01878297e-01 -2.31188759e-01 -8.12163800e-02 -2.28855282e-01\n",
      "  -3.00299991e-02 -4.23133671e-02 -1.04217827e+00 -6.11826181e-01\n",
      "  -3.73570830e-01 -1.55018806e-01 -4.69247103e-01 -1.30502924e-01\n",
      "  -2.41182998e-01 -1.59397900e+00 -1.22797608e+00 -1.71295717e-01\n",
      "  -2.74330825e-01 -2.49579206e-01 -6.12912849e-02 -1.28118306e-01\n",
      "  -3.79818715e-02 -1.59606159e-01 -1.12126723e-01 -8.65212679e-02\n",
      "   4.84323986e-02 -1.42082378e-01 -5.43808974e-02 -4.48178984e-02\n",
      "  -2.47545596e-02 -1.83837041e-01 -4.27981585e-01 -2.10218087e-01\n",
      "  -2.48106614e-01 -2.27542028e-01 -4.83986884e-01 -2.70154625e-02\n",
      "  -2.34167695e-01 -1.69544920e-01 -8.65664005e-01 -2.52997547e-01\n",
      "   6.45381026e-03 -2.78555006e-01 -6.79874539e-01 -1.03048730e+00\n",
      "  -2.05945268e-01 -9.59006906e-01 -1.00317132e+00 -7.91461319e-02\n",
      "  -1.08377016e+00 -2.52659261e-01 -2.60577887e-01 -6.30785108e-01\n",
      "   1.49821684e-01  6.68555871e-02  8.90273750e-02  6.77794293e-02\n",
      "  -1.92878693e-01  2.48005427e-02  1.12450793e-02  1.35698825e-01\n",
      "   1.29051479e-02 -2.93536633e-02  4.18186001e-02  4.46014702e-02\n",
      "   4.38103266e-02  5.81528544e-02 -2.38147401e-03  3.56321596e-02\n",
      "   9.09658223e-02  1.00762054e-01  8.17708895e-02  6.45990670e-02\n",
      "  -6.67726696e-02 -6.46650651e-03 -3.70672420e-02  3.80646214e-02\n",
      "  -2.81415600e-02 -1.11570463e-01  7.05933869e-02  2.16270369e-02\n",
      "   1.47439763e-01 -6.98865801e-02  1.14714773e-02  2.99242456e-02\n",
      "   5.01181670e-02  8.54599327e-02  5.52028269e-02  4.83804606e-02\n",
      "   2.47438229e-03  3.79328169e-02  4.26062495e-02  1.10239774e-01\n",
      "  -3.39336111e-03  9.75073955e-04  9.58559662e-02  3.00223734e-02\n",
      "   2.84011979e-02 -2.36153863e-02  9.84238237e-02  1.44978510e-02\n",
      "   4.99162413e-02  3.93915214e-02  2.59531662e-02  1.94711871e-02\n",
      "  -2.92552449e-02  1.20635696e-01 -2.49496326e-01 -4.92408872e-02\n",
      "   8.84187967e-02 -5.66504784e-02 -3.99532735e-01 -9.18463022e-02\n",
      "   8.66982713e-02  1.41114146e-01 -4.94908988e-02 -2.87140816e-01\n",
      "  -9.76711605e-03 -4.24504966e-01  2.50435937e-02 -3.06164566e-02\n",
      "  -4.06671949e-02  7.45109245e-02 -1.23927653e-01 -5.26629612e-02\n",
      "  -1.31035700e-01 -3.81407291e-02 -1.75145734e-02  8.48584473e-02]\n",
      " [-5.59984267e-01 -6.82627082e-01 -9.92738456e-03  5.34020998e-02\n",
      "  -1.24357872e-01 -2.28802711e-01 -1.75561234e-01 -5.19108653e-01\n",
      "  -4.37404424e-01 -6.52006012e-04 -2.26530254e-01  7.99639709e-03\n",
      "  -9.10081923e-01 -4.57529277e-02 -1.08816408e-01  1.56409442e-02\n",
      "  -8.12622011e-01 -9.05821249e-02 -1.53647229e-01 -8.15788582e-02\n",
      "  -6.35310039e-02 -8.86265814e-01 -1.75141916e-01 -2.02043504e-01\n",
      "   2.70733777e-02 -2.36252338e-01 -2.03092009e-01 -2.04317197e-01\n",
      "  -8.57067943e-01 -1.09308571e-01 -4.14367497e-01 -8.59474018e-02\n",
      "  -1.61139835e-02  3.53300832e-02 -2.47457270e-02 -1.88391693e-02\n",
      "  -8.24197158e-02  6.78471327e-02 -5.53927235e-02 -1.21374704e-01\n",
      "   9.51129124e-02 -2.25718588e-01 -1.31653443e-01  5.93146086e-02\n",
      "   4.33296338e-02  5.73184434e-03  6.19242899e-02  1.10815205e-01\n",
      "   5.22540323e-02 -1.13196425e-01 -4.07022424e-02  4.14549075e-02\n",
      "  -1.87881395e-01  5.36479652e-02 -5.42734377e-03  8.50906968e-02\n",
      "   2.35738717e-02  1.94015186e-02  5.01505584e-02  8.17204714e-02\n",
      "  -2.18463972e-01  4.37908322e-02 -1.58633828e-01  1.32927328e-01\n",
      "   9.45300013e-02 -1.56144306e-01  7.22953975e-02  4.81650792e-02\n",
      "  -5.53606115e-02  1.05514891e-01 -2.18740925e-01 -6.45693839e-02\n",
      "   3.65958512e-02  4.32078205e-02  1.45974830e-02 -2.09221598e-02\n",
      "   8.27529952e-02 -5.94085827e-03  5.22023812e-02  6.44112825e-02\n",
      "   5.67542948e-02  5.01674749e-02 -2.25761440e-02 -3.12908343e-03\n",
      "   5.85840046e-02  2.92986929e-02 -2.73472909e-02 -6.99925870e-02\n",
      "  -7.98656419e-02 -5.37899993e-02 -7.92298317e-02 -1.04422905e-01\n",
      "   8.35667923e-02 -6.13508373e-02  2.23230720e-02 -9.94722638e-03\n",
      "  -9.00412127e-02 -1.12831712e-01 -1.29148021e-01 -9.58437920e-02\n",
      "  -1.87731758e-01 -7.50930190e-01 -1.60662651e-01 -2.12436393e-01\n",
      "  -6.33424938e-01 -5.49709201e-01  4.78945263e-02 -1.44379079e-01\n",
      "  -4.86425698e-01 -4.82635796e-02 -7.83488452e-02 -1.46976948e-01\n",
      "   8.14758459e-05 -6.59118732e-03 -6.55931607e-02 -1.06678240e-01\n",
      "  -1.76003911e-02 -5.80156920e-03  1.31140146e-02 -1.17807742e-02\n",
      "   1.09335606e-03  5.98302819e-02  3.67962196e-02  3.42560373e-02\n",
      "   9.44864824e-02  2.32311487e-02  9.73637328e-02  8.88718814e-02\n",
      "   2.87710913e-02  5.54050282e-02  7.57789612e-03 -5.04102647e-01\n",
      "   9.44543406e-02  6.89233691e-02  5.35029508e-02  9.41367820e-02\n",
      "   6.55685589e-02  8.32854509e-02  5.72802313e-02  2.46875733e-02\n",
      "   4.11601625e-02  6.33502677e-02  1.07838370e-01  2.78717838e-02\n",
      "   6.35051206e-02  6.98288679e-02  1.16283469e-01  7.10097849e-02\n",
      "   7.00013041e-02  6.92555383e-02  1.77883133e-02  6.19065762e-02\n",
      "   3.46106142e-02  3.44320387e-02  5.09943664e-02  3.47221009e-02\n",
      "   9.43630263e-02  8.90483558e-02  8.42994452e-02  6.60070851e-02\n",
      "   5.27298860e-02  8.50644410e-02  8.62208456e-02  4.20504995e-02\n",
      "   6.17342442e-02  9.05550718e-02 -3.57592478e-02  5.49469292e-02\n",
      "  -2.66569965e-02  2.65254709e-03  3.41053680e-02  3.70263755e-02\n",
      "  -1.07202269e-01  5.06137349e-02 -1.65715814e-02 -7.78721422e-02\n",
      "   3.34200123e-03  8.07642005e-03  7.42124859e-03  1.93332192e-02\n",
      "   5.89366630e-02  6.73641935e-02  2.21535806e-02  2.60743499e-02\n",
      "   6.07976131e-02  2.07042377e-02  4.91747819e-02  6.96578845e-02\n",
      "   1.84040274e-02  4.27438617e-02  7.36144409e-02  7.99695775e-02\n",
      "   7.71499798e-02  5.08197621e-02  4.20183502e-02 -3.38192843e-02\n",
      "   5.06114028e-02 -1.41905770e-02  4.14593518e-02  5.43882698e-02\n",
      "  -5.31402111e-01 -4.63387847e-01 -5.36085844e-01 -2.42812544e-01\n",
      "  -6.85423613e-01 -3.90427917e-01 -5.04305959e-01 -8.24861884e-01\n",
      "  -9.10609066e-01 -6.42809331e-01 -7.49246895e-01 -1.15481749e-01\n",
      "  -7.46308044e-02 -4.79188293e-01 -9.18700993e-02 -6.31969333e-01\n",
      "  -4.05516773e-01 -4.57058102e-01 -2.85628557e-01 -1.06361687e+00\n",
      "  -4.52644408e-01 -6.10651851e-01 -4.26311702e-01 -1.09728381e-01\n",
      "  -1.16371192e-01 -4.12123352e-01 -5.08846879e-01 -4.74895746e-01\n",
      "  -6.24065816e-01 -1.03001690e+00 -5.76822162e-01 -4.18488294e-01\n",
      "  -4.83702540e-01 -8.87348056e-01 -3.94015938e-01 -4.91584212e-01\n",
      "  -5.41523576e-01 -5.21452546e-01 -5.37647128e-01 -8.37719977e-01\n",
      "  -1.01047707e+00 -3.76997769e-01 -3.69911075e-01 -2.01367512e-01\n",
      "  -1.32346213e-01 -1.04192364e+00 -8.57438326e-01 -3.48001033e-01\n",
      "  -4.63302732e-01 -2.24359453e-01 -2.77228296e-01 -4.23977464e-01\n",
      "  -4.07468826e-01 -3.01336348e-01 -8.80536795e-01 -8.80926382e-03\n",
      "  -1.91429213e-01 -3.22725214e-02 -3.98213714e-01 -3.28848034e-01\n",
      "  -1.89339802e-01 -6.12373769e-01 -7.76336908e-01 -7.29711711e-01\n",
      "   1.77718580e-01 -1.83562949e-01 -9.20197964e-01 -6.09025538e-01\n",
      "  -5.14182210e-01 -8.23938549e-01 -4.81037855e-01 -4.82687771e-01\n",
      "  -7.83143878e-01 -2.83214301e-01 -2.82217115e-01 -4.36800331e-01]\n",
      " [-1.18469134e-01 -7.84379765e-02 -1.58699267e-02  1.08385663e-02\n",
      "   1.70724839e-02 -8.78044311e-03 -1.14801619e-02 -6.36160821e-02\n",
      "  -2.66923364e-02 -9.02088452e-03  1.63523629e-02 -9.52644460e-03\n",
      "  -1.39462091e-02 -3.20445583e-03 -1.02044381e-02 -1.48271574e-02\n",
      "  -3.53663832e-01 -5.53489208e-01 -7.45513082e-01 -1.16479687e-01\n",
      "  -8.86306524e-01 -4.48820531e-01 -1.06032109e+00 -8.63791287e-01\n",
      "  -8.23017836e-01 -1.11500621e+00 -1.06303811e+00 -1.31657147e+00\n",
      "  -6.02951288e-01 -6.77386403e-01 -6.16027653e-01 -6.89238429e-01\n",
      "  -2.72084028e-01 -3.82960647e-01 -8.54836285e-01 -8.65511239e-01\n",
      "  -1.05662310e+00 -6.81849048e-02 -1.83538839e-01 -2.10979119e-01\n",
      "  -6.45719290e-01 -6.06925845e-01 -8.26914549e-01 -9.03133750e-01\n",
      "  -2.62744963e-01  8.56507756e-03 -8.02414238e-01 -4.75652397e-01\n",
      "  -8.77096951e-02 -6.46282971e-01 -3.55878770e-01 -2.71362275e-01\n",
      "  -7.93522596e-01 -3.97425592e-01 -7.28182793e-01 -1.13918591e+00\n",
      "  -1.25664219e-01 -2.77501971e-01 -4.66639251e-01 -1.82236791e-01\n",
      "  -6.70818806e-01 -5.36438107e-01 -8.91933680e-01 -9.11112189e-01\n",
      "  -4.35715914e-01 -6.28005108e-03 -8.03608060e-01 -1.07803392e+00\n",
      "  -7.47555971e-01 -2.33022004e-01 -6.78364217e-01 -6.29948854e-01\n",
      "  -7.89670467e-01 -1.99415788e-01 -1.43805504e-01 -2.00950295e-01\n",
      "  -1.15658939e+00 -5.21480143e-01 -5.13648927e-01 -5.31326234e-01\n",
      "  -3.00448239e-01 -3.96792173e-01 -1.38476402e-01 -1.00657308e+00\n",
      "  -7.19972789e-01 -4.38430011e-01 -6.42543316e-01 -3.23635876e-01\n",
      "  -4.60947305e-02 -4.06530768e-01 -3.94311309e-01  1.48696061e-02\n",
      "  -9.42084014e-01 -6.92695796e-01 -7.82153085e-02 -1.13080472e-01\n",
      "  -7.39443362e-01 -1.13597679e+00 -3.59837532e-01 -8.49441707e-01\n",
      "  -4.43417281e-02  4.74705501e-03  2.96360743e-03  2.62525817e-03\n",
      "   3.42416316e-02  2.68383138e-02 -1.29063755e-01 -8.89985338e-02\n",
      "   3.87652740e-02  1.03665218e-02  8.80847964e-03  1.04107745e-02\n",
      "  -3.70469466e-02 -9.19097587e-02  2.48896461e-02 -1.82049500e-03\n",
      "  -4.42097560e-02 -5.07382257e-03 -7.08097173e-03 -3.36916349e-03\n",
      "  -1.91464834e-02 -6.74183905e-01 -5.22839487e-01 -9.57644224e-01\n",
      "  -6.83216035e-01 -1.40845180e-01 -6.62237167e-01 -7.36111999e-01\n",
      "  -4.85071577e-02 -7.55131245e-01 -2.48461422e-02  2.70272195e-02\n",
      "  -1.44579470e-01 -3.69441211e-02 -7.89258257e-03 -7.18234181e-02\n",
      "  -3.42201740e-02 -2.13387776e-02 -2.81045446e-03  6.73602428e-03\n",
      "   3.24012116e-02  4.98594642e-02 -5.71533740e-02 -1.11358874e-02\n",
      "   2.19170377e-02  1.04995386e-04 -1.20517202e-01  1.22008091e-02\n",
      "   2.79642213e-02 -7.06788674e-02 -1.44700930e-02  4.52826656e-02\n",
      "   2.46672388e-02  2.97543015e-02  5.18205427e-02  2.78860629e-02\n",
      "  -5.58305800e-01 -8.00596237e-01 -8.65365565e-01 -4.02143806e-01\n",
      "  -2.99488902e-01 -8.27982426e-01 -6.45378530e-01 -3.53208154e-01\n",
      "  -9.77798998e-01 -6.75893247e-01 -8.58042419e-01 -9.26435173e-01\n",
      "  -5.78484058e-01 -3.61389518e-01 -2.15392724e-01 -8.61548066e-01\n",
      "  -5.57325244e-01 -6.35843396e-01 -3.40495110e-01 -6.48173690e-01\n",
      "  -2.11495072e-01 -8.04565787e-01 -1.99948311e-01 -2.62642175e-01\n",
      "  -5.81772149e-01 -5.96381247e-01 -3.01435262e-01 -8.74746025e-01\n",
      "  -8.45411301e-01 -9.05775487e-01 -1.40628189e-01 -5.55505216e-01\n",
      "  -8.20787132e-01 -2.53322065e-01 -2.21530378e-01 -1.97911933e-01\n",
      "  -9.54155803e-01 -1.89064324e-01 -8.73692110e-02 -6.40173554e-02\n",
      "  -1.90817431e-01 -6.40933394e-01 -1.12657286e-01 -7.79708028e-01\n",
      "  -5.06564695e-03  3.49197388e-02  5.70746101e-02  4.83186059e-02\n",
      "  -1.61254779e-02  7.99597576e-02  1.86195376e-03  3.69843133e-02\n",
      "  -2.07662992e-02 -1.36951841e-02 -1.68022746e-03  5.27265761e-03\n",
      "   2.03716569e-02  2.45697740e-02  7.82952877e-04  4.58716676e-02\n",
      "   3.54205333e-02  4.87849824e-02  6.85136020e-02  1.67898387e-02\n",
      "   8.02343041e-02  6.72441497e-02  3.81220654e-02  2.08796524e-02\n",
      "   4.18190435e-02  6.54169125e-03  6.77809715e-02  7.05211535e-02\n",
      "   7.56620849e-03  4.63915765e-02  7.39312693e-02  5.90751581e-02\n",
      "   5.12439944e-02  3.13341431e-02  6.82820007e-02  7.63681531e-02\n",
      "   7.06344545e-02  6.60957918e-02  6.64844140e-02  4.24143858e-02\n",
      "   3.16295512e-02  6.58136532e-02  5.90018742e-02  3.72174941e-02\n",
      "   2.48304475e-02  2.23598741e-02  8.20401590e-03  8.27321410e-02\n",
      "   7.27013573e-02  6.09127134e-02  6.94415346e-02  8.33644718e-02\n",
      "   7.50961378e-02  4.55274098e-02  4.82217818e-02  1.94988027e-02\n",
      "  -6.73642382e-02  2.69461311e-02  5.03680035e-02  1.60153247e-02\n",
      "   3.05124559e-03  4.21320051e-02  4.26027909e-05 -6.04273304e-02\n",
      "  -7.15871574e-03 -8.18310753e-02 -2.60431897e-02 -2.10991632e-02\n",
      "  -1.51460692e-02  3.98185626e-02 -1.68032199e-03  2.24979703e-05\n",
      "   4.24152538e-02 -3.73096300e-05 -3.39150801e-03 -6.18018955e-02]\n",
      " [-5.65699115e-02 -3.90460253e-01  1.80421211e-02 -1.55404368e-02\n",
      "  -2.43607566e-01 -2.70538867e-01 -2.23902941e-01 -4.31256056e-01\n",
      "  -2.08834842e-01 -2.60136295e-02 -1.81018278e-01 -6.83742613e-02\n",
      "  -6.97810769e-01 -3.40741910e-02 -9.85598043e-02 -1.78576075e-02\n",
      "  -5.44703841e-01 -3.08131009e-01 -3.24637532e-01 -7.74294585e-02\n",
      "  -9.14048105e-02 -1.67089552e-01 -2.53842235e-01 -1.20666690e-01\n",
      "  -1.85624868e-01 -5.84525645e-01 -3.60254347e-01 -2.92086840e-01\n",
      "  -8.78678441e-01 -4.01351415e-02 -2.43482530e-01 -1.14073709e-01\n",
      "  -1.39887258e-01  2.74214018e-02 -6.03304664e-03 -3.10910791e-01\n",
      "  -1.23478198e+00  2.78419275e-02 -1.66194096e-01 -6.40980482e-01\n",
      "  -2.76709169e-01 -3.39097142e-01 -1.60265952e-01 -4.23650295e-01\n",
      "  -1.54812619e-01 -1.76244497e-01 -1.76208820e-02 -2.34175511e-02\n",
      "   6.40213443e-03 -2.68239796e-01 -2.45226711e-01 -4.24729846e-02\n",
      "  -9.87607390e-02 -3.50696415e-01 -4.76403385e-02  5.39837666e-02\n",
      "  -4.32251781e-01  5.07960320e-02  1.48213264e-02  2.08711028e-02\n",
      "  -2.00149357e-01 -1.50202632e-01 -6.00793287e-02  8.46002400e-02\n",
      "   5.36867939e-02 -2.35930100e-01 -3.92911404e-01  3.91990803e-02\n",
      "  -2.22022459e-01 -9.65285208e-03 -1.96168944e-01 -3.71834397e-01\n",
      "  -1.84616268e-01 -1.22579120e-01  2.45790952e-03 -7.27422178e-01\n",
      "  -1.17523983e-01 -1.57216683e-01 -1.19337300e-02 -1.87657878e-01\n",
      "  -1.24432936e-01 -3.44037175e-01 -3.29762787e-01 -6.00464463e-01\n",
      "  -3.19084860e-02 -7.30548752e-03 -1.45061210e-01 -7.01317266e-02\n",
      "  -2.35659286e-01 -2.25535527e-01 -6.11107409e-01 -8.18429738e-02\n",
      "  -1.38490424e-02 -4.47417721e-02 -6.03317954e-02 -3.69255304e-01\n",
      "  -7.57481515e-01 -4.52460825e-01 -2.84181774e-01 -2.29334950e-01\n",
      "  -1.52896926e-01 -8.24327245e-02 -6.28910214e-02 -8.81786495e-02\n",
      "  -3.83976161e-01 -5.24940826e-02  5.97353913e-02 -1.69720110e-02\n",
      "  -9.30049568e-02 -1.45122975e-01 -1.77073345e-01 -9.91270840e-02\n",
      "   3.68219167e-02 -1.81921646e-01 -1.66429117e-01 -2.28322759e-01\n",
      "   2.29317527e-02  2.79884767e-02  2.87226047e-02  7.76618952e-03\n",
      "   2.24768221e-02  6.27589375e-02  7.90603161e-02  3.86336930e-02\n",
      "   1.00715920e-01  5.69607057e-02  1.04517281e-01  9.95060951e-02\n",
      "   2.82118972e-02  6.52014986e-02  3.37744020e-02 -3.57379653e-02\n",
      "   8.91462117e-02  7.62793720e-02  6.26767650e-02  8.31126943e-02\n",
      "   7.99555779e-02  8.13098475e-02  5.31793274e-02  5.13525456e-02\n",
      "   4.78003770e-02  6.23652823e-02  1.00426257e-01  2.93598957e-02\n",
      "   6.43970445e-02  7.47855604e-02  9.55427811e-02  6.94085360e-02\n",
      "   7.08557218e-02  7.95677677e-02  2.36898754e-02  6.22528680e-02\n",
      "   5.39949089e-02  4.30310033e-02  5.25069945e-02  6.00226335e-02\n",
      "   8.92535076e-02  9.14017782e-02  8.98316652e-02  8.18185359e-02\n",
      "   8.16610977e-02  8.93862844e-02  7.43575096e-02  6.60945922e-02\n",
      "   7.13855848e-02  8.39211196e-02  1.66495796e-02  3.01342867e-02\n",
      "  -6.07444672e-03  2.53602024e-02  2.31004716e-03  4.77505177e-02\n",
      "  -4.97406255e-03  6.28229305e-02  2.47170795e-02  1.05153245e-03\n",
      "   1.32310875e-02  1.21765099e-02  1.48894247e-02  3.52574997e-02\n",
      "   7.21885040e-02  7.19311312e-02  5.15119024e-02  6.13967925e-02\n",
      "   5.82609475e-02  4.79002073e-02 -1.13080358e-02  8.38366896e-02\n",
      "   3.11733931e-02  3.36491913e-02  6.64654896e-02  7.43485838e-02\n",
      "   8.33424628e-02  4.46895324e-02  1.99988130e-02 -6.48693461e-03\n",
      "   4.90600578e-02  6.30084053e-03  4.44082320e-02  7.13260397e-02\n",
      "  -5.66385627e-01 -4.07261461e-01 -4.04601097e-01 -1.80610418e-01\n",
      "  -1.04060292e+00 -3.35263163e-01 -5.02053797e-01 -7.06631124e-01\n",
      "  -4.37527031e-01 -5.60013354e-01 -4.82793987e-01 -6.01440191e-01\n",
      "  -5.95392846e-02 -6.52396739e-01 -5.08702219e-01 -4.45951343e-01\n",
      "  -4.56655025e-01 -3.64165783e-01 -2.60641128e-01 -6.38357341e-01\n",
      "  -2.18632162e-01 -5.78816354e-01 -5.32605290e-01 -1.01202883e-01\n",
      "  -5.00827014e-01 -6.20151877e-01 -1.67271212e-01 -3.44296426e-01\n",
      "  -4.75318193e-01 -5.95164061e-01 -4.28471804e-01 -4.46608692e-01\n",
      "  -5.33868670e-01 -5.16197920e-01 -3.96005511e-01 -2.97961056e-01\n",
      "  -3.06586415e-01 -3.44689757e-01 -4.49084133e-01 -5.92346191e-01\n",
      "  -6.61144316e-01 -4.21565175e-01 -3.98673832e-01 -7.19427541e-02\n",
      "  -9.95425507e-02 -6.35639787e-01 -6.36472344e-01 -2.67704993e-01\n",
      "  -3.02439153e-01 -2.36619160e-01 -5.10940552e-01 -1.84609249e-01\n",
      "  -2.35397726e-01 -3.76591355e-01 -6.84801757e-01 -1.01060688e-01\n",
      "  -1.79392993e-01 -1.03178687e-01 -4.62342411e-01 -6.10776365e-01\n",
      "  -2.95633376e-01 -4.44826305e-01 -7.58022308e-01 -1.07353342e+00\n",
      "  -1.04400241e+00 -1.38617301e+00 -7.89138734e-01 -9.13574100e-01\n",
      "  -5.89127541e-01 -7.71885574e-01 -8.12154770e-01 -7.78560519e-01\n",
      "  -8.58266711e-01 -3.57835963e-02 -8.05999219e-01 -7.13721037e-01]\n",
      " [-3.94823968e-01 -3.05610031e-01 -6.02135584e-02 -2.44162321e-01\n",
      "  -2.58110434e-01 -4.20707345e-01 -3.53598557e-02 -5.33024967e-01\n",
      "   1.79646477e-01 -1.40056700e-01  1.54452503e-01  8.92702192e-02\n",
      "  -7.69210041e-01 -6.91146925e-02  1.08156756e-01 -1.95157260e-01\n",
      "  -8.73146772e-01  7.99774304e-02 -4.44653295e-02 -2.13686049e-01\n",
      "  -8.46323445e-02 -1.00348210e+00 -1.17415495e-01 -3.47375661e-01\n",
      "  -2.96760332e-02 -5.79813011e-02 -8.12146664e-02 -1.83375344e-01\n",
      "  -8.87826979e-01 -1.17830671e-01 -1.50587887e-01 -3.43417339e-02\n",
      "   1.34370830e-02  1.93535492e-01  9.68781039e-02  1.01318777e-01\n",
      "   1.26705775e-02  2.41663143e-01  1.54736750e-02 -2.18884330e-02\n",
      "   6.28855228e-02 -2.94139504e-01  1.19889751e-01  1.70954734e-01\n",
      "   7.98999444e-02  4.66944762e-02  1.03302710e-01  1.60653129e-01\n",
      "   6.36724755e-02  1.38629824e-01  6.15609661e-02  1.92787219e-02\n",
      "  -1.51296183e-01  1.07654415e-01  2.38162316e-02  1.50833070e-01\n",
      "   1.55331865e-01  5.93778603e-02  1.27765909e-01  2.13217840e-01\n",
      "  -2.16765270e-01  2.98678204e-02  1.07769901e-03  1.31467879e-01\n",
      "   1.74775645e-01 -1.13907903e-01  1.16557114e-01  6.55833855e-02\n",
      "   3.42163146e-02  1.26450032e-01 -2.57533155e-02  3.03604100e-02\n",
      "   6.68595955e-02 -2.91761737e-02  8.31346884e-02  4.88413423e-02\n",
      "  -1.36811314e-02  4.57148328e-02  1.15655392e-01  5.89127988e-02\n",
      "   6.78110123e-02  9.61179063e-02  3.54733206e-02  1.00331098e-01\n",
      "  -2.07201522e-02  4.70540449e-02  1.31308034e-01  9.48027372e-02\n",
      "   1.47695318e-02 -9.32010356e-03 -7.28015155e-02 -2.81310063e-02\n",
      "   9.19161662e-02 -9.40609425e-02  4.05030586e-02  2.53165569e-02\n",
      "   2.35817730e-02  1.71918839e-01 -9.06402469e-02  1.22908063e-01\n",
      "  -1.36022761e-01 -1.38413996e-01  1.47200469e-02 -1.65552944e-01\n",
      "  -8.67787540e-01  7.38997161e-02 -1.44708812e-01 -5.21888375e-01\n",
      "  -4.39019889e-01 -2.41556093e-01  2.28031933e-01 -1.01186633e+00\n",
      "   2.19812647e-01 -5.17967224e-01  1.65957063e-01 -2.03405127e-01\n",
      "  -3.29972982e-01  3.69541720e-02 -1.09756105e-01  1.11144148e-01\n",
      "   6.01056293e-02 -1.31313860e-01 -5.87580085e-01 -3.25330719e-02\n",
      "  -1.03960490e+00 -2.01792404e-01 -8.59946072e-01 -8.10203373e-01\n",
      "  -7.76100624e-03 -3.01111370e-01  1.23445503e-01 -3.86460781e-01\n",
      "   4.67485674e-02  1.62057847e-01  2.04402834e-01  1.65189058e-02\n",
      "   2.68029347e-02 -4.99350816e-01 -1.64288636e-02 -1.30756900e-01\n",
      "   1.52889267e-01  2.09803522e-01 -3.77674028e-02  1.85415968e-01\n",
      "   1.85701698e-01  6.63390532e-02 -4.96363528e-02  2.12741435e-01\n",
      "  -4.46130894e-03 -1.99205548e-01  1.64738186e-02  1.37484983e-01\n",
      "  -6.16608858e-02  1.88343432e-02  2.32103616e-02  1.34969264e-01\n",
      "   1.06687285e-01 -6.17627501e-02 -2.23814636e-01 -2.17751056e-01\n",
      "  -4.50752527e-01 -3.19292545e-02 -1.02033220e-01 -1.36517376e-01\n",
      "  -1.74005687e-01 -5.67619056e-02 -2.99999982e-01 -6.89946353e-01\n",
      "  -1.02620751e-01 -9.47200283e-02 -1.53740972e-01 -5.47420263e-01\n",
      "  -1.01867294e+00 -5.12928903e-01 -5.08094609e-01 -3.03836584e-01\n",
      "  -1.76570609e-01 -2.16527924e-01 -1.03757540e-02 -2.63594061e-01\n",
      "   3.98603193e-02 -6.89747185e-02 -1.73837870e-01 -2.10950702e-01\n",
      "  -3.90919745e-01  3.07820350e-01 -5.70890978e-02 -5.91629505e-01\n",
      "  -9.24870074e-01 -4.87568863e-02  1.55873522e-01 -4.22110975e-01\n",
      "  -4.50057566e-01 -1.71013102e-01  1.97655689e-02 -3.28644007e-01\n",
      "   9.70845018e-03 -3.48516911e-01 -9.98768285e-02 -2.69441634e-01\n",
      "  -1.00600922e+00  1.19514123e-01 -6.03335559e-01 -1.52845800e-01\n",
      "   1.69495732e-01 -4.48984653e-01  8.95486400e-02 -3.02197840e-02\n",
      "   2.64584254e-02  1.26533374e-01 -4.13089842e-02 -2.38892958e-01\n",
      "  -5.90924583e-02 -1.76336706e-01  2.81219259e-02 -1.42530948e-01\n",
      "  -3.19570482e-01 -5.76669812e-01 -4.28101212e-01  9.82999355e-02\n",
      "  -1.08927119e+00 -7.65091836e-01  1.58812925e-02 -2.38428563e-02\n",
      "   2.00903020e-03 -8.14314559e-02 -5.48149109e-01 -3.40337723e-01\n",
      "  -1.04951710e-01  9.25574228e-02 -3.44886273e-01 -2.27553859e-01\n",
      "  -4.99307901e-01  3.54966260e-02 -1.10901868e+00 -8.52370083e-01\n",
      "  -1.22881699e+00 -1.33194298e-01 -2.75918841e-01 -1.73010770e-02\n",
      "   6.99975938e-02 -1.78241670e-01 -3.07982445e-01 -5.89796454e-02\n",
      "   5.76494448e-02  2.31449351e-01  7.18615577e-02 -7.10250616e-01\n",
      "   1.02224700e-01 -8.60838711e-01 -1.12782168e+00 -5.92352629e-01\n",
      "  -3.03813905e-01  2.39199027e-01 -2.25976989e-01  4.28842790e-02\n",
      "  -8.31575096e-02  2.35626608e-01  3.16425353e-01  9.76740345e-02\n",
      "   5.32988422e-02 -5.86946726e-01  8.35219473e-02 -1.91379189e-02\n",
      "   8.65724906e-02  3.52689587e-02 -7.13243783e-02  7.77754709e-02\n",
      "   8.53057206e-02 -9.25389826e-01  1.46486744e-01 -6.28425255e-02\n",
      "  -8.90619099e-01 -6.20513186e-02  4.77226861e-02 -5.82070202e-02]\n",
      " [-5.93208969e-01 -6.96605325e-01  1.06891263e-02  8.13758895e-02\n",
      "   4.69707586e-02  2.97327172e-02 -1.19522195e-02 -5.07273853e-01\n",
      "  -1.32904887e-01  3.17610018e-02 -4.50270250e-02  4.45340388e-02\n",
      "  -7.22829103e-01  2.63996515e-03 -6.66547239e-01  1.93239767e-02\n",
      "  -1.07678092e+00 -4.80573803e-01 -1.05674088e-01  2.90305465e-02\n",
      "  -2.35454086e-02 -9.84543681e-01 -2.24604473e-01 -9.30083513e-01\n",
      "  -7.18805566e-02 -4.95667383e-02 -1.25004604e-01 -8.32583979e-02\n",
      "  -8.55563104e-01 -2.57849187e-01 -1.39380670e+00  1.40620451e-02\n",
      "   3.88883278e-02  4.95316535e-02  6.15495164e-03 -5.42259291e-02\n",
      "  -6.71344846e-02  7.17177913e-02  3.32578011e-02  4.54084128e-02\n",
      "   4.13728617e-02 -1.72370493e-01 -1.57513052e-01  5.16883470e-02\n",
      "   9.37300920e-02  3.63072604e-02  6.56387210e-02  8.55481774e-02\n",
      "   4.07355167e-02 -5.06575406e-02 -1.81053244e-02  6.51758909e-02\n",
      "  -3.85927260e-02  7.56963417e-02  4.02457640e-03 -3.32455002e-02\n",
      "   4.90223430e-02  7.51500726e-02  6.88753799e-02  8.26073214e-02\n",
      "  -4.24207263e-02  6.29183576e-02 -4.57122549e-02  6.28508329e-02\n",
      "   4.42810953e-02 -4.95776534e-02  8.51536244e-02  5.65776192e-02\n",
      "  -7.82324150e-02  7.66343847e-02 -1.96941812e-02  5.43921292e-02\n",
      "   1.63915195e-02  3.55327427e-02  7.35245198e-02  5.04483357e-02\n",
      "   4.19607460e-02  7.14002177e-02  9.27971005e-02  6.81115985e-02\n",
      "   7.89613426e-02  6.00259900e-02  3.99493091e-02  4.36784439e-02\n",
      "   4.68124002e-02  3.46218720e-02  2.26563588e-02 -1.78342596e-01\n",
      "  -2.84387097e-02 -4.54486273e-02 -5.93218952e-02  4.03871313e-02\n",
      "  -1.17991969e-01 -1.67960599e-01 -4.87519875e-02 -3.73598957e-03\n",
      "  -2.17121281e-03 -5.04089668e-02 -6.47218302e-02 -1.04284845e-01\n",
      "  -5.94228029e-01 -7.03311682e-01 -6.16898417e-01 -6.40408337e-01\n",
      "  -8.17321241e-01 -8.57539833e-01 -2.16653824e-01 -8.46765637e-01\n",
      "  -6.16217315e-01 -4.72758979e-01 -6.24935031e-01 -8.71657372e-01\n",
      "  -3.02618355e-01 -9.74813849e-02 -3.61715585e-01 -2.91739255e-01\n",
      "  -1.44634292e-01 -8.13605607e-01 -4.34836268e-01 -5.12686253e-01\n",
      "  -6.82714403e-01 -8.94136965e-01 -7.63400376e-01 -3.91089208e-02\n",
      "  -6.60282671e-01 -2.26000398e-02 -4.80037779e-02 -1.21761668e+00\n",
      "  -6.41209185e-02 -5.27976975e-02 -5.74556403e-02 -3.86932522e-01\n",
      "   5.30941673e-02  2.61449143e-02  7.08332732e-02  8.45773295e-02\n",
      "   7.13170245e-02  3.74853387e-02  3.57626975e-02  5.69771193e-02\n",
      "   8.30090791e-02  8.63492936e-02  8.40752497e-02 -3.33378674e-03\n",
      "   7.56537393e-02  6.44626319e-02  6.99075162e-02  6.30111322e-02\n",
      "   8.77604336e-02  4.54169847e-02  5.93686737e-02  9.31182653e-02\n",
      "   6.95149526e-02  8.01155120e-02  1.01014711e-01  7.63909668e-02\n",
      "  -9.22936916e-01  5.48367463e-02 -2.11768761e-01 -2.93254614e-01\n",
      "  -1.20537949e+00  3.53037231e-02 -1.35496229e-01 -1.03725362e+00\n",
      "  -2.88064107e-02 -5.02074122e-01 -6.22993708e-01 -1.47961840e-01\n",
      "  -4.17092890e-02 -4.53877822e-02  6.16548099e-02 -5.83478928e-01\n",
      "  -1.22484553e+00 -1.06984520e+00  3.97305237e-03 -6.17900640e-02\n",
      "  -1.92505997e-02  4.08919342e-02  4.81242537e-02  3.23880762e-02\n",
      "  -4.43114936e-02 -5.26459932e-01 -9.94012877e-02 -6.47203982e-01\n",
      "  -7.81239718e-02 -1.77196115e-01  8.42419937e-02 -6.53479040e-01\n",
      "  -1.09458566e-01  1.35291060e-02  2.52551269e-02  5.34858890e-02\n",
      "  -2.35310987e-01  4.79250625e-02  6.99371099e-02  3.77579816e-02\n",
      "   8.04446340e-02 -4.65202004e-01  3.85444984e-02 -1.31038092e-02\n",
      "  -8.59906971e-01 -1.15974438e+00 -9.32761610e-01 -1.08899391e+00\n",
      "   3.51232067e-02 -6.21162713e-01  4.04933877e-02 -6.93849251e-02\n",
      "  -8.49721283e-02 -5.94814643e-02 -8.53769481e-02 -6.33144677e-02\n",
      "   3.86919896e-03  4.48500291e-02 -1.91825293e-02 -5.83373070e-01\n",
      "  -8.18832994e-01 -5.88197529e-01 -6.55891180e-01  6.20388007e-03\n",
      "  -4.83273596e-01 -3.97846669e-01  4.70247976e-02 -5.16405925e-02\n",
      "   1.80555917e-02 -1.89600922e-02 -7.95999825e-01 -7.23783910e-01\n",
      "  -1.31585509e-01  7.08146468e-02 -6.95176363e-01 -5.03404737e-01\n",
      "  -5.19515455e-01  6.87967911e-02 -6.50960803e-01 -5.07812619e-01\n",
      "  -6.81803465e-01 -6.32043064e-01 -7.46214807e-01 -5.95506616e-02\n",
      "   4.98606637e-02 -6.38747633e-01 -8.66427243e-01  1.71326585e-02\n",
      "   2.84275990e-02  3.72947119e-02  4.51246314e-02 -3.40485841e-01\n",
      "  -5.02690196e-01 -2.72310853e-01 -6.86119258e-01 -5.66620350e-01\n",
      "  -4.45599705e-01  8.76008570e-02  6.61486313e-02  5.14614508e-02\n",
      "  -4.06298220e-01  9.45818946e-02  1.51199952e-01  7.03795925e-02\n",
      "  -1.60412729e-01 -4.19831902e-01 -1.21860495e-02  4.47238013e-02\n",
      "   2.52711494e-02  7.63633251e-02  1.77422408e-02 -2.36082114e-02\n",
      "   3.64005417e-02 -1.49957137e-02  4.35672067e-02  4.17597182e-02\n",
      "   3.42293344e-02 -1.04599781e-01  6.51910752e-02 -6.32818267e-02]\n",
      " [-4.69651461e-01 -7.47313648e-02 -1.41381636e-01  2.40259664e-03\n",
      "  -2.56637335e-02 -6.13705404e-02 -8.44557434e-02 -1.76610693e-03\n",
      "   9.36252181e-04 -1.21139288e-01  2.76492462e-02 -1.26648813e-01\n",
      "  -1.22870160e-02 -4.70811650e-02  1.39047671e-02 -1.12754166e-01\n",
      "  -6.50087237e-01 -1.24227238e+00 -9.14472491e-02  6.70805722e-02\n",
      "  -2.77884334e-01 -6.94551766e-01 -8.48189771e-01 -7.43188500e-01\n",
      "  -5.25679708e-01 -3.25372964e-01 -8.58807206e-01 -2.96055168e-01\n",
      "  -7.35074282e-01 -5.18303514e-01 -9.57686454e-02 -4.98503357e-01\n",
      "  -4.12946284e-01 -2.08171103e-02 -9.72963944e-02 -3.34276855e-01\n",
      "  -4.33359236e-01  2.18821391e-02 -2.96931528e-02 -7.66123831e-02\n",
      "  -1.95474491e-01 -1.07623255e+00 -5.15330017e-01 -2.57192194e-01\n",
      "  -1.29481122e-01 -1.48255408e-01 -1.10237978e-01 -7.19047934e-02\n",
      "   5.76299392e-02 -4.22138087e-02 -4.73614708e-02  6.50089458e-02\n",
      "  -7.61413574e-02 -7.72167519e-02 -7.10614622e-02 -3.58955748e-02\n",
      "  -2.17042603e-02 -5.01147434e-02 -1.33258760e-01 -1.62150130e-01\n",
      "  -7.21457526e-02 -1.01182990e-01 -5.33543937e-02 -3.12557399e-01\n",
      "  -1.58244029e-01 -2.68924147e-01 -2.96921164e-01 -4.53361869e-01\n",
      "  -1.30301714e-01  1.14527903e-01 -3.94017063e-02  1.90901477e-02\n",
      "  -6.73529506e-03 -2.61473536e-01  2.48416141e-02  1.54361287e-02\n",
      "  -3.11262250e-01 -2.15315655e-01 -1.12829886e-01 -1.02108769e-01\n",
      "  -1.94604576e-01 -5.64220920e-03 -1.80588692e-01 -1.68791234e-01\n",
      "  -3.44504535e-01 -3.01976830e-01 -3.49469751e-01 -1.04616773e+00\n",
      "  -3.04859549e-01 -2.56887197e-01 -5.76976418e-01  2.76367553e-02\n",
      "  -7.45216727e-01 -4.98016089e-01  7.09717944e-02 -4.77542244e-02\n",
      "  -4.88030344e-01 -5.98371267e-01 -6.54620051e-01 -1.40164554e+00\n",
      "  -5.67515055e-03  3.87972072e-02 -1.81097105e-01 -3.34483422e-02\n",
      "   5.84363155e-02  4.55311090e-02 -9.64336753e-01 -7.40437865e-01\n",
      "  -2.45573036e-02  4.50080298e-02 -1.19344369e-02 -4.50139958e-03\n",
      "  -3.41263950e-01 -9.63670731e-01 -3.07551138e-02 -1.40166372e-01\n",
      "  -4.63619456e-02  8.19682027e-05 -6.30322248e-02 -1.44070312e-01\n",
      "  -5.48521101e-01 -5.20463645e-01 -7.14059889e-01 -1.38390911e+00\n",
      "  -5.59150577e-01 -2.63598233e-01 -7.23121166e-01 -4.75031853e-01\n",
      "  -2.71906763e-01 -8.14550638e-01 -1.74419284e-01  1.00288585e-01\n",
      "  -1.15673482e+00 -3.04522991e-01 -6.43983558e-02 -2.85091788e-01\n",
      "  -1.68970808e-01 -5.80708683e-01  4.84938249e-02 -4.41968180e-02\n",
      "   2.77189482e-02  3.25654298e-02 -2.83925712e-01  1.32744992e-02\n",
      "  -4.73675579e-02 -2.19511036e-02 -2.98374206e-01  2.60510966e-02\n",
      "  -2.39099264e-02 -4.52921629e-01  2.83020660e-02  4.61051092e-02\n",
      "  -6.66848337e-03 -6.01949263e-03  8.61080363e-02  6.19690074e-03\n",
      "  -4.51508194e-01 -6.88565373e-01 -6.26328945e-01 -3.84858489e-01\n",
      "  -4.33097392e-01 -5.79020619e-01 -7.17152357e-01 -5.27943373e-01\n",
      "  -5.65780818e-01 -3.16978186e-01 -7.10602164e-01 -7.30755627e-01\n",
      "  -9.00676966e-01 -7.72637725e-01 -3.33174884e-01 -7.59110510e-01\n",
      "  -9.45210457e-01 -5.90275526e-01 -9.75028038e-01  1.36715248e-02\n",
      "  -6.41184986e-01 -7.04782486e-01 -2.37625271e-01  6.05186000e-02\n",
      "  -9.78133678e-01 -7.63493240e-01 -7.90340245e-01 -5.51606059e-01\n",
      "  -8.28643024e-01 -8.42413008e-01 -3.10473919e-01 -4.19429570e-01\n",
      "  -5.96135557e-01 -3.93909156e-01 -7.65413463e-01 -6.45219088e-01\n",
      "  -7.70426750e-01  1.17570860e-03 -2.76212953e-02 -4.88618940e-01\n",
      "  -1.46457732e-01 -1.19819343e+00 -1.91011027e-01 -8.02317142e-01\n",
      "   4.02348787e-02  6.60968721e-02  8.13299194e-02  8.35229382e-02\n",
      "  -4.00405470e-03  1.01504028e-01  4.57784496e-02  6.38371482e-02\n",
      "   2.40151770e-02  3.23306732e-02  2.84883212e-02 -3.34137194e-02\n",
      "  -6.36870861e-02 -2.03668303e-03  1.70742217e-02  7.81056583e-02\n",
      "   7.61243105e-02  7.91531876e-02  9.24810618e-02  4.09351476e-02\n",
      "   9.42564607e-02  8.88301507e-02  1.25756217e-02 -1.32502094e-01\n",
      "  -8.73142630e-02 -9.19894036e-03  1.05373211e-01  9.86127183e-02\n",
      "   3.56141552e-02  8.73744115e-02  1.00066379e-01  8.92883092e-02\n",
      "   8.28127936e-02  5.82776330e-02  9.07375515e-02  9.47128683e-02\n",
      "   9.12679210e-02  9.10401195e-02  8.99796858e-02  7.57331625e-02\n",
      "   5.60802370e-02  8.26458633e-02  8.42064172e-02 -2.41914671e-02\n",
      "   3.10793184e-02  4.69021462e-02  4.76800092e-02  8.41355249e-02\n",
      "   9.95700285e-02  9.16560143e-02  8.87529999e-02  1.01554148e-01\n",
      "   9.38554555e-02  7.06412196e-02  6.61746413e-02  2.83553712e-02\n",
      "  -2.79897600e-02  5.34746833e-02  8.53967220e-02  4.63204794e-02\n",
      "  -1.91723198e-01  6.58671185e-02  4.98714345e-03 -1.77957732e-02\n",
      "  -5.59509383e-04 -1.02022039e-02  8.19224678e-03  5.21123642e-03\n",
      "   7.78024038e-03  3.45506184e-02 -4.55731293e-03  1.15648285e-02\n",
      "   3.68947312e-02 -3.07705626e-02  1.58197861e-02 -5.00270166e-02]\n",
      " [-8.60209316e-02 -8.70063677e-02 -5.75103819e-01 -6.79433465e-01\n",
      "  -9.91738379e-01 -8.91176879e-01 -1.67316034e-01 -3.51798795e-02\n",
      "  -1.95458055e-01 -1.72316670e-01 -2.23726511e-01 -2.49604091e-01\n",
      "  -4.95281547e-01 -5.59610724e-01  1.45390993e-02 -2.98295654e-02\n",
      "   8.91717225e-02  5.89041896e-02  5.74697256e-02  4.40545939e-02\n",
      "   4.63725142e-02  8.45846310e-02  5.73119111e-02  6.64559752e-02\n",
      "   4.50843535e-02  6.02343008e-02  5.89751676e-02  5.39563149e-02\n",
      "   8.80879536e-02  6.30857795e-02  7.69311264e-02  4.36422564e-02\n",
      "   4.85161804e-02 -9.29070711e-02  6.86109588e-02  5.13013676e-02\n",
      "   1.08809412e-01 -4.49239492e-01  2.85057407e-02  3.81277427e-02\n",
      "   8.17247778e-02  7.17917979e-02  5.98839484e-02  7.99842924e-02\n",
      "  -6.37857094e-02  8.68929997e-02  3.90890390e-02  2.27213856e-02\n",
      "   2.31617196e-06  4.29536738e-02  4.36037481e-02  6.29487932e-02\n",
      "   4.54151556e-02  2.05646344e-02  5.24988445e-03  4.30690199e-02\n",
      "   4.38930392e-02  2.83799935e-02  1.94965675e-02 -4.90191840e-02\n",
      "   5.05452715e-02  5.25981560e-02  5.39518856e-02  5.80905750e-03\n",
      "  -4.02758121e-02  3.97601947e-02  8.62991437e-02  2.84320824e-02\n",
      "   3.41196023e-02 -1.16716791e-02  4.76555713e-02  5.36455400e-02\n",
      "   6.55890033e-02  4.15771194e-02  3.99025604e-02  8.74920860e-02\n",
      "   8.09625238e-02  6.34917468e-02  5.54866120e-02  4.24312279e-02\n",
      "  -5.37191033e-02  4.41686362e-02 -3.46009731e-02  8.24989006e-02\n",
      "  -2.49131210e-02  1.26395538e-01 -4.89648394e-02  1.13024432e-02\n",
      "   4.93302150e-03  2.65576728e-02  4.18407805e-02 -5.86774871e-02\n",
      "   8.39397237e-02  2.13470422e-02 -4.51132050e-03  5.92035316e-02\n",
      "   3.95533964e-02  1.75937451e-02  3.81898582e-02  4.75677438e-02\n",
      "   1.26603330e-02  3.43545899e-02  2.59832852e-02  1.60476211e-02\n",
      "  -5.58771670e-01 -1.03351325e-01 -2.44955067e-02  2.38205064e-02\n",
      "  -1.39763117e-01  1.06037259e-02 -4.49093729e-02  1.48734357e-02\n",
      "  -4.63802256e-02  4.23372090e-02  1.95818581e-02 -6.29935740e-03\n",
      "  -2.48734448e-02 -4.10617515e-02 -4.21523452e-02 -2.90234741e-02\n",
      "  -2.86133913e-03  5.44814803e-02 -9.77214396e-01  1.31926602e-02\n",
      "  -7.29878902e-01 -6.51144803e-01 -1.23992956e+00 -2.70316362e-01\n",
      "  -5.31292818e-02 -8.39027762e-02 -1.61482263e-02 -8.21742296e-01\n",
      "  -9.56676602e-01 -6.55568421e-01 -8.06133211e-01 -1.18567681e+00\n",
      "  -1.04275513e+00 -6.88920736e-01 -6.86864614e-01 -5.35513282e-01\n",
      "  -4.17626768e-01 -4.17467266e-01 -7.26560235e-01 -4.23960574e-02\n",
      "  -6.03864074e-01 -8.23888302e-01 -5.58254719e-01 -5.81524253e-01\n",
      "  -4.50619698e-01 -7.23492444e-01 -9.27892089e-01 -5.97614706e-01\n",
      "  -4.93381381e-01 -5.42556047e-01 -6.16194367e-01 -4.99211371e-01\n",
      "  -1.66415095e-01 -1.91908106e-01 -5.28239667e-01 -7.37067387e-02\n",
      "   4.20362316e-03 -3.27798784e-01  9.11673345e-03 -3.93408490e-03\n",
      "  -1.50311738e-01 -1.68051794e-01  2.61412933e-02  3.34082693e-02\n",
      "   5.38446195e-02 -1.73921004e-01 -1.46424383e-01 -9.12742764e-02\n",
      "   8.32751989e-02  5.66873662e-02  2.41957754e-02  2.62879059e-02\n",
      "  -8.24843645e-02 -1.26673266e-01 -3.92961413e-01 -9.04354453e-02\n",
      "  -7.78871298e-01 -2.23537385e-01 -3.49405915e-01 -7.10543916e-02\n",
      "  -2.41156027e-01 -2.93969274e-01 -4.41913396e-01 -1.95406348e-01\n",
      "  -3.80469747e-02 -2.95891434e-01 -1.27308166e+00 -1.14541912e+00\n",
      "  -1.22130060e+00 -8.10584426e-01 -9.26437438e-01  1.74252037e-02\n",
      "  -8.47719669e-01  5.50131835e-02 -8.30516398e-01 -9.33976233e-01\n",
      "   1.17300130e-01  6.14033043e-02 -1.62429605e-02 -4.25386801e-02\n",
      "   4.30731364e-02 -6.25612540e-03 -1.27631202e-01 -2.32135415e-01\n",
      "  -1.35406196e-01 -1.46331964e-02 -2.10808635e-01 -2.27051631e-01\n",
      "   2.23305877e-02  2.42270641e-02 -6.32489920e-02  4.39804308e-02\n",
      "  -1.32906646e-01 -1.41390413e-01 -1.95145935e-01 -2.15905845e-01\n",
      "  -6.98566735e-02 -3.59412909e-01 -9.06928837e-01 -1.66224197e-01\n",
      "  -8.05897474e-01 -1.05474954e-02 -5.05246580e-01 -5.63167259e-02\n",
      "  -2.69697100e-01 -6.01399481e-01 -1.44437969e-01 -5.18151045e-01\n",
      "  -2.04399824e-01 -5.75752318e-01 -1.37930587e-01 -1.43706352e-01\n",
      "  -1.93411589e-01 -1.82993665e-01 -1.06766909e-01 -3.02060902e-01\n",
      "  -5.54952860e-01 -2.18687892e-01 -3.88196930e-02 -5.84750712e-01\n",
      "  -6.21736944e-01 -5.78272581e-01 -7.10912108e-01 -2.17589796e-01\n",
      "  -3.83835137e-01 -4.90444928e-01 -1.32849649e-01 -2.90695250e-01\n",
      "  -3.55178326e-01 -1.03367031e+00 -9.41502392e-01 -7.10682571e-03\n",
      "   3.29640359e-02 -2.71463394e-01 -1.25117838e+00 -4.05291498e-01\n",
      "  -5.23632839e-02 -5.44727258e-02  3.27887386e-02 -2.94383280e-02\n",
      "   7.49659725e-04 -6.49937838e-02 -4.15640771e-02 -3.04397047e-02\n",
      "  -2.15956569e-01 -2.21885473e-01  3.92567664e-02 -3.54211837e-01\n",
      "  -5.50290167e-01 -7.89597929e-02 -1.96361169e-01  2.05270164e-02]\n",
      " [ 6.71384260e-02  9.16286781e-02  5.98723181e-02 -7.46763051e-01\n",
      "  -3.17817405e-02  1.52961677e-02 -3.30247805e-02  9.75224972e-02\n",
      "   4.51654904e-02  1.21958060e-02  5.26320748e-02 -1.01146653e-01\n",
      "   9.46922004e-02  6.17314596e-04 -8.70695978e-04 -2.00003222e-01\n",
      "   7.85273388e-02 -2.88878884e-02  9.11026355e-03 -4.10271287e-02\n",
      "  -6.50205538e-02  9.08520073e-02  1.15843583e-02  9.04520825e-02\n",
      "  -6.50280043e-02 -3.18462204e-04 -1.82360539e-03  2.33102385e-02\n",
      "   9.66432244e-02  2.14368515e-02  5.62123433e-02 -1.89484321e-02\n",
      "  -2.88605273e-01 -4.51844811e-01 -5.19744575e-01  8.27608071e-03\n",
      "  -2.64185011e-01 -6.02825701e-01  1.02233268e-01 -1.83450393e-02\n",
      "  -2.49852240e-01  5.97263388e-02 -6.31627291e-02 -1.07796833e-01\n",
      "  -1.27063811e+00 -3.01976442e-01  5.11107631e-02 -9.59460914e-01\n",
      "   1.94532737e-01 -4.83965389e-02 -1.13592790e-02 -9.77755338e-02\n",
      "  -1.56989619e-02 -1.07498157e+00 -4.21393096e-01 -3.11543159e-02\n",
      "  -1.06141889e+00 -1.00197816e+00 -1.39295554e+00 -1.52425110e+00\n",
      "  -1.44220451e-02 -2.61901617e-01  3.27570317e-03  4.36801501e-02\n",
      "  -9.71011877e-01  2.19916031e-02 -1.41045451e+00 -4.64025363e-02\n",
      "  -1.10929556e-01 -9.63135421e-01 -4.96190675e-02 -6.17714763e-01\n",
      "  -3.92163724e-01 -1.63159981e-01 -3.53675544e-01 -4.19959366e-01\n",
      "  -1.29955232e-01 -4.77481782e-01 -1.31141579e+00 -3.97098750e-01\n",
      "   3.70673686e-02 -3.90567124e-01 -2.29382724e-01 -2.58380085e-01\n",
      "   1.02771722e-01 -3.68840843e-01 -2.24153623e-01 -6.74040243e-02\n",
      "  -4.91693884e-01  1.20185524e-01 -6.49413541e-02 -6.26514077e-01\n",
      "  -1.93426877e-01 -8.81809443e-02 -1.16476440e-03 -3.41585755e-01\n",
      "  -1.47527739e-01 -1.08417086e-01 -4.39349823e-02 -2.60856282e-02\n",
      "   3.34307440e-02  7.44685903e-02  8.35705325e-02  3.57077532e-02\n",
      "   8.32298398e-02  8.89601260e-02 -4.86616381e-02  3.31998840e-02\n",
      "   1.03280954e-01  4.85617742e-02  8.34906399e-02  4.24419157e-02\n",
      "   4.74407412e-02  4.29205894e-02  9.02525336e-02  1.16835408e-01\n",
      "  -9.45055299e-03  3.90405431e-02  3.80356424e-02  9.61777791e-02\n",
      "   3.91473025e-02  8.63840804e-02  4.47272956e-02 -2.82012075e-01\n",
      "   3.65957133e-02  1.09741203e-01  2.25808695e-02  9.82907042e-02\n",
      "  -8.69070739e-02 -1.12358388e-02 -3.24672423e-02  7.36339688e-02\n",
      "  -3.07552125e-02  4.86595407e-02  3.37224789e-02 -1.08018525e-01\n",
      "  -1.29892686e-02  8.42860565e-02  3.11414432e-02 -1.56760722e-01\n",
      "  -1.94976911e-01 -1.15296149e+00 -8.68357182e-01  3.14893317e-03\n",
      "  -7.97479630e-01 -1.85783327e-01 -7.54483581e-01 -2.88270295e-01\n",
      "  -2.65576839e-01 -1.74034581e-01 -3.01712360e-02 -9.80094314e-01\n",
      "  -1.42371655e-01  5.02498914e-03 -9.21974778e-01 -3.19293588e-01\n",
      "   3.67448255e-02  2.50173490e-02 -4.55295891e-02  2.42394935e-02\n",
      "   2.26971954e-02 -1.53134540e-01 -5.38214505e-01  5.62029704e-02\n",
      "  -1.85680762e-01  8.78594629e-03 -6.78798407e-02 -1.77512765e-01\n",
      "  -3.61450948e-02 -5.67969074e-03  3.86627652e-02  3.19365002e-02\n",
      "  -4.86867083e-03 -7.67727643e-02  1.45387966e-02 -8.37271288e-02\n",
      "   5.37837967e-02 -2.52187662e-02 -8.45795795e-02 -1.97758502e-03\n",
      "  -3.76722286e-03  1.12100489e-01 -1.68655105e-02  5.60434572e-02\n",
      "  -2.79223942e-03 -4.50573675e-02 -4.48097646e-01  9.68345404e-02\n",
      "  -5.47286049e-02  6.52718022e-02 -2.71942109e-01 -1.20875776e-01\n",
      "  -6.25452474e-02 -3.27551961e-01 -4.76393938e-01 -1.12930462e-01\n",
      "  -2.99690723e-01  1.76699944e-02 -1.25744298e-01 -3.73466313e-02\n",
      "   3.69883776e-02  4.56065573e-02  5.66546507e-02  6.79710805e-02\n",
      "  -3.50397564e-02  5.71279004e-02 -2.39323899e-02  3.14394124e-02\n",
      "  -4.72889803e-02 -1.29831377e-02  7.42595969e-03 -9.88712981e-02\n",
      "  -3.43893677e-01 -5.97250722e-02 -2.21535116e-01  3.29044014e-02\n",
      "   1.02098256e-01  2.45567206e-02  7.52177387e-02 -5.53128170e-03\n",
      "   5.08350246e-02  3.12895328e-02 -3.70295763e-01 -1.76704437e-01\n",
      "  -9.95511770e-01 -1.61045134e-01  6.17470108e-02  3.96405607e-02\n",
      "   4.10735607e-03  4.29744972e-03  4.05178964e-02  3.64171416e-02\n",
      "   6.91059753e-02 -2.39190459e-02  6.03330880e-02  6.67235181e-02\n",
      "   6.73703477e-02  5.74523509e-02  6.54842108e-02  5.31989299e-02\n",
      "   2.65084170e-02  5.31047285e-02  5.82987145e-02 -3.09171885e-01\n",
      "  -3.99341643e-01 -3.82515192e-02  4.06198762e-03  5.70425987e-02\n",
      "   5.22716716e-02  1.36308884e-02  4.91113700e-02  5.96387424e-02\n",
      "   3.41067836e-02  6.14957372e-03 -1.73467368e-01 -4.78291549e-02\n",
      "  -2.63561215e-02 -9.88908187e-02 -1.47302106e-01 -7.87347406e-02\n",
      "   3.68179157e-02  1.38830170e-02 -7.23017454e-02 -6.63053513e-01\n",
      "  -1.00595959e-01 -8.10410738e-01 -7.65733123e-02 -8.05037543e-02\n",
      "  -5.61300106e-02 -5.24448790e-03 -3.82110961e-02 -4.17400412e-02\n",
      "  -1.21943928e-01 -1.17205352e-01 -9.75911468e-02 -1.47588132e-02]\n",
      " [-4.01605874e-01 -4.21869457e-01 -2.01157629e-02 -3.57309937e-01\n",
      "  -5.74486256e-01 -8.97856057e-01 -3.50171804e-01 -9.34161425e-01\n",
      "  -2.01509818e-01 -1.24600351e-01 -8.67311656e-01 -1.45272031e-01\n",
      "  -8.24836314e-01 -8.73002052e-01  1.22906929e-02 -1.25697972e-02\n",
      "   3.48405726e-02  3.13940123e-02  1.33948522e-02 -2.39299387e-02\n",
      "   1.80905424e-02  4.27161790e-02  9.58670955e-03  3.47937644e-02\n",
      "   2.29466353e-02  1.58392191e-02  7.25457026e-03  4.30524210e-03\n",
      "   6.39051013e-03  1.88810714e-02  2.37430874e-02 -4.06275392e-02\n",
      "   3.00686527e-02  7.07147585e-04  7.09898099e-02  5.11539243e-02\n",
      "   7.29337037e-02 -4.49345261e-02  1.49735240e-02 -3.81701700e-02\n",
      "   8.12024772e-02  2.11800402e-03  2.29060203e-02  8.51197094e-02\n",
      "   5.13008162e-02  5.39009087e-02  8.52522254e-02  8.40875581e-02\n",
      "   5.42595088e-02 -1.37622533e-02 -2.57114507e-02  5.29894046e-02\n",
      "  -1.16827376e-02  8.27125907e-02  2.12582890e-02  5.07322587e-02\n",
      "   6.52897581e-02  3.24335322e-02  6.15605861e-02  6.37337789e-02\n",
      "  -1.30025251e-02  5.39003201e-02 -2.53804005e-03  7.19202831e-02\n",
      "   5.72821274e-02 -2.60137860e-02  8.96467343e-02  4.77138571e-02\n",
      "   2.32706722e-02  6.84785023e-02 -2.09210832e-02  5.36620431e-02\n",
      "   4.67051230e-02  4.65718210e-02  2.38384958e-02  6.67803362e-02\n",
      "   4.32055555e-02  7.48728737e-02  6.99967369e-02  4.88024056e-02\n",
      "   1.72551349e-02  4.14268896e-02 -2.25066487e-02  5.27408645e-02\n",
      "   4.58851568e-02  6.56665489e-02 -2.17905678e-02  2.50567478e-04\n",
      "  -2.83968337e-02  2.99872877e-03  1.76769048e-02 -3.98673229e-02\n",
      "   7.16647655e-02 -1.63161643e-02 -9.01225675e-03  2.02891789e-02\n",
      "   5.36353402e-02 -5.64594707e-03 -2.18707304e-02 -4.10881964e-03\n",
      "  -1.41799033e-01 -1.21668354e-01 -1.49661258e-01 -5.84493019e-02\n",
      "  -6.82200551e-01 -7.50566125e-01 -1.92735065e-02 -7.22366348e-02\n",
      "  -7.78921604e-01 -8.79692674e-01 -1.63578078e-01 -3.96499373e-02\n",
      "  -8.84267986e-02  1.80155337e-02 -3.18123966e-01 -2.00368959e-04\n",
      "  -3.74147408e-02 -4.02739018e-01 -2.05752507e-01  1.51659716e-02\n",
      "   1.06490729e-02  6.37893453e-02 -1.23964213e-01  3.05044353e-02\n",
      "   3.98797169e-02 -1.61227211e-01 -1.74605370e-01 -3.64328735e-02\n",
      "  -4.95369770e-02 -3.21744531e-02 -4.76006791e-02 -7.29810357e-01\n",
      "  -1.80154085e-01 -1.08652189e-01 -1.67212486e-01  7.20746964e-02\n",
      "  -8.59743282e-02 -9.35963392e-02 -1.75298706e-01 -1.45546108e-01\n",
      "  -9.05508459e-01 -8.99452925e-01 -1.59068391e-01 -4.05761331e-01\n",
      "  -2.89625004e-02 -6.40844628e-02 -4.90094461e-02 -1.96403921e-01\n",
      "  -1.97062507e-01 -1.19121298e-01 -8.73874903e-01 -1.17186582e+00\n",
      "  -1.10351372e+00 -5.89605272e-01 -7.36978590e-01 -1.16608843e-01\n",
      "   9.97070894e-02  8.62541273e-02  4.46710326e-02  9.04325992e-02\n",
      "   1.16155826e-01  1.23310164e-01  9.08291433e-03  7.67522082e-02\n",
      "   9.00141373e-02  9.15595889e-02 -6.82093203e-03  1.46128349e-02\n",
      "   2.97105080e-03  4.48773280e-02  1.78805869e-02  4.83249240e-02\n",
      "   6.00176789e-02  6.86214119e-02 -4.68067965e-03 -3.03384792e-02\n",
      "   1.15437964e-02 -2.57697608e-03 -7.98369944e-02  1.62669141e-02\n",
      "   3.11462698e-03  2.42746957e-02  1.84962153e-02  8.88646487e-03\n",
      "   2.09053271e-02  3.45413424e-02  1.82829099e-03  9.07407328e-02\n",
      "   1.13128126e-02  4.69691083e-02  3.02885305e-02 -9.40797571e-03\n",
      "   2.99391188e-02 -1.15533527e-02 -4.02010828e-02 -3.41907479e-02\n",
      "   1.39469933e-02  4.58184145e-02 -5.13288267e-02  1.44401612e-02\n",
      "  -3.97680104e-01 -4.88089949e-01 -5.52518368e-01 -2.64766395e-01\n",
      "  -1.27470136e-01 -3.76817703e-01 -1.01575994e+00 -1.14929724e+00\n",
      "  -9.03323054e-01 -7.99105167e-01 -1.02583432e+00 -9.47206736e-01\n",
      "  -5.88325143e-01 -3.41181159e-01 -9.96316731e-01 -4.11399305e-01\n",
      "  -3.55185419e-01 -3.88085723e-01 -3.23495418e-01 -1.15951931e+00\n",
      "  -3.26712519e-01 -5.34570038e-01 -9.98572111e-01 -1.31944090e-01\n",
      "  -6.27287626e-01 -2.55163908e-01 -3.39784175e-01 -4.63886082e-01\n",
      "  -7.81979740e-01 -1.03499389e+00 -3.53139162e-01 -4.05022949e-01\n",
      "  -6.21523380e-01 -1.17536032e+00 -4.00146067e-01 -3.23572457e-01\n",
      "  -3.62116516e-01 -2.95419842e-01 -3.93997103e-01 -1.10860312e+00\n",
      "  -1.18583202e+00 -4.54138398e-01 -5.41917145e-01 -6.57725871e-01\n",
      "  -6.78894594e-02 -1.21114516e+00 -1.20356691e+00 -4.26577479e-01\n",
      "  -3.40114444e-01 -2.89226264e-01 -5.95432818e-01 -4.28468078e-01\n",
      "  -4.10691112e-01 -3.91438752e-01 -5.21353304e-01 -1.05763316e+00\n",
      "  -6.68046251e-02 -5.84724426e-01 -6.98303401e-01 -7.18106568e-01\n",
      "  -9.39919204e-02 -5.96154273e-01 -1.12186290e-01 -1.25341460e-01\n",
      "  -1.80105582e-01 -9.94696021e-02 -1.23767495e-01 -1.56619176e-01\n",
      "  -2.21229166e-01 -7.61176109e-01 -2.97184557e-01 -3.32968116e-01\n",
      "  -5.49443901e-01 -1.02457441e-01 -1.69711143e-01 -1.06604114e-01]\n",
      " [ 9.04752761e-02  5.01678772e-02  5.18369302e-02  4.25842777e-02\n",
      "   4.21282649e-02  6.13735504e-02  2.93091517e-02 -1.03207016e-02\n",
      "   5.89822754e-02  4.09765281e-02  6.63418090e-03  4.34446372e-02\n",
      "   5.52072190e-02  4.22156826e-02  1.39739141e-02  1.68212466e-02\n",
      "  -7.63857722e-01 -2.69097775e-01 -1.44167936e+00 -4.97050397e-02\n",
      "  -3.80891174e-01 -6.29672229e-01 -1.29778609e-01 -1.98914185e-02\n",
      "  -9.44479585e-01 -1.41308069e+00 -7.71553278e-01 -3.14150184e-01\n",
      "  -9.50508863e-02 -3.13462555e-01 -9.89530921e-01 -3.75224143e-01\n",
      "  -6.67398944e-02 -1.53281959e-02 -7.62543142e-01 -1.23399600e-01\n",
      "  -6.67530715e-01 -2.91678216e-03 -6.94443226e-01 -2.18077511e-01\n",
      "  -1.24189520e+00 -3.23475629e-01 -8.27255426e-04 -7.35679030e-01\n",
      "  -8.59037936e-02 -9.67029333e-01 -1.22232199e-01 -8.02057803e-01\n",
      "  -2.48332724e-01 -4.16522026e-01 -2.09871694e-01 -8.77752721e-01\n",
      "  -2.60265768e-02 -8.32656696e-02 -7.05577955e-02 -1.07121336e+00\n",
      "  -4.84523214e-02 -3.41054142e-01 -1.99790105e-01 -3.10482919e-01\n",
      "  -1.26199618e-01 -6.42269701e-02 -1.20286763e+00 -1.11836350e+00\n",
      "  -1.70438915e-01 -1.78130150e-01 -1.38078439e+00 -9.08552170e-01\n",
      "  -3.55564728e-02 -2.67463416e-01 -1.88899249e-01 -7.50623286e-01\n",
      "  -3.84936452e-01 -2.10879259e-02 -9.71232951e-02 -1.70805228e+00\n",
      "  -1.59573257e-01 -9.03976932e-02 -7.41360366e-01 -7.45220542e-01\n",
      "  -3.62594910e-02 -2.37360016e-01  1.14684254e-02 -6.40726388e-01\n",
      "  -1.54961333e-01 -6.33862317e-01  3.84691022e-02  8.90426990e-03\n",
      "   9.51202400e-03  4.62731011e-02 -1.11437561e-02  4.69993539e-02\n",
      "  -4.45988655e-01  3.91619094e-02 -3.30787338e-02 -7.22123608e-02\n",
      "   5.05941473e-02 -1.27836484e-02 -1.01709872e-01 -9.51784849e-02\n",
      "  -2.41808109e-02 -6.73005581e-02  4.98580225e-02 -4.12168913e-02\n",
      "   3.82828079e-02  2.69855261e-02 -4.56703529e-02  7.77385710e-03\n",
      "   6.98193237e-02  5.10574430e-02  2.11073644e-02  1.12753054e-02\n",
      "   1.18485698e-02 -2.59818342e-02  2.09998600e-02  3.06085590e-03\n",
      "   2.74932887e-02  3.69169773e-03  1.33357570e-02 -5.62849641e-02\n",
      "   3.40947062e-02 -9.23482358e-01  8.85616057e-03 -3.45083892e-01\n",
      "  -2.90689975e-01  8.35284591e-02 -2.95764226e-02 -1.16214857e-01\n",
      "   3.99027355e-02 -2.01516822e-01 -4.53565881e-05  8.60082954e-02\n",
      "  -1.97200645e-02  2.43821479e-02  1.74491312e-02  1.96748227e-02\n",
      "  -1.82762630e-02  9.56926215e-03  9.71783604e-03  6.62978292e-02\n",
      "   1.39009580e-02  5.98581955e-02  3.99822928e-02  1.50538748e-02\n",
      "  -2.73477919e-02  6.89791574e-04  5.79898581e-02  3.43924910e-02\n",
      "   5.76841570e-02  9.17587504e-02 -1.15708467e-02  4.38289791e-02\n",
      "   7.22219497e-02  3.99621204e-02  3.35860811e-02  3.56507413e-02\n",
      "  -1.37543404e+00 -7.41463184e-01 -5.33472607e-03 -6.15233362e-01\n",
      "  -1.11934149e+00 -5.43115675e-01 -2.13422596e-01 -2.53328681e-01\n",
      "   9.89283621e-03 -4.68280971e-01 -2.21184809e-02  5.39678335e-02\n",
      "  -1.03268335e-02  3.98896523e-02 -2.89190933e-03  2.54860502e-02\n",
      "  -6.88006997e-01 -4.60110337e-01 -1.30643144e-01 -3.96290421e-02\n",
      "  -1.84476133e-02 -5.38888164e-02 -1.04026310e-01 -6.35466948e-02\n",
      "  -2.07378548e-02  3.47086415e-02  3.49411555e-02  2.92750560e-02\n",
      "   2.29498129e-02  6.44759536e-02 -3.23263779e-02 -2.44297966e-01\n",
      "   3.79783995e-02  5.39490394e-02  6.57082722e-02  2.61087455e-02\n",
      "   1.07628472e-01  1.78399514e-02  2.42078230e-02  2.18453798e-02\n",
      "  -1.42521234e-02 -2.28281133e-02  5.94517738e-02  1.11438252e-01\n",
      "  -2.04431415e-02 -1.03766993e-01 -3.17835689e-01 -1.17994070e+00\n",
      "  -1.97058856e-01 -7.42862940e-01 -1.66048124e-01 -1.19409099e-01\n",
      "   2.65983902e-02 -1.33537248e-01 -1.14032947e-01  6.74540177e-02\n",
      "   7.29400888e-02  6.15804270e-02  5.06154895e-02 -5.76566383e-02\n",
      "  -4.17925537e-01 -1.70891657e-01 -5.12216628e-01 -3.12118530e-02\n",
      "  -2.86071956e-01  1.51817119e-02  7.51373768e-02  7.94392899e-02\n",
      "   7.92082325e-02  4.35388200e-02 -3.55171919e-01 -8.88645574e-02\n",
      "  -4.38191146e-02 -7.37815082e-01 -3.34383130e-01 -5.27547337e-02\n",
      "  -3.31881307e-02 -1.49948999e-01 -9.82414544e-01 -3.98743480e-01\n",
      "  -1.17909811e-01 -3.21890473e-01 -1.99146584e-01 -2.15667784e-01\n",
      "  -3.96935433e-01 -3.82181220e-02 -2.43485317e-01  7.57168680e-02\n",
      "   4.95936237e-02 -4.68365997e-01 -8.87456387e-02 -5.34289896e-01\n",
      "  -2.17448801e-01 -7.29133487e-01 -3.35555561e-02 -7.63034046e-01\n",
      "  -2.47182131e-01  2.20747627e-02  6.59083501e-02 -9.86951068e-02\n",
      "  -1.49588054e-03 -5.02425320e-02 -4.73037153e-01 -4.12457958e-02\n",
      "   7.61038363e-02 -2.98169497e-02 -1.11214504e-01 -6.65244758e-02\n",
      "  -2.38329262e-01 -1.80642828e-01 -7.96140432e-02 -6.19193837e-02\n",
      "  -1.51937366e-01  9.06085297e-02 -1.65046409e-01 -2.01517910e-01\n",
      "   5.47101274e-02  4.19956283e-04  3.84467207e-02  4.65066992e-02]\n",
      " [ 3.04171052e-02  2.71158330e-02 -1.50715411e-01 -6.70888484e-01\n",
      "  -4.48096156e-01 -2.03528348e-02 -1.84860900e-02  6.85537457e-02\n",
      "   1.47375492e-02 -1.06152797e+00  3.11128609e-02 -2.00489551e-01\n",
      "   3.22137438e-02 -2.05549169e-02 -1.70655504e-01 -4.58395153e-01\n",
      "   4.17667814e-02  1.74300943e-03  2.61293985e-02  5.24955569e-03\n",
      "   1.34237565e-03  7.67910480e-02  2.31181365e-02 -2.31861528e-02\n",
      "  -1.04418807e-02  2.37011481e-02  4.31680121e-02  1.02435919e-02\n",
      "   4.23838943e-02  3.08749937e-02  3.20964716e-02  6.77381828e-03\n",
      "  -9.37096417e-01 -1.19338311e-01 -1.16832972e+00 -2.08408117e-01\n",
      "  -9.17212725e-01 -3.95648986e-01 -9.14250836e-02  7.27382116e-03\n",
      "  -8.78135264e-01  6.67948797e-02 -1.18019879e+00 -8.79925609e-01\n",
      "  -1.13749492e+00 -4.31626916e-01 -9.04431760e-01 -1.05362964e+00\n",
      "  -2.61923045e-01 -2.88625993e-02  3.23947296e-02 -7.85955906e-01\n",
      "   1.25922058e-02 -3.38303477e-01 -1.57449469e-01 -7.42807519e-03\n",
      "  -1.27372110e+00  3.44094187e-02 -1.09590554e+00 -3.92190874e-01\n",
      "   2.40403805e-02 -3.97261918e-01  3.56405750e-02 -1.08348705e-01\n",
      "  -3.24609786e-01 -6.93275314e-03 -1.28426766e+00  4.54493165e-02\n",
      "  -1.84032023e-01 -6.14050508e-01  3.30339111e-02 -1.17988241e+00\n",
      "  -1.13694406e+00 -4.55091119e-01 -9.21486095e-02 -9.85693872e-01\n",
      "  -4.00622278e-01 -1.49690878e+00 -4.17178959e-01 -3.81077826e-01\n",
      "  -1.97542652e-01 -2.20757917e-01 -2.99617916e-01 -4.47964281e-01\n",
      "  -2.04989314e-01 -1.00366485e+00 -2.58500546e-01 -1.43687293e-01\n",
      "  -3.28782707e-01 -7.52757251e-01 -1.05849588e+00 -2.82211393e-01\n",
      "  -1.81960672e-01 -1.54613987e-01 -1.20357990e+00 -1.21337712e+00\n",
      "  -1.11833799e+00 -3.41911584e-01 -4.65834178e-02  3.08879768e-03\n",
      "  -2.17822790e-02  4.73126732e-02 -1.47781476e-01 -5.59060136e-04\n",
      "   3.34362462e-02  2.06096787e-02  4.36700694e-02 -1.27086798e-02\n",
      "  -1.03323236e-02 -2.77973637e-02 -7.75557309e-02  2.47908421e-02\n",
      "   1.62653849e-02 -8.11023235e-01 -4.12524134e-01 -1.98392704e-01\n",
      "  -9.22184903e-03 -3.20188701e-03  1.02340588e-02  2.17135251e-02\n",
      "  -7.39206327e-03  4.21524867e-02  8.68592560e-02  6.05050894e-03\n",
      "   1.17994547e-01  2.68834028e-02  9.16533470e-02  1.15962043e-01\n",
      "  -8.70487764e-02  5.29434159e-02  1.29016989e-03  6.18838258e-02\n",
      "   5.90583086e-02  4.37328033e-02  1.41251022e-02  5.52648641e-02\n",
      "   5.30752465e-02  2.15738229e-02  4.02872562e-02 -2.73270030e-02\n",
      "  -1.06362097e-01 -1.65988639e-01  7.21711293e-02  2.33955774e-02\n",
      "   2.58910432e-02  4.32754233e-02  6.65493831e-02  3.07231378e-02\n",
      "   1.18491072e-02  4.44112644e-02  2.50663292e-02 -1.82630822e-01\n",
      "  -1.61356434e-01 -6.75014779e-02 -4.46707606e-01 -3.49314651e-03\n",
      "   1.09747559e-01  1.08070299e-01  9.83819813e-02  9.95026752e-02\n",
      "   1.04426958e-01  1.07122548e-01  6.01579770e-02  9.35755074e-02\n",
      "   8.33725929e-02  1.13665171e-01  1.64319817e-02 -9.87007469e-03\n",
      "  -1.36993574e-02  8.75222776e-03 -8.72663260e-02  4.46044989e-02\n",
      "   4.42006625e-02  5.27121127e-02  1.98887587e-02 -1.08309053e-02\n",
      "   2.04811990e-02  6.45006169e-03  1.36690764e-02  3.34255360e-02\n",
      "   8.10555518e-02  8.26330706e-02  6.75268322e-02  7.37913847e-02\n",
      "   3.44737545e-02  5.82576618e-02 -2.92674273e-01  1.01648502e-01\n",
      "   2.64276136e-02 -6.04832135e-02  4.25596312e-02  8.43752921e-02\n",
      "   9.38203111e-02  3.41206975e-02 -2.46581882e-02 -3.85504565e-03\n",
      "   1.13105001e-02 -4.12426107e-02  1.78581215e-02  5.72115928e-02\n",
      "  -4.41688061e-01 -3.97786409e-01 -3.40948284e-01 -3.17854881e-01\n",
      "  -5.41653752e-01 -2.29390666e-01 -4.48594302e-01 -4.41572249e-01\n",
      "  -3.02559167e-01 -6.49198830e-01 -3.33507240e-01 -5.51199615e-01\n",
      "  -7.21708477e-01 -6.14140928e-01 -8.14845383e-01 -5.63731492e-01\n",
      "  -5.36763251e-01 -3.45507741e-01 -3.17169666e-01 -1.29456282e-01\n",
      "  -4.00276870e-01 -3.89691383e-01 -6.48315966e-01 -6.77003026e-01\n",
      "  -4.69066828e-01 -8.07423115e-01 -3.10637563e-01 -3.25165212e-01\n",
      "  -1.84471399e-01 -6.16348803e-01 -3.43559176e-01 -4.38661039e-01\n",
      "  -4.95750844e-01 -3.18024039e-01 -2.73312718e-01 -1.54032707e-01\n",
      "  -2.16300845e-01 -2.78603345e-01 -3.46841574e-01 -3.28305393e-01\n",
      "  -5.68477929e-01 -4.42784399e-01 -3.12064230e-01 -5.70872128e-01\n",
      "  -8.56006563e-01 -2.81824023e-01 -1.66231081e-01 -2.03016326e-01\n",
      "  -2.41007254e-01 -1.73359379e-01 -4.05877739e-01 -2.14565605e-01\n",
      "  -2.08937988e-01 -1.44318819e-01 -1.65512457e-01  7.14400113e-02\n",
      "  -1.40451163e-01 -7.41745412e-01 -4.67953205e-01 -3.46789360e-01\n",
      "  -7.30952322e-01 -3.77173901e-01 -5.14578283e-01 -5.18745840e-01\n",
      "  -7.23728776e-01 -7.97033548e-01 -5.06075203e-01 -3.39763343e-01\n",
      "  -3.77013654e-01 -4.54337656e-01 -5.09058595e-01 -8.74108970e-01\n",
      "  -5.73563635e-01 -6.69199944e-01 -1.73652574e-01 -1.54066071e-01]\n",
      " [-6.65947676e-01 -5.01278758e-01 -1.03497937e-01  5.51470891e-02\n",
      "   5.72117567e-02 -4.42389324e-02 -3.57224382e-02 -5.85454643e-01\n",
      "  -6.78898275e-01  2.00436227e-02 -7.22845137e-01  5.65952808e-02\n",
      "  -5.06074905e-01 -1.87397987e-01 -2.42870096e-02 -3.56596448e-02\n",
      "  -2.06504717e-01  2.61806455e-02 -3.13294157e-02 -1.66327469e-02\n",
      "   2.88735237e-02 -5.84710479e-01 -1.54537882e-03 -3.39969359e-02\n",
      "   3.50373536e-02  1.00698485e-03 -1.27541181e-02 -3.93002369e-02\n",
      "  -8.32434654e-01 -3.21273468e-02 -1.04828730e-01 -4.73202206e-02\n",
      "   6.23182617e-02  3.88226286e-02  6.48073927e-02  4.95337956e-02\n",
      "   9.12434533e-02  3.85169238e-02  3.39043662e-02 -5.20374253e-03\n",
      "   8.26418623e-02 -1.20583862e-01  5.40315770e-02  8.87630433e-02\n",
      "   7.51216337e-02  7.53132254e-02  7.28166178e-02  7.75934160e-02\n",
      "   4.87830825e-02  1.36693777e-03 -3.84469442e-02  6.09100796e-02\n",
      "  -4.86844070e-02  8.20501372e-02  4.99318577e-02  3.18219326e-02\n",
      "   6.48194626e-02  5.85343428e-02  6.90649003e-02  5.93125261e-02\n",
      "  -2.98535153e-02  5.86278401e-02 -5.25596775e-02  6.64905310e-02\n",
      "   7.69823119e-02 -4.95587401e-02  9.39487889e-02  6.70292899e-02\n",
      "   4.90861349e-02  6.78088963e-02 -3.12553048e-02  6.96330965e-02\n",
      "   6.41185790e-02  7.59257376e-02  3.35029773e-02  7.15498328e-02\n",
      "   8.29252973e-02  7.10266531e-02  7.51554072e-02  6.75095543e-02\n",
      "   6.21711649e-02  6.40590861e-02  5.76733090e-02  6.56818002e-02\n",
      "   6.50161728e-02  7.60485381e-02  4.01676521e-02  1.38069531e-02\n",
      "   3.54332924e-02  6.00854233e-02  6.12385906e-02 -9.05844662e-03\n",
      "   6.41210377e-02  3.75338271e-02  4.42616306e-02  6.29531890e-02\n",
      "   6.67595863e-02  3.00734621e-02  1.36052202e-02 -6.95166830e-03\n",
      "  -7.14232177e-02 -5.88164628e-01 -1.27189964e-01 -4.44542497e-01\n",
      "  -5.47998130e-01 -5.76499522e-01 -3.00002955e-02 -1.33838087e-01\n",
      "  -7.69134879e-01 -7.26829708e-01 -1.26208961e-01 -7.11436033e-01\n",
      "  -4.41692062e-02  8.37181881e-02 -7.76958168e-02  5.08392556e-03\n",
      "  -6.02662303e-02 -6.10272408e-01 -3.65223677e-04 -3.48060243e-02\n",
      "  -3.23913433e-02  7.14285597e-02 -9.64407325e-01  9.70607325e-02\n",
      "  -1.05078685e+00 -1.08024693e+00 -1.03579557e+00 -9.73707676e-01\n",
      "   1.29823321e-02  6.44953772e-02 -1.35370135e-01 -5.93049467e-01\n",
      "  -7.72780538e-01 -6.52214170e-01 -1.13365509e-01 -4.10726875e-01\n",
      "  -1.05785973e-01 -7.09765255e-02 -6.83078885e-01 -9.24801409e-01\n",
      "  -3.79295796e-01 -4.51628864e-01  3.62595990e-02 -6.35330141e-01\n",
      "  -2.71504313e-01 -6.03538156e-01 -3.37328434e-01 -3.54062229e-01\n",
      "  -7.37708151e-01 -6.84169948e-01 -3.22088838e-01  2.56422833e-02\n",
      "  -7.34084189e-01 -2.49001030e-02 -4.00095820e-01 -5.64935029e-01\n",
      "  -1.01389313e+00 -1.02408174e-02 -4.81918722e-01 -6.19402885e-01\n",
      "  -7.97115862e-01  3.20816587e-04  4.65079956e-02 -2.62960374e-01\n",
      "  -5.14769256e-02 -6.57145202e-01  7.04761920e-03  4.30737389e-03\n",
      "  -9.67982486e-02  5.30622527e-02  7.90605992e-02 -5.47060668e-01\n",
      "   3.76279466e-02  1.52050536e-02 -2.27098055e-02 -6.36452362e-02\n",
      "   6.40312536e-03  4.65360619e-02  6.45574629e-02  1.32907294e-02\n",
      "  -6.53489590e-01 -4.71798867e-01 -1.90311804e-01 -3.42978716e-01\n",
      "  -9.55669135e-02 -4.51222919e-02  8.89262259e-02 -6.21093094e-01\n",
      "  -1.56030441e-02  2.20899507e-02 -7.21437559e-02  4.61253785e-02\n",
      "  -3.27172488e-01  4.45163175e-02  5.80406412e-02 -5.84278181e-02\n",
      "   6.24986887e-02  6.39410764e-02  3.25223058e-02 -2.27425992e-01\n",
      "  -3.77436608e-01 -7.55867898e-01 -6.06034935e-01 -1.11673450e+00\n",
      "   9.75411832e-02 -8.15563917e-01 -5.56551367e-02 -7.27276579e-02\n",
      "   4.12544496e-02 -1.80512130e-01  2.27293633e-02 -1.37680843e-01\n",
      "  -9.00743604e-02 -2.06795394e-01 -6.82809949e-02 -6.55569673e-01\n",
      "  -1.11305332e+00 -9.74594057e-01 -9.26771879e-01 -1.08071022e-01\n",
      "  -7.08386362e-01 -7.16565132e-01 -3.89445983e-02 -1.10325105e-01\n",
      "  -4.57977280e-02 -1.96099672e-02 -8.16248536e-01 -8.22206080e-01\n",
      "  -2.39148401e-02 -7.90249556e-04 -6.76949978e-01 -4.57165331e-01\n",
      "  -3.00449193e-01 -6.66866228e-02 -9.54191089e-01 -8.56838703e-01\n",
      "  -8.21631730e-01 -1.04285097e+00 -5.10948896e-01 -1.49083370e-02\n",
      "   7.33990397e-04 -3.11673343e-01 -1.03188205e+00 -1.13458961e-01\n",
      "  -1.07449234e-01 -1.55257378e-02  2.46214122e-03 -9.33816135e-01\n",
      "  -9.94664967e-01 -8.07678103e-01 -5.77693999e-01 -1.01537716e+00\n",
      "  -7.58612931e-01 -4.20048356e-01 -3.03884270e-03 -9.66127887e-02\n",
      "  -1.27147779e-01 -1.54727608e-01  1.82663801e-03 -2.22848937e-01\n",
      "  -1.49132669e-01 -5.33344567e-01  8.03682357e-02  9.95625705e-02\n",
      "   7.29947463e-02  9.60234702e-02  8.42061043e-02  8.01023543e-02\n",
      "   7.84138367e-02 -1.66052371e-01  7.78148845e-02  6.34485334e-02\n",
      "  -1.02215588e-01 -1.98285878e-02  9.01556462e-02 -2.28805151e-02]\n",
      " [-4.36827600e-01 -6.99748993e-01 -4.01181728e-01  3.83071080e-02\n",
      "  -4.88000922e-02 -1.87578708e-01 -3.96074682e-01 -7.21728086e-01\n",
      "  -6.13034308e-01 -1.96375191e-01 -7.89220989e-01 -4.15426865e-02\n",
      "  -8.81812036e-01 -7.41067886e-01 -8.14386383e-02 -8.39221105e-02\n",
      "   4.22123335e-02  8.64123031e-02  1.23949414e-02 -4.35044803e-02\n",
      "   6.02089465e-02 -2.23820165e-01  3.37413102e-02 -9.93106291e-02\n",
      "   4.85709757e-02  4.93090637e-02  2.43109223e-02 -1.89059775e-03\n",
      "  -6.26860186e-02  1.60199981e-02  4.39577177e-03 -5.88314421e-02\n",
      "   6.09210804e-02  2.88440268e-02  8.68140608e-02  6.27649352e-02\n",
      "   1.14596143e-01 -1.07173748e-01  4.36107777e-02  1.24405511e-02\n",
      "   8.44992027e-02 -1.65088460e-01  7.86604583e-02  9.29680765e-02\n",
      "   7.79682770e-02  8.00058097e-02  7.59318471e-02  6.98154494e-02\n",
      "   2.86244322e-02  2.34246701e-02 -2.63742078e-02  6.74906150e-02\n",
      "  -2.96947006e-02  9.31402445e-02  6.33229092e-02  3.63713317e-02\n",
      "   6.37792274e-02  6.98396564e-02  6.23802729e-02  3.69747356e-02\n",
      "  -1.99619196e-02  5.77142946e-02 -2.59025563e-02  6.89811483e-02\n",
      "   5.06752431e-02 -4.14419100e-02  1.07177205e-01  7.96368420e-02\n",
      "   6.22391887e-02  7.31818601e-02 -3.25069055e-02  8.67982134e-02\n",
      "   7.80111179e-02  7.01364502e-02  3.38632055e-02  9.62237269e-02\n",
      "   9.15265009e-02  8.48689005e-02  8.20451230e-02  6.83971941e-02\n",
      "   4.92261276e-02  8.10501128e-02  4.42152694e-02  7.91206881e-02\n",
      "   5.56199104e-02  8.34547654e-02  1.62467845e-02  1.83052162e-03\n",
      "   2.27491260e-02  4.24973778e-02  7.35887960e-02 -3.03040631e-02\n",
      "   5.02394624e-02 -1.14853866e-02  3.30905281e-02  6.37376755e-02\n",
      "   6.89038187e-02  4.81146313e-02  1.51869627e-02  5.92163997e-03\n",
      "  -1.29046699e-03 -5.72749257e-01  2.19221655e-02 -1.99849933e-01\n",
      "  -6.94822669e-01 -8.89456630e-01 -3.98652218e-02 -1.42600253e-01\n",
      "  -5.90058565e-01 -5.75821579e-01 -2.80781418e-01 -7.02982068e-01\n",
      "   7.95499533e-02  5.44710979e-02 -6.37226254e-02 -7.38350302e-02\n",
      "  -1.09448582e-01 -5.99421501e-01 -2.43930495e-03 -3.29390056e-02\n",
      "  -3.65665227e-01 -7.74603412e-02 -6.37508869e-01  9.11474898e-02\n",
      "  -5.27225256e-01 -7.10896194e-01 -3.87090534e-01 -4.68498379e-01\n",
      "   2.09339720e-04  6.68193400e-02 -4.66609895e-01 -1.05233443e+00\n",
      "  -3.47915441e-01 -4.25684392e-01 -7.01998591e-01 -3.57819647e-01\n",
      "  -5.66476285e-01 -5.67792475e-01 -6.85552657e-01 -6.48823678e-01\n",
      "  -7.04170167e-01 -5.97263455e-01 -2.90145010e-01 -1.78359851e-01\n",
      "  -4.48961318e-01 -5.28850853e-01 -3.07367086e-01 -4.72641528e-01\n",
      "  -4.21834886e-01 -4.33330864e-01 -8.74688566e-01 -5.19444883e-01\n",
      "  -4.96428072e-01 -6.65053904e-01 -6.12244368e-01 -5.33074379e-01\n",
      "  -5.23040950e-01 -6.74503982e-01 -5.65215230e-01 -5.06368279e-01\n",
      "  -7.11768866e-01 -5.90883613e-01  9.15778726e-02 -7.02630281e-01\n",
      "  -8.40122938e-01 -6.95517480e-01  6.86880946e-02 -1.38511118e-02\n",
      "  -1.85566232e-01 -1.52689457e-01 -2.20119655e-02 -8.23622823e-01\n",
      "   2.48903380e-05  7.97547307e-03 -6.37259543e-01 -1.57057658e-01\n",
      "  -3.77069801e-01 -1.14773829e-02 -1.63993612e-02 -2.66707271e-01\n",
      "  -7.01383531e-01 -7.47607470e-01 -7.14935243e-01 -7.19403803e-01\n",
      "  -7.54417181e-01 -8.64393413e-01  5.27311154e-02 -4.79987413e-01\n",
      "  -9.37192500e-01 -7.44830787e-01 -7.61001468e-01 -7.51447082e-01\n",
      "  -3.98450166e-01 -5.06434917e-01 -8.16289708e-02 -1.16395839e-01\n",
      "   2.67343465e-02  9.94485319e-02 -9.41690266e-01 -7.32299745e-01\n",
      "   2.17320099e-02  3.93768176e-02  4.73911390e-02  5.29051274e-02\n",
      "   1.04559787e-01  6.10460341e-02  9.28860437e-03 -5.32565173e-03\n",
      "   6.99072108e-02 -2.34358627e-02  3.63379791e-02 -6.70244731e-03\n",
      "  -3.29249687e-02 -1.70868412e-01 -3.52954492e-02 -1.95738561e-02\n",
      "  -4.14430387e-02  5.85879982e-02  9.48745087e-02  2.96999831e-02\n",
      "   5.73754385e-02  3.15396301e-02 -7.38002837e-01 -5.81979811e-01\n",
      "  -7.77175963e-01  2.34476328e-02  3.76750082e-02  3.58524956e-02\n",
      "   1.36358931e-03  1.14134094e-02  8.27385262e-02 -3.27147171e-02\n",
      "  -2.44416669e-02  5.25170267e-02  1.98581070e-02  5.60584553e-02\n",
      "   7.72343352e-02  1.36338398e-02  2.43207533e-02  4.33883406e-02\n",
      "   4.99408990e-02 -6.26844633e-03  4.88924198e-02 -5.17807186e-01\n",
      "  -1.69420332e-01  5.72667047e-02  1.41009903e-02  1.79259814e-02\n",
      "   3.39969695e-02 -4.15461473e-02  9.69686825e-03  3.36345248e-02\n",
      "   5.11144698e-02 -1.09961867e-01 -3.62299889e-01  9.47456434e-02\n",
      "  -1.40091121e-01 -9.94713679e-02 -1.84046719e-02 -1.04053020e-01\n",
      "  -1.49599046e-01  1.19539984e-01  9.17633176e-02  1.17394581e-01\n",
      "   7.66199529e-02  1.05475865e-01  9.23985392e-02  8.22822154e-02\n",
      "   7.88016841e-02 -9.26056206e-02  7.91525990e-02  7.17272535e-02\n",
      "  -1.15271568e-01  5.29867597e-02  8.93836245e-02  3.60017158e-02]\n",
      " [ 7.61940554e-02  7.57173523e-02 -5.80709726e-02 -5.84984243e-01\n",
      "  -1.23864830e-01  4.67862934e-02  2.86499243e-02  8.88381824e-02\n",
      "   4.90973853e-02 -5.92075810e-02  4.72555794e-02 -3.54957804e-02\n",
      "   7.02519193e-02  2.98616439e-02 -1.04545411e-02 -4.69921887e-01\n",
      "   1.94098556e-03 -1.29643247e-01  3.65218194e-03 -3.52453329e-02\n",
      "  -8.10632259e-02  3.79756317e-02  9.35744867e-03 -3.00769135e-02\n",
      "  -2.10187465e-01 -7.69436476e-04 -3.92557401e-03 -1.08043421e-02\n",
      "   2.84735598e-02 -3.33460118e-03 -8.64423998e-03 -8.98533780e-03\n",
      "  -7.58678734e-01 -6.59254074e-01 -5.56606591e-01 -7.25660324e-01\n",
      "  -6.52211010e-01 -5.55895925e-01 -1.99867919e-01 -1.64737236e-02\n",
      "  -8.52240145e-01  4.17447574e-02 -9.86694098e-01 -6.83729112e-01\n",
      "  -6.28581583e-01 -8.13798070e-01 -5.64491868e-01 -5.36215425e-01\n",
      "  -8.30019712e-01 -2.37386506e-02 -1.74186546e-02 -8.47878695e-01\n",
      "  -1.54697383e-02 -5.34067392e-01 -9.48934793e-01 -5.14383793e-01\n",
      "  -5.73052764e-01 -7.07207263e-01 -4.41588074e-01 -5.34777999e-01\n",
      "  -1.35950865e-02 -8.40776622e-01 -4.43329709e-03 -9.27846789e-01\n",
      "  -5.32630086e-01 -1.68521199e-02 -2.64284015e-01 -2.67137319e-01\n",
      "  -5.48617065e-01 -6.63851976e-01 -7.70154921e-03 -7.82366335e-01\n",
      "  -7.86941886e-01 -7.52248049e-01 -8.64726603e-01 -8.46903086e-01\n",
      "  -5.20182788e-01 -8.09697330e-01 -5.70150375e-01 -9.36280787e-01\n",
      "  -8.40829134e-01 -7.33270943e-01 -7.06842601e-01 -9.25781429e-01\n",
      "  -7.88158059e-01 -5.41696727e-01 -7.27023840e-01 -1.86723605e-01\n",
      "  -5.26176274e-01 -9.07145739e-01 -7.62976229e-01 -5.49711943e-01\n",
      "  -7.93330848e-01 -3.83053750e-01 -8.88276160e-01 -8.56365502e-01\n",
      "  -8.50880504e-01 -6.27653897e-01 -3.59318368e-02 -1.12446425e-02\n",
      "   2.23454125e-02  7.46666938e-02  4.40793522e-02  3.52919325e-02\n",
      "   7.57441372e-02  7.00369999e-02  3.95148508e-02  6.20709267e-03\n",
      "   8.95951688e-02  5.06851934e-02  6.62441999e-02  4.11093496e-02\n",
      "   2.85411589e-02 -2.32327580e-01  3.43581401e-02  3.59812169e-03\n",
      "  -1.45606324e-03  4.20294851e-02  2.43762080e-02 -2.20587011e-03\n",
      "  -6.48582773e-03  5.34265675e-02  1.13907009e-01 -1.11789387e-02\n",
      "   1.47093311e-01  5.13956882e-02  1.32652074e-01  1.42483562e-01\n",
      "  -3.87552083e-02  7.27650151e-02 -1.85292866e-02  6.65042102e-02\n",
      "   9.10148397e-02  5.32322563e-02 -4.37723175e-02  4.77590039e-02\n",
      "   6.51511475e-02 -7.16789393e-03  3.60651799e-02 -2.87817568e-02\n",
      "  -1.28184468e-01 -1.60816446e-01  5.10106049e-03  1.23802070e-02\n",
      "   1.47991143e-02  2.76319329e-02  2.76980102e-02 -4.02507745e-02\n",
      "  -1.29498625e-02  1.92462951e-02  1.09442016e-02 -1.87998787e-01\n",
      "   2.14010961e-02 -2.03745775e-02 -3.62795383e-01 -1.05912909e-01\n",
      "   1.09662086e-01  8.44468698e-02  8.55981410e-02  8.90820920e-02\n",
      "   9.77431238e-02  7.67901912e-02  4.53171097e-02  6.40952513e-02\n",
      "   5.11072353e-02  9.03363526e-02  1.06192855e-02 -3.84635702e-02\n",
      "  -5.19758426e-02  2.09390260e-02 -4.74196300e-02  5.75893521e-02\n",
      "   1.23410230e-03  9.27735120e-03  3.49872746e-02 -3.15421782e-02\n",
      "   2.44645290e-02 -2.46212818e-02  1.02025960e-02  2.45768651e-02\n",
      "   8.67051780e-02  1.10942297e-01  5.53864203e-02  6.78403303e-02\n",
      "   5.54746874e-02  4.69752848e-02 -2.07717329e-01  9.88005325e-02\n",
      "   2.53871642e-02 -1.80477574e-02  4.40782756e-02  4.71001677e-02\n",
      "   9.86321270e-02  2.92791496e-03 -4.67259623e-03 -7.36400485e-02\n",
      "  -7.03321164e-03 -3.82092781e-02  2.57732365e-02  4.60970737e-02\n",
      "  -4.22057882e-02 -5.40984683e-02 -8.81936178e-02 -8.84794965e-02\n",
      "  -5.49036205e-01 -1.53892800e-01 -1.49840936e-01 -8.23166966e-02\n",
      "  -2.34654233e-01 -8.56786892e-02 -9.34619904e-02 -3.88831139e-01\n",
      "  -7.71523774e-01 -8.96596387e-02 -5.90765774e-02 -1.03979290e-01\n",
      "  -8.11595321e-02 -7.54285231e-02 -1.81599379e-01 -2.48078614e-01\n",
      "  -7.17600659e-02 -9.46460217e-02 -1.37703240e-01 -1.67301044e-01\n",
      "  -3.87056708e-01 -5.97560763e-01 -1.47246763e-01 -8.85788500e-02\n",
      "  -1.54828280e-01 -8.62361565e-02 -1.28474370e-01 -8.29193294e-02\n",
      "  -1.33934896e-02 -2.82916933e-01 -1.59695938e-01 -1.39729187e-01\n",
      "  -4.02060784e-02 -1.49377733e-01 -5.43743111e-02 -4.87839244e-02\n",
      "  -8.35089982e-02 -4.67296839e-02 -4.20906730e-02 -9.48460221e-01\n",
      "  -5.64664483e-01 -2.91860968e-01 -3.75278652e-01 -2.40546599e-01\n",
      "  -1.30735934e-01 -2.18354285e-01 -1.56389669e-01 -1.16437167e-01\n",
      "  -5.08871935e-02 -2.97159433e-01 -2.56531775e-01 -4.14274514e-01\n",
      "  -6.51866719e-02 -1.14213347e-01 -4.45267648e-01 -3.84141505e-02\n",
      "  -1.85798541e-01 -3.70539188e-01 -4.03433800e-01 -8.36773753e-01\n",
      "  -2.75425941e-01 -6.04538500e-01  6.28283620e-02 -2.11098611e-01\n",
      "  -4.98115152e-01 -6.07575536e-01 -1.09451756e-01 -9.65780169e-02\n",
      "  -3.95396978e-01 -3.91135037e-01 -3.55381429e-01 -1.58231765e-01]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(weights_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "id": "51351361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt('weights.txt', weights_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "id": "8d1ad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def format_weights_for_arduino(weights_array, max_line_length=80):\n",
    "    # Convert the array to a string with desired precision\n",
    "    weights_str = np.array2string(weights_array, separator=',', precision=8, floatmode='fixed', suppress_small=True)\n",
    "    \n",
    "    # Remove unnecessary characters\n",
    "    weights_str = weights_str.replace('[', '{').replace(']', '}').replace('\\n', '').replace(' ', '')\n",
    "    \n",
    "    # Split the string into lines\n",
    "    lines = [weights_str[i:i+max_line_length] for i in range(0, len(weights_str), max_line_length)]\n",
    "    \n",
    "    # Join the lines with newline characters\n",
    "    formatted_weights = '\\n'.join(lines)\n",
    "    \n",
    "    return formatted_weights\n",
    "\n",
    "formatted_weights = format_weights_for_arduino(weights_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "id": "d38780f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1231], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pd\u001b[39m.\u001b[39mset_option(\u001b[39m'\u001b[39m\u001b[39mdisplay.max_colwidth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1000\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(formatted_weights\u001b[39m.\u001b[39;49mto_list())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "print(formatted_weights.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf173aa",
   "metadata": {},
   "source": [
    "# rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43274a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 3), name='in')\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_T')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu', name='bottle_T')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid', name='out_T')(d_T)\n",
    "\n",
    "# STUDENT MODEL\n",
    "x_S = Conv1D(kernel_size = 7, filters=8, activation='relu', name='l1_S')(dataInp) # 194, 8\n",
    "x_S = BatchNormalization()(x_S)\n",
    "x_S = MaxPooling1D(4)(x_S) # 48, 32\n",
    "x_b_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "avg_S = GlobalAveragePooling1D()(x_b_S)\n",
    "max_S = GlobalMaxPooling1D()(x_b_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "\n",
    "#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\n",
    "abs_sub = tf.abs(x_b_T - x_b_S)\n",
    "l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "833ac35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model_t.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0cbb10ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_130 (MaxPool  (None, 97, 32)               0         ['batch_normalization_62[0][0]\n",
      " ing1D)                                                             ']                            \n",
      "                                                                                                  \n",
      " conv1d_203 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_130[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_131 (MaxPool  (None, 46, 64)               0         ['conv1d_203[0][0]']          \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 46, 64)               256       ['max_pooling1d_131[0][0]']   \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_204 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['conv1d_204[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_132 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " conv1d_205 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_132[0][0]']   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 512)                  0         ['conv1d_205[0][0]']          \n",
      " 8 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_43 (G  (None, 512)                  0         ['conv1d_205[0][0]']          \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " add_36 (Add)                (None, 512)                  0         ['global_average_pooling1d_38[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_43[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_71 (Dense)            (None, 300)                  153900    ['add_36[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)        (None, 300)                  0         ['dense_71[0][0]']            \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_36[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 372048 (1.42 MB)\n",
      "Trainable params: 371856 (1.42 MB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.fit(np.array(x_train), np.array(y_train), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8ea2e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s = Model(inputs=dataInp, outputs=out_S, name='teacher' )\n",
    "model_s.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "03a70285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_133 (MaxPool  (None, 48, 8)                0         ['batch_normalization_64[0][0]\n",
      " ing1D)                                                             ']                            \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_133[0][0]']   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 9 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_44 (G  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " add_37 (Add)                (None, 32)                   0         ['global_average_pooling1d_39[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_44[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 32)                   0         ['add_37[0][0]']              \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11140 (43.52 KB)\n",
      "Trainable params: 11124 (43.45 KB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b855a094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 5.4584 - accuracy: 0.0060\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 5.1703 - accuracy: 0.0200\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 4.8890 - accuracy: 0.0313\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 4.6056 - accuracy: 0.0427\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 4.3571 - accuracy: 0.0620\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 4.1456 - accuracy: 0.0860\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.9200 - accuracy: 0.1100\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.7307 - accuracy: 0.1273\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.5086 - accuracy: 0.1627\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.4272 - accuracy: 0.1813\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.2892 - accuracy: 0.1867\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.1195 - accuracy: 0.2160\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 3.0296 - accuracy: 0.2507\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.8730 - accuracy: 0.2633\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.7938 - accuracy: 0.2713\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.6788 - accuracy: 0.2960\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.6070 - accuracy: 0.3153\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.4729 - accuracy: 0.3600\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.4610 - accuracy: 0.3453\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.3441 - accuracy: 0.3820\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.2850 - accuracy: 0.4007\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 2.2519 - accuracy: 0.3967\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 2.1319 - accuracy: 0.4327\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 2.1158 - accuracy: 0.4080\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.9995 - accuracy: 0.4560\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.9842 - accuracy: 0.4560\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.9502 - accuracy: 0.4740\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.9223 - accuracy: 0.4713\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.8394 - accuracy: 0.4913\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.8067 - accuracy: 0.4940\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.7423 - accuracy: 0.5207\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.7528 - accuracy: 0.5020\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.6820 - accuracy: 0.5333\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.7009 - accuracy: 0.5160\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.6956 - accuracy: 0.5240\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.6773 - accuracy: 0.5320\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.6072 - accuracy: 0.5473\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.6106 - accuracy: 0.5473\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.4960 - accuracy: 0.5867\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.5226 - accuracy: 0.5647\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.4436 - accuracy: 0.5880\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.4614 - accuracy: 0.5627\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.4522 - accuracy: 0.5893\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.4266 - accuracy: 0.5967\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.4276 - accuracy: 0.5773\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.3902 - accuracy: 0.6000\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.3958 - accuracy: 0.5920\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.3782 - accuracy: 0.6133\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.3459 - accuracy: 0.6220\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.3421 - accuracy: 0.6093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x37cc585d0>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s.fit(np.array(x_train), np.array(y_train), epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "d584459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_st = Model(inputs=dataInp, outputs=[out_T, out_S], name='teacher' )\n",
    "model_st.compile(loss = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"], optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "29cb19dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " in (InputLayer)             [(None, 200, 3)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_T (Conv1D)               (None, 194, 32)              704       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 194, 32)              128       ['l1_T[0][0]']                \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_130 (MaxPool  (None, 97, 32)               0         ['batch_normalization_62[0][0]\n",
      " ing1D)                                                             ']                            \n",
      "                                                                                                  \n",
      " conv1d_203 (Conv1D)         (None, 93, 64)               10304     ['max_pooling1d_130[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_131 (MaxPool  (None, 46, 64)               0         ['conv1d_203[0][0]']          \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 46, 64)               256       ['max_pooling1d_131[0][0]']   \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_204 (Conv1D)         (None, 44, 256)              49408     ['batch_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " bottle_T (Conv1D)           (None, 42, 32)               24608     ['conv1d_204[0][0]']          \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 8)               176       ['in[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_132 (MaxPool  (None, 21, 32)               0         ['bottle_T[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 194, 8)               32        ['l1_S[0][0]']                \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv1d_205 (Conv1D)         (None, 19, 512)              49664     ['max_pooling1d_132[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_133 (MaxPool  (None, 48, 8)                0         ['batch_normalization_64[0][0]\n",
      " ing1D)                                                             ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 512)                  0         ['conv1d_205[0][0]']          \n",
      " 8 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_43 (G  (None, 512)                  0         ['conv1d_205[0][0]']          \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               1824      ['max_pooling1d_133[0][0]']   \n",
      "                                                                                                  \n",
      " add_36 (Add)                (None, 512)                  0         ['global_average_pooling1d_38[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_43[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " 9 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_44 (G  (None, 32)                   0         ['bottle_S[0][0]']            \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " dense_71 (Dense)            (None, 300)                  153900    ['add_36[0][0]']              \n",
      "                                                                                                  \n",
      " add_37 (Add)                (None, 32)                   0         ['global_average_pooling1d_39[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'global_max_pooling1d_44[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)        (None, 300)                  0         ['dense_71[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 32)                   0         ['add_37[0][0]']              \n",
      "                                                                                                  \n",
      " out_T (Dense)               (None, 276)                  83076     ['dropout_36[0][0]']          \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  9108      ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 383188 (1.46 MB)\n",
      "Trainable params: 382980 (1.46 MB)\n",
      "Non-trainable params: 208 (832.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_st.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "bd7b7e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 2s 19ms/step - loss: 1.4437 - out_T_loss: 0.0919 - out_S_loss: 1.3518 - out_T_accuracy: 0.9707 - out_S_accuracy: 0.5967 - val_loss: 1.7984 - val_out_T_loss: 0.5001 - val_out_S_loss: 1.2983 - val_out_T_accuracy: 0.8780 - val_out_S_accuracy: 0.6840\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 1.3476 - out_T_loss: 0.0890 - out_S_loss: 1.2586 - out_T_accuracy: 0.9733 - out_S_accuracy: 0.6433 - val_loss: 1.9806 - val_out_T_loss: 0.7878 - val_out_S_loss: 1.1928 - val_out_T_accuracy: 0.8320 - val_out_S_accuracy: 0.7120\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 1.3809 - out_T_loss: 0.0786 - out_S_loss: 1.3023 - out_T_accuracy: 0.9787 - out_S_accuracy: 0.6260 - val_loss: 1.5285 - val_out_T_loss: 0.3646 - val_out_S_loss: 1.1639 - val_out_T_accuracy: 0.9200 - val_out_S_accuracy: 0.7260\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.3268 - out_T_loss: 0.0813 - out_S_loss: 1.2455 - out_T_accuracy: 0.9753 - out_S_accuracy: 0.6420 - val_loss: 1.8357 - val_out_T_loss: 0.7107 - val_out_S_loss: 1.1250 - val_out_T_accuracy: 0.8700 - val_out_S_accuracy: 0.7280\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.4249 - out_T_loss: 0.1329 - out_S_loss: 1.2920 - out_T_accuracy: 0.9680 - out_S_accuracy: 0.6140 - val_loss: 1.5745 - val_out_T_loss: 0.4721 - val_out_S_loss: 1.1024 - val_out_T_accuracy: 0.9080 - val_out_S_accuracy: 0.7340\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 1.2964 - out_T_loss: 0.0692 - out_S_loss: 1.2272 - out_T_accuracy: 0.9753 - out_S_accuracy: 0.6453 - val_loss: 1.7195 - val_out_T_loss: 0.5984 - val_out_S_loss: 1.1211 - val_out_T_accuracy: 0.8880 - val_out_S_accuracy: 0.7340\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.3804 - out_T_loss: 0.1165 - out_S_loss: 1.2639 - out_T_accuracy: 0.9727 - out_S_accuracy: 0.6393 - val_loss: 1.5615 - val_out_T_loss: 0.4548 - val_out_S_loss: 1.1067 - val_out_T_accuracy: 0.9020 - val_out_S_accuracy: 0.7260\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.3622 - out_T_loss: 0.1222 - out_S_loss: 1.2401 - out_T_accuracy: 0.9640 - out_S_accuracy: 0.6440 - val_loss: 1.5647 - val_out_T_loss: 0.4573 - val_out_S_loss: 1.1074 - val_out_T_accuracy: 0.9060 - val_out_S_accuracy: 0.7200\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 1.2305 - out_T_loss: 0.0564 - out_S_loss: 1.1740 - out_T_accuracy: 0.9847 - out_S_accuracy: 0.6473 - val_loss: 1.4168 - val_out_T_loss: 0.3649 - val_out_S_loss: 1.0519 - val_out_T_accuracy: 0.9080 - val_out_S_accuracy: 0.7380\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 1.1999 - out_T_loss: 0.0334 - out_S_loss: 1.1665 - out_T_accuracy: 0.9913 - out_S_accuracy: 0.6627 - val_loss: 1.4041 - val_out_T_loss: 0.3243 - val_out_S_loss: 1.0798 - val_out_T_accuracy: 0.9320 - val_out_S_accuracy: 0.7360\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 1.2331 - out_T_loss: 0.0682 - out_S_loss: 1.1649 - out_T_accuracy: 0.9793 - out_S_accuracy: 0.6487 - val_loss: 1.4721 - val_out_T_loss: 0.3554 - val_out_S_loss: 1.1167 - val_out_T_accuracy: 0.9080 - val_out_S_accuracy: 0.7180\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 1.2529 - out_T_loss: 0.0927 - out_S_loss: 1.1602 - out_T_accuracy: 0.9767 - out_S_accuracy: 0.6520 - val_loss: 1.4346 - val_out_T_loss: 0.4029 - val_out_S_loss: 1.0317 - val_out_T_accuracy: 0.9080 - val_out_S_accuracy: 0.7380\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 1.2148 - out_T_loss: 0.0720 - out_S_loss: 1.1428 - out_T_accuracy: 0.9773 - out_S_accuracy: 0.6580 - val_loss: 1.3370 - val_out_T_loss: 0.2920 - val_out_S_loss: 1.0450 - val_out_T_accuracy: 0.9380 - val_out_S_accuracy: 0.7460\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 1.1459 - out_T_loss: 0.0338 - out_S_loss: 1.1121 - out_T_accuracy: 0.9893 - out_S_accuracy: 0.6847 - val_loss: 1.4101 - val_out_T_loss: 0.3326 - val_out_S_loss: 1.0775 - val_out_T_accuracy: 0.9400 - val_out_S_accuracy: 0.7480\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 1.1878 - out_T_loss: 0.0452 - out_S_loss: 1.1427 - out_T_accuracy: 0.9873 - out_S_accuracy: 0.6560 - val_loss: 1.3550 - val_out_T_loss: 0.2542 - val_out_S_loss: 1.1009 - val_out_T_accuracy: 0.9400 - val_out_S_accuracy: 0.7160\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 1.1410 - out_T_loss: 0.0563 - out_S_loss: 1.0847 - out_T_accuracy: 0.9860 - out_S_accuracy: 0.6800 - val_loss: 1.4409 - val_out_T_loss: 0.4302 - val_out_S_loss: 1.0107 - val_out_T_accuracy: 0.9020 - val_out_S_accuracy: 0.7460\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.2056 - out_T_loss: 0.0664 - out_S_loss: 1.1392 - out_T_accuracy: 0.9807 - out_S_accuracy: 0.6647 - val_loss: 1.4976 - val_out_T_loss: 0.4765 - val_out_S_loss: 1.0212 - val_out_T_accuracy: 0.8960 - val_out_S_accuracy: 0.7420\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.2956 - out_T_loss: 0.1120 - out_S_loss: 1.1836 - out_T_accuracy: 0.9747 - out_S_accuracy: 0.6520 - val_loss: 2.0899 - val_out_T_loss: 0.7334 - val_out_S_loss: 1.3564 - val_out_T_accuracy: 0.8580 - val_out_S_accuracy: 0.6220\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.2131 - out_T_loss: 0.1032 - out_S_loss: 1.1099 - out_T_accuracy: 0.9740 - out_S_accuracy: 0.6633 - val_loss: 1.3537 - val_out_T_loss: 0.3624 - val_out_S_loss: 0.9914 - val_out_T_accuracy: 0.9260 - val_out_S_accuracy: 0.7600\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.1690 - out_T_loss: 0.0532 - out_S_loss: 1.1158 - out_T_accuracy: 0.9853 - out_S_accuracy: 0.6693 - val_loss: 1.3238 - val_out_T_loss: 0.3261 - val_out_S_loss: 0.9977 - val_out_T_accuracy: 0.9280 - val_out_S_accuracy: 0.7440\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0747 - out_T_loss: 0.0534 - out_S_loss: 1.0212 - out_T_accuracy: 0.9827 - out_S_accuracy: 0.6967 - val_loss: 1.2805 - val_out_T_loss: 0.3093 - val_out_S_loss: 0.9712 - val_out_T_accuracy: 0.9260 - val_out_S_accuracy: 0.7600\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 1.1487 - out_T_loss: 0.0308 - out_S_loss: 1.1179 - out_T_accuracy: 0.9907 - out_S_accuracy: 0.6720 - val_loss: 1.2843 - val_out_T_loss: 0.3128 - val_out_S_loss: 0.9715 - val_out_T_accuracy: 0.9340 - val_out_S_accuracy: 0.7580\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 1.1486 - out_T_loss: 0.0619 - out_S_loss: 1.0867 - out_T_accuracy: 0.9847 - out_S_accuracy: 0.6860 - val_loss: 1.5729 - val_out_T_loss: 0.6049 - val_out_S_loss: 0.9680 - val_out_T_accuracy: 0.8520 - val_out_S_accuracy: 0.7600\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0644 - out_T_loss: 0.0555 - out_S_loss: 1.0089 - out_T_accuracy: 0.9853 - out_S_accuracy: 0.7147 - val_loss: 1.1891 - val_out_T_loss: 0.1847 - val_out_S_loss: 1.0044 - val_out_T_accuracy: 0.9540 - val_out_S_accuracy: 0.7460\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0829 - out_T_loss: 0.0443 - out_S_loss: 1.0386 - out_T_accuracy: 0.9880 - out_S_accuracy: 0.7040 - val_loss: 1.4364 - val_out_T_loss: 0.4493 - val_out_S_loss: 0.9871 - val_out_T_accuracy: 0.9060 - val_out_S_accuracy: 0.7540\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0714 - out_T_loss: 0.0409 - out_S_loss: 1.0305 - out_T_accuracy: 0.9860 - out_S_accuracy: 0.7033 - val_loss: 1.3941 - val_out_T_loss: 0.4074 - val_out_S_loss: 0.9867 - val_out_T_accuracy: 0.9060 - val_out_S_accuracy: 0.7600\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0838 - out_T_loss: 0.0400 - out_S_loss: 1.0438 - out_T_accuracy: 0.9873 - out_S_accuracy: 0.6907 - val_loss: 1.2471 - val_out_T_loss: 0.2242 - val_out_S_loss: 1.0229 - val_out_T_accuracy: 0.9460 - val_out_S_accuracy: 0.7460\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.1688 - out_T_loss: 0.1445 - out_S_loss: 1.0243 - out_T_accuracy: 0.9673 - out_S_accuracy: 0.6887 - val_loss: 1.5762 - val_out_T_loss: 0.5524 - val_out_S_loss: 1.0238 - val_out_T_accuracy: 0.8620 - val_out_S_accuracy: 0.7440\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.3023 - out_T_loss: 0.2745 - out_S_loss: 1.0278 - out_T_accuracy: 0.9273 - out_S_accuracy: 0.6920 - val_loss: 2.8870 - val_out_T_loss: 1.9261 - val_out_S_loss: 0.9608 - val_out_T_accuracy: 0.6300 - val_out_S_accuracy: 0.7600\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.2391 - out_T_loss: 0.1613 - out_S_loss: 1.0778 - out_T_accuracy: 0.9587 - out_S_accuracy: 0.6713 - val_loss: 1.4434 - val_out_T_loss: 0.4480 - val_out_S_loss: 0.9954 - val_out_T_accuracy: 0.8920 - val_out_S_accuracy: 0.7440\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.1650 - out_T_loss: 0.0892 - out_S_loss: 1.0758 - out_T_accuracy: 0.9767 - out_S_accuracy: 0.6847 - val_loss: 1.7562 - val_out_T_loss: 0.7310 - val_out_S_loss: 1.0252 - val_out_T_accuracy: 0.8260 - val_out_S_accuracy: 0.7520\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.1242 - out_T_loss: 0.1065 - out_S_loss: 1.0177 - out_T_accuracy: 0.9740 - out_S_accuracy: 0.6867 - val_loss: 1.3157 - val_out_T_loss: 0.2837 - val_out_S_loss: 1.0320 - val_out_T_accuracy: 0.9340 - val_out_S_accuracy: 0.7480\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0026 - out_T_loss: 0.0386 - out_S_loss: 0.9639 - out_T_accuracy: 0.9867 - out_S_accuracy: 0.7080 - val_loss: 1.3340 - val_out_T_loss: 0.3622 - val_out_S_loss: 0.9718 - val_out_T_accuracy: 0.9200 - val_out_S_accuracy: 0.7520\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 1.0919 - out_T_loss: 0.0496 - out_S_loss: 1.0423 - out_T_accuracy: 0.9880 - out_S_accuracy: 0.6893 - val_loss: 1.2693 - val_out_T_loss: 0.3490 - val_out_S_loss: 0.9203 - val_out_T_accuracy: 0.9360 - val_out_S_accuracy: 0.7640\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.1359 - out_T_loss: 0.0607 - out_S_loss: 1.0752 - out_T_accuracy: 0.9827 - out_S_accuracy: 0.6720 - val_loss: 1.2939 - val_out_T_loss: 0.3324 - val_out_S_loss: 0.9615 - val_out_T_accuracy: 0.9240 - val_out_S_accuracy: 0.7580\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.9761 - out_T_loss: 0.0354 - out_S_loss: 0.9407 - out_T_accuracy: 0.9907 - out_S_accuracy: 0.7280 - val_loss: 1.2101 - val_out_T_loss: 0.2671 - val_out_S_loss: 0.9430 - val_out_T_accuracy: 0.9380 - val_out_S_accuracy: 0.7620\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0297 - out_T_loss: 0.0274 - out_S_loss: 1.0023 - out_T_accuracy: 0.9907 - out_S_accuracy: 0.6980 - val_loss: 1.4066 - val_out_T_loss: 0.4807 - val_out_S_loss: 0.9259 - val_out_T_accuracy: 0.9160 - val_out_S_accuracy: 0.7760\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0829 - out_T_loss: 0.0486 - out_S_loss: 1.0343 - out_T_accuracy: 0.9853 - out_S_accuracy: 0.6900 - val_loss: 1.5460 - val_out_T_loss: 0.5959 - val_out_S_loss: 0.9501 - val_out_T_accuracy: 0.8800 - val_out_S_accuracy: 0.7620\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0328 - out_T_loss: 0.0411 - out_S_loss: 0.9917 - out_T_accuracy: 0.9873 - out_S_accuracy: 0.7093 - val_loss: 1.5507 - val_out_T_loss: 0.5203 - val_out_S_loss: 1.0304 - val_out_T_accuracy: 0.8980 - val_out_S_accuracy: 0.7320\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0168 - out_T_loss: 0.0437 - out_S_loss: 0.9731 - out_T_accuracy: 0.9840 - out_S_accuracy: 0.7100 - val_loss: 1.3765 - val_out_T_loss: 0.3970 - val_out_S_loss: 0.9795 - val_out_T_accuracy: 0.9160 - val_out_S_accuracy: 0.7620\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.9859 - out_T_loss: 0.0393 - out_S_loss: 0.9466 - out_T_accuracy: 0.9907 - out_S_accuracy: 0.7213 - val_loss: 1.3696 - val_out_T_loss: 0.4907 - val_out_S_loss: 0.8789 - val_out_T_accuracy: 0.9040 - val_out_S_accuracy: 0.7700\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 1.0368 - out_T_loss: 0.0873 - out_S_loss: 0.9495 - out_T_accuracy: 0.9780 - out_S_accuracy: 0.7187 - val_loss: 1.2775 - val_out_T_loss: 0.3695 - val_out_S_loss: 0.9080 - val_out_T_accuracy: 0.9040 - val_out_S_accuracy: 0.7660\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 1.0191 - out_T_loss: 0.1088 - out_S_loss: 0.9104 - out_T_accuracy: 0.9747 - out_S_accuracy: 0.7287 - val_loss: 1.5883 - val_out_T_loss: 0.6583 - val_out_S_loss: 0.9300 - val_out_T_accuracy: 0.8560 - val_out_S_accuracy: 0.7580\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 1s 19ms/step - loss: 1.1103 - out_T_loss: 0.1480 - out_S_loss: 0.9623 - out_T_accuracy: 0.9647 - out_S_accuracy: 0.7173 - val_loss: 1.4764 - val_out_T_loss: 0.5424 - val_out_S_loss: 0.9340 - val_out_T_accuracy: 0.8880 - val_out_S_accuracy: 0.7560\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 1.0477 - out_T_loss: 0.1034 - out_S_loss: 0.9443 - out_T_accuracy: 0.9793 - out_S_accuracy: 0.7213 - val_loss: 1.4766 - val_out_T_loss: 0.5678 - val_out_S_loss: 0.9089 - val_out_T_accuracy: 0.8820 - val_out_S_accuracy: 0.7740\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 1.0661 - out_T_loss: 0.0719 - out_S_loss: 0.9943 - out_T_accuracy: 0.9787 - out_S_accuracy: 0.7093 - val_loss: 1.2717 - val_out_T_loss: 0.3890 - val_out_S_loss: 0.8828 - val_out_T_accuracy: 0.9100 - val_out_S_accuracy: 0.7660\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.9356 - out_T_loss: 0.0289 - out_S_loss: 0.9067 - out_T_accuracy: 0.9927 - out_S_accuracy: 0.7333 - val_loss: 1.1515 - val_out_T_loss: 0.2583 - val_out_S_loss: 0.8931 - val_out_T_accuracy: 0.9420 - val_out_S_accuracy: 0.7600\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.9465 - out_T_loss: 0.0239 - out_S_loss: 0.9226 - out_T_accuracy: 0.9900 - out_S_accuracy: 0.7273 - val_loss: 1.2692 - val_out_T_loss: 0.4022 - val_out_S_loss: 0.8670 - val_out_T_accuracy: 0.9280 - val_out_S_accuracy: 0.7760\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.9184 - out_T_loss: 0.0350 - out_S_loss: 0.8834 - out_T_accuracy: 0.9907 - out_S_accuracy: 0.7313 - val_loss: 1.1551 - val_out_T_loss: 0.2854 - val_out_S_loss: 0.8698 - val_out_T_accuracy: 0.9460 - val_out_S_accuracy: 0.7980\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.9180 - out_T_loss: 0.0197 - out_S_loss: 0.8983 - out_T_accuracy: 0.9933 - out_S_accuracy: 0.7340 - val_loss: 1.1307 - val_out_T_loss: 0.2132 - val_out_S_loss: 0.9175 - val_out_T_accuracy: 0.9540 - val_out_S_accuracy: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x387d37310>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_st.fit(x_train, [y_train, y_train], validation_data = (x_val, [y_val, y_val]), epochs = 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "8ed2c79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model_st.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "53a5ff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 760, 276)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "5576ee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.95677981e-31, 2.03815988e-23, 4.12583683e-26, 1.22317056e-23,\n",
       "       5.02427151e-23, 3.03972279e-16, 7.54588317e-18, 1.50512466e-35,\n",
       "       1.30996009e-30, 3.25779682e-19, 1.03229010e-31, 3.68153183e-16,\n",
       "       4.91248846e-32, 4.36651748e-28, 9.11452869e-22, 1.74038861e-21,\n",
       "       7.89699663e-30, 1.39705870e-31, 3.79959039e-28, 2.99021197e-23,\n",
       "       1.49527136e-26, 2.56050978e-34, 8.97722187e-38, 1.64455745e-29,\n",
       "       3.64636437e-32, 1.07598093e-26, 2.50732876e-34, 1.50500184e-38,\n",
       "       8.20593269e-32, 1.07264371e-36, 0.00000000e+00, 1.56693755e-38,\n",
       "       0.00000000e+00, 2.31338905e-32, 1.45414945e-28, 3.83033585e-38,\n",
       "       6.81657551e-30, 4.42722790e-38, 8.05416583e-37, 7.21647183e-26,\n",
       "       1.90007626e-25, 0.00000000e+00, 1.13994915e-33, 1.36026179e-32,\n",
       "       5.79087412e-29, 1.22692839e-37, 7.06336914e-29, 1.11028703e-29,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.03406071e-34, 1.42276386e-35,\n",
       "       7.02247329e-38, 2.04675740e-27, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.95361640e-36, 7.44300296e-38, 2.19248465e-35, 2.40646190e-32,\n",
       "       7.00314504e-27, 0.00000000e+00, 0.00000000e+00, 1.98594352e-34,\n",
       "       1.34737596e-33, 0.00000000e+00, 2.71737010e-22, 1.80894561e-35,\n",
       "       0.00000000e+00, 8.13596128e-31, 0.00000000e+00, 6.96878611e-29,\n",
       "       0.00000000e+00, 2.11775915e-30, 0.00000000e+00, 1.89948831e-26,\n",
       "       0.00000000e+00, 7.04030486e-32, 2.89847960e-37, 5.29593199e-37,\n",
       "       2.60403120e-32, 7.83702084e-34, 3.31750220e-28, 1.46941408e-32,\n",
       "       8.98349531e-37, 3.29384652e-35, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.97785193e-36, 0.00000000e+00, 1.41517961e-36, 6.47289801e-27,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.43305105e-38, 1.33759596e-31,\n",
       "       7.24486574e-23, 2.25086228e-38, 8.47938425e-38, 8.81744558e-28,\n",
       "       9.35011292e-25, 3.30030625e-23, 1.78631507e-14, 8.41065460e-19,\n",
       "       2.46611015e-21, 7.99575340e-23, 6.71071729e-22, 3.50281237e-16,\n",
       "       9.26362153e-19, 1.91700622e-22, 1.21010502e-17, 7.45669077e-18,\n",
       "       8.76892471e-13, 1.67509761e-18, 1.21993107e-04, 2.62771077e-17,\n",
       "       1.54676223e-17, 4.19994649e-19, 5.11057964e-16, 5.24734065e-17,\n",
       "       1.01382688e-24, 1.84753396e-26, 2.14477770e-28, 5.75953246e-19,\n",
       "       8.56244607e-35, 4.38467533e-29, 4.70907210e-32, 4.41863184e-34,\n",
       "       3.53056613e-32, 1.12704663e-33, 2.35378443e-25, 2.20210435e-30,\n",
       "       4.79792671e-30, 1.34590413e-37, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.49970647e-38, 4.33277579e-32, 1.75733055e-32, 4.34000116e-26,\n",
       "       7.68582361e-33, 7.09022854e-37, 6.84844184e-33, 2.04651579e-19,\n",
       "       6.83983299e-31, 0.00000000e+00, 1.19715800e-38, 2.61795893e-37,\n",
       "       2.97236748e-31, 2.61325194e-35, 2.36602562e-34, 9.96253374e-31,\n",
       "       1.30172516e-31, 1.48417871e-33, 3.19502186e-31, 0.00000000e+00,\n",
       "       0.00000000e+00, 7.47658199e-38, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.63866762e-26, 1.39673297e-32,\n",
       "       6.84405051e-19, 2.10117131e-18, 1.33220572e-20, 6.02548430e-37,\n",
       "       1.44546023e-29, 5.95204220e-28, 2.89517778e-28, 1.71229338e-16,\n",
       "       1.07492799e-31, 4.35778013e-21, 6.52053738e-25, 1.58950098e-24,\n",
       "       7.24595051e-36, 1.60145310e-33, 4.68658958e-36, 5.24005443e-34,\n",
       "       1.09762584e-33, 8.69324024e-27, 3.19461288e-20, 0.00000000e+00,\n",
       "       7.51245565e-17, 4.19862122e-17, 1.83538843e-26, 2.08310054e-35,\n",
       "       3.71811214e-32, 1.72826661e-23, 2.02739965e-20, 3.01448014e-16,\n",
       "       1.03967043e-27, 4.90519010e-30, 4.73365551e-25, 1.17934531e-29,\n",
       "       1.59700988e-28, 8.34483953e-37, 0.00000000e+00, 1.50823490e-34,\n",
       "       1.38709232e-37, 7.47468216e-35, 0.00000000e+00, 3.80574971e-31,\n",
       "       0.00000000e+00, 7.77619474e-38, 0.00000000e+00, 4.83613292e-36,\n",
       "       5.58416885e-24, 2.02851681e-32, 4.53523491e-34, 3.33237575e-38,\n",
       "       0.00000000e+00, 4.61422787e-37, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.97486527e-35, 1.90922289e-26, 1.27573777e-26, 3.61412942e-30,\n",
       "       1.73805190e-30, 6.70508278e-31, 8.15481809e-32, 1.29109938e-28,\n",
       "       2.87748476e-35, 0.00000000e+00, 1.83026058e-29, 2.44473422e-37,\n",
       "       5.25394501e-34, 0.00000000e+00, 0.00000000e+00, 2.95776001e-27,\n",
       "       1.85077008e-35, 2.21714970e-38, 1.65089703e-32, 0.00000000e+00,\n",
       "       2.75667806e-38, 5.31472922e-31, 2.87544961e-31, 2.73246811e-26,\n",
       "       3.94296994e-35, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.72982707e-33, 0.00000000e+00, 0.00000000e+00, 9.65147443e-34,\n",
       "       2.02798035e-27, 4.90682222e-36, 1.00788383e-33, 0.00000000e+00,\n",
       "       1.69974873e-28, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       9.45588647e-26, 0.00000000e+00, 4.06147325e-24, 0.00000000e+00,\n",
       "       3.10550453e-20, 9.61935944e-32, 1.36913089e-27, 8.36838873e-24,\n",
       "       8.83784696e-29, 3.22243921e-24, 2.67325245e-29, 3.46464655e-38,\n",
       "       1.31546305e-38, 4.42591836e-27, 3.67089988e-32, 1.61887832e-26],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f227f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t = []\n",
    "for i in range(760):\n",
    "    result_t.append(np.argmax(result[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4355a77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760,)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(result_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6d5dcdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_s = []\n",
    "for i in range(760):\n",
    "    result_s.append(np.argmax(result[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "2b04f343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760,)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(result_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f2829919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "24792bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9434210526315789"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_t = accuracy_score(y_test, result_t)\n",
    "accuracy_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "60e81a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7802631578947369"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_s = accuracy_score(y_test, result_s)\n",
    "accuracy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d20592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb850c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04436b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bfacd45",
   "metadata": {},
   "source": [
    "# for deplyoment purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "id": "f2e8c893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 200, 3)"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "id": "f5ef247e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109.88904211,  43.81725954, 142.76502976],\n",
       "       [ 74.92160267,  10.40813613,  50.53895719],\n",
       "       [ 49.70739987,  73.68885845,  10.23688286],\n",
       "       [ 84.14285397,  56.75922489,  11.6891557 ],\n",
       "       [ 52.80200525,  78.27646388,  10.87419466],\n",
       "       [ 51.56836837,  76.44765582,  10.62013599],\n",
       "       [ 76.09318052,  10.57089215,  51.32925426],\n",
       "       [ 86.72747808,  58.50270344,  12.04821261],\n",
       "       [ 12.2136161 ,   5.84926127,  87.91811339],\n",
       "       [ 12.35885212,  24.37517908,  88.96357583],\n",
       "       [ 13.14942672,  20.19110708,  94.65442341],\n",
       "       [ 95.5144376 ,  54.88593878,  13.26890021],\n",
       "       [ 86.39309395,  59.34447472,  12.00175985],\n",
       "       [ 11.51118408,  15.78642099,  82.8617486 ],\n",
       "       [ 11.63813147,  17.87049457,  83.77556281],\n",
       "       [103.56130367,  45.20185944,  14.38677375],\n",
       "       [ 86.99435731,  45.0259831 ,  12.08528757],\n",
       "       [ 80.55439857,  54.58121184,  11.19064617],\n",
       "       [ 12.85197641,  16.51554552,  92.51326632],\n",
       "       [ 10.74907024,  14.63299878,  77.37577212],\n",
       "       [ 12.07276167,  19.07119165,  86.90419128],\n",
       "       [ 12.45806003,  15.55828172,  89.67771099],\n",
       "       [ 12.35749386,  18.71488482,  88.95379856],\n",
       "       [ 12.76671214,  18.05485745,  91.89950266],\n",
       "       [ 12.91745005,  21.28675185,  92.98456977],\n",
       "       [ 12.73394208,  20.87773742,  91.66361168],\n",
       "       [ 13.5918059 ,  22.09327808,  97.83883192],\n",
       "       [ 12.50855464,  16.6445683 ,  90.0411898 ],\n",
       "       [ 12.35735027,  16.94685207,  88.95276495],\n",
       "       [ 12.56635849,  18.80143984,  90.45728324],\n",
       "       [100.4024804 ,  62.10470189,  13.94794888],\n",
       "       [ 97.11326182,  62.24080339,  13.49100944],\n",
       "       [ 93.24623693,  58.84081751,  12.95380095],\n",
       "       [ 95.09860683,  58.33192085,  13.21113284],\n",
       "       [ 86.2518112 ,  58.57667868,  11.9821328 ],\n",
       "       [ 95.63376031,  60.46803521,  13.28547656],\n",
       "       [ 11.79475069,  23.29002644,  84.90296568],\n",
       "       [ 99.41040203,  64.85623264,  13.81012899],\n",
       "       [ 82.39487895,  61.28834399,  11.44632637],\n",
       "       [ 94.8490475 ,  63.1164372 ,  13.17646397],\n",
       "       [ 98.66114996,  62.40983706,  13.70604262],\n",
       "       [ 84.81036578,  55.34254573,  11.78188668],\n",
       "       [ 99.80305925,  63.75760205,  13.86467708],\n",
       "       [101.34658612,  61.87109896,  14.07910439],\n",
       "       [ 12.45429541,  19.67389576,  89.65061186],\n",
       "       [ 12.56982996,  19.74659841,  90.48227217],\n",
       "       [ 12.19057955,  20.88748221,  87.75228781],\n",
       "       [ 12.73004602,  16.93929686,  91.6355664 ],\n",
       "       [ 11.59142134,  17.55470145,  83.43932606],\n",
       "       [ 11.53144243,  18.28292457,  83.0075757 ],\n",
       "       [ 11.4160709 ,  17.28914085,  82.17708886],\n",
       "       [ 11.59955218,  21.34229689,  83.49785482],\n",
       "       [ 11.65564624,  17.65196725,  83.90164055],\n",
       "       [ 11.32995988,  18.19149045,  81.55722997],\n",
       "       [ 13.1365286 ,  19.89469892,  94.56157798],\n",
       "       [ 13.09023411,  19.13868635,  94.22833313],\n",
       "       [ 95.14484428,  55.7620563 ,  13.21755617],\n",
       "       [ 12.67542709,  19.4632748 ,  91.24239926],\n",
       "       [ 12.60822998,  23.7879346 ,  90.75868966],\n",
       "       [ 95.95940576,  65.16945209,  13.33071534],\n",
       "       [ 95.56892314,  63.1745164 ,  13.27646936],\n",
       "       [102.37157799,  70.64935376,  14.22149663],\n",
       "       [104.56136217,  71.07946364,  14.52570224],\n",
       "       [100.38205481,  69.66155897,  13.94511135],\n",
       "       [ 96.81514782,  66.83954682,  13.44959534],\n",
       "       [ 97.08006322,  58.35851186,  13.48639748],\n",
       "       [110.72894784,  69.18598464,  15.38250547],\n",
       "       [ 15.01674781,  22.51363809, 108.09608929],\n",
       "       [ 12.65877501,  21.95914133,  91.1225314 ],\n",
       "       [ 12.65107806,  16.25737933,  91.06712595],\n",
       "       [ 12.42035852,  13.56414924,  89.40632158],\n",
       "       [ 11.31020792,  14.08317877,  81.4150481 ],\n",
       "       [102.60723427,  53.64809258,  14.25423408],\n",
       "       [ 98.41674129,  46.2995365 ,  13.67208928],\n",
       "       [100.45571873,  50.44084064,  13.95534477],\n",
       "       [101.73373436,  43.33906875,  14.13288716],\n",
       "       [105.45808463,  59.99576992,  14.65027525],\n",
       "       [107.92138844,  53.90334006,  14.9924783 ],\n",
       "       [ 12.50552939,  15.15126254,  90.01941293],\n",
       "       [ 12.84061202,  14.81004834,  92.43146126],\n",
       "       [ 13.03779454,  22.61662544,  93.85085377],\n",
       "       [ 11.51940897,  22.63903944,  82.9209544 ],\n",
       "       [ 95.43866393,  55.89099191,  13.2583737 ],\n",
       "       [100.35175401,  53.62839598,  13.94090195],\n",
       "       [ 97.08674723,  51.69020717,  13.48732603],\n",
       "       [ 98.89207065,  59.44777832,  13.73812221],\n",
       "       [ 85.57133936,  57.72282096,  11.8876014 ],\n",
       "       [ 87.7037122 ,  59.16122986,  12.18383141],\n",
       "       [ 93.21497275,  53.73663524,  12.94945772],\n",
       "       [ 12.01749968,  20.7511682 ,  86.50639506],\n",
       "       [101.45959199,  47.27058779,  14.0948032 ],\n",
       "       [ 11.60277577,  13.42842501,  83.52105938],\n",
       "       [ 12.57146283,  13.46255756,  90.49402612],\n",
       "       [ 12.70783405,  13.87809839,  91.47567648],\n",
       "       [109.38831796,  58.16691871,  15.19626468],\n",
       "       [108.52533991,  59.53992338,  15.07637946],\n",
       "       [ 14.82198372,  17.15416214, 106.69410553],\n",
       "       [104.67715104,  56.0611571 ,  14.54178767],\n",
       "       [ 14.45377512,  18.20920964, 104.04360422],\n",
       "       [103.14365342,  59.25406101,  14.32875363],\n",
       "       [ 14.71131117,  14.50744943, 105.89744373],\n",
       "       [ 12.69289735,  14.79034745,  91.3681566 ],\n",
       "       [ 10.89018262,  12.73253997,  78.39155112],\n",
       "       [ 11.21825015,  14.00994182,  80.75310219],\n",
       "       [ 10.93600579,  11.56978791,  78.72140316],\n",
       "       [ 12.28155347,  17.12805247,  88.40715162],\n",
       "       [ 11.91013557,  17.14974534,  85.73354859],\n",
       "       [ 11.2595951 ,  17.38986451,  81.05071834],\n",
       "       [ 11.70322124,  13.54467516,  84.24410293],\n",
       "       [ 92.21602451,  54.77882355,  12.81068347],\n",
       "       [ 13.93392857,  14.37438674, 100.30155702],\n",
       "       [103.64240318,  59.87503879,  14.39804012],\n",
       "       [108.12515254,  55.14298696,  15.02078528],\n",
       "       [107.82742837,  57.71266939,  14.97942534],\n",
       "       [103.09855828,  53.36109245,  14.32248899],\n",
       "       [ 10.49349394,   5.02547216,  75.53603964],\n",
       "       [ 10.53722984,  12.31987604,  75.85086678],\n",
       "       [ 11.10377238,  13.86697595,  79.92904899],\n",
       "       [ 11.95277126,  10.36721931,  86.04045603],\n",
       "       [103.8716368 ,  51.60372095,  14.42988534],\n",
       "       [101.904052  ,  54.23805417,  14.15654775],\n",
       "       [ 11.66979855,  14.53093173,  84.0035141 ],\n",
       "       [ 11.91485311,  15.35375941,  85.76750722],\n",
       "       [ 11.69534054,  16.31051047,  84.18737474],\n",
       "       [101.09846725,  47.10233776,  14.04463562],\n",
       "       [ 13.63773744,  14.89363657,  98.16946408],\n",
       "       [ 11.45677577,  13.39498707,  82.47009758],\n",
       "       [ 11.69867709,  15.60717749,  84.21139244],\n",
       "       [ 13.16029169,  18.04800475,  94.73263342],\n",
       "       [ 12.84309291,  21.5187524 ,  92.44931962],\n",
       "       [ 12.82240727,  19.18453297,  92.30041677],\n",
       "       [ 10.2446784 ,  10.56851761,  73.74497366],\n",
       "       [ 88.78541332,  52.63498849,  12.33410172],\n",
       "       [102.36051424,  54.27631622,  14.21995965],\n",
       "       [105.30145522,  64.16323382,  14.62851623],\n",
       "       [105.296467  ,  62.5489617 ,  14.62782327],\n",
       "       [ 95.49450369,  59.06883635,  13.26613098],\n",
       "       [ 96.64044089,  62.05778963,  13.42532499],\n",
       "       [ 96.66060274,  57.41902375,  13.42812588],\n",
       "       [ 98.87245877,  50.61262333,  13.73539772],\n",
       "       [ 11.69822731,   9.03365468,  84.20815479],\n",
       "       [ 11.19071121,  13.72670699,  80.55486668],\n",
       "       [ 11.28971476,  13.67822401,  81.2675308 ],\n",
       "       [ 10.59452502,  12.3868642 ,  76.26329864],\n",
       "       [ 11.34489766,  14.57886069,  81.66475762],\n",
       "       [ 11.92653952,  11.42355305,  85.85163031],\n",
       "       [ 11.7080298 ,  14.5785363 ,  84.27871675],\n",
       "       [ 12.61227838,  15.70448327,  90.78783152],\n",
       "       [ 11.91734784,  16.34343338,  85.7854652 ],\n",
       "       [ 12.55607679,  15.81844416,  90.38327174],\n",
       "       [ 12.58477071,  18.1211718 ,  90.58982114],\n",
       "       [ 12.18113358,  15.21244137,  87.68429223],\n",
       "       [100.47138466,  57.40840778,  13.95752108],\n",
       "       [ 88.29873484,  53.01466528,  12.26649217],\n",
       "       [ 92.75385535,  53.76909543,  12.88539912],\n",
       "       [ 99.47362993,  54.33261086,  13.81891264],\n",
       "       [ 12.80545914,  19.19839611,  92.17841789],\n",
       "       [ 11.64160195,  12.31627599,  83.80054458],\n",
       "       [101.56785123,  50.02293899,  14.10984262],\n",
       "       [107.80354799,  57.61066576,  14.97610786],\n",
       "       [ 95.76381515,  54.3314174 ,  13.30354383],\n",
       "       [ 96.43128255,  52.99807707,  13.39626864],\n",
       "       [ 11.4662419 ,  13.89209812,  82.53823827],\n",
       "       [ 12.59051558,  15.86183096,  90.63117484],\n",
       "       [104.23853765,  55.51524003,  14.48085534],\n",
       "       [104.76200545,  63.63635806,  14.55357568],\n",
       "       [ 12.45510518,  16.04994091,  89.65644087],\n",
       "       [ 12.5041005 ,  15.75296319,  90.00912723],\n",
       "       [ 11.6059231 ,  16.03238403,  83.54371504],\n",
       "       [ 11.51891423,  16.4019103 ,  82.91739312],\n",
       "       [ 11.33516329,  16.85477864,  81.59468606],\n",
       "       [ 11.51186225,  17.43421268,  82.86663037],\n",
       "       [ 11.55321632,  12.90503272,  83.16431218],\n",
       "       [ 97.8160579 ,  48.63014017,  13.58864213],\n",
       "       [ 11.41395619,  13.16457834,  82.16186639],\n",
       "       [ 13.47649159,  18.48164084,  97.00875698],\n",
       "       [ 98.43642131,  48.05416136,  13.67482324],\n",
       "       [ 95.94604923,  54.82267352,  13.32885985],\n",
       "       [ 96.91228599,  57.2353848 ,  13.46308981],\n",
       "       [ 13.82787047,  16.11287031,  99.53811174],\n",
       "       [ 91.95661597,  51.57976399,  12.77464634],\n",
       "       [ 97.91649016,  51.91988749,  13.60259422],\n",
       "       [ 11.28747351,  13.37301126,  81.25139742],\n",
       "       [ 94.28892737,  57.72630773,  13.09865188],\n",
       "       [ 95.66282274,  56.92602318,  13.28951392],\n",
       "       [ 96.16372513,  58.91581162,  13.35909946],\n",
       "       [ 98.51083138,  58.53278889,  13.68516031],\n",
       "       [ 95.8798607 ,  61.71460861,  13.31966491],\n",
       "       [ 97.75014931,  63.62699822,  13.57948608],\n",
       "       [ 96.78926123,  62.16669674,  13.44599916],\n",
       "       [ 97.78900724,  62.56574323,  13.58488424],\n",
       "       [ 95.00895796,  52.07845984,  13.19867879],\n",
       "       [ 11.12132096,  14.83692805,  80.05537011],\n",
       "       [ 12.08607158,  17.40308065,  87.00000091],\n",
       "       [ 12.57324823,  17.24293422,  90.50687811],\n",
       "       [ 12.99625233,  19.95590591,  93.55181764],\n",
       "       [104.85070473,  57.52391958,  14.56589781],\n",
       "       [ 95.63949213,  46.32312572,  13.28627283],\n",
       "       [ 93.53164143,  58.05513744,  12.99344945],\n",
       "       [ 97.17723969,  56.71784855,  13.49989727]])"
      ]
     },
     "execution_count": 1157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "id": "09314ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([114], dtype=int32)>"
      ]
     },
     "execution_count": 1158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57011549",
   "metadata": {},
   "source": [
    "# Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0ab4de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, BatchNormalization, Dense, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "1ff6987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=64, activation='relu')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=128, activation='relu')(x_T) # 44, 256\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "# flat_T = Flatten()(x_T)\n",
    "# d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(flat_T)\n",
    "# d_T = Dense(300, activation = 'relu')(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid')(d_T)\n",
    "\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model_T.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5b3506d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# TEACHER RNN MODEL\n",
    "x_T = LSTM(32, return_sequences=True)(dataInp) # Output shape: (200, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(64, return_sequences=True)(x_T) # Output shape: (200, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(128, return_sequences=True)(x_T) # Output shape: (200, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(128, return_sequences=True)(x_T) # Output shape: (200, 256)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(256, return_sequences=True)(x_T) # Output shape: (200, 512)\n",
    "\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "\n",
    "d_T = Dropout(0.3)(flat_T)\n",
    "out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher_rnn')\n",
    "model_T.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "80862438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher_gru\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)       [(None, 200, 6)]             0         []                            \n",
      "                                                                                                  \n",
      " gru_25 (GRU)                (None, 200, 32)              3840      ['input_29[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_88 (Ba  (None, 200, 32)              128       ['gru_25[0][0]']              \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " gru_26 (GRU)                (None, 200, 64)              18816     ['batch_normalization_88[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_89 (Ba  (None, 200, 64)              256       ['gru_26[0][0]']              \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " gru_27 (GRU)                (None, 200, 128)             74496     ['batch_normalization_89[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_90 (Ba  (None, 200, 128)             512       ['gru_27[0][0]']              \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " gru_28 (GRU)                (None, 200, 256)             296448    ['batch_normalization_90[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_91 (Ba  (None, 200, 256)             1024      ['gru_28[0][0]']              \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " gru_29 (GRU)                (None, 200, 256)             394752    ['batch_normalization_91[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 256)                  0         ['gru_29[0][0]']              \n",
      " 6 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_26 (G  (None, 256)                  0         ['gru_29[0][0]']              \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenat  (None, 512)                  0         ['global_average_pooling1d_26[\n",
      " e)                                                                 0][0]',                       \n",
      "                                                                     'global_max_pooling1d_26[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)        (None, 512)                  0         ['concatenate_26[0][0]']      \n",
      "                                                                                                  \n",
      " dense_29 (Dense)            (None, 276)                  141588    ['dropout_27[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 931860 (3.55 MB)\n",
      "Trainable params: 930900 (3.55 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# TEACHER GRU MODEL\n",
    "x_T = GRU(32, return_sequences=True)(dataInp) # Output shape: (200, 32)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(64, return_sequences=True)(x_T) # Output shape: (200, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(128, return_sequences=True)(x_T) # Output shape: (200, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(256, return_sequences=True)(x_T) # Output shape: (200, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(256, return_sequences=True)(x_T) # Output shape: (200, 256)\n",
    "\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "\n",
    "d_T = Dropout(0.3)(flat_T)\n",
    "out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher_gru')\n",
    "model_T.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model_T.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6f2e9530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher_bilstm\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)       [(None, 200, 6)]             0         []                            \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 200, 64)              9984      ['input_30[0][0]']            \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_92 (Ba  (None, 200, 64)              256       ['bidirectional[0][0]']       \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 200, 128)             66048     ['batch_normalization_92[0][0]\n",
      " onal)                                                              ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_93 (Ba  (None, 200, 128)             512       ['bidirectional_1[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirecti  (None, 200, 256)             263168    ['batch_normalization_93[0][0]\n",
      " onal)                                                              ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_94 (Ba  (None, 200, 256)             1024      ['bidirectional_2[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirecti  (None, 200, 256)             394240    ['batch_normalization_94[0][0]\n",
      " onal)                                                              ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_95 (Ba  (None, 200, 256)             1024      ['bidirectional_3[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirecti  (None, 200, 512)             1050624   ['batch_normalization_95[0][0]\n",
      " onal)                                                              ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 512)                  0         ['bidirectional_4[0][0]']     \n",
      " 7 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_27 (G  (None, 512)                  0         ['bidirectional_4[0][0]']     \n",
      " lobalMaxPooling1D)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenat  (None, 1024)                 0         ['global_average_pooling1d_27[\n",
      " e)                                                                 0][0]',                       \n",
      "                                                                     'global_max_pooling1d_27[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)        (None, 1024)                 0         ['concatenate_27[0][0]']      \n",
      "                                                                                                  \n",
      " dense_30 (Dense)            (None, 276)                  282900    ['dropout_28[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2069780 (7.90 MB)\n",
      "Trainable params: 2068372 (7.89 MB)\n",
      "Non-trainable params: 1408 (5.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# TEACHER Bi-LSTM MODEL\n",
    "x_T = Bidirectional(LSTM(32, return_sequences=True))(dataInp) # Output shape: (200, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(64, return_sequences=True))(x_T) # Output shape: (200, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(128, return_sequences=True))(x_T) # Output shape: (200, 256)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(128, return_sequences=True))(x_T) # Output shape: (200, 256)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(256, return_sequences=True))(x_T) # Output shape: (200, 512)\n",
    "\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "\n",
    "d_T = Dropout(0.3)(flat_T)\n",
    "out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher_bilstm')\n",
    "model_T.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model_T.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "f28182f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=16, activation='relu')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=32, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "# x_T = Conv1D(kernel_size = kernel_size, filters=128, activation='relu')(x_T) # 44, 256\n",
    "# x_T = BatchNormalization()(x_T)\n",
    "# x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 42, 32\n",
    "# x_T = BatchNormalization()(x_T)\n",
    "# x_T = MaxPooling1D(2)(x_T) # 21, 32\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "# flat_T = Flatten()(x_T)\n",
    "# d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "# d_T = Dropout(0.3)(flat_T)\n",
    "# d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "out_T = Dense(276, activation = 'sigmoid')(flat_T)\n",
    "\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model_T.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "18fe4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "x_S = Conv1D(kernel_size = 7, filters= 16, activation='relu', name='l1_S')(dataInp) # 194, 32\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(2)(x_S) # 97, 32\n",
    "\n",
    "x_S1 = Conv1D(kernel_size = 4, filters=32, activation='relu', name='l2_S')(x_S) #94, 64\n",
    "x_S = MaxPooling1D(2)(x_S1) #47, 64\n",
    "x_S = BatchNormalization()(x_S)\n",
    "\n",
    "x_b_S = Conv1D(kernel_size = 6, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "x_b2 = BatchNormalization()(x_b_S) #old\n",
    "# x_S = MaxPooling1D(2)(x_b2) \n",
    "\n",
    "x_S3 = Conv1D(kernel_size = 5, filters=64, activation='relu', name='l4_S')(x_b2) #38, 128\n",
    "x_S = MaxPooling1D(2)(x_S3) \n",
    "x_S = BatchNormalization()(x_S3) \n",
    "# x_S = MaxPooling1D(2)(x_b3) \n",
    "\n",
    "# x_S = Conv1D(kernel_size = 3, filters=128, activation='relu', name='l5_S')(x_S)\n",
    "# x_S = BatchNormalization()(x_S) \n",
    "# x_S = Conv1D(kernel_size = 3, filters=256, activation='relu', name='l6_S')(x_S)\n",
    "# x_S = BatchNormalization()(x_S) \n",
    "# x_S = MaxPooling1D(2)(x_S) \n",
    "\n",
    "# x_S = Conv1D(kernel_size = 1, filters=256, activation='relu', name='l7_S')(x_S)\n",
    "# x_S = MaxPooling1D(2)(x_S) \n",
    "\n",
    "avg_S = GlobalAveragePooling1D()(x_S)\n",
    "max_S = GlobalMaxPooling1D()(x_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "\n",
    "# flat_S = Flatten()(x_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_1')(flat_S)\n",
    "\n",
    "# d_S = Dropout(0.3)(flat_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_2')(d_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(flat_S)\n",
    "# model_s = Model(inputs=dataInp, outputs=[x_b_S, out_S], name='student' )\n",
    "# model_s.compile(loss = [\"mean_squared_error\", \"binary_crossentropy\" ], optimizer = 'adam',loss_weights = [1, 10], metrics=[\"accuracy\"])\n",
    "# # print(model_s.summary())\n",
    "\n",
    "model_T = Model(inputs=dataInp, outputs=out_S, name='teacher' )\n",
    "model_T.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "37d22f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_177 (InputLayer)      [(None, 200, 6)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 16)              688       ['input_177[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_786 (B  (None, 194, 16)              64        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_591 (MaxPool  (None, 97, 16)               0         ['batch_normalization_786[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " l2_S (Conv1D)               (None, 94, 32)               2080      ['max_pooling1d_591[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_592 (MaxPool  (None, 47, 32)               0         ['l2_S[0][0]']                \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_787 (B  (None, 47, 32)               128       ['max_pooling1d_592[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               6176      ['batch_normalization_787[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_788 (B  (None, 42, 32)               128       ['bottle_S[0][0]']            \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " l4_S (Conv1D)               (None, 38, 64)               10304     ['batch_normalization_788[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_789 (B  (None, 38, 64)               256       ['l4_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 64)                   0         ['batch_normalization_789[0][0\n",
      " 60 (GlobalAveragePooling1D                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_161 (  (None, 64)                   0         ['batch_normalization_789[0][0\n",
      " GlobalMaxPooling1D)                                                ]']                           \n",
      "                                                                                                  \n",
      " add_162 (Add)               (None, 64)                   0         ['global_average_pooling1d_160\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_161[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  17940     ['add_162[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37764 (147.52 KB)\n",
      "Trainable params: 37476 (146.39 KB)\n",
      "Non-trainable params: 288 (1.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_T.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "82c6c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "24/24 [==============================] - 4s 15ms/step - loss: 6.1488 - accuracy: 0.0113 - val_loss: 6.7767 - val_accuracy: 0.0100\n",
      "Epoch 2/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 4.5790 - accuracy: 0.0773 - val_loss: 5.8041 - val_accuracy: 0.0260\n",
      "Epoch 3/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 3.8892 - accuracy: 0.2040 - val_loss: 5.2024 - val_accuracy: 0.0320\n",
      "Epoch 4/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 3.3981 - accuracy: 0.3073 - val_loss: 4.7171 - val_accuracy: 0.0420\n",
      "Epoch 5/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.9912 - accuracy: 0.3960 - val_loss: 4.4380 - val_accuracy: 0.1040\n",
      "Epoch 6/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.6773 - accuracy: 0.4620 - val_loss: 4.1164 - val_accuracy: 0.1600\n",
      "Epoch 7/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.3414 - accuracy: 0.5320 - val_loss: 3.7899 - val_accuracy: 0.2400\n",
      "Epoch 8/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.0451 - accuracy: 0.6153 - val_loss: 3.4675 - val_accuracy: 0.3080\n",
      "Epoch 9/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 1.7954 - accuracy: 0.6567 - val_loss: 3.3478 - val_accuracy: 0.3500\n",
      "Epoch 10/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 1.5465 - accuracy: 0.6907 - val_loss: 3.1500 - val_accuracy: 0.3400\n",
      "Epoch 11/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 1.3053 - accuracy: 0.7313 - val_loss: 3.0393 - val_accuracy: 0.3840\n",
      "Epoch 12/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.1690 - accuracy: 0.7553 - val_loss: 2.7696 - val_accuracy: 0.4460\n",
      "Epoch 13/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.9397 - accuracy: 0.7947 - val_loss: 2.6912 - val_accuracy: 0.4620\n",
      "Epoch 14/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.8042 - accuracy: 0.8280 - val_loss: 2.6412 - val_accuracy: 0.4680\n",
      "Epoch 15/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.7028 - accuracy: 0.8447 - val_loss: 2.4765 - val_accuracy: 0.5240\n",
      "Epoch 16/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5773 - accuracy: 0.8813 - val_loss: 2.3104 - val_accuracy: 0.5600\n",
      "Epoch 17/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5231 - accuracy: 0.8913 - val_loss: 2.1489 - val_accuracy: 0.5980\n",
      "Epoch 18/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4604 - accuracy: 0.9053 - val_loss: 2.0199 - val_accuracy: 0.6080\n",
      "Epoch 19/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4032 - accuracy: 0.9107 - val_loss: 1.9660 - val_accuracy: 0.6340\n",
      "Epoch 20/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3631 - accuracy: 0.9140 - val_loss: 1.8778 - val_accuracy: 0.6060\n",
      "Epoch 21/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3308 - accuracy: 0.9273 - val_loss: 1.6230 - val_accuracy: 0.6640\n",
      "Epoch 22/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3201 - accuracy: 0.9273 - val_loss: 1.5114 - val_accuracy: 0.6940\n",
      "Epoch 23/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2734 - accuracy: 0.9433 - val_loss: 1.4512 - val_accuracy: 0.6840\n",
      "Epoch 24/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2525 - accuracy: 0.9453 - val_loss: 1.3674 - val_accuracy: 0.7160\n",
      "Epoch 25/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2044 - accuracy: 0.9553 - val_loss: 1.2044 - val_accuracy: 0.7280\n",
      "Epoch 26/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2063 - accuracy: 0.9540 - val_loss: 1.2143 - val_accuracy: 0.7080\n",
      "Epoch 27/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1962 - accuracy: 0.9580 - val_loss: 1.2008 - val_accuracy: 0.6880\n",
      "Epoch 28/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1789 - accuracy: 0.9573 - val_loss: 1.2067 - val_accuracy: 0.6840\n",
      "Epoch 29/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1730 - accuracy: 0.9620 - val_loss: 1.1958 - val_accuracy: 0.7020\n",
      "Epoch 30/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1369 - accuracy: 0.9720 - val_loss: 1.0808 - val_accuracy: 0.7020\n",
      "Epoch 31/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1493 - accuracy: 0.9680 - val_loss: 1.1901 - val_accuracy: 0.6880\n",
      "Epoch 32/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1185 - accuracy: 0.9787 - val_loss: 1.0844 - val_accuracy: 0.7280\n",
      "Epoch 33/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1252 - accuracy: 0.9707 - val_loss: 1.1186 - val_accuracy: 0.6920\n",
      "Epoch 34/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1001 - accuracy: 0.9773 - val_loss: 1.1040 - val_accuracy: 0.7060\n",
      "Epoch 35/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1192 - accuracy: 0.9720 - val_loss: 1.1149 - val_accuracy: 0.7140\n",
      "Epoch 36/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0985 - accuracy: 0.9780 - val_loss: 1.0808 - val_accuracy: 0.7320\n",
      "Epoch 37/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0958 - accuracy: 0.9813 - val_loss: 1.0768 - val_accuracy: 0.7200\n",
      "Epoch 38/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0613 - accuracy: 0.9913 - val_loss: 1.0100 - val_accuracy: 0.7600\n",
      "Epoch 39/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0597 - accuracy: 0.9887 - val_loss: 1.0471 - val_accuracy: 0.7160\n",
      "Epoch 40/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0720 - accuracy: 0.9840 - val_loss: 1.0387 - val_accuracy: 0.7640\n",
      "Epoch 41/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0575 - accuracy: 0.9900 - val_loss: 0.9864 - val_accuracy: 0.7540\n",
      "Epoch 42/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0427 - accuracy: 0.9927 - val_loss: 1.0014 - val_accuracy: 0.7560\n",
      "Epoch 43/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0359 - accuracy: 0.9953 - val_loss: 0.9795 - val_accuracy: 0.7780\n",
      "Epoch 44/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0404 - accuracy: 0.9960 - val_loss: 0.9954 - val_accuracy: 0.7500\n",
      "Epoch 45/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9920 - val_loss: 0.9658 - val_accuracy: 0.7640\n",
      "Epoch 46/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9907 - val_loss: 1.2336 - val_accuracy: 0.7040\n",
      "Epoch 47/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0316 - accuracy: 0.9980 - val_loss: 0.8928 - val_accuracy: 0.7860\n",
      "Epoch 48/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0262 - accuracy: 0.9960 - val_loss: 0.8617 - val_accuracy: 0.7820\n",
      "Epoch 49/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0202 - accuracy: 0.9987 - val_loss: 0.9206 - val_accuracy: 0.7700\n",
      "Epoch 50/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.9254 - val_accuracy: 0.7880\n",
      "Epoch 51/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0137 - accuracy: 0.9993 - val_loss: 0.9360 - val_accuracy: 0.7720\n",
      "Epoch 52/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0174 - accuracy: 0.9973 - val_loss: 0.9694 - val_accuracy: 0.7640\n",
      "Epoch 53/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0137 - accuracy: 0.9980 - val_loss: 0.9243 - val_accuracy: 0.7680\n",
      "Epoch 54/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0123 - accuracy: 0.9987 - val_loss: 0.9971 - val_accuracy: 0.7640\n",
      "Epoch 55/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0081 - accuracy: 0.9993 - val_loss: 0.9249 - val_accuracy: 0.7900\n",
      "Epoch 56/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.9281 - val_accuracy: 0.7740\n",
      "Epoch 57/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.9822 - val_accuracy: 0.7640\n",
      "Epoch 58/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.9834 - val_accuracy: 0.7520\n",
      "Epoch 59/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.9516 - val_accuracy: 0.7880\n",
      "Epoch 60/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0092 - accuracy: 0.9993 - val_loss: 0.9411 - val_accuracy: 0.7760\n",
      "Epoch 61/2000\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.9836 - val_accuracy: 0.7760\n",
      "Epoch 62/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0152 - accuracy: 0.9973 - val_loss: 1.0100 - val_accuracy: 0.7820\n",
      "Epoch 63/2000\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.9373 - val_accuracy: 0.7860\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_T.fit(\n",
    "    np.array(x_train), np.array(y_train),  #pairTrain[:, 1]\n",
    "  validation_data = (np.array(x_val), np.array(y_val) ),  #pairVal[:, 1]\n",
    "    epochs=2000\n",
    "    ,batch_size = 64    #8192\n",
    "  , callbacks=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "15425da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: 0.9575 - accuracy: 0.7855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9575217962265015, 0.7855263352394104]"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_T.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079e8be",
   "metadata": {},
   "source": [
    "# Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e757d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 42, 32)]          0         \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 32)                0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 1)                 1089      \n",
      "                                                                 \n",
      " tf.reshape_1 (TFOpLambda)   (None, 42, 1)             0         \n",
      "                                                                 \n",
      " conv1d_82 (Conv1D)          (None, 42, 1)             6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1095 (4.28 KB)\n",
      "Trainable params: 1095 (4.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 40, 128)]         0         \n",
      "                                                                 \n",
      " tf.reshape_2 (TFOpLambda)   (None, 128)               0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 1)                 4161      \n",
      "                                                                 \n",
      " tf.reshape_3 (TFOpLambda)   (None, 40, 1)             0         \n",
      "                                                                 \n",
      " conv1d_83 (Conv1D)          (None, 40, 1)             6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4167 (16.28 KB)\n",
      "Trainable params: 4167 (16.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def spatial_mlp_attention(shape, kernel):\n",
    "  inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "  # CHANNEL SUMMARIZATION\n",
    "  w, C = shape[0], shape[1]\n",
    "  # Reshape the feature map to (w * h, C)\n",
    "  reshaped_feature_map = tf.reshape(inp, [-1, C])\n",
    "  # Define the MLP architecture\n",
    "  hidden_units = 32\n",
    "  output_units = 1\n",
    "  mlp = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      Dropout(0.4),\n",
    "      #BatchNormalization(),\n",
    "      tf.keras.layers.Dense(output_units, activation='sigmoid')\n",
    "  ])\n",
    "  # Apply the MLP on the reshaped feature map\n",
    "  output_mlp = mlp(reshaped_feature_map)\n",
    "  reshaped_output = tf.reshape(output_mlp, ( -1, w, 1 ))\n",
    "  # CONV & ACTIVATION\n",
    "  out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( reshaped_output )\n",
    "\n",
    "  model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "\n",
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "x_S = Conv1D(kernel_size = 7, filters=64, activation='relu', name='l1_S')(dataInp) # 194, 32\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(2)(x_S) # 97, 32\n",
    "\n",
    "x_S1 = Conv1D(kernel_size = 4, filters=64, activation='relu', name='l2_S')(x_S) #94, 64\n",
    "x_S = MaxPooling1D(2)(x_S1) #47, 64\n",
    "\n",
    "x_b_S = Conv1D(kernel_size = 6, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "x_b2 = BatchNormalization()(x_b_S) #old\n",
    "\n",
    "at1 = spatial_mlp_attention(shape = (42, 32), kernel = 5)\n",
    "x_a1 = at1(x_b2) \n",
    "x_S = tf.keras.layers.Multiply()([x_a1, x_b2])\n",
    "# x_T = tf.multiply([x_a1, x_b2])\n",
    "x_S = Add()([x_S, x_b_S])\n",
    "\n",
    "# x_S = Conv1D(kernel_size = 3, filters=64, activation='relu', name='l3_s')(x_b_S)\n",
    "x_S3 = Conv1D(kernel_size = 3, filters=128, activation='relu', name='l4_S')(x_S) #40, 128\n",
    "x_b3 = BatchNormalization()(x_S3) \n",
    "\n",
    "at2 = spatial_mlp_attention(shape = (40, 128), kernel = 5)\n",
    "x_a2 = at2(x_b3) \n",
    "x_S = tf.keras.layers.Multiply()([x_a2, x_b3])\n",
    "# x_T = tf.multiply([x_a1, x_b2])\n",
    "x_S = Add()([x_S, x_S3])\n",
    "\n",
    "x_S = Conv1D(kernel_size = 1, filters=256, activation='relu', name='l5_S')(x_S)\n",
    "avg_S = GlobalAveragePooling1D()(x_S)\n",
    "max_S = GlobalMaxPooling1D()(x_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "# flat_S = Flatten()(x_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_1')(flat_S)\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_2')(d_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "model_s = Model(inputs=dataInp, outputs=[x_b_S, out_S], name='student' )\n",
    "model_s.compile(loss = [\"mean_squared_error\", \"binary_crossentropy\" ], optimizer = 'adam',loss_weights = [1, 10], metrics=[\"accuracy\"])\n",
    "# print(model_s.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "1cc2eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spatial_mlp_attention(shape, kernel):\n",
    "#   inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "#   # CHANNEL SUMMARIZATION\n",
    "#   w, C = shape[0], shape[1]\n",
    "#   # Reshape the feature map to (w * h, C)\n",
    "#   reshaped_feature_map = tf.reshape(inp, [-1, C])\n",
    "#   # Define the MLP architecture\n",
    "#   hidden_units = 32\n",
    "#   output_units = 1\n",
    "#   mlp = tf.keras.Sequential([\n",
    "#       tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "#       Dropout(0.4),\n",
    "#       #BatchNormalization(),\n",
    "#       tf.keras.layers.Dense(output_units, activation='sigmoid')\n",
    "#   ])\n",
    "#   # Apply the MLP on the reshaped feature map\n",
    "#   output_mlp = mlp(reshaped_feature_map)\n",
    "#   reshaped_output = tf.reshape(output_mlp, ( -1, w, 1 ))\n",
    "#   # CONV & ACTIVATION\n",
    "#   out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( reshaped_output )\n",
    "\n",
    "#   model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "#   model.summary()\n",
    "#   return model\n",
    "\n",
    "\n",
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "x_S = Conv1D(kernel_size = 7, filters= 32, activation='relu', name='l1_S')(dataInp) # 194, 32\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(2)(x_S) # 97, 32\n",
    "\n",
    "x_S1 = Conv1D(kernel_size = 4, filters=64, activation='relu', name='l2_S')(x_S) #94, 64\n",
    "x_S = MaxPooling1D(2)(x_S1) #47, 64\n",
    "x_S = BatchNormalization()(x_S)\n",
    "\n",
    "x_b_S = Conv1D(kernel_size = 6, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "x_b2 = BatchNormalization()(x_b_S) #old\n",
    "# x_S = MaxPooling1D(2)(x_b2) \n",
    "\n",
    "# at1 = spatial_mlp_attention(shape = (42, 32), kernel = 5)\n",
    "# x_a1 = at1(x_b2) \n",
    "# x_S = tf.keras.layers.Multiply()([x_a1, x_b2])\n",
    "# x_S = Add()([x_S, x_b_S])\n",
    "\n",
    "\n",
    "x_S3 = Conv1D(kernel_size = 5, filters=64, activation='relu', name='l4_S')(x_b2) #38, 128\n",
    "# x_S = MaxPooling1D(2)(x_S3) \n",
    "x_S = BatchNormalization()(x_S3) \n",
    "# x_S = MaxPooling1D(2)(x_b3) \n",
    "\n",
    "# at2 = spatial_mlp_attention(shape = (38, 64), kernel = 5)\n",
    "# x_a2 = at2(x_b3) \n",
    "# x_S = tf.keras.layers.Multiply()([x_a2, x_b3])\n",
    "# # x_T = tf.multiply([x_a1, x_b2])\n",
    "# x_S = Add()([x_S, x_S3])\n",
    "\n",
    "x_S = Conv1D(kernel_size = 3, filters=128, activation='relu', name='l5_S')(x_S)\n",
    "x_S = BatchNormalization()(x_S) \n",
    "x_S = Conv1D(kernel_size = 3, filters=256, activation='relu', name='l6_S')(x_S)\n",
    "x_S = BatchNormalization()(x_S) \n",
    "x_S = MaxPooling1D(2)(x_S) \n",
    "\n",
    "# x_S = Conv1D(kernel_size = 1, filters=256, activation='relu', name='l7_S')(x_S)\n",
    "# x_S = MaxPooling1D(2)(x_S) \n",
    "\n",
    "avg_S = GlobalAveragePooling1D()(x_S)\n",
    "max_S = GlobalMaxPooling1D()(x_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "\n",
    "# flat_S = Flatten()(x_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_1')(flat_S)\n",
    "\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "d_S = Dense(300, activation = 'relu', name='dense_2')(d_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "model_s = Model(inputs=dataInp, outputs=[x_b_S, out_S], name='student' )\n",
    "model_s.compile(loss = [\"mean_squared_error\", \"binary_crossentropy\" ], optimizer = 'adam',loss_weights = [1, 10], metrics=[\"accuracy\"])\n",
    "# print(model_s.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "a9a101bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"student\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_159 (InputLayer)      [(None, 200, 6)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 32)              1376      ['input_159[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_710 (B  (None, 194, 32)              128       ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_542 (MaxPool  (None, 97, 32)               0         ['batch_normalization_710[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " l2_S (Conv1D)               (None, 94, 64)               8256      ['max_pooling1d_542[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_543 (MaxPool  (None, 47, 64)               0         ['l2_S[0][0]']                \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_711 (B  (None, 47, 64)               256       ['max_pooling1d_543[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               12320     ['batch_normalization_711[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_712 (B  (None, 42, 32)               128       ['bottle_S[0][0]']            \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " l4_S (Conv1D)               (None, 38, 64)               10304     ['batch_normalization_712[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_713 (B  (None, 38, 64)               256       ['l4_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " l5_S (Conv1D)               (None, 36, 128)              24704     ['batch_normalization_713[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_714 (B  (None, 36, 128)              512       ['l5_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " l6_S (Conv1D)               (None, 34, 256)              98560     ['batch_normalization_714[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_715 (B  (None, 34, 256)              1024      ['l6_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_544 (MaxPool  (None, 17, 256)              0         ['batch_normalization_715[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 256)                  0         ['max_pooling1d_544[0][0]']   \n",
      " 42 (GlobalAveragePooling1D                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_143 (  (None, 256)                  0         ['max_pooling1d_544[0][0]']   \n",
      " GlobalMaxPooling1D)                                                                              \n",
      "                                                                                                  \n",
      " add_144 (Add)               (None, 256)                  0         ['global_average_pooling1d_142\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_143[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_111 (Dropout)       (None, 256)                  0         ['add_144[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 300)                  77100     ['dropout_111[0][0]']         \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  83076     ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 318000 (1.21 MB)\n",
      "Trainable params: 316848 (1.21 MB)\n",
      "Non-trainable params: 1152 (4.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "46a2404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "x_S = Conv1D(kernel_size = 7, filters= 16, activation='relu', name='l1_S')(dataInp) # 194, 32\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(2)(x_S) # 97, 32\n",
    "\n",
    "x_S1 = Conv1D(kernel_size = 4, filters=32, activation='relu', name='l2_S')(x_S) #94, 64\n",
    "x_S = MaxPooling1D(2)(x_S1) #47, 64\n",
    "x_S = BatchNormalization()(x_S)\n",
    "\n",
    "x_b_S = Conv1D(kernel_size = 6, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "x_b2 = BatchNormalization()(x_b_S) #old\n",
    "# x_S = MaxPooling1D(2)(x_b2) \n",
    "\n",
    "x_S3 = Conv1D(kernel_size = 5, filters=64, activation='relu', name='l4_S')(x_b2) #38, 128\n",
    "x_S = MaxPooling1D(2)(x_S3) \n",
    "x_S = BatchNormalization()(x_S3) \n",
    "# x_S = MaxPooling1D(2)(x_b3) \n",
    "\n",
    "# x_S = Conv1D(kernel_size = 3, filters=128, activation='relu', name='l5_S')(x_S)\n",
    "# x_S = BatchNormalization()(x_S) \n",
    "# x_S = Conv1D(kernel_size = 3, filters=256, activation='relu', name='l6_S')(x_S)\n",
    "# x_S = BatchNormalization()(x_S) \n",
    "# x_S = MaxPooling1D(2)(x_S) \n",
    "\n",
    "# x_S = Conv1D(kernel_size = 1, filters=256, activation='relu', name='l7_S')(x_S)\n",
    "# x_S = MaxPooling1D(2)(x_S) \n",
    "\n",
    "avg_S = GlobalAveragePooling1D()(x_S)\n",
    "max_S = GlobalMaxPooling1D()(x_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "\n",
    "# flat_S = Flatten()(x_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_1')(flat_S)\n",
    "\n",
    "# d_S = Dropout(0.3)(flat_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_2')(d_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(flat_S)\n",
    "model_s = Model(inputs=dataInp, outputs=[x_b_S, out_S], name='student' )\n",
    "model_s.compile(loss = [\"mean_squared_error\", \"binary_crossentropy\" ], optimizer = 'adam',loss_weights = [1, 10], metrics=[\"accuracy\"])\n",
    "# print(model_s.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "512e0d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"student\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_175 (InputLayer)      [(None, 200, 6)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 16)              688       ['input_175[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_778 (B  (None, 194, 16)              64        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_585 (MaxPool  (None, 97, 16)               0         ['batch_normalization_778[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " l2_S (Conv1D)               (None, 94, 32)               2080      ['max_pooling1d_585[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_586 (MaxPool  (None, 47, 32)               0         ['l2_S[0][0]']                \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_779 (B  (None, 47, 32)               128       ['max_pooling1d_586[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               6176      ['batch_normalization_779[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_780 (B  (None, 42, 32)               128       ['bottle_S[0][0]']            \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " l4_S (Conv1D)               (None, 38, 64)               10304     ['batch_normalization_780[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_781 (B  (None, 38, 64)               256       ['l4_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 64)                   0         ['batch_normalization_781[0][0\n",
      " 58 (GlobalAveragePooling1D                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_159 (  (None, 64)                   0         ['batch_normalization_781[0][0\n",
      " GlobalMaxPooling1D)                                                ]']                           \n",
      "                                                                                                  \n",
      " add_160 (Add)               (None, 64)                   0         ['global_average_pooling1d_158\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_159[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  17940     ['add_160[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37764 (147.52 KB)\n",
      "Trainable params: 37476 (146.39 KB)\n",
      "Non-trainable params: 288 (1.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the student model using the teacher model's outputs as labels\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_s.fit(np.array(x_train), \n",
    "                      [np.array(bottleneck_train), np.array(output_train)], \n",
    "                      epochs=1000, \n",
    "                      batch_size=32,\n",
    "                      validation_data = ( np.array(x_val), [np.array(bottleneck_val), np.array(output_val)] ),\n",
    "                      callbacks = callback)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "7a73beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7282 - bottle_S_loss: 0.5443 - out_S_loss: 0.0184 - bottle_S_accuracy: 0.8547 - out_S_accuracy: 0.8237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7282442450523376,\n",
       " 0.5443060994148254,\n",
       " 0.01839381456375122,\n",
       " 0.8546992540359497,\n",
       " 0.8236842155456543]"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s.evaluate(np.array(x_test), [bottleneck_test, output_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae3286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "60985b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "x_S = Conv1D(kernel_size = 7, filters= 16, activation='relu', name='l1_S')(dataInp) # 194, 32\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(2)(x_S) # 97, 32\n",
    "\n",
    "x_S1 = Conv1D(kernel_size = 4, filters=32, activation='relu', name='l2_S')(x_S) #94, 64\n",
    "x_S = MaxPooling1D(2)(x_S1) #47, 64\n",
    "x_S = BatchNormalization()(x_S)\n",
    "\n",
    "x_b_S = Conv1D(kernel_size = 6, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "x_b2 = BatchNormalization()(x_b_S) #old\n",
    "# x_S = MaxPooling1D(2)(x_b2) \n",
    "\n",
    "x_S3 = Conv1D(kernel_size = 5, filters=64, activation='relu', name='l4_S')(x_b2) #38, 128\n",
    "x_S = MaxPooling1D(2)(x_S3) \n",
    "x_S = BatchNormalization()(x_S3) \n",
    "# x_S = MaxPooling1D(2)(x_b3) \n",
    "\n",
    "# x_S = Conv1D(kernel_size = 3, filters=128, activation='relu', name='l5_S')(x_S)\n",
    "# x_S = BatchNormalization()(x_S) \n",
    "# x_S = Conv1D(kernel_size = 3, filters=256, activation='relu', name='l6_S')(x_S)\n",
    "# x_S = BatchNormalization()(x_S) \n",
    "# x_S = MaxPooling1D(2)(x_S) \n",
    "\n",
    "# x_S = Conv1D(kernel_size = 1, filters=256, activation='relu', name='l7_S')(x_S)\n",
    "# x_S = MaxPooling1D(2)(x_S) \n",
    "\n",
    "avg_S = GlobalAveragePooling1D()(x_S)\n",
    "max_S = GlobalMaxPooling1D()(x_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "\n",
    "# flat_S = Flatten()(x_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_1')(flat_S)\n",
    "\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_2')(d_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "# model_s = Model(inputs=dataInp, outputs=[x_b_S, out_S], name='student' )\n",
    "# model_s.compile(loss = [\"mean_squared_error\", \"binary_crossentropy\" ], optimizer = 'adam',loss_weights = [1, 10], metrics=[\"accuracy\"])\n",
    "# print(model_s.summary())\n",
    "model_s = Model(inputs=dataInp, outputs=out_S, name='student' )\n",
    "model_s.compile(loss = \"binary_crossentropy\" , optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "# print(model_s.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "0907c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"student\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_180 (InputLayer)      [(None, 200, 6)]             0         []                            \n",
      "                                                                                                  \n",
      " l1_S (Conv1D)               (None, 194, 16)              688       ['input_180[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_798 (B  (None, 194, 16)              64        ['l1_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling1d_600 (MaxPool  (None, 97, 16)               0         ['batch_normalization_798[0][0\n",
      " ing1D)                                                             ]']                           \n",
      "                                                                                                  \n",
      " l2_S (Conv1D)               (None, 94, 32)               2080      ['max_pooling1d_600[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_601 (MaxPool  (None, 47, 32)               0         ['l2_S[0][0]']                \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_799 (B  (None, 47, 32)               128       ['max_pooling1d_601[0][0]']   \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " bottle_S (Conv1D)           (None, 42, 32)               6176      ['batch_normalization_799[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_800 (B  (None, 42, 32)               128       ['bottle_S[0][0]']            \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " l4_S (Conv1D)               (None, 38, 64)               10304     ['batch_normalization_800[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_801 (B  (None, 38, 64)               256       ['l4_S[0][0]']                \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 64)                   0         ['batch_normalization_801[0][0\n",
      " 63 (GlobalAveragePooling1D                                         ]']                           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_164 (  (None, 64)                   0         ['batch_normalization_801[0][0\n",
      " GlobalMaxPooling1D)                                                ]']                           \n",
      "                                                                                                  \n",
      " add_165 (Add)               (None, 64)                   0         ['global_average_pooling1d_163\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'global_max_pooling1d_164[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout_113 (Dropout)       (None, 64)                   0         ['add_165[0][0]']             \n",
      "                                                                                                  \n",
      " out_S (Dense)               (None, 276)                  17940     ['dropout_113[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37764 (147.52 KB)\n",
      "Trainable params: 37476 (146.39 KB)\n",
      "Non-trainable params: 288 (1.12 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48403121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the student model using the teacher model's outputs as labels\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_s.fit(np.array(x_train), \n",
    "                      np.array(output_train), \n",
    "                      epochs=1000, \n",
    "                      batch_size=32,\n",
    "                      validation_data = ( np.array(x_val), np.array(output_val) ),\n",
    "                      callbacks = callback)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "2f26a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.6224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02433028630912304, 0.6223683953285217]"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s.evaluate(np.array(x_test), output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac76ba",
   "metadata": {},
   "source": [
    "# plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "a25f2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_par = [21000, 45285, 107700, 156980, 218004, 427284]\n",
    "base_acc = [66.58, 85.39, 88.68, 90.26, 92.89, 94.21]\n",
    "base_loss = [1.27, 0.7261, 0.6315, 0.6333, 0.5135, 0.3788]\n",
    "kd_par = [18000, 37500, 80000, 166000, 319000]\n",
    "kd_acc = [77.89, 87.89, 91.32, 93, 96]\n",
    "kd_loss = [0.0201, 0.015, 0.0133, 0.0123, 0.0112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c11a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_par = [18000, 37500, 80000, 166000, 319000]\n",
    "kd_acc = [77.89, 87.89, 91.32, 93, 96]\n",
    "kd_loss = [0.0201, 0.015, 0.0133, 0.0123, 0.0112]\n",
    "\n",
    "base_par = [21000, 45285, 107700, 156980, 218004, 319000]\n",
    "base_acc = [66.58, 85.39, 88.68, 90.26, 92.89, 90.39]\n",
    "base_loss = [1.27, 0.7261, 0.6315, 0.6333, 0.5135, 0.3455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "084f7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "e01b056c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(base_par, base_acc, 'o-', label = 'Base Model Accuracy', linewidth = 2)\n",
    "plt.plot(kd_par, kd_acc, 's-', label = 'Knowledge Distillation', linewidth = 2)\n",
    "plt.xlabel('parameters')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy Improvment using Knowledge Distillation')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf4eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d50ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
