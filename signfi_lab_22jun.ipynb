{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import random \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('dataset_lab_276_ul.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'csiu_lab', 'label_lab'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture = data['csiu_lab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['label_lab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 30, 3, 5520), (5520, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gesture.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude = np.zeros(shape = (200, 30, 3, 5520))\n",
    "for timestep in range(200):\n",
    "    for antenna in range(30):\n",
    "        for subcar in range(3):\n",
    "            for sample in range(5520):\n",
    "                amplitude[timestep][antenna][subcar][sample] = np.abs(gesture[timestep][antenna][subcar][sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = np.zeros(shape = (200, 30, 3, 5520))\n",
    "for timestep in range(200):\n",
    "    for antenna in range(30):\n",
    "        for subcar in range(3):\n",
    "            for sample in range(5520):\n",
    "                phase[timestep][antenna][subcar][sample] = np.angle(gesture[timestep][antenna][subcar][sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  8.30181255,   7.30990203,   9.17665868, ...,  33.81686943,\n",
       "          33.17240603,  29.86606089],\n",
       "        [ 40.31124725,  35.49481108,  66.05697378, ...,   6.96434197,\n",
       "           6.83161935,   6.15070125],\n",
       "        [ 59.75950865,  52.61937089,  44.55925195, ...,  50.13190211,\n",
       "          49.17651575,  44.27501617]],\n",
       "\n",
       "       [[ 80.9746449 ,  44.81034436,  58.01165631, ...,  87.70039126,\n",
       "          28.22822676,  20.03043134],\n",
       "        [ 91.98223529,  81.75417724, 109.90219935, ...,  68.36663358,\n",
       "          63.4753673 ,  60.55445993],\n",
       "        [103.57062713,  94.08776056,  99.95492042, ...,  44.71312752,\n",
       "          73.32521719,  43.99979362]],\n",
       "\n",
       "       [[100.98651727,  31.19453836,  56.26059382, ...,  20.05631095,\n",
       "          76.5897575 ,  76.31682658],\n",
       "        [103.57062713, 110.48586027, 156.01310191, ...,  44.69321963,\n",
       "           0.92539129,  17.68372612],\n",
       "        [ 72.00565414,  90.17146755,   1.96542435, ...,  28.86926504,\n",
       "          63.12023391,  35.98038452]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 36.05547564,  91.53123849,  55.38173831, ...,  23.35673181,\n",
       "          85.54240226,  17.17595317],\n",
       "        [ 58.32447145,  48.85603236, 136.94368295, ...,  63.37098305,\n",
       "          83.75941467,  41.64113892],\n",
       "        [ 82.55222128,  42.76147954,  25.62599828, ...,  66.73630903,\n",
       "          84.48979604,  91.31520168]],\n",
       "\n",
       "       [[ 49.30050449,  21.70501055,  56.25372733, ..., 105.66580801,\n",
       "         104.96768541,  27.93869471],\n",
       "        [127.0729509 ,  79.22971593, 138.00022407, ...,  20.68970865,\n",
       "          63.47199445,  58.31200017],\n",
       "        [ 72.63949972,  93.08200569, 103.40448472, ..., 104.21991837,\n",
       "          64.62518309,  61.50701254]],\n",
       "\n",
       "       [[ 77.13553315,  69.96713458,  57.13269182, ...,  63.38502493,\n",
       "          57.38172249,  52.92017141],\n",
       "        [ 36.55185502,  22.41610953,  77.71259043, ...,   0.66706298,\n",
       "          20.2954155 ,  38.882602  ],\n",
       "        [ 26.54008255,  87.52020382,  36.67505006, ...,  78.48698281,\n",
       "           2.06923784,  58.75079912]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amplitude[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.29145679,  0.29145679,  0.29145679, ..., -1.31146458,\n",
       "         -1.31146458, -1.31146458],\n",
       "        [-1.31146458, -1.31146458,  1.13095374, ...,  0.29145679,\n",
       "          0.29145679,  0.29145679],\n",
       "        [ 1.13095374,  1.13095374, -1.31146458, ...,  1.13095374,\n",
       "          1.13095374,  1.13095374]],\n",
       "\n",
       "       [[ 2.83226382,  0.        ,  3.14159265, ..., -2.88001035,\n",
       "         -0.80179014, -1.57079633],\n",
       "        [ 2.17798123,  0.5903779 ,  1.59479172, ...,  2.84452844,\n",
       "          3.13128374,  2.8352255 ],\n",
       "        [ 2.88544127,  2.37724401, -0.97303794, ...,  3.11175077,\n",
       "          2.14642332,  2.02845598]],\n",
       "\n",
       "       [[ 0.        ,  2.37206617,  3.12596892, ..., -0.06656816,\n",
       "          2.14932054, -0.265575  ],\n",
       "        [ 0.25615138,  2.4955567 , -0.79734979, ...,  3.14159265,\n",
       "          0.78539816,  0.033321  ],\n",
       "        [-2.32495466,  0.24309543,  0.46364761, ..., -0.70361378,\n",
       "          1.90893886, -1.52165559]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.4186133 ,  2.9021798 ,  0.01587168, ...,  3.113029  ,\n",
       "         -1.81020918,  1.67387794],\n",
       "        [-2.69013956,  0.44441921, -0.63965004, ...,  0.        ,\n",
       "          3.13378031,  2.70325609],\n",
       "        [ 1.20638874, -1.61993706, -3.03851105, ..., -0.029991  ,\n",
       "         -0.28255495, -2.49809154]],\n",
       "\n",
       "       [[ 0.        ,  1.57079633,  3.14159265, ..., -2.22187305,\n",
       "          0.91358037, -2.40094072],\n",
       "        [ 0.65226184,  0.59045358, -2.22066678, ...,  0.03224688,\n",
       "          0.        ,  0.31855568],\n",
       "        [-2.30198409, -2.36683239, -0.61629694, ...,  0.62173758,\n",
       "         -1.29388749,  0.29145679]],\n",
       "\n",
       "       [[ 3.13128374,  0.31542152,  0.        , ..., -1.5497468 ,\n",
       "          2.37232212, -2.37981215],\n",
       "        [ 0.80078157,  1.60203616, -2.3321991 , ...,  1.57079633,\n",
       "          1.53854944,  3.14159265],\n",
       "        [-2.99122123,  3.14159265,  0.80234569, ...,  2.58659693,\n",
       "          1.89254688,  2.8674252 ]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.zeros(shape = (200, 3, 30, 5520))\n",
    "csi_amp = np.zeros(shape = (5520, 200, 90))\n",
    "\n",
    "for sample in range(5520):\n",
    "  for timestep in range(200):\n",
    "    for antenna in range(30):\n",
    "      for subcar in range(3):\n",
    "        csi_amp[sample][timestep][3*antenna+subcar] = amplitude[timestep][antenna][subcar][sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.zeros(shape = (200, 3, 30, 5520))\n",
    "csi_phase = np.zeros(shape = (5520, 200, 90))\n",
    "\n",
    "for sample in range(5520):\n",
    "  for timestep in range(200):\n",
    "    for antenna in range(30):\n",
    "      for subcar in range(3):\n",
    "        csi_phase[sample][timestep][3*antenna+subcar] = phase[timestep][antenna][subcar][sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5520, 200, 90)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(csi_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2,  0,  1, 21, 57, 18, 24, 60, 48, 54, 45, 51, 78,  3, 15, 75, 72,\n",
       "        69,  6, 63, 42, 27, 39,  9, 65, 66, 81, 71, 56, 89, 74, 35, 38, 83,\n",
       "         5, 80, 62, 59, 41, 77, 26, 50, 14, 17, 53, 20, 68, 29, 44, 11, 32,\n",
       "         8, 86, 47, 12, 23, 30, 84, 36, 87, 33, 52, 55, 49, 79, 76, 58,  4,\n",
       "        22,  7, 46, 73, 25, 28, 19, 82, 61, 43, 70, 31, 85, 10, 88, 64, 40,\n",
       "        16, 34, 37, 13, 67]),\n",
       " array([ 2,  0,  1, 42, 45, 48,  6, 75, 21, 12,  9, 72, 15, 69, 78, 84, 63,\n",
       "        81, 60, 24, 66, 57,  3, 30, 27, 18, 87, 54, 51, 36, 33, 39, 32, 29,\n",
       "        50, 23, 22, 62, 80, 86,  4, 83, 77, 89, 13, 10, 68, 14, 11, 56, 17,\n",
       "        16,  7, 35, 59, 53, 49, 74, 71, 67, 20, 25, 26, 64, 73, 79, 70, 76,\n",
       "        31, 82, 55, 88, 61, 19, 65, 52, 85, 40, 28, 43, 58, 34,  8, 46, 37,\n",
       "        38, 47, 41, 44,  5]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming x_train and x_test are numpy arrays with shape (num_samples, num_timesteps, num_subcarriers=90)\n",
    "\n",
    "# Compute variances across subcarriers for x_train and x_test\n",
    "variances_amp = np.var(csi_amp, axis=(0, 1))  # Compute variance across all samples and time steps (shape: num_subcarriers)\n",
    "variances_phase = np.var(csi_phase, axis=(0, 1))    # Compute variance across all samples and time steps (shape: num_subcarriers)\n",
    "\n",
    "# Get indices of top three subcarriers with the highest variances\n",
    "top_subcarrier_indices_amp = np.argsort(variances_amp)\n",
    "top_subcarrier_indices_phase = np.argsort(variances_phase)\n",
    "\n",
    "top_subcarrier_indices_amp, top_subcarrier_indices_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_amp_30 = np.array(csi_amp)[:, :,[2,0,1]]\n",
    "csi_phase_30 = np.array(csi_phase)[:,:,[2,0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5520, 200, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(csi_amp_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.concatenate((csi_amp_30, csi_phase_30), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5520, 200, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1],\n",
       "       [  2],\n",
       "       [  3],\n",
       "       ...,\n",
       "       [274],\n",
       "       [275],\n",
       "       [276]], dtype=uint16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0],\n",
       "       [  1],\n",
       "       [  2],\n",
       "       ...,\n",
       "       [273],\n",
       "       [274],\n",
       "       [275]], dtype=uint16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(zip(combined, labels))\n",
    "random.shuffle(temp)\n",
    "combined, labels = zip(*temp)\n",
    "combined, labels = list(combined), list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.constant(labels[:3000])\n",
    "y_val = tf.constant(labels[3000:4000])\n",
    "y_test = tf.constant(labels[4000:5520])\n",
    "x_train = tf.constant(combined[:3000])\n",
    "x_val = tf.constant(combined[3000:4000])\n",
    "x_test = tf.constant(combined[4000:5520])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data and labels together\n",
    "combined_shuffled, labels_shuffled = shuffle(combined, labels, random_state=42)\n",
    "\n",
    "# Split the data and labels\n",
    "x_train = tf.constant(combined_shuffled[:3000])\n",
    "x_val = tf.constant(combined_shuffled[3000:4000])\n",
    "x_test = tf.constant(combined_shuffled[4000:5520])\n",
    "\n",
    "y_train = tf.constant(labels_shuffled[:3000])\n",
    "y_val = tf.constant(labels_shuffled[3000:4000])\n",
    "y_test = tf.constant(labels_shuffled[4000:5520])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (3000, 200, 6), Train labels shape: (3000, 1)\n",
      "Validation data shape: (1000, 200, 6), Validation labels shape: (1000, 1)\n",
      "Test data shape: (1520, 200, 6), Test labels shape: (1520, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes\n",
    "print(f'Train data shape: {x_train.shape}, Train labels shape: {y_train.shape}')\n",
    "print(f'Validation data shape: {x_val.shape}, Validation labels shape: {y_val.shape}')\n",
    "print(f'Test data shape: {x_test.shape}, Test labels shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "x_train shape: (3000, 200, 6)\n",
      "x_val shape: (1000, 200, 6)\n",
      "x_test shape: (1520, 200, 6)\n",
      "y_train shape: (3000, 1)\n",
      "y_val shape: (1000, 1)\n",
      "y_test shape: (1520, 1)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes (optional)\n",
    "print(\"Shapes:\")\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,504</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ conv1d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ conv1d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ conv1d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ conv1d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ conv1d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ conv1d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,944</span> │ conv1d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,944</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">786,944</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">282,900</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │      \u001b[38;5;34m5,504\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_15 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m49,280\u001b[0m │ conv1d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_16 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m49,280\u001b[0m │ conv1d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_17 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m49,280\u001b[0m │ conv1d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_18 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m49,280\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_19 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m49,280\u001b[0m │ conv1d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ conv1d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_20 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m98,560\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_22 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m33,024\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_21 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m196,864\u001b[0m │ conv1d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_23 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m393,728\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_25 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_24 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m786,944\u001b[0m │ conv1d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_26 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m786,944\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_27 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m786,944\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │    \u001b[38;5;34m282,900\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,749,396</span> (14.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,749,396\u001b[0m (14.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,749,396</span> (14.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,749,396\u001b[0m (14.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Add, Concatenate, Input\n",
    "\n",
    "# Input Layer\n",
    "inputs = Input(shape=(200, 6))\n",
    "\n",
    "# Initial Convolution Layer\n",
    "x = Conv1D(128, kernel_size=7, padding='same', activation='relu')(inputs)\n",
    "x = Conv1D(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)  # Replaces MaxPooling\n",
    "\n",
    "# Residual Block 1\n",
    "res1 = Conv1D(128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "res1 = Conv1D(128, kernel_size=3, padding='same', activation='relu')(res1)\n",
    "x = Add()([x, res1])\n",
    "\n",
    "# Residual Block 2\n",
    "res2 = Conv1D(128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "res2 = Conv1D(128, kernel_size=3, padding='same', activation='relu')(res2)\n",
    "x = Add()([x, res2])\n",
    "\n",
    "# Residual Block 3 (Fix: Match dimensions)\n",
    "res3 = Conv1D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
    "res3 = Conv1D(256, kernel_size=3, padding='same', activation='relu')(res3)\n",
    "x = Conv1D(256, kernel_size=1, padding='same', activation='relu')(x)  # Match dimensions\n",
    "x = Add()([x, res3])\n",
    "\n",
    "# Residual Block 4 (Fix: Match dimensions)\n",
    "res4 = Conv1D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "res4 = Conv1D(512, kernel_size=3, padding='same', activation='relu')(res4)\n",
    "x = Conv1D(512, kernel_size=1, padding='same', activation='relu')(x)  # Match dimensions\n",
    "x = Add()([x, res4])\n",
    "\n",
    "# Two Parallel Paths\n",
    "branch1 = Conv1D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "branch1 = GlobalAveragePooling1D()(branch1)\n",
    "\n",
    "branch2 = Conv1D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "branch2 = GlobalAveragePooling1D()(branch2)\n",
    "\n",
    "# Concatenation\n",
    "merged = Concatenate()([branch1, branch2])\n",
    "\n",
    "# Fully Connected Output Layer\n",
    "outputs = Dense(276, activation='softmax')(merged)\n",
    "\n",
    "# Create Model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 314ms/step - accuracy: 0.0050 - loss: 6.9546 - val_accuracy: 0.0090 - val_loss: 5.1014\n",
      "Epoch 2/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 349ms/step - accuracy: 0.0230 - loss: 4.8899 - val_accuracy: 0.0280 - val_loss: 4.4736\n",
      "Epoch 3/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 361ms/step - accuracy: 0.0451 - loss: 4.3424 - val_accuracy: 0.0770 - val_loss: 4.0272\n",
      "Epoch 4/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 654ms/step - accuracy: 0.0868 - loss: 3.9242 - val_accuracy: 0.1090 - val_loss: 3.7162\n",
      "Epoch 5/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 453ms/step - accuracy: 0.1115 - loss: 3.6767 - val_accuracy: 0.1410 - val_loss: 3.4792\n",
      "Epoch 6/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 415ms/step - accuracy: 0.1403 - loss: 3.5573 - val_accuracy: 0.1960 - val_loss: 3.2811\n",
      "Epoch 7/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 455ms/step - accuracy: 0.1948 - loss: 3.2778 - val_accuracy: 0.2140 - val_loss: 3.1717\n",
      "Epoch 8/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 434ms/step - accuracy: 0.2271 - loss: 3.0627 - val_accuracy: 0.2220 - val_loss: 3.0221\n",
      "Epoch 9/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 439ms/step - accuracy: 0.2571 - loss: 2.9805 - val_accuracy: 0.2310 - val_loss: 3.3204\n",
      "Epoch 10/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 506ms/step - accuracy: 0.2929 - loss: 2.7835 - val_accuracy: 0.3250 - val_loss: 2.6643\n",
      "Epoch 11/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 495ms/step - accuracy: 0.3463 - loss: 2.5288 - val_accuracy: 0.3450 - val_loss: 2.6319\n",
      "Epoch 12/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 542ms/step - accuracy: 0.3604 - loss: 2.4785 - val_accuracy: 0.3470 - val_loss: 2.5743\n",
      "Epoch 13/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 561ms/step - accuracy: 0.3908 - loss: 2.3377 - val_accuracy: 0.4030 - val_loss: 2.4212\n",
      "Epoch 14/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 542ms/step - accuracy: 0.4119 - loss: 2.2766 - val_accuracy: 0.3560 - val_loss: 2.6154\n",
      "Epoch 15/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 598ms/step - accuracy: 0.4045 - loss: 2.3050 - val_accuracy: 0.3960 - val_loss: 2.3846\n",
      "Epoch 16/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 598ms/step - accuracy: 0.4477 - loss: 2.1008 - val_accuracy: 0.4310 - val_loss: 2.2583\n",
      "Epoch 17/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 672ms/step - accuracy: 0.4657 - loss: 2.0443 - val_accuracy: 0.4440 - val_loss: 2.2126\n",
      "Epoch 18/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 653ms/step - accuracy: 0.4782 - loss: 1.9856 - val_accuracy: 0.4320 - val_loss: 2.2370\n",
      "Epoch 19/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 695ms/step - accuracy: 0.4922 - loss: 1.9045 - val_accuracy: 0.4550 - val_loss: 2.2280\n",
      "Epoch 20/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 698ms/step - accuracy: 0.5151 - loss: 1.8248 - val_accuracy: 0.4800 - val_loss: 2.0611\n",
      "Epoch 21/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 719ms/step - accuracy: 0.5429 - loss: 1.6971 - val_accuracy: 0.4880 - val_loss: 2.0986\n",
      "Epoch 22/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 644ms/step - accuracy: 0.5546 - loss: 1.6700 - val_accuracy: 0.4880 - val_loss: 2.0358\n",
      "Epoch 23/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 669ms/step - accuracy: 0.5609 - loss: 1.6158 - val_accuracy: 0.4940 - val_loss: 2.1596\n",
      "Epoch 24/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 726ms/step - accuracy: 0.4837 - loss: 2.0540 - val_accuracy: 0.5020 - val_loss: 2.0947\n",
      "Epoch 25/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 747ms/step - accuracy: 0.5729 - loss: 1.5748 - val_accuracy: 0.5200 - val_loss: 1.9936\n",
      "Epoch 26/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 665ms/step - accuracy: 0.6012 - loss: 1.4334 - val_accuracy: 0.5520 - val_loss: 1.8459\n",
      "Epoch 27/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 693ms/step - accuracy: 0.6436 - loss: 1.2782 - val_accuracy: 0.5270 - val_loss: 1.9647\n",
      "Epoch 28/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 643ms/step - accuracy: 0.6551 - loss: 1.2773 - val_accuracy: 0.5580 - val_loss: 1.8928\n",
      "Epoch 29/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 716ms/step - accuracy: 0.6724 - loss: 1.1799 - val_accuracy: 0.5810 - val_loss: 1.7466\n",
      "Epoch 30/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 655ms/step - accuracy: 0.7052 - loss: 1.0696 - val_accuracy: 0.5890 - val_loss: 1.6548\n",
      "Epoch 31/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 683ms/step - accuracy: 0.6900 - loss: 1.1002 - val_accuracy: 0.5580 - val_loss: 1.8848\n",
      "Epoch 32/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 679ms/step - accuracy: 0.7089 - loss: 1.0062 - val_accuracy: 0.6300 - val_loss: 1.5686\n",
      "Epoch 33/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 707ms/step - accuracy: 0.7758 - loss: 0.8267 - val_accuracy: 0.6000 - val_loss: 1.8222\n",
      "Epoch 34/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 635ms/step - accuracy: 0.7498 - loss: 0.8712 - val_accuracy: 0.6430 - val_loss: 1.5256\n",
      "Epoch 35/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 695ms/step - accuracy: 0.8021 - loss: 0.6870 - val_accuracy: 0.6310 - val_loss: 1.6921\n",
      "Epoch 36/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 586ms/step - accuracy: 0.8147 - loss: 0.6347 - val_accuracy: 0.6340 - val_loss: 1.7555\n",
      "Epoch 37/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 741ms/step - accuracy: 0.8378 - loss: 0.5547 - val_accuracy: 0.6570 - val_loss: 1.6930\n",
      "Epoch 38/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 832ms/step - accuracy: 0.8472 - loss: 0.5129 - val_accuracy: 0.6460 - val_loss: 1.7444\n",
      "Epoch 39/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - accuracy: 0.8677 - loss: 0.4580 - val_accuracy: 0.6750 - val_loss: 1.5906\n",
      "Epoch 40/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 722ms/step - accuracy: 0.8868 - loss: 0.3710 - val_accuracy: 0.7110 - val_loss: 1.5025\n",
      "Epoch 41/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 574ms/step - accuracy: 0.9110 - loss: 0.2835 - val_accuracy: 0.7270 - val_loss: 1.4936\n",
      "Epoch 42/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 626ms/step - accuracy: 0.9140 - loss: 0.2663 - val_accuracy: 0.7350 - val_loss: 1.3236\n",
      "Epoch 43/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 511ms/step - accuracy: 0.9372 - loss: 0.2124 - val_accuracy: 0.7260 - val_loss: 1.3597\n",
      "Epoch 44/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 494ms/step - accuracy: 0.9533 - loss: 0.1504 - val_accuracy: 0.7030 - val_loss: 1.5619\n",
      "Epoch 45/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 502ms/step - accuracy: 0.9242 - loss: 0.2756 - val_accuracy: 0.7020 - val_loss: 1.4652\n",
      "Epoch 46/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 476ms/step - accuracy: 0.9538 - loss: 0.1396 - val_accuracy: 0.7030 - val_loss: 2.1236\n",
      "Epoch 47/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 516ms/step - accuracy: 0.9339 - loss: 0.2213 - val_accuracy: 0.6930 - val_loss: 1.5571\n",
      "Epoch 48/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 522ms/step - accuracy: 0.9280 - loss: 0.2805 - val_accuracy: 0.7250 - val_loss: 1.4636\n",
      "Epoch 49/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 536ms/step - accuracy: 0.9631 - loss: 0.1157 - val_accuracy: 0.7200 - val_loss: 1.5069\n",
      "Epoch 50/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 631ms/step - accuracy: 0.9120 - loss: 0.4083 - val_accuracy: 0.7520 - val_loss: 1.1066\n",
      "Epoch 51/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 522ms/step - accuracy: 0.9875 - loss: 0.0497 - val_accuracy: 0.7620 - val_loss: 1.2845\n",
      "Epoch 52/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 521ms/step - accuracy: 0.9936 - loss: 0.0274 - val_accuracy: 0.7530 - val_loss: 1.3690\n",
      "Epoch 53/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 507ms/step - accuracy: 0.9781 - loss: 0.0711 - val_accuracy: 0.7640 - val_loss: 1.2494\n",
      "Epoch 54/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 515ms/step - accuracy: 0.9812 - loss: 0.0691 - val_accuracy: 0.7680 - val_loss: 1.3297\n",
      "Epoch 55/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 505ms/step - accuracy: 0.9958 - loss: 0.0152 - val_accuracy: 0.7840 - val_loss: 1.3283\n",
      "Epoch 56/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 480ms/step - accuracy: 0.9963 - loss: 0.0092 - val_accuracy: 0.7500 - val_loss: 1.5054\n",
      "Epoch 57/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 529ms/step - accuracy: 0.9773 - loss: 0.0805 - val_accuracy: 0.7620 - val_loss: 1.3556\n",
      "Epoch 58/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 465ms/step - accuracy: 0.9776 - loss: 0.0822 - val_accuracy: 0.7530 - val_loss: 1.3619\n",
      "Epoch 59/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 493ms/step - accuracy: 0.9764 - loss: 0.0769 - val_accuracy: 0.7270 - val_loss: 1.3845\n",
      "Epoch 60/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 480ms/step - accuracy: 0.9683 - loss: 0.0999 - val_accuracy: 0.7330 - val_loss: 1.4746\n",
      "Epoch 61/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 445ms/step - accuracy: 0.9771 - loss: 0.0808 - val_accuracy: 0.6870 - val_loss: 1.7629\n",
      "Epoch 62/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 488ms/step - accuracy: 0.9506 - loss: 0.1633 - val_accuracy: 0.6740 - val_loss: 2.3710\n",
      "Epoch 63/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 482ms/step - accuracy: 0.9570 - loss: 0.1484 - val_accuracy: 0.7420 - val_loss: 1.4567\n",
      "Epoch 64/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 450ms/step - accuracy: 0.9859 - loss: 0.0528 - val_accuracy: 0.7760 - val_loss: 1.2251\n",
      "Epoch 65/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 478ms/step - accuracy: 0.9938 - loss: 0.0307 - val_accuracy: 0.7600 - val_loss: 1.4715\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    np.array(x_train), np.array(y_train), \n",
    "    validation_data=(x_val, y_val), \n",
    "    epochs=2000, \n",
    "    batch_size=32, \n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.7464 - loss: 1.0295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0880067348480225, 0.7440789341926575]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping 1d to 2d for 2d cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to add an extra dimension for the CNN\n",
    "x_train = np.expand_dims(x_train, axis=-1)  # Shape: (num_samples, 200, 6, 1)\n",
    "x_val = np.expand_dims(x_val, axis=-1)      # Shape: (num_samples, 200, 6, 1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)    # Shape: (num_samples, 200, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "x_train shape: (3000, 200, 4, 1)\n",
      "x_val shape: (1000, 200, 4, 1)\n",
      "x_test shape: (1520, 200, 4, 1)\n",
      "y_train shape: (3000, 1)\n",
      "y_val shape: (1000, 1)\n",
      "y_test shape: (1520, 1)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes (optional)\n",
    "print(\"Shapes:\")\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming x_train and x_test are numpy arrays with shape (num_samples, num_timesteps, num_subcarriers=90)\n",
    "\n",
    "# Compute variances across subcarriers for x_train and x_test\n",
    "variances_train = np.var(x_train, axis=(0, 1))  # Compute variance across all samples and time steps (shape: num_subcarriers)\n",
    "variances_test = np.var(x_test, axis=(0, 1))    # Compute variance across all samples and time steps (shape: num_subcarriers)\n",
    "\n",
    "# Get indices of top three subcarriers with the highest variances\n",
    "top_subcarrier_indices_train = np.argsort(variances_train)\n",
    "top_subcarrier_indices_test = np.argsort(variances_test)\n",
    "\n",
    "top_subcarrier_indices_train, top_subcarrier_indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.33113705e+01, 2.11406693e+02, 3.37323854e+02, 2.51454746e-01,\n",
       "       9.25452902e-01, 1.07058845e+00])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variances_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing model on 3 sub carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3000, 1])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 200, 6, 1)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selecting top three variance subcarriers out of 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)[:, :,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 200, 6, 1)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 200, 6, 1)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = np.array(x_test)[:, :, [2,  0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 200, 6, 1)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_val = np.array(x_val)[:, :, [2,  0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 200, 6, 1), (1520, 200, 6, 1), (1000, 200, 6, 1))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train), np.shape(x_test), np.shape(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Student Teacher with attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Reshape,  GlobalAveragePooling1D, Masking, Input, MaxPooling1D, GlobalMaxPooling1D, Add, Dropout, BatchNormalization, UpSampling1D, Lambda, Conv2D,Concatenate,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import models, layers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signfi 2D CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │            \u001b[38;5;34m30\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │            \u001b[38;5;34m12\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │            \u001b[38;5;34m84\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │            \u001b[38;5;34m12\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m0\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m0\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)            │           \u001b[38;5;34m276\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">414</span> (1.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m414\u001b[0m (1.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">402</span> (1.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m402\u001b[0m (1.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> (48.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12\u001b[0m (48.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def create_signfi_model():\n",
    "#     model = models.Sequential()\n",
    "    \n",
    "#     # Input layer\n",
    "#     model.add(layers.Input(shape=(200, 6, 1)))\n",
    "    \n",
    "#     # Convolutional Layer 1\n",
    "#     model.add(layers.Conv2D(3, (3, 3), padding='same', activation='relu'))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.AveragePooling2D(pool_size=(3, 3)))\n",
    "\n",
    "#     # Convolutional Layer 2\n",
    "#     model.add(layers.Conv2D(3, (3, 3), padding='same', activation='relu'))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.AveragePooling2D(pool_size=(3, 3)))\n",
    "    \n",
    "#     # Dropout Layer\n",
    "#     model.add(layers.Dropout(0.6))\n",
    "    \n",
    "#     # Flatten the output to feed into fully connected layer\n",
    "#     model.add(layers.Flatten())\n",
    "\n",
    "#     # Fully Connected Layer\n",
    "#     model.add(layers.Dense(276, activation='softmax'))  # Output layer with softmax activation for 276 classes\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Create the model\n",
    "# signfi_model = create_signfi_model()\n",
    "\n",
    "# # Compile the model\n",
    "# signfi_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary\n",
    "# signfi_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 200, 4, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 200, 4, 3)         30        \n",
      "                                                                 \n",
      " batch_normalization_68 (Ba  (None, 200, 4, 3)         12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " average_pooling2d_4 (Avera  (None, 100, 2, 3)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_69 (Conv2D)          (None, 100, 2, 3)         84        \n",
      "                                                                 \n",
      " batch_normalization_69 (Ba  (None, 100, 2, 3)         12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " average_pooling2d_5 (Avera  (None, 50, 1, 3)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50, 1, 3)          0         \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 150)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 276)               41676     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41814 (163.34 KB)\n",
      "Trainable params: 41802 (163.29 KB)\n",
      "Non-trainable params: 12 (48.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(200, 4, 1))\n",
    "    \n",
    "# Convolutional Layer 1\n",
    "x = layers.Conv2D(3, (3, 3), padding='same', activation='relu')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "# Convolutional Layer 2\n",
    "x = layers.Conv2D(3, (3, 3), padding='same', activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x_b_T = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "# Dropout Layer\n",
    "x = layers.Dropout(0.6)(x_b_T)\n",
    "    \n",
    "# Flatten the output to feed into fully connected layer\n",
    "x = layers.Flatten()(x)\n",
    "    \n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(276, activation='softmax')(x)  # Output layer with softmax activation for 276 classes\n",
    "    \n",
    "# Create the model\n",
    "signfi_model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "signfi_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "signfi_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_53\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_53\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │ conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │ conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │ conv2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │ conv2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │ conv2d_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │ conv2d_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │ conv2d_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,832</span> │ flatten_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,108</span> │ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m30\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m30\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m30\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m30\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m30\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m30\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m30\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m12\u001b[0m │ conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m12\u001b[0m │ conv2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m12\u001b[0m │ conv2d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m12\u001b[0m │ conv2d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m12\u001b[0m │ conv2d_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m12\u001b[0m │ conv2d_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │         \u001b[38;5;34m12\u001b[0m │ conv2d_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "│                     │                   │            │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m4,832\u001b[0m │ flatten_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │      \u001b[38;5;34m9,108\u001b[0m │ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,234</span> (55.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,234\u001b[0m (55.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,192</span> (55.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,192\u001b[0m (55.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> (168.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m42\u001b[0m (168.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "def create_St_model():\n",
    "    inputs = Input(shape=(200, 6, 1))\n",
    "    \n",
    "    # First Convolutional Layer repeated thrice\n",
    "    branch1 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "    branch1 = layers.BatchNormalization()(branch1)\n",
    "    \n",
    "    branch2 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "    branch2 = layers.BatchNormalization()(branch2)\n",
    "    \n",
    "    branch3 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "    branch3 = layers.BatchNormalization()(branch3)\n",
    "\n",
    "    branch4 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "    branch4 = layers.BatchNormalization()(branch4)\n",
    "    \n",
    "    branch5 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "    branch5 = layers.BatchNormalization()(branch5)\n",
    "\n",
    "    branch6 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "    branch6 = layers.BatchNormalization()(branch6)\n",
    "    \n",
    "    branch7 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "    branch7 = layers.BatchNormalization()(branch7)\n",
    "    \n",
    "    # Adding the outputs of the three branches\n",
    "    x = layers.Add()([branch1, branch2, branch3, branch4, branch5, branch6, branch7])\n",
    "    \n",
    "    # Dropout Layer\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Flatten the output\n",
    "    cat = layers.Flatten()(x)\n",
    "    cat = layers.Dense(32, activation='relu')(cat)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    outputs = layers.Dense(276, activation='softmax')(cat)  # Output layer with softmax activation for 276 classes\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "st_model = create_St_model()\n",
    "\n",
    "# Compile the model\n",
    "st_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "st_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 5.4656 - accuracy: 0.0120 - val_loss: 5.4343 - val_accuracy: 0.0240\n",
      "Epoch 2/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 4.9003 - accuracy: 0.0587 - val_loss: 5.1734 - val_accuracy: 0.0670\n",
      "Epoch 3/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 4.3737 - accuracy: 0.1243 - val_loss: 4.7337 - val_accuracy: 0.3040\n",
      "Epoch 4/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 3.7911 - accuracy: 0.2497 - val_loss: 3.9630 - val_accuracy: 0.4870\n",
      "Epoch 5/2000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 3.2472 - accuracy: 0.3560 - val_loss: 3.2185 - val_accuracy: 0.5670\n",
      "Epoch 6/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 2.7887 - accuracy: 0.4597 - val_loss: 2.6671 - val_accuracy: 0.6070\n",
      "Epoch 7/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 2.3845 - accuracy: 0.5457 - val_loss: 2.1121 - val_accuracy: 0.6850\n",
      "Epoch 8/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 2.0669 - accuracy: 0.6183 - val_loss: 1.8686 - val_accuracy: 0.7760\n",
      "Epoch 9/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 1.8031 - accuracy: 0.6723 - val_loss: 1.5652 - val_accuracy: 0.7330\n",
      "Epoch 10/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 1.5844 - accuracy: 0.7077 - val_loss: 1.4372 - val_accuracy: 0.8420\n",
      "Epoch 11/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 1.3765 - accuracy: 0.7490 - val_loss: 1.1577 - val_accuracy: 0.9110\n",
      "Epoch 12/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 1.2166 - accuracy: 0.7807 - val_loss: 0.9721 - val_accuracy: 0.9360\n",
      "Epoch 13/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 1.0484 - accuracy: 0.8113 - val_loss: 0.9415 - val_accuracy: 0.9460\n",
      "Epoch 14/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.9126 - accuracy: 0.8303 - val_loss: 0.7247 - val_accuracy: 0.9690\n",
      "Epoch 15/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.8061 - accuracy: 0.8587 - val_loss: 0.6213 - val_accuracy: 0.9770\n",
      "Epoch 16/2000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.7171 - accuracy: 0.8790 - val_loss: 0.4939 - val_accuracy: 0.9770\n",
      "Epoch 17/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.6252 - accuracy: 0.8900 - val_loss: 0.5865 - val_accuracy: 0.9650\n",
      "Epoch 18/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.5724 - accuracy: 0.9050 - val_loss: 0.3571 - val_accuracy: 0.9830\n",
      "Epoch 19/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.5363 - accuracy: 0.9003 - val_loss: 0.3353 - val_accuracy: 0.9900\n",
      "Epoch 20/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.4599 - accuracy: 0.9207 - val_loss: 0.3397 - val_accuracy: 0.9820\n",
      "Epoch 21/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.4299 - accuracy: 0.9233 - val_loss: 0.2389 - val_accuracy: 0.9940\n",
      "Epoch 22/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.4134 - accuracy: 0.9250 - val_loss: 0.1930 - val_accuracy: 0.9940\n",
      "Epoch 23/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.3879 - accuracy: 0.9317 - val_loss: 0.1683 - val_accuracy: 0.9960\n",
      "Epoch 24/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.3601 - accuracy: 0.9223 - val_loss: 0.1810 - val_accuracy: 0.9950\n",
      "Epoch 25/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.3385 - accuracy: 0.9363 - val_loss: 0.1896 - val_accuracy: 0.9930\n",
      "Epoch 26/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.2792 - accuracy: 0.9473 - val_loss: 0.1207 - val_accuracy: 0.9980\n",
      "Epoch 27/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2633 - accuracy: 0.9503 - val_loss: 0.1279 - val_accuracy: 0.9980\n",
      "Epoch 28/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2460 - accuracy: 0.9507 - val_loss: 0.1089 - val_accuracy: 0.9980\n",
      "Epoch 29/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2305 - accuracy: 0.9550 - val_loss: 0.1105 - val_accuracy: 0.9970\n",
      "Epoch 30/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2324 - accuracy: 0.9500 - val_loss: 0.0808 - val_accuracy: 0.9980\n",
      "Epoch 31/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2261 - accuracy: 0.9577 - val_loss: 0.0859 - val_accuracy: 0.9990\n",
      "Epoch 32/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.1849 - accuracy: 0.9653 - val_loss: 0.1113 - val_accuracy: 0.9890\n",
      "Epoch 33/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.1774 - accuracy: 0.9640 - val_loss: 0.0606 - val_accuracy: 0.9990\n",
      "Epoch 34/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1755 - accuracy: 0.9613 - val_loss: 0.0596 - val_accuracy: 0.9990\n",
      "Epoch 35/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1670 - accuracy: 0.9717 - val_loss: 0.0566 - val_accuracy: 0.9990\n",
      "Epoch 36/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1599 - accuracy: 0.9650 - val_loss: 0.0391 - val_accuracy: 0.9980\n",
      "Epoch 37/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1618 - accuracy: 0.9663 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 38/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.1459 - accuracy: 0.9690 - val_loss: 0.0339 - val_accuracy: 0.9990\n",
      "Epoch 39/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.1531 - accuracy: 0.9643 - val_loss: 0.0409 - val_accuracy: 0.9990\n",
      "Epoch 40/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1334 - accuracy: 0.9673 - val_loss: 0.0321 - val_accuracy: 0.9990\n",
      "Epoch 41/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.1159 - accuracy: 0.9757 - val_loss: 0.0301 - val_accuracy: 0.9990\n",
      "Epoch 42/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1330 - accuracy: 0.9717 - val_loss: 0.0293 - val_accuracy: 0.9990\n",
      "Epoch 43/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1220 - accuracy: 0.9733 - val_loss: 0.0243 - val_accuracy: 0.9990\n",
      "Epoch 44/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.1189 - accuracy: 0.9770 - val_loss: 0.0220 - val_accuracy: 0.9990\n",
      "Epoch 45/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1263 - accuracy: 0.9667 - val_loss: 0.0610 - val_accuracy: 0.9950\n",
      "Epoch 46/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1047 - accuracy: 0.9753 - val_loss: 0.0192 - val_accuracy: 0.9990\n",
      "Epoch 47/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.1058 - accuracy: 0.9777 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 48/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0977 - accuracy: 0.9803 - val_loss: 0.0215 - val_accuracy: 0.9990\n",
      "Epoch 49/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0814 - accuracy: 0.9850 - val_loss: 0.0197 - val_accuracy: 0.9990\n",
      "Epoch 50/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0918 - accuracy: 0.9837 - val_loss: 0.0335 - val_accuracy: 0.9970\n",
      "Epoch 51/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0942 - accuracy: 0.9770 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
      "Epoch 52/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0859 - accuracy: 0.9803 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
      "Epoch 53/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0835 - accuracy: 0.9827 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
      "Epoch 54/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0777 - accuracy: 0.9817 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
      "Epoch 55/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0875 - accuracy: 0.9813 - val_loss: 0.0808 - val_accuracy: 0.9900\n",
      "Epoch 56/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0727 - accuracy: 0.9823 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
      "Epoch 57/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0773 - accuracy: 0.9843 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 58/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0726 - accuracy: 0.9857 - val_loss: 0.0205 - val_accuracy: 0.9990\n",
      "Epoch 59/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0687 - accuracy: 0.9847 - val_loss: 0.0128 - val_accuracy: 0.9990\n",
      "Epoch 60/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0679 - accuracy: 0.9823 - val_loss: 0.0145 - val_accuracy: 0.9990\n",
      "Epoch 61/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0757 - accuracy: 0.9810 - val_loss: 0.0099 - val_accuracy: 0.9990\n",
      "Epoch 62/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0688 - accuracy: 0.9833 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
      "Epoch 63/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0562 - accuracy: 0.9870 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 64/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0704 - accuracy: 0.9810 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 65/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0779 - accuracy: 0.9793 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 66/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0641 - accuracy: 0.9840 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 67/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0612 - accuracy: 0.9847 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 68/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0517 - accuracy: 0.9877 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 69/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0682 - accuracy: 0.9827 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 70/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0566 - accuracy: 0.9867 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 71/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0559 - accuracy: 0.9843 - val_loss: 0.0084 - val_accuracy: 0.9990\n",
      "Epoch 72/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0529 - accuracy: 0.9857 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 73/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0652 - accuracy: 0.9853 - val_loss: 0.0076 - val_accuracy: 0.9990\n",
      "Epoch 74/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0559 - accuracy: 0.9840 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 75/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0585 - accuracy: 0.9843 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 76/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0575 - accuracy: 0.9860 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
      "Epoch 77/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0495 - accuracy: 0.9867 - val_loss: 0.0355 - val_accuracy: 0.9970\n",
      "Epoch 78/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 79/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0585 - accuracy: 0.9833 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 80/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0590 - accuracy: 0.9853 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 81/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0577 - accuracy: 0.9857 - val_loss: 0.0105 - val_accuracy: 0.9990\n",
      "Epoch 82/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0541 - accuracy: 0.9853 - val_loss: 0.0088 - val_accuracy: 0.9980\n",
      "Epoch 83/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0482 - accuracy: 0.9880 - val_loss: 0.0129 - val_accuracy: 0.9990\n",
      "Epoch 84/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0578 - accuracy: 0.9830 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 85/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0413 - accuracy: 0.9907 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 86/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0559 - accuracy: 0.9847 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 87/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0481 - accuracy: 0.9863 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 88/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0494 - accuracy: 0.9843 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 89/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 90/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0493 - accuracy: 0.9870 - val_loss: 0.0102 - val_accuracy: 0.9960\n",
      "Epoch 91/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0484 - accuracy: 0.9837 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
      "Epoch 92/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0464 - accuracy: 0.9883 - val_loss: 0.0166 - val_accuracy: 0.9990\n",
      "Epoch 93/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0497 - accuracy: 0.9847 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 94/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0454 - accuracy: 0.9877 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 95/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0379 - accuracy: 0.9887 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 96/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0391 - accuracy: 0.9887 - val_loss: 0.0216 - val_accuracy: 0.9930\n",
      "Epoch 97/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0413 - accuracy: 0.9890 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 98/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0384 - accuracy: 0.9880 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 99/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0490 - accuracy: 0.9860 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 100/2000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.0513 - accuracy: 0.9887 - val_loss: 0.0043 - val_accuracy: 0.9990\n",
      "Epoch 101/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 0.0161 - val_accuracy: 0.9960\n",
      "Epoch 102/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0526 - accuracy: 0.9870 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 103/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0509 - accuracy: 0.9840 - val_loss: 0.0114 - val_accuracy: 0.9980\n",
      "Epoch 104/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0470 - accuracy: 0.9860 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 105/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0445 - accuracy: 0.9847 - val_loss: 0.0068 - val_accuracy: 0.9990\n",
      "Epoch 106/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0473 - accuracy: 0.9837 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 107/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0362 - accuracy: 0.9910 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 108/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0471 - accuracy: 0.9873 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 109/2000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.9853 - val_loss: 0.0066 - val_accuracy: 0.9990\n",
      "Epoch 110/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0432 - accuracy: 0.9857 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 111/2000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 0.9870 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 112/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0528 - accuracy: 0.9843 - val_loss: 0.0043 - val_accuracy: 0.9990\n",
      "Epoch 113/2000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0420 - accuracy: 0.9877 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 114/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0475 - accuracy: 0.9857 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 115/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0365 - accuracy: 0.9880 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 116/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0399 - accuracy: 0.9900 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 117/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0428 - accuracy: 0.9860 - val_loss: 0.0032 - val_accuracy: 0.9990\n",
      "Epoch 118/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0398 - accuracy: 0.9870 - val_loss: 0.0038 - val_accuracy: 0.9990\n",
      "Epoch 119/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0465 - accuracy: 0.9877 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 120/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0368 - accuracy: 0.9883 - val_loss: 0.0088 - val_accuracy: 0.9990\n",
      "Epoch 121/2000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.9880 - val_loss: 0.0045 - val_accuracy: 0.9980\n",
      "Epoch 122/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0472 - accuracy: 0.9840 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 123/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.0066 - val_accuracy: 0.9990\n",
      "Epoch 124/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0436 - accuracy: 0.9850 - val_loss: 0.0057 - val_accuracy: 0.9990\n",
      "Epoch 125/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0271 - accuracy: 0.9933 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 126/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0350 - accuracy: 0.9890 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 127/2000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0376 - accuracy: 0.9883 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 128/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0444 - accuracy: 0.9890 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 129/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0437 - accuracy: 0.9857 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 130/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0330 - accuracy: 0.9917 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 131/2000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0339 - accuracy: 0.9890 - val_loss: 0.0195 - val_accuracy: 0.9940\n",
      "Epoch 132/2000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0389 - accuracy: 0.9907 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 133/2000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 134/2000\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0370 - accuracy: 0.9863 - val_loss: 0.0019 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = signfi_model.fit(\n",
    "    np.array(x_train), np.array(y_train), \n",
    "    validation_data=(x_val, y_val), \n",
    "    epochs=2000, \n",
    "    batch_size=32, \n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.001718524843454361, 1.0]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signfi_model.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"teacher\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"teacher\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,376</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9728</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,918,700</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,076</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_38 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m1,376\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_29 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_39 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m10,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_30 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_40 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_41 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │        \u001b[38;5;34m24,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_31 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_42 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │        \u001b[38;5;34m49,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9728\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │     \u001b[38;5;34m2,918,700\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │        \u001b[38;5;34m90,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)            │        \u001b[38;5;34m83,076\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,227,820</span> (12.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,227,820\u001b[0m (12.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,227,628</span> (12.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,227,628\u001b[0m (12.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Main Model \n",
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "# avg_T = GlobalAveragePooling1D()(x_T)\n",
    "# max_T = GlobalMaxPooling1D()(x_T)\n",
    "# flat_T = Concatenate()([avg_T, max_T])\n",
    "flat_T = Flatten()(x_T)\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "d_T = Dense(300, activation = 'relu')(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid')(d_T)\n",
    "\n",
    "model_sub3_1 = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model_sub3_1.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "model_sub3_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_25k\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_25k\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">792</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,376</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,108</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │           \u001b[38;5;34m152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m8\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m792\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m25,376\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)            │         \u001b[38;5;34m9,108\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,636</span> (135.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,636\u001b[0m (135.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,636</span> (135.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,636\u001b[0m (135.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input layer 34k params\n",
    "# dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# # Model with very few parameters\n",
    "# x_T = Conv1D(kernel_size=3, filters=8, activation='relu')(dataInp)  # 198, 8\n",
    "# x_T = MaxPooling1D(2)(x_T)  # 99, 8\n",
    "\n",
    "# x_T = Flatten()(x_T)\n",
    "# d_T = Dense(32, activation='relu')(x_T)  # 32 units\n",
    "# d_T = Dropout(0.3)(d_T)\n",
    "# out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "# # Create and compile the model\n",
    "# model_25k = Model(inputs=dataInp, outputs=out_T, name='model_25k')\n",
    "# model_25k.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=1e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "# # Summary of the model\n",
    "# model_25k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_50k\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"model_50k\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,850</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,076</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │           \u001b[38;5;34m152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m8\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │           \u001b[38;5;34m400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m1,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m736\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m36,850\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)            │        \u001b[38;5;34m14,076\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,046</span> (207.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,046\u001b[0m (207.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,046</span> (207.21 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m53,046\u001b[0m (207.21 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Input layer\n",
    "# dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# # Model with around 50k parameters\n",
    "# x_T = Conv1D(kernel_size=3, filters=8, activation='relu')(dataInp)  # 198, 8\n",
    "# x_T = MaxPooling1D(2)(x_T)  # 99, 8\n",
    "\n",
    "# x_T = Conv1D(kernel_size=3, filters=16, activation='relu')(x_T)  # 97, 16\n",
    "# x_T = MaxPooling1D(2)(x_T)  # 48, 16\n",
    "\n",
    "# x_T = Conv1D(kernel_size=3, filters=32, activation='relu')(x_T)  # 46, 32\n",
    "# x_T = MaxPooling1D(2)(x_T)  # 23, 32\n",
    "\n",
    "# x_T = Flatten()(x_T)\n",
    "# d_T = Dense(50, activation='relu')(x_T)  # Reduced to 50 units\n",
    "# d_T = Dropout(0.3)(d_T)\n",
    "# out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "# # Create and compile the model\n",
    "# model_50k = Model(inputs=dataInp, outputs=out_T, name='model_50k')\n",
    "# model_50k.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=1e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "# # Summary of the model\n",
    "# model_50k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"teacher_reduced_params_further\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"teacher_reduced_params_further\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">430</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,020</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,440</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1520</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">106,470</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,970</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,596</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_24 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │           \u001b[38;5;34m430\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │            \u001b[38;5;34m40\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_22 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m10\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_25 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │         \u001b[38;5;34m1,020\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_23 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │            \u001b[38;5;34m80\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_26 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │         \u001b[38;5;34m2,440\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_27 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m10\u001b[0m)         │         \u001b[38;5;34m1,210\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_24 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m10\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_28 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m80\u001b[0m)         │         \u001b[38;5;34m2,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1520\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │       \u001b[38;5;34m106,470\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │         \u001b[38;5;34m4,970\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)            │        \u001b[38;5;34m19,596\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,736</span> (541.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,736\u001b[0m (541.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,676</span> (541.70 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m138,676\u001b[0m (541.70 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> (240.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m60\u001b[0m (240.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #Input layer # 138k\n",
    "# dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# # TEACHER MODEL with further reduced parameters\n",
    "# x_T = Conv1D(kernel_size=7, filters=10, activation='relu')(dataInp)  # Reduced filters to 10\n",
    "# x_T = BatchNormalization()(x_T)\n",
    "# x_T = MaxPooling1D(2)(x_T)  # 97, 10\n",
    "# x_T = Conv1D(kernel_size=5, filters=20, activation='relu')(x_T)  # Reduced filters to 20\n",
    "# x_T = MaxPooling1D(2)(x_T)  # 46, 20\n",
    "# x_T = BatchNormalization()(x_T)\n",
    "# x_T = Conv1D(kernel_size=3, filters=40, activation='relu')(x_T)  # Reduced filters to 40\n",
    "# x_b_T = Conv1D(kernel_size=3, filters=10, activation='relu')(x_T)  # Reduced filters to 10\n",
    "# x_T = MaxPooling1D(2)(x_b_T)  # 21, 10\n",
    "# x_T = Conv1D(kernel_size=3, filters=80, activation='relu')(x_T)  # Reduced filters to 80\n",
    "# flat_T = Flatten()(x_T)\n",
    "# d_T = Dense(70, activation='relu')(flat_T)  # Reduced dense layer size to 70\n",
    "# d_T = Dropout(0.3)(d_T)\n",
    "# d_T = Dense(70, activation='relu')(d_T)  # Reduced dense layer size to 70\n",
    "# out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "# # Create and compile the model\n",
    "# model_100k = Model(inputs=dataInp, outputs=out_T, name='teacher_reduced_params_further')\n",
    "# model_100k.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# # Summary of the model\n",
    "# model_100k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"teacher_increased_params_refined\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"teacher_increased_params_refined\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,504</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,740</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,552</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1824</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">146,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,356</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_34 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m12\u001b[0m)        │           \u001b[38;5;34m516\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m12\u001b[0m)        │            \u001b[38;5;34m48\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_28 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m12\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_35 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m24\u001b[0m)         │         \u001b[38;5;34m1,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_29 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m24\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m24\u001b[0m)         │            \u001b[38;5;34m96\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_36 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m48\u001b[0m)         │         \u001b[38;5;34m3,504\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_37 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m12\u001b[0m)         │         \u001b[38;5;34m1,740\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_30 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m12\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_38 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m96\u001b[0m)         │         \u001b[38;5;34m3,552\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1824\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │       \u001b[38;5;34m146,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │         \u001b[38;5;34m6,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)            │        \u001b[38;5;34m22,356\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,756</span> (725.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m185,756\u001b[0m (725.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,684</span> (725.33 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m185,684\u001b[0m (725.33 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> (288.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m72\u001b[0m (288.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Input layer\n",
    "# dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# # TEACHER MODEL with increased parameters\n",
    "# x_T = Conv1D(kernel_size=7, filters=12, activation='relu')(dataInp)  # Slightly increased filters to 12\n",
    "# x_T = BatchNormalization()(x_T)\n",
    "# x_T = MaxPooling1D(2)(x_T)  # 97, 12\n",
    "# x_T = Conv1D(kernel_size=5, filters=24, activation='relu')(x_T)  # Slightly increased filters to 24\n",
    "# x_T = MaxPooling1D(2)(x_T)  # 46, 24\n",
    "# x_T = BatchNormalization()(x_T)\n",
    "# x_T = Conv1D(kernel_size=3, filters=48, activation='relu')(x_T)  # Slightly increased filters to 48\n",
    "# x_b_T = Conv1D(kernel_size=3, filters=12, activation='relu')(x_T)  # Increased filters to 12\n",
    "# x_T = MaxPooling1D(2)(x_b_T)  # 21, 12\n",
    "# x_T = Conv1D(kernel_size=3, filters=96, activation='relu')(x_T)  # Slightly increased filters to 96\n",
    "# flat_T = Flatten()(x_T)\n",
    "# d_T = Dense(80, activation='relu')(flat_T)  # Increased dense layer size to 80\n",
    "# d_T = Dropout(0.3)(d_T)\n",
    "# d_T = Dense(80, activation='relu')(d_T)  # Increased dense layer size to 80\n",
    "# out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "# # Create and compile the model\n",
    "# model_200k = Model(inputs=dataInp, outputs=out_T, name='teacher_increased_params_refined')\n",
    "# model_200k.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# # Summary of the model\n",
    "# model_200k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"teacher_increased_params\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"teacher_increased_params\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,680</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,820</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3040</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">304,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,876</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_28 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │           \u001b[38;5;34m860\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │            \u001b[38;5;34m80\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_23 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_29 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │         \u001b[38;5;34m4,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_24 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │           \u001b[38;5;34m160\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_30 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m80\u001b[0m)         │         \u001b[38;5;34m9,680\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_31 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │         \u001b[38;5;34m4,820\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_25 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m20\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_32 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m160\u001b[0m)        │         \u001b[38;5;34m9,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3040\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m304,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m10,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)            │        \u001b[38;5;34m27,876\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">371,476</span> (1.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m371,476\u001b[0m (1.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">371,356</span> (1.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m371,356\u001b[0m (1.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input layer\n",
    "dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# TEACHER MODEL with increased parameters\n",
    "x_T = Conv1D(kernel_size=7, filters=20, activation='relu')(dataInp)  # Increased filters to 20\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T)  # 97, 20\n",
    "x_T = Conv1D(kernel_size=5, filters=40, activation='relu')(x_T)  # Increased filters to 40\n",
    "x_T = MaxPooling1D(2)(x_T)  # 46, 40\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Conv1D(kernel_size=3, filters=80, activation='relu')(x_T)  # Increased filters to 80\n",
    "x_b_T = Conv1D(kernel_size=3, filters=20, activation='relu')(x_T)  # Increased filters to 20\n",
    "x_T = MaxPooling1D(2)(x_b_T)  # 21, 20\n",
    "x_T = Conv1D(kernel_size=3, filters=160, activation='relu')(x_T)  # Increased filters to 160\n",
    "flat_T = Flatten()(x_T)\n",
    "d_T = Dense(100, activation='relu')(flat_T)  # Increased dense layer size to 100\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "d_T = Dense(100, activation='relu')(d_T)  # Increased dense layer size to 100\n",
    "out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "# Create and compile the model\n",
    "model_350k = Model(inputs=dataInp, outputs=out_T, name='teacher_increased_params')\n",
    "model_350k.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Summary of the model\n",
    "model_350k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9661 - loss: 0.1260 - val_accuracy: 0.9000 - val_loss: 0.3966\n",
      "Epoch 2/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9719 - loss: 0.1093 - val_accuracy: 0.8630 - val_loss: 0.5239\n",
      "Epoch 3/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9702 - loss: 0.1071 - val_accuracy: 0.9390 - val_loss: 0.2848\n",
      "Epoch 4/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.9877 - loss: 0.0452 - val_accuracy: 0.9420 - val_loss: 0.2608\n",
      "Epoch 5/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9802 - loss: 0.0826 - val_accuracy: 0.9200 - val_loss: 0.3532\n",
      "Epoch 6/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9820 - loss: 0.0537 - val_accuracy: 0.9360 - val_loss: 0.3314\n",
      "Epoch 7/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9696 - loss: 0.1363 - val_accuracy: 0.9090 - val_loss: 0.3684\n",
      "Epoch 8/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9777 - loss: 0.1013 - val_accuracy: 0.9390 - val_loss: 0.2834\n",
      "Epoch 9/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.9885 - loss: 0.0462 - val_accuracy: 0.9240 - val_loss: 0.3345\n",
      "Epoch 10/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step - accuracy: 0.9739 - loss: 0.0873 - val_accuracy: 0.8080 - val_loss: 0.8423\n",
      "Epoch 11/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9695 - loss: 0.1181 - val_accuracy: 0.9370 - val_loss: 0.2498\n",
      "Epoch 12/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9813 - loss: 0.0628 - val_accuracy: 0.9480 - val_loss: 0.2101\n",
      "Epoch 13/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.9861 - loss: 0.0449 - val_accuracy: 0.9400 - val_loss: 0.2397\n",
      "Epoch 14/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9839 - loss: 0.0586 - val_accuracy: 0.7700 - val_loss: 2.3275\n",
      "Epoch 15/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.9812 - loss: 0.0677 - val_accuracy: 0.9150 - val_loss: 0.4184\n",
      "Epoch 16/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9738 - loss: 0.0987 - val_accuracy: 0.9020 - val_loss: 0.3931\n",
      "Epoch 17/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.9856 - loss: 0.0501 - val_accuracy: 0.8420 - val_loss: 0.7773\n",
      "Epoch 18/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - accuracy: 0.9786 - loss: 0.0974 - val_accuracy: 0.9050 - val_loss: 0.3511\n",
      "Epoch 19/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9833 - loss: 0.0462 - val_accuracy: 0.9240 - val_loss: 0.2894\n",
      "Epoch 20/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9839 - loss: 0.0793 - val_accuracy: 0.9460 - val_loss: 0.2149\n",
      "Epoch 21/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9838 - loss: 0.0508 - val_accuracy: 0.9440 - val_loss: 0.2367\n",
      "Epoch 22/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9826 - loss: 0.0600 - val_accuracy: 0.8710 - val_loss: 0.4873\n",
      "Epoch 23/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9801 - loss: 0.0766 - val_accuracy: 0.9040 - val_loss: 0.3961\n",
      "Epoch 24/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9839 - loss: 0.0507 - val_accuracy: 0.9040 - val_loss: 0.4160\n",
      "Epoch 25/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9715 - loss: 0.1085 - val_accuracy: 0.9220 - val_loss: 0.4064\n",
      "Epoch 26/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.9725 - loss: 0.1451 - val_accuracy: 0.9090 - val_loss: 0.3206\n",
      "Epoch 27/2000\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.9750 - loss: 0.1046 - val_accuracy: 0.9420 - val_loss: 0.2148\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_sub3_1.fit(np.array(x_train), np.array(y_train), validation_data = (x_val, y_val), epochs = 2000, batch_size = 32,\n",
    "callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9362 - loss: 0.2739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.245758518576622, 0.9467105269432068]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sub3_1.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375692\n"
     ]
    }
   ],
   "source": [
    "# print(model_sub3_1.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "\u001b[1m 1/94\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:55\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 5.6170"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 16:59:20.006858: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at sparse_xent_op.cc:103 : INVALID_ARGUMENT: Received a label value of 65535 which is outside the valid range of [0, 276).  Label values: 216 72 147 126 234 77 248 6 63 203 214 183 191 119 18 129 53 207 49 78 217 210 65535 109 230 117 152 24 51 272 10 135\n",
      "2024-07-20 16:59:20.006938: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Received a label value of 65535 which is outside the valid range of [0, 276).  Label values: 216 72 147 126 234 77 248 6 63 203 214 183 191 119 18 129 53 207 49 78 217 210 65535 109 230 117 152 24 51 272 10 135\n",
      "\t [[{{function_node __inference_one_step_on_data_348658}}{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelapp.py\", line 728, in start\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1919, in _run_once\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/var/folders/qh/0gr3y4rs1w58xpknzybvhdvm0000gn/T/ipykernel_4743/1716946166.py\", line 2, in <module>\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 314, in fit\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 117, in one_step_on_iterator\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 104, in one_step_on_data\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/trainer.py\", line 316, in compute_loss\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/compile_utils.py\", line 609, in __call__\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/compile_utils.py\", line 645, in call\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/losses/loss.py\", line 43, in __call__\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/losses/losses.py\", line 22, in call\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/losses/losses.py\", line 1722, in sparse_categorical_crossentropy\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/ops/nn.py\", line 1567, in sparse_categorical_crossentropy\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py\", line 638, in sparse_categorical_crossentropy\n\nReceived a label value of 65535 which is outside the valid range of [0, 276).  Label values: 216 72 147 126 234 77 248 6 63 203 214 183 191 119 18 129 53 207 49 78 217 210 65535 109 230 117 152 24 51 272 10 135\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_348805]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m callback \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model_sub3_1\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(x_train), np\u001b[39m.\u001b[39;49marray(y_train), validation_data \u001b[39m=\u001b[39;49m (x_val, y_val), epochs \u001b[39m=\u001b[39;49m \u001b[39m2000\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m callbacks \u001b[39m=\u001b[39;49m callback)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelapp.py\", line 728, in start\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1919, in _run_once\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/Users/amitanand/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/var/folders/qh/0gr3y4rs1w58xpknzybvhdvm0000gn/T/ipykernel_4743/1716946166.py\", line 2, in <module>\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 314, in fit\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 117, in one_step_on_iterator\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 104, in one_step_on_data\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/trainer.py\", line 316, in compute_loss\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/compile_utils.py\", line 609, in __call__\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/compile_utils.py\", line 645, in call\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/losses/loss.py\", line 43, in __call__\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/losses/losses.py\", line 22, in call\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/losses/losses.py\", line 1722, in sparse_categorical_crossentropy\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/ops/nn.py\", line 1567, in sparse_categorical_crossentropy\n\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py\", line 638, in sparse_categorical_crossentropy\n\nReceived a label value of 65535 which is outside the valid range of [0, 276).  Label values: 216 72 147 126 234 77 248 6 63 203 214 183 191 119 18 129 53 207 49 78 217 210 65535 109 230 117 152 24 51 272 10 135\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_348805]"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_sub3_1.fit(np.array(x_train), np.array(y_train), validation_data = (x_val, y_val), epochs = 2000, batch_size = 32,\n",
    "callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16856060922145844, 0.9552631378173828]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sub3_1.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Probablity ouptputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Reshape,  GlobalAveragePooling1D, Masking, Input, MaxPooling1D, GlobalMaxPooling1D, Add, Dropout, BatchNormalization, UpSampling1D, Lambda, Conv2D,Concatenate,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import models, layers, regularizers, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "48/48 [==============================] - 0s 2ms/step\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "48/48 [==============================] - 0s 2ms/step\n",
      "Bottleneck Train Shape: (3000, 50, 1, 3)\n",
      "Bottleneck Val Shape: (1000, 50, 1, 3)\n",
      "Bottleneck Test Shape: (1520, 50, 1, 3)\n",
      "Final Output Train Shape: (3000, 276)\n",
      "Final Output Val Shape: (1000, 276)\n",
      "Final Output Test Shape: (1520, 276)\n"
     ]
    }
   ],
   "source": [
    "model_bottleneck = Model(inputs=signfi_model.input, outputs=x_b_T)\n",
    "\n",
    "# Create a model to extract the final output\n",
    "model_final_output = Model(inputs=signfi_model.input, outputs=outputs)\n",
    "\n",
    "# Assuming train_data, val_data, and test_data are your datasets\n",
    "# For demonstration, using random data with correct shape\n",
    "# train_data = np.random.rand(1000, 200, 6)\n",
    "# val_data = np.random.rand(200, 200, 6)\n",
    "# test_data = np.random.rand(200, 200, 6)\n",
    "\n",
    "# Predict the bottleneck outputs\n",
    "bottleneck_train = model_bottleneck.predict(np.array(x_train))\n",
    "bottleneck_val = model_bottleneck.predict(np.array(x_val))\n",
    "bottleneck_test = model_bottleneck.predict(np.array(x_test))\n",
    "\n",
    "# Predict the final outputs (probability scores)\n",
    "output_train = model_final_output.predict(np.array(x_train))\n",
    "output_val = model_final_output.predict(np.array(x_val))\n",
    "output_test = model_final_output.predict(np.array(x_test))\n",
    "\n",
    "# Print shapes of the outputs\n",
    "print(f\"Bottleneck Train Shape: {bottleneck_train.shape}\")\n",
    "print(f\"Bottleneck Val Shape: {bottleneck_val.shape}\")\n",
    "print(f\"Bottleneck Test Shape: {bottleneck_test.shape}\")\n",
    "print(f\"Final Output Train Shape: {output_train.shape}\")\n",
    "print(f\"Final Output Val Shape: {output_val.shape}\")\n",
    "print(f\"Final Output Test Shape: {output_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck Train Shape: (3000, 50, 1, 3)\n",
      "Bottleneck Val Shape: (1000, 50, 1, 3)\n",
      "Bottleneck Test Shape: (1520, 50, 1, 3)\n",
      "Final Output Train Shape: (3000, 276)\n",
      "Final Output Val Shape: (1000, 276)\n",
      "Final Output Test Shape: (1520, 276)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bottleneck Train Shape: {bottleneck_train.shape}\")\n",
    "print(f\"Bottleneck Val Shape: {bottleneck_val.shape}\")\n",
    "print(f\"Bottleneck Test Shape: {bottleneck_test.shape}\")\n",
    "print(f\"Final Output Train Shape: {output_train.shape}\")\n",
    "print(f\"Final Output Val Shape: {output_val.shape}\")\n",
    "print(f\"Final Output Test Shape: {output_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 276)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prob = np.concatenate((output_train, output_val), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 276)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_output = np.concatenate((bottleneck_train, bottleneck_val), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 42, 32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# st  model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(200, 6, 1))\n",
    "\n",
    "# First Convolutional Layer repeated five times\n",
    "branch1 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch1 = layers.BatchNormalization()(branch1)\n",
    "\n",
    "branch2 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch2 = layers.BatchNormalization()(branch2)\n",
    "\n",
    "branch3 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch3 = layers.BatchNormalization()(branch3)\n",
    "\n",
    "branch4 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch4 = layers.BatchNormalization()(branch4)\n",
    "\n",
    "branch5 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch5 = layers.BatchNormalization()(branch5)\n",
    "\n",
    "\n",
    "\n",
    "# Adding the outputs of the five branches\n",
    "x_b_S = layers.Add()([branch1, branch2, branch3, branch4, branch5])\n",
    "\n",
    "# Dropout Layer (commented out)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Flatten the output\n",
    "cat = layers.Flatten()(x_b_S)\n",
    "cat = layers.Dense(32, activation='relu')(cat)\n",
    "\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(276, activation='softmax', name = 'out_s')(cat)  # Output layer with softmax activation for 276 classes\n",
    "\n",
    "# Create the model\n",
    "st_model = models.Model(inputs=inputs, outputs=[x_b_S, outputs])\n",
    "\n",
    "# Compile the model\n",
    "st_model.compile(optimizer='adam', loss=['mean_squared_error', 'binary_crossentropy'], metrics= ['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "# st_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)       [(None, 200, 6, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)          (None, 50, 1, 3)             30        ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)          (None, 50, 1, 3)             30        ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)          (None, 50, 1, 3)             30        ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)          (None, 50, 1, 3)             30        ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)          (None, 50, 1, 3)             30        ['input_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_61 (Ba  (None, 50, 1, 3)             12        ['conv2d_61[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 50, 1, 3)             12        ['conv2d_62[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 50, 1, 3)             12        ['conv2d_63[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 50, 1, 3)             12        ['conv2d_64[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_65 (Ba  (None, 50, 1, 3)             12        ['conv2d_65[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 50, 1, 3)             0         ['batch_normalization_61[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_62[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_63[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_64[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_65[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)        (None, 150)                  0         ['add_13[0][0]']              \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 32)                   4832      ['flatten_14[0][0]']          \n",
      "                                                                                                  \n",
      " out_s (Dense)               (None, 276)                  9108      ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14150 (55.27 KB)\n",
      "Trainable params: 14120 (55.16 KB)\n",
      "Non-trainable params: 30 (120.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "st_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "47/47 [==============================] - 1s 6ms/step - loss: 8.5157 - add_13_loss: 8.1207 - out_s_loss: 0.3950 - add_13_accuracy: 0.5043 - out_s_accuracy: 0.0063 - val_loss: 76.1832 - val_add_13_loss: 76.1186 - val_out_s_loss: 0.0645 - val_add_13_accuracy: 0.5071 - val_out_s_accuracy: 0.0040\n",
      "Epoch 2/1000\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 5.6991 - add_13_loss: 5.6367 - out_s_loss: 0.0624 - add_13_accuracy: 0.5242 - out_s_accuracy: 0.0063 - val_loss: 15.7383 - val_add_13_loss: 15.6880 - val_out_s_loss: 0.0503 - val_add_13_accuracy: 0.5527 - val_out_s_accuracy: 0.0060\n",
      "Epoch 3/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.7520 - add_13_loss: 4.7044 - out_s_loss: 0.0476 - add_13_accuracy: 0.5601 - out_s_accuracy: 0.0073 - val_loss: 8.4331 - val_add_13_loss: 8.3614 - val_out_s_loss: 0.0718 - val_add_13_accuracy: 0.4994 - val_out_s_accuracy: 0.0080\n",
      "Epoch 4/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 4.4626 - add_13_loss: 4.4236 - out_s_loss: 0.0390 - add_13_accuracy: 0.6123 - out_s_accuracy: 0.0107 - val_loss: 6.5479 - val_add_13_loss: 6.4754 - val_out_s_loss: 0.0725 - val_add_13_accuracy: 0.5376 - val_out_s_accuracy: 0.0100\n",
      "Epoch 5/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 4.2756 - add_13_loss: 4.2424 - out_s_loss: 0.0332 - add_13_accuracy: 0.6336 - out_s_accuracy: 0.0123 - val_loss: 5.6579 - val_add_13_loss: 5.5911 - val_out_s_loss: 0.0668 - val_add_13_accuracy: 0.5474 - val_out_s_accuracy: 0.0090\n",
      "Epoch 6/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 4.1405 - add_13_loss: 4.1097 - out_s_loss: 0.0308 - add_13_accuracy: 0.6425 - out_s_accuracy: 0.0150 - val_loss: 5.1023 - val_add_13_loss: 5.0348 - val_out_s_loss: 0.0675 - val_add_13_accuracy: 0.5539 - val_out_s_accuracy: 0.0090\n",
      "Epoch 7/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.0308 - add_13_loss: 4.0019 - out_s_loss: 0.0290 - add_13_accuracy: 0.6501 - out_s_accuracy: 0.0170 - val_loss: 4.6944 - val_add_13_loss: 4.6251 - val_out_s_loss: 0.0693 - val_add_13_accuracy: 0.5651 - val_out_s_accuracy: 0.0070\n",
      "Epoch 8/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.9383 - add_13_loss: 3.9101 - out_s_loss: 0.0282 - add_13_accuracy: 0.6558 - out_s_accuracy: 0.0207 - val_loss: 4.3995 - val_add_13_loss: 4.3381 - val_out_s_loss: 0.0614 - val_add_13_accuracy: 0.5747 - val_out_s_accuracy: 0.0050\n",
      "Epoch 9/1000\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 3.8511 - add_13_loss: 3.8245 - out_s_loss: 0.0267 - add_13_accuracy: 0.6635 - out_s_accuracy: 0.0280 - val_loss: 4.3327 - val_add_13_loss: 4.2814 - val_out_s_loss: 0.0513 - val_add_13_accuracy: 0.5694 - val_out_s_accuracy: 0.0080\n",
      "Epoch 10/1000\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 3.7904 - add_13_loss: 3.7645 - out_s_loss: 0.0259 - add_13_accuracy: 0.6672 - out_s_accuracy: 0.0343 - val_loss: 4.2114 - val_add_13_loss: 4.1709 - val_out_s_loss: 0.0405 - val_add_13_accuracy: 0.5575 - val_out_s_accuracy: 0.0070\n",
      "Epoch 11/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.7459 - add_13_loss: 3.7209 - out_s_loss: 0.0250 - add_13_accuracy: 0.6707 - out_s_accuracy: 0.0367 - val_loss: 3.9838 - val_add_13_loss: 3.9531 - val_out_s_loss: 0.0307 - val_add_13_accuracy: 0.5707 - val_out_s_accuracy: 0.0120\n",
      "Epoch 12/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.7235 - add_13_loss: 3.6987 - out_s_loss: 0.0247 - add_13_accuracy: 0.6721 - out_s_accuracy: 0.0527 - val_loss: 3.9187 - val_add_13_loss: 3.8918 - val_out_s_loss: 0.0269 - val_add_13_accuracy: 0.5744 - val_out_s_accuracy: 0.0180\n",
      "Epoch 13/1000\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 3.6974 - add_13_loss: 3.6738 - out_s_loss: 0.0236 - add_13_accuracy: 0.6743 - out_s_accuracy: 0.0687 - val_loss: 3.7634 - val_add_13_loss: 3.7388 - val_out_s_loss: 0.0247 - val_add_13_accuracy: 0.6025 - val_out_s_accuracy: 0.0260\n",
      "Epoch 14/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6802 - add_13_loss: 3.6573 - out_s_loss: 0.0229 - add_13_accuracy: 0.6757 - out_s_accuracy: 0.0837 - val_loss: 3.7102 - val_add_13_loss: 3.6864 - val_out_s_loss: 0.0238 - val_add_13_accuracy: 0.6193 - val_out_s_accuracy: 0.0390\n",
      "Epoch 15/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6647 - add_13_loss: 3.6425 - out_s_loss: 0.0223 - add_13_accuracy: 0.6754 - out_s_accuracy: 0.0980 - val_loss: 3.6996 - val_add_13_loss: 3.6765 - val_out_s_loss: 0.0231 - val_add_13_accuracy: 0.6225 - val_out_s_accuracy: 0.0520\n",
      "Epoch 16/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6573 - add_13_loss: 3.6358 - out_s_loss: 0.0215 - add_13_accuracy: 0.6772 - out_s_accuracy: 0.1133 - val_loss: 3.6706 - val_add_13_loss: 3.6484 - val_out_s_loss: 0.0222 - val_add_13_accuracy: 0.6285 - val_out_s_accuracy: 0.0700\n",
      "Epoch 17/1000\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 3.6467 - add_13_loss: 3.6256 - out_s_loss: 0.0210 - add_13_accuracy: 0.6772 - out_s_accuracy: 0.1423 - val_loss: 3.6797 - val_add_13_loss: 3.6580 - val_out_s_loss: 0.0217 - val_add_13_accuracy: 0.6319 - val_out_s_accuracy: 0.0780\n",
      "Epoch 18/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6472 - add_13_loss: 3.6267 - out_s_loss: 0.0205 - add_13_accuracy: 0.6770 - out_s_accuracy: 0.1573 - val_loss: 3.6371 - val_add_13_loss: 3.6162 - val_out_s_loss: 0.0209 - val_add_13_accuracy: 0.6424 - val_out_s_accuracy: 0.0930\n",
      "Epoch 19/1000\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 3.6355 - add_13_loss: 3.6157 - out_s_loss: 0.0198 - add_13_accuracy: 0.6779 - out_s_accuracy: 0.1797 - val_loss: 3.5869 - val_add_13_loss: 3.5668 - val_out_s_loss: 0.0201 - val_add_13_accuracy: 0.6661 - val_out_s_accuracy: 0.1130\n",
      "Epoch 20/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6325 - add_13_loss: 3.6133 - out_s_loss: 0.0192 - add_13_accuracy: 0.6772 - out_s_accuracy: 0.2070 - val_loss: 3.5907 - val_add_13_loss: 3.5711 - val_out_s_loss: 0.0196 - val_add_13_accuracy: 0.6595 - val_out_s_accuracy: 0.1340\n",
      "Epoch 21/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6321 - add_13_loss: 3.6134 - out_s_loss: 0.0187 - add_13_accuracy: 0.6771 - out_s_accuracy: 0.2303 - val_loss: 3.5939 - val_add_13_loss: 3.5748 - val_out_s_loss: 0.0191 - val_add_13_accuracy: 0.6606 - val_out_s_accuracy: 0.1430\n",
      "Epoch 22/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6287 - add_13_loss: 3.6105 - out_s_loss: 0.0182 - add_13_accuracy: 0.6771 - out_s_accuracy: 0.2590 - val_loss: 3.5510 - val_add_13_loss: 3.5326 - val_out_s_loss: 0.0184 - val_add_13_accuracy: 0.6761 - val_out_s_accuracy: 0.1850\n",
      "Epoch 23/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6304 - add_13_loss: 3.6127 - out_s_loss: 0.0177 - add_13_accuracy: 0.6782 - out_s_accuracy: 0.2803 - val_loss: 3.5550 - val_add_13_loss: 3.5370 - val_out_s_loss: 0.0180 - val_add_13_accuracy: 0.6751 - val_out_s_accuracy: 0.2100\n",
      "Epoch 24/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6252 - add_13_loss: 3.6080 - out_s_loss: 0.0172 - add_13_accuracy: 0.6782 - out_s_accuracy: 0.3070 - val_loss: 3.5462 - val_add_13_loss: 3.5288 - val_out_s_loss: 0.0175 - val_add_13_accuracy: 0.6764 - val_out_s_accuracy: 0.2470\n",
      "Epoch 25/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6220 - add_13_loss: 3.6053 - out_s_loss: 0.0167 - add_13_accuracy: 0.6775 - out_s_accuracy: 0.3367 - val_loss: 3.5471 - val_add_13_loss: 3.5300 - val_out_s_loss: 0.0171 - val_add_13_accuracy: 0.6762 - val_out_s_accuracy: 0.2630\n",
      "Epoch 26/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6145 - add_13_loss: 3.5981 - out_s_loss: 0.0163 - add_13_accuracy: 0.6784 - out_s_accuracy: 0.3567 - val_loss: 3.5506 - val_add_13_loss: 3.5339 - val_out_s_loss: 0.0167 - val_add_13_accuracy: 0.6732 - val_out_s_accuracy: 0.2860\n",
      "Epoch 27/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6117 - add_13_loss: 3.5958 - out_s_loss: 0.0159 - add_13_accuracy: 0.6787 - out_s_accuracy: 0.3930 - val_loss: 3.5310 - val_add_13_loss: 3.5149 - val_out_s_loss: 0.0162 - val_add_13_accuracy: 0.6797 - val_out_s_accuracy: 0.3210\n",
      "Epoch 28/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6139 - add_13_loss: 3.5984 - out_s_loss: 0.0155 - add_13_accuracy: 0.6775 - out_s_accuracy: 0.4073 - val_loss: 3.5306 - val_add_13_loss: 3.5148 - val_out_s_loss: 0.0158 - val_add_13_accuracy: 0.6786 - val_out_s_accuracy: 0.3500\n",
      "Epoch 29/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6100 - add_13_loss: 3.5949 - out_s_loss: 0.0150 - add_13_accuracy: 0.6781 - out_s_accuracy: 0.4310 - val_loss: 3.5276 - val_add_13_loss: 3.5122 - val_out_s_loss: 0.0154 - val_add_13_accuracy: 0.6816 - val_out_s_accuracy: 0.3630\n",
      "Epoch 30/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6023 - add_13_loss: 3.5878 - out_s_loss: 0.0145 - add_13_accuracy: 0.6779 - out_s_accuracy: 0.4547 - val_loss: 3.5278 - val_add_13_loss: 3.5127 - val_out_s_loss: 0.0151 - val_add_13_accuracy: 0.6768 - val_out_s_accuracy: 0.3700\n",
      "Epoch 31/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6043 - add_13_loss: 3.5900 - out_s_loss: 0.0143 - add_13_accuracy: 0.6775 - out_s_accuracy: 0.4830 - val_loss: 3.5349 - val_add_13_loss: 3.5199 - val_out_s_loss: 0.0150 - val_add_13_accuracy: 0.6801 - val_out_s_accuracy: 0.3970\n",
      "Epoch 32/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5994 - add_13_loss: 3.5855 - out_s_loss: 0.0139 - add_13_accuracy: 0.6776 - out_s_accuracy: 0.5023 - val_loss: 3.5205 - val_add_13_loss: 3.5062 - val_out_s_loss: 0.0143 - val_add_13_accuracy: 0.6817 - val_out_s_accuracy: 0.4160\n",
      "Epoch 33/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.6013 - add_13_loss: 3.5877 - out_s_loss: 0.0136 - add_13_accuracy: 0.6777 - out_s_accuracy: 0.5203 - val_loss: 3.5298 - val_add_13_loss: 3.5157 - val_out_s_loss: 0.0141 - val_add_13_accuracy: 0.6798 - val_out_s_accuracy: 0.4290\n",
      "Epoch 34/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5988 - add_13_loss: 3.5856 - out_s_loss: 0.0132 - add_13_accuracy: 0.6777 - out_s_accuracy: 0.5330 - val_loss: 3.5146 - val_add_13_loss: 3.5010 - val_out_s_loss: 0.0136 - val_add_13_accuracy: 0.6817 - val_out_s_accuracy: 0.4570\n",
      "Epoch 35/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5992 - add_13_loss: 3.5863 - out_s_loss: 0.0129 - add_13_accuracy: 0.6783 - out_s_accuracy: 0.5550 - val_loss: 3.5155 - val_add_13_loss: 3.5021 - val_out_s_loss: 0.0134 - val_add_13_accuracy: 0.6786 - val_out_s_accuracy: 0.4790\n",
      "Epoch 36/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.6015 - add_13_loss: 3.5889 - out_s_loss: 0.0126 - add_13_accuracy: 0.6774 - out_s_accuracy: 0.5733 - val_loss: 3.5157 - val_add_13_loss: 3.5026 - val_out_s_loss: 0.0131 - val_add_13_accuracy: 0.6819 - val_out_s_accuracy: 0.4820\n",
      "Epoch 37/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5935 - add_13_loss: 3.5814 - out_s_loss: 0.0121 - add_13_accuracy: 0.6774 - out_s_accuracy: 0.5873 - val_loss: 3.5159 - val_add_13_loss: 3.5032 - val_out_s_loss: 0.0127 - val_add_13_accuracy: 0.6807 - val_out_s_accuracy: 0.5030\n",
      "Epoch 38/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5860 - add_13_loss: 3.5742 - out_s_loss: 0.0119 - add_13_accuracy: 0.6775 - out_s_accuracy: 0.6123 - val_loss: 3.5105 - val_add_13_loss: 3.4981 - val_out_s_loss: 0.0125 - val_add_13_accuracy: 0.6808 - val_out_s_accuracy: 0.5290\n",
      "Epoch 39/1000\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 3.5933 - add_13_loss: 3.5816 - out_s_loss: 0.0117 - add_13_accuracy: 0.6769 - out_s_accuracy: 0.6167 - val_loss: 3.5086 - val_add_13_loss: 3.4964 - val_out_s_loss: 0.0122 - val_add_13_accuracy: 0.6813 - val_out_s_accuracy: 0.5440\n",
      "Epoch 40/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5885 - add_13_loss: 3.5773 - out_s_loss: 0.0113 - add_13_accuracy: 0.6773 - out_s_accuracy: 0.6447 - val_loss: 3.5093 - val_add_13_loss: 3.4975 - val_out_s_loss: 0.0119 - val_add_13_accuracy: 0.6788 - val_out_s_accuracy: 0.5670\n",
      "Epoch 41/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5820 - add_13_loss: 3.5711 - out_s_loss: 0.0109 - add_13_accuracy: 0.6774 - out_s_accuracy: 0.6637 - val_loss: 3.5050 - val_add_13_loss: 3.4934 - val_out_s_loss: 0.0116 - val_add_13_accuracy: 0.6807 - val_out_s_accuracy: 0.5900\n",
      "Epoch 42/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5829 - add_13_loss: 3.5722 - out_s_loss: 0.0107 - add_13_accuracy: 0.6774 - out_s_accuracy: 0.6723 - val_loss: 3.5043 - val_add_13_loss: 3.4929 - val_out_s_loss: 0.0114 - val_add_13_accuracy: 0.6801 - val_out_s_accuracy: 0.6180\n",
      "Epoch 43/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5844 - add_13_loss: 3.5738 - out_s_loss: 0.0105 - add_13_accuracy: 0.6773 - out_s_accuracy: 0.6787 - val_loss: 3.5054 - val_add_13_loss: 3.4941 - val_out_s_loss: 0.0113 - val_add_13_accuracy: 0.6786 - val_out_s_accuracy: 0.6040\n",
      "Epoch 44/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5776 - add_13_loss: 3.5675 - out_s_loss: 0.0101 - add_13_accuracy: 0.6780 - out_s_accuracy: 0.6983 - val_loss: 3.5019 - val_add_13_loss: 3.4911 - val_out_s_loss: 0.0109 - val_add_13_accuracy: 0.6796 - val_out_s_accuracy: 0.6280\n",
      "Epoch 45/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5849 - add_13_loss: 3.5750 - out_s_loss: 0.0100 - add_13_accuracy: 0.6769 - out_s_accuracy: 0.7043 - val_loss: 3.5115 - val_add_13_loss: 3.5004 - val_out_s_loss: 0.0110 - val_add_13_accuracy: 0.6820 - val_out_s_accuracy: 0.6110\n",
      "Epoch 46/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5777 - add_13_loss: 3.5680 - out_s_loss: 0.0097 - add_13_accuracy: 0.6772 - out_s_accuracy: 0.7270 - val_loss: 3.5095 - val_add_13_loss: 3.4990 - val_out_s_loss: 0.0105 - val_add_13_accuracy: 0.6745 - val_out_s_accuracy: 0.6650\n",
      "Epoch 47/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5817 - add_13_loss: 3.5723 - out_s_loss: 0.0094 - add_13_accuracy: 0.6756 - out_s_accuracy: 0.7287 - val_loss: 3.5008 - val_add_13_loss: 3.4905 - val_out_s_loss: 0.0103 - val_add_13_accuracy: 0.6794 - val_out_s_accuracy: 0.6550\n",
      "Epoch 48/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5791 - add_13_loss: 3.5699 - out_s_loss: 0.0092 - add_13_accuracy: 0.6762 - out_s_accuracy: 0.7423 - val_loss: 3.4993 - val_add_13_loss: 3.4893 - val_out_s_loss: 0.0100 - val_add_13_accuracy: 0.6788 - val_out_s_accuracy: 0.6890\n",
      "Epoch 49/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5772 - add_13_loss: 3.5683 - out_s_loss: 0.0089 - add_13_accuracy: 0.6772 - out_s_accuracy: 0.7537 - val_loss: 3.5044 - val_add_13_loss: 3.4944 - val_out_s_loss: 0.0100 - val_add_13_accuracy: 0.6790 - val_out_s_accuracy: 0.6640\n",
      "Epoch 50/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5692 - add_13_loss: 3.5605 - out_s_loss: 0.0086 - add_13_accuracy: 0.6770 - out_s_accuracy: 0.7657 - val_loss: 3.4987 - val_add_13_loss: 3.4891 - val_out_s_loss: 0.0096 - val_add_13_accuracy: 0.6784 - val_out_s_accuracy: 0.6740\n",
      "Epoch 51/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5756 - add_13_loss: 3.5672 - out_s_loss: 0.0085 - add_13_accuracy: 0.6769 - out_s_accuracy: 0.7767 - val_loss: 3.5176 - val_add_13_loss: 3.5078 - val_out_s_loss: 0.0099 - val_add_13_accuracy: 0.6731 - val_out_s_accuracy: 0.6980\n",
      "Epoch 52/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5717 - add_13_loss: 3.5633 - out_s_loss: 0.0083 - add_13_accuracy: 0.6771 - out_s_accuracy: 0.7840 - val_loss: 3.4987 - val_add_13_loss: 3.4895 - val_out_s_loss: 0.0091 - val_add_13_accuracy: 0.6766 - val_out_s_accuracy: 0.7030\n",
      "Epoch 53/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5719 - add_13_loss: 3.5638 - out_s_loss: 0.0081 - add_13_accuracy: 0.6766 - out_s_accuracy: 0.7827 - val_loss: 3.5080 - val_add_13_loss: 3.4989 - val_out_s_loss: 0.0091 - val_add_13_accuracy: 0.6726 - val_out_s_accuracy: 0.7160\n",
      "Epoch 54/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5635 - add_13_loss: 3.5557 - out_s_loss: 0.0078 - add_13_accuracy: 0.6765 - out_s_accuracy: 0.8010 - val_loss: 3.5084 - val_add_13_loss: 3.4993 - val_out_s_loss: 0.0091 - val_add_13_accuracy: 0.6723 - val_out_s_accuracy: 0.7110\n",
      "Epoch 55/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5570 - add_13_loss: 3.5495 - out_s_loss: 0.0075 - add_13_accuracy: 0.6769 - out_s_accuracy: 0.8033 - val_loss: 3.5230 - val_add_13_loss: 3.5140 - val_out_s_loss: 0.0091 - val_add_13_accuracy: 0.6716 - val_out_s_accuracy: 0.7080\n",
      "Epoch 56/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5618 - add_13_loss: 3.5544 - out_s_loss: 0.0073 - add_13_accuracy: 0.6762 - out_s_accuracy: 0.8093 - val_loss: 3.5117 - val_add_13_loss: 3.5032 - val_out_s_loss: 0.0085 - val_add_13_accuracy: 0.6698 - val_out_s_accuracy: 0.7440\n",
      "Epoch 57/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5574 - add_13_loss: 3.5502 - out_s_loss: 0.0072 - add_13_accuracy: 0.6767 - out_s_accuracy: 0.8240 - val_loss: 3.5099 - val_add_13_loss: 3.5008 - val_out_s_loss: 0.0091 - val_add_13_accuracy: 0.6766 - val_out_s_accuracy: 0.7340\n",
      "Epoch 58/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5629 - add_13_loss: 3.5559 - out_s_loss: 0.0070 - add_13_accuracy: 0.6761 - out_s_accuracy: 0.8317 - val_loss: 3.5286 - val_add_13_loss: 3.5195 - val_out_s_loss: 0.0090 - val_add_13_accuracy: 0.6729 - val_out_s_accuracy: 0.7200\n",
      "Epoch 59/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5561 - add_13_loss: 3.5492 - out_s_loss: 0.0069 - add_13_accuracy: 0.6761 - out_s_accuracy: 0.8387 - val_loss: 3.4926 - val_add_13_loss: 3.4845 - val_out_s_loss: 0.0082 - val_add_13_accuracy: 0.6790 - val_out_s_accuracy: 0.7410\n",
      "Epoch 60/1000\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 3.5498 - add_13_loss: 3.5433 - out_s_loss: 0.0066 - add_13_accuracy: 0.6766 - out_s_accuracy: 0.8370 - val_loss: 3.4776 - val_add_13_loss: 3.4700 - val_out_s_loss: 0.0076 - val_add_13_accuracy: 0.6802 - val_out_s_accuracy: 0.7700\n",
      "Epoch 61/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5550 - add_13_loss: 3.5485 - out_s_loss: 0.0064 - add_13_accuracy: 0.6756 - out_s_accuracy: 0.8487 - val_loss: 3.4755 - val_add_13_loss: 3.4680 - val_out_s_loss: 0.0075 - val_add_13_accuracy: 0.6795 - val_out_s_accuracy: 0.7850\n",
      "Epoch 62/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5541 - add_13_loss: 3.5478 - out_s_loss: 0.0063 - add_13_accuracy: 0.6756 - out_s_accuracy: 0.8463 - val_loss: 3.4794 - val_add_13_loss: 3.4721 - val_out_s_loss: 0.0073 - val_add_13_accuracy: 0.6769 - val_out_s_accuracy: 0.7990\n",
      "Epoch 63/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5490 - add_13_loss: 3.5430 - out_s_loss: 0.0061 - add_13_accuracy: 0.6762 - out_s_accuracy: 0.8607 - val_loss: 3.4694 - val_add_13_loss: 3.4623 - val_out_s_loss: 0.0071 - val_add_13_accuracy: 0.6823 - val_out_s_accuracy: 0.7900\n",
      "Epoch 64/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5582 - add_13_loss: 3.5522 - out_s_loss: 0.0060 - add_13_accuracy: 0.6770 - out_s_accuracy: 0.8603 - val_loss: 3.4883 - val_add_13_loss: 3.4809 - val_out_s_loss: 0.0074 - val_add_13_accuracy: 0.6759 - val_out_s_accuracy: 0.7920\n",
      "Epoch 65/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5497 - add_13_loss: 3.5440 - out_s_loss: 0.0058 - add_13_accuracy: 0.6764 - out_s_accuracy: 0.8740 - val_loss: 3.4718 - val_add_13_loss: 3.4649 - val_out_s_loss: 0.0069 - val_add_13_accuracy: 0.6818 - val_out_s_accuracy: 0.8050\n",
      "Epoch 66/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5477 - add_13_loss: 3.5421 - out_s_loss: 0.0056 - add_13_accuracy: 0.6767 - out_s_accuracy: 0.8723 - val_loss: 3.4715 - val_add_13_loss: 3.4648 - val_out_s_loss: 0.0067 - val_add_13_accuracy: 0.6798 - val_out_s_accuracy: 0.8040\n",
      "Epoch 67/1000\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 3.5486 - add_13_loss: 3.5431 - out_s_loss: 0.0055 - add_13_accuracy: 0.6767 - out_s_accuracy: 0.8837 - val_loss: 3.4747 - val_add_13_loss: 3.4681 - val_out_s_loss: 0.0066 - val_add_13_accuracy: 0.6757 - val_out_s_accuracy: 0.8290\n",
      "Epoch 68/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5573 - add_13_loss: 3.5518 - out_s_loss: 0.0055 - add_13_accuracy: 0.6750 - out_s_accuracy: 0.8827 - val_loss: 3.4688 - val_add_13_loss: 3.4623 - val_out_s_loss: 0.0064 - val_add_13_accuracy: 0.6799 - val_out_s_accuracy: 0.8250\n",
      "Epoch 69/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5461 - add_13_loss: 3.5409 - out_s_loss: 0.0052 - add_13_accuracy: 0.6768 - out_s_accuracy: 0.8943 - val_loss: 3.4667 - val_add_13_loss: 3.4604 - val_out_s_loss: 0.0063 - val_add_13_accuracy: 0.6801 - val_out_s_accuracy: 0.8320\n",
      "Epoch 70/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5455 - add_13_loss: 3.5404 - out_s_loss: 0.0051 - add_13_accuracy: 0.6753 - out_s_accuracy: 0.8920 - val_loss: 3.4702 - val_add_13_loss: 3.4640 - val_out_s_loss: 0.0063 - val_add_13_accuracy: 0.6787 - val_out_s_accuracy: 0.8230\n",
      "Epoch 71/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5529 - add_13_loss: 3.5479 - out_s_loss: 0.0050 - add_13_accuracy: 0.6759 - out_s_accuracy: 0.8883 - val_loss: 3.4635 - val_add_13_loss: 3.4575 - val_out_s_loss: 0.0060 - val_add_13_accuracy: 0.6808 - val_out_s_accuracy: 0.8340\n",
      "Epoch 72/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5533 - add_13_loss: 3.5484 - out_s_loss: 0.0049 - add_13_accuracy: 0.6751 - out_s_accuracy: 0.8963 - val_loss: 3.4636 - val_add_13_loss: 3.4576 - val_out_s_loss: 0.0060 - val_add_13_accuracy: 0.6813 - val_out_s_accuracy: 0.8420\n",
      "Epoch 73/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5519 - add_13_loss: 3.5472 - out_s_loss: 0.0048 - add_13_accuracy: 0.6761 - out_s_accuracy: 0.9043 - val_loss: 3.4618 - val_add_13_loss: 3.4561 - val_out_s_loss: 0.0058 - val_add_13_accuracy: 0.6825 - val_out_s_accuracy: 0.8470\n",
      "Epoch 74/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5466 - add_13_loss: 3.5420 - out_s_loss: 0.0046 - add_13_accuracy: 0.6757 - out_s_accuracy: 0.9120 - val_loss: 3.4676 - val_add_13_loss: 3.4618 - val_out_s_loss: 0.0058 - val_add_13_accuracy: 0.6786 - val_out_s_accuracy: 0.8570\n",
      "Epoch 75/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5453 - add_13_loss: 3.5408 - out_s_loss: 0.0045 - add_13_accuracy: 0.6768 - out_s_accuracy: 0.9167 - val_loss: 3.4714 - val_add_13_loss: 3.4658 - val_out_s_loss: 0.0056 - val_add_13_accuracy: 0.6739 - val_out_s_accuracy: 0.8500\n",
      "Epoch 76/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5420 - add_13_loss: 3.5377 - out_s_loss: 0.0043 - add_13_accuracy: 0.6757 - out_s_accuracy: 0.9257 - val_loss: 3.4670 - val_add_13_loss: 3.4614 - val_out_s_loss: 0.0055 - val_add_13_accuracy: 0.6765 - val_out_s_accuracy: 0.8470\n",
      "Epoch 77/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5440 - add_13_loss: 3.5397 - out_s_loss: 0.0043 - add_13_accuracy: 0.6764 - out_s_accuracy: 0.9243 - val_loss: 3.4602 - val_add_13_loss: 3.4549 - val_out_s_loss: 0.0053 - val_add_13_accuracy: 0.6780 - val_out_s_accuracy: 0.8700\n",
      "Epoch 78/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5459 - add_13_loss: 3.5417 - out_s_loss: 0.0041 - add_13_accuracy: 0.6754 - out_s_accuracy: 0.9267 - val_loss: 3.4666 - val_add_13_loss: 3.4610 - val_out_s_loss: 0.0056 - val_add_13_accuracy: 0.6776 - val_out_s_accuracy: 0.8570\n",
      "Epoch 79/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5458 - add_13_loss: 3.5418 - out_s_loss: 0.0041 - add_13_accuracy: 0.6759 - out_s_accuracy: 0.9327 - val_loss: 3.4630 - val_add_13_loss: 3.4577 - val_out_s_loss: 0.0052 - val_add_13_accuracy: 0.6804 - val_out_s_accuracy: 0.8700\n",
      "Epoch 80/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5447 - add_13_loss: 3.5408 - out_s_loss: 0.0040 - add_13_accuracy: 0.6754 - out_s_accuracy: 0.9317 - val_loss: 3.4747 - val_add_13_loss: 3.4694 - val_out_s_loss: 0.0053 - val_add_13_accuracy: 0.6751 - val_out_s_accuracy: 0.8600\n",
      "Epoch 81/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5463 - add_13_loss: 3.5425 - out_s_loss: 0.0038 - add_13_accuracy: 0.6750 - out_s_accuracy: 0.9323 - val_loss: 3.4619 - val_add_13_loss: 3.4568 - val_out_s_loss: 0.0052 - val_add_13_accuracy: 0.6808 - val_out_s_accuracy: 0.8570\n",
      "Epoch 82/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5368 - add_13_loss: 3.5331 - out_s_loss: 0.0037 - add_13_accuracy: 0.6760 - out_s_accuracy: 0.9417 - val_loss: 3.4634 - val_add_13_loss: 3.4583 - val_out_s_loss: 0.0051 - val_add_13_accuracy: 0.6818 - val_out_s_accuracy: 0.8650\n",
      "Epoch 83/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5409 - add_13_loss: 3.5373 - out_s_loss: 0.0037 - add_13_accuracy: 0.6771 - out_s_accuracy: 0.9440 - val_loss: 3.4579 - val_add_13_loss: 3.4531 - val_out_s_loss: 0.0048 - val_add_13_accuracy: 0.6779 - val_out_s_accuracy: 0.8770\n",
      "Epoch 84/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5444 - add_13_loss: 3.5409 - out_s_loss: 0.0035 - add_13_accuracy: 0.6760 - out_s_accuracy: 0.9463 - val_loss: 3.4570 - val_add_13_loss: 3.4522 - val_out_s_loss: 0.0048 - val_add_13_accuracy: 0.6795 - val_out_s_accuracy: 0.8800\n",
      "Epoch 85/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5343 - add_13_loss: 3.5309 - out_s_loss: 0.0034 - add_13_accuracy: 0.6768 - out_s_accuracy: 0.9547 - val_loss: 3.4635 - val_add_13_loss: 3.4587 - val_out_s_loss: 0.0048 - val_add_13_accuracy: 0.6764 - val_out_s_accuracy: 0.8730\n",
      "Epoch 86/1000\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 3.5423 - add_13_loss: 3.5390 - out_s_loss: 0.0033 - add_13_accuracy: 0.6764 - out_s_accuracy: 0.9520 - val_loss: 3.4598 - val_add_13_loss: 3.4551 - val_out_s_loss: 0.0047 - val_add_13_accuracy: 0.6806 - val_out_s_accuracy: 0.8860\n",
      "Epoch 87/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5441 - add_13_loss: 3.5408 - out_s_loss: 0.0033 - add_13_accuracy: 0.6771 - out_s_accuracy: 0.9567 - val_loss: 3.4556 - val_add_13_loss: 3.4511 - val_out_s_loss: 0.0044 - val_add_13_accuracy: 0.6806 - val_out_s_accuracy: 0.8970\n",
      "Epoch 88/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5410 - add_13_loss: 3.5378 - out_s_loss: 0.0032 - add_13_accuracy: 0.6759 - out_s_accuracy: 0.9577 - val_loss: 3.4585 - val_add_13_loss: 3.4539 - val_out_s_loss: 0.0046 - val_add_13_accuracy: 0.6808 - val_out_s_accuracy: 0.8910\n",
      "Epoch 89/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5404 - add_13_loss: 3.5373 - out_s_loss: 0.0031 - add_13_accuracy: 0.6760 - out_s_accuracy: 0.9560 - val_loss: 3.4586 - val_add_13_loss: 3.4543 - val_out_s_loss: 0.0044 - val_add_13_accuracy: 0.6814 - val_out_s_accuracy: 0.8960\n",
      "Epoch 90/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5388 - add_13_loss: 3.5358 - out_s_loss: 0.0030 - add_13_accuracy: 0.6752 - out_s_accuracy: 0.9600 - val_loss: 3.4534 - val_add_13_loss: 3.4491 - val_out_s_loss: 0.0043 - val_add_13_accuracy: 0.6798 - val_out_s_accuracy: 0.9020\n",
      "Epoch 91/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5347 - add_13_loss: 3.5318 - out_s_loss: 0.0029 - add_13_accuracy: 0.6769 - out_s_accuracy: 0.9617 - val_loss: 3.4709 - val_add_13_loss: 3.4664 - val_out_s_loss: 0.0045 - val_add_13_accuracy: 0.6755 - val_out_s_accuracy: 0.8960\n",
      "Epoch 92/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5395 - add_13_loss: 3.5366 - out_s_loss: 0.0029 - add_13_accuracy: 0.6765 - out_s_accuracy: 0.9610 - val_loss: 3.4552 - val_add_13_loss: 3.4509 - val_out_s_loss: 0.0043 - val_add_13_accuracy: 0.6813 - val_out_s_accuracy: 0.9020\n",
      "Epoch 93/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5362 - add_13_loss: 3.5335 - out_s_loss: 0.0027 - add_13_accuracy: 0.6766 - out_s_accuracy: 0.9673 - val_loss: 3.4566 - val_add_13_loss: 3.4524 - val_out_s_loss: 0.0043 - val_add_13_accuracy: 0.6802 - val_out_s_accuracy: 0.9050\n",
      "Epoch 94/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5346 - add_13_loss: 3.5319 - out_s_loss: 0.0027 - add_13_accuracy: 0.6758 - out_s_accuracy: 0.9683 - val_loss: 3.4588 - val_add_13_loss: 3.4545 - val_out_s_loss: 0.0043 - val_add_13_accuracy: 0.6801 - val_out_s_accuracy: 0.9040\n",
      "Epoch 95/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5366 - add_13_loss: 3.5340 - out_s_loss: 0.0026 - add_13_accuracy: 0.6766 - out_s_accuracy: 0.9680 - val_loss: 3.4649 - val_add_13_loss: 3.4608 - val_out_s_loss: 0.0041 - val_add_13_accuracy: 0.6802 - val_out_s_accuracy: 0.9100\n",
      "Epoch 96/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5342 - add_13_loss: 3.5316 - out_s_loss: 0.0026 - add_13_accuracy: 0.6757 - out_s_accuracy: 0.9743 - val_loss: 3.4607 - val_add_13_loss: 3.4567 - val_out_s_loss: 0.0040 - val_add_13_accuracy: 0.6762 - val_out_s_accuracy: 0.9170\n",
      "Epoch 97/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5351 - add_13_loss: 3.5326 - out_s_loss: 0.0025 - add_13_accuracy: 0.6767 - out_s_accuracy: 0.9743 - val_loss: 3.4556 - val_add_13_loss: 3.4517 - val_out_s_loss: 0.0039 - val_add_13_accuracy: 0.6801 - val_out_s_accuracy: 0.9120\n",
      "Epoch 98/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5331 - add_13_loss: 3.5306 - out_s_loss: 0.0024 - add_13_accuracy: 0.6775 - out_s_accuracy: 0.9700 - val_loss: 3.4624 - val_add_13_loss: 3.4581 - val_out_s_loss: 0.0043 - val_add_13_accuracy: 0.6828 - val_out_s_accuracy: 0.9010\n",
      "Epoch 99/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5315 - add_13_loss: 3.5292 - out_s_loss: 0.0023 - add_13_accuracy: 0.6774 - out_s_accuracy: 0.9770 - val_loss: 3.4615 - val_add_13_loss: 3.4576 - val_out_s_loss: 0.0039 - val_add_13_accuracy: 0.6790 - val_out_s_accuracy: 0.9200\n",
      "Epoch 100/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5388 - add_13_loss: 3.5364 - out_s_loss: 0.0023 - add_13_accuracy: 0.6763 - out_s_accuracy: 0.9760 - val_loss: 3.4642 - val_add_13_loss: 3.4605 - val_out_s_loss: 0.0038 - val_add_13_accuracy: 0.6755 - val_out_s_accuracy: 0.9220\n",
      "Epoch 101/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5356 - add_13_loss: 3.5333 - out_s_loss: 0.0023 - add_13_accuracy: 0.6765 - out_s_accuracy: 0.9790 - val_loss: 3.4578 - val_add_13_loss: 3.4540 - val_out_s_loss: 0.0038 - val_add_13_accuracy: 0.6798 - val_out_s_accuracy: 0.9250\n",
      "Epoch 102/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5427 - add_13_loss: 3.5404 - out_s_loss: 0.0023 - add_13_accuracy: 0.6761 - out_s_accuracy: 0.9770 - val_loss: 3.4711 - val_add_13_loss: 3.4671 - val_out_s_loss: 0.0040 - val_add_13_accuracy: 0.6797 - val_out_s_accuracy: 0.9220\n",
      "Epoch 103/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5358 - add_13_loss: 3.5337 - out_s_loss: 0.0021 - add_13_accuracy: 0.6767 - out_s_accuracy: 0.9780 - val_loss: 3.4573 - val_add_13_loss: 3.4537 - val_out_s_loss: 0.0036 - val_add_13_accuracy: 0.6783 - val_out_s_accuracy: 0.9280\n",
      "Epoch 104/1000\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 3.5252 - add_13_loss: 3.5231 - out_s_loss: 0.0020 - add_13_accuracy: 0.6779 - out_s_accuracy: 0.9837 - val_loss: 3.4556 - val_add_13_loss: 3.4521 - val_out_s_loss: 0.0035 - val_add_13_accuracy: 0.6797 - val_out_s_accuracy: 0.9300\n",
      "Epoch 105/1000\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.5334 - add_13_loss: 3.5314 - out_s_loss: 0.0020 - add_13_accuracy: 0.6760 - out_s_accuracy: 0.9810 - val_loss: 3.4634 - val_add_13_loss: 3.4594 - val_out_s_loss: 0.0039 - val_add_13_accuracy: 0.6819 - val_out_s_accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = st_model.fit(\n",
    "    np.array(x_train), [np.array(bottleneck_train),np.array(output_train)], \n",
    "    validation_data=(x_val, [np.array(bottleneck_val), np.array(output_val)]), \n",
    "    epochs=1000, \n",
    "    batch_size=64, \n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 3.5416 - add_13_loss: 3.5372 - out_s_loss: 0.0043 - add_13_accuracy: 0.6786 - out_s_accuracy: 0.8974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.541557550430298,\n",
       " 3.537235975265503,\n",
       " 0.004321706015616655,\n",
       " 0.6786184310913086,\n",
       " 0.8973684310913086]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_model.evaluate(np.array(x_test), [bottleneck_test, output_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(200, 6, 1))\n",
    "\n",
    "# First Convolutional Layer repeated five times\n",
    "branch1 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch1 = layers.BatchNormalization()(branch1)\n",
    "\n",
    "branch2 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch2 = layers.BatchNormalization()(branch2)\n",
    "\n",
    "branch3 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch3 = layers.BatchNormalization()(branch3)\n",
    "\n",
    "branch4 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch4 = layers.BatchNormalization()(branch4)\n",
    "\n",
    "branch5 = layers.Conv2D(3, (3, 3), activation='relu', strides=(4, 6), padding='same')(inputs)\n",
    "branch5 = layers.BatchNormalization()(branch5)\n",
    "\n",
    "# Adding the outputs of the five branches\n",
    "x_b_S = layers.Add()([branch1, branch2, branch3, branch4, branch5])\n",
    "\n",
    "# Dropout Layer (commented out)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Flatten the output\n",
    "cat = layers.Flatten()(x_b_S)\n",
    "cat = layers.Dense(64, activation='relu')(cat)\n",
    "\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(276, activation='softmax', name = 'out_s')(cat)  # Output layer with softmax activation for 276 classes\n",
    "\n",
    "# Create the model\n",
    "st_model_o = models.Model(inputs=inputs, outputs= outputs)\n",
    "\n",
    "# Compile the model\n",
    "st_model_o.compile(optimizer='adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "# st_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)       [(None, 200, 6, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 50, 1, 3)             30        ['input_12[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 50, 1, 3)             30        ['input_12[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)          (None, 50, 1, 3)             30        ['input_12[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)          (None, 50, 1, 3)             30        ['input_12[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)          (None, 50, 1, 3)             30        ['input_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_52 (Ba  (None, 50, 1, 3)             12        ['conv2d_52[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_53 (Ba  (None, 50, 1, 3)             12        ['conv2d_53[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_54 (Ba  (None, 50, 1, 3)             12        ['conv2d_54[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_55 (Ba  (None, 50, 1, 3)             12        ['conv2d_55[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_56 (Ba  (None, 50, 1, 3)             12        ['conv2d_56[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 50, 1, 3)             0         ['batch_normalization_52[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_53[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_54[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_55[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_56[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)        (None, 150)                  0         ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 64)                   9664      ['flatten_11[0][0]']          \n",
      "                                                                                                  \n",
      " out_s (Dense)               (None, 276)                  17940     ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27814 (108.65 KB)\n",
      "Trainable params: 27784 (108.53 KB)\n",
      "Non-trainable params: 30 (120.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "st_model_o.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "94/94 [==============================] - 1s 3ms/step - loss: 0.1661 - accuracy: 0.0037 - val_loss: 0.0735 - val_accuracy: 0.0010\n",
      "Epoch 2/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0133 - val_loss: 0.0390 - val_accuracy: 0.0160\n",
      "Epoch 3/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0380 - val_loss: 0.0230 - val_accuracy: 0.0460\n",
      "Epoch 4/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0830 - val_loss: 0.0200 - val_accuracy: 0.1200\n",
      "Epoch 5/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.1500 - val_loss: 0.0192 - val_accuracy: 0.1680\n",
      "Epoch 6/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.2140 - val_loss: 0.0175 - val_accuracy: 0.2050\n",
      "Epoch 7/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.2767 - val_loss: 0.0166 - val_accuracy: 0.2520\n",
      "Epoch 8/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.3480 - val_loss: 0.0157 - val_accuracy: 0.3140\n",
      "Epoch 9/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 0.4043 - val_loss: 0.0149 - val_accuracy: 0.3770\n",
      "Epoch 10/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.4563 - val_loss: 0.0138 - val_accuracy: 0.4330\n",
      "Epoch 11/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.5250 - val_loss: 0.0130 - val_accuracy: 0.4560\n",
      "Epoch 12/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.5760 - val_loss: 0.0125 - val_accuracy: 0.4950\n",
      "Epoch 13/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.6283 - val_loss: 0.0127 - val_accuracy: 0.5340\n",
      "Epoch 14/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 0.6783 - val_loss: 0.0115 - val_accuracy: 0.5500\n",
      "Epoch 15/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.7310 - val_loss: 0.0109 - val_accuracy: 0.5950\n",
      "Epoch 16/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.7677 - val_loss: 0.0100 - val_accuracy: 0.6150\n",
      "Epoch 17/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0065 - accuracy: 0.8057 - val_loss: 0.0091 - val_accuracy: 0.6360\n",
      "Epoch 18/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0056 - accuracy: 0.8490 - val_loss: 0.0084 - val_accuracy: 0.6670\n",
      "Epoch 19/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0048 - accuracy: 0.8850 - val_loss: 0.0078 - val_accuracy: 0.7070\n",
      "Epoch 20/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 0.9113 - val_loss: 0.0068 - val_accuracy: 0.7400\n",
      "Epoch 21/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 0.9387 - val_loss: 0.0064 - val_accuracy: 0.8320\n",
      "Epoch 22/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 0.9557 - val_loss: 0.0050 - val_accuracy: 0.8390\n",
      "Epoch 23/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 0.9727 - val_loss: 0.0046 - val_accuracy: 0.8740\n",
      "Epoch 24/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.9813 - val_loss: 0.0041 - val_accuracy: 0.9200\n",
      "Epoch 25/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 0.9873 - val_loss: 0.0034 - val_accuracy: 0.9100\n",
      "Epoch 26/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9907 - val_loss: 0.0032 - val_accuracy: 0.9270\n",
      "Epoch 27/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.9933 - val_loss: 0.0028 - val_accuracy: 0.9520\n",
      "Epoch 28/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 0.9947 - val_loss: 0.0024 - val_accuracy: 0.9610\n",
      "Epoch 29/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 9.8067e-04 - accuracy: 0.9967 - val_loss: 0.0023 - val_accuracy: 0.9630\n",
      "Epoch 30/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 8.0499e-04 - accuracy: 0.9973 - val_loss: 0.0021 - val_accuracy: 0.9650\n",
      "Epoch 31/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 7.0742e-04 - accuracy: 0.9993 - val_loss: 0.0024 - val_accuracy: 0.9690\n",
      "Epoch 32/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 6.2293e-04 - accuracy: 0.9993 - val_loss: 0.0019 - val_accuracy: 0.9780\n",
      "Epoch 33/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 5.2419e-04 - accuracy: 0.9997 - val_loss: 0.0018 - val_accuracy: 0.9800\n",
      "Epoch 34/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.4472e-04 - accuracy: 0.9993 - val_loss: 0.0017 - val_accuracy: 0.9840\n",
      "Epoch 35/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.4126e-04 - accuracy: 0.9990 - val_loss: 0.0015 - val_accuracy: 0.9860\n",
      "Epoch 36/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6722e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 0.9850\n",
      "Epoch 37/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1378e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 0.9860\n",
      "Epoch 38/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7798e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 0.9830\n",
      "Epoch 39/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.5913e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 0.9870\n",
      "Epoch 40/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.3008e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 0.9850\n",
      "Epoch 41/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.0193e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 0.9880\n",
      "Epoch 42/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.0115e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 0.9870\n",
      "Epoch 43/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.7433e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 0.9890\n",
      "Epoch 44/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.5293e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9890\n",
      "Epoch 45/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.4501e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9910\n",
      "Epoch 46/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.4626e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9930\n",
      "Epoch 47/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.3076e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9920\n",
      "Epoch 48/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 9.8041e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9900\n",
      "Epoch 49/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.0189e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9860\n",
      "Epoch 50/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 9.2704e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9890\n",
      "Epoch 51/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 8.1022e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9870\n",
      "Epoch 52/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 7.3842e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9890\n",
      "Epoch 53/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 6.9585e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9910\n",
      "Epoch 54/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 6.0279e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9890\n",
      "Epoch 55/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 6.2715e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9900\n",
      "Epoch 56/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 5.1675e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9890\n",
      "Epoch 57/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 6.6065e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9890\n",
      "Epoch 58/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 5.6388e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9890\n",
      "Epoch 59/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.8634e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9900\n",
      "Epoch 60/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5823e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9910\n",
      "Epoch 61/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 5.2960e-05 - accuracy: 1.0000 - val_loss: 9.9312e-04 - val_accuracy: 0.9910\n",
      "Epoch 62/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3735e-05 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 0.9910\n",
      "Epoch 63/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6586e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9900\n",
      "Epoch 64/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.7633e-05 - accuracy: 1.0000 - val_loss: 9.2860e-04 - val_accuracy: 0.9910\n",
      "Epoch 65/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7274e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9900\n",
      "Epoch 66/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.3982e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9910\n",
      "Epoch 67/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.3306e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9900\n",
      "Epoch 68/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.5489e-05 - accuracy: 1.0000 - val_loss: 9.8827e-04 - val_accuracy: 0.9890\n",
      "Epoch 69/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.1025e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9930\n",
      "Epoch 70/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.2365e-05 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 0.9890\n",
      "Epoch 71/1000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 1.9232e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9920\n",
      "Epoch 72/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.3663e-05 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 0.9890\n",
      "Epoch 73/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.1345e-05 - accuracy: 1.0000 - val_loss: 9.7788e-04 - val_accuracy: 0.9910\n",
      "Epoch 74/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.0560e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9910\n",
      "Epoch 75/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.6278e-05 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9920\n",
      "Epoch 76/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.8049e-05 - accuracy: 1.0000 - val_loss: 9.9507e-04 - val_accuracy: 0.9920\n",
      "Epoch 77/1000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.4583e-05 - accuracy: 1.0000 - val_loss: 9.9690e-04 - val_accuracy: 0.9900\n",
      "Epoch 78/1000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 1.5935e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9920\n",
      "Epoch 79/1000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 1.5615e-05 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# Train the student model using the teacher model's outputs as labels\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = st_model_o.fit(np.array(x_train), \n",
    "                      np.array(output_train), \n",
    "                      epochs=1000, \n",
    "                      batch_size=32,\n",
    "                      validation_data = ( np.array(x_val), np.array(output_val) ),\n",
    "                      callbacks = callback)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 0s 945us/step - loss: 0.0011 - accuracy: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0011070554610341787, 0.9901315569877625]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_model_o.evaluate(np.array(x_test), output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, GlobalAveragePooling1D, Masking, Input, MaxPooling1D, GlobalMaxPooling1D, Add, Dropout, BatchNormalization, UpSampling1D, Lambda, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using subcarriers = 1, 2, 3\n",
    "x_train = np.array(x_train)[:, :, [0, 1, 2]]\n",
    "x_test = np.array(x_test)[:, :, [0, 1, 2]]\n",
    "x_val = np.array(x_val)[:, :, [0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 200, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"distillationFramework\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"distillationFramework\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_T (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │ in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ l1_T[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bottle_T (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,608</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_S (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span> │ in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bottle_T[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ l1_S[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bottle_S (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,824</span> │ max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bottle_S[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bottle_S[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">153,900</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out_T (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">83,076</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out_S (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,108</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ in (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_T (\u001b[38;5;33mConv1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m704\u001b[0m │ in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ l1_T[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m10,304\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │     \u001b[38;5;34m49,408\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bottle_T (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │     \u001b[38;5;34m24,608\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_S (\u001b[38;5;33mConv1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │        \u001b[38;5;34m176\u001b[0m │ in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ bottle_T[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │         \u001b[38;5;34m32\u001b[0m │ l1_S[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │     \u001b[38;5;34m49,664\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m8\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bottle_S (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m1,824\u001b[0m │ max_pooling1d_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bottle_S[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bottle_S[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │    \u001b[38;5;34m153,900\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out_T (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │     \u001b[38;5;34m83,076\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out_S (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │      \u001b[38;5;34m9,108\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">383,188</span> (1.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m383,188\u001b[0m (1.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">382,980</span> (1.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m382,980\u001b[0m (1.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> (832.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m208\u001b[0m (832.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataInp = Input(shape = (200, 3), name='in')\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_T')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T) #old\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T) #old\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 44, 256\n",
    "x_b_T = Conv1D(kernel_size = kernel_size, filters=32, activation='relu', name='bottle_T')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_b_T) # 21, 32\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Add()([avg_T, max_T])\n",
    "d_T = Dense(300, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(276, activation = 'sigmoid', name='out_T')(d_T)\n",
    "\n",
    "# STUDENT MODEL\n",
    "x_S = Conv1D(kernel_size = 7, filters=8, activation='relu', name='l1_S')(dataInp) # 194, 8\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(4)(x_S) # 48, 32\n",
    "x_b_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "avg_S = GlobalAveragePooling1D()(x_b_S)\n",
    "max_S = GlobalMaxPooling1D()(x_b_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "\n",
    "#l2_internal = tf.norm(x_b_T - x_b_S, ord='euclidean', axis=1)\n",
    "# abs_sub = tf.abs(x_b_T - x_b_S)\n",
    "# l2_output = tf.norm(out_T - out_S, ord='euclidean', axis=-1)\n",
    "# abs_sub = tf.abs(tf.keras.layers.Subtract()([x_b_T, x_b_S]))  \n",
    "# l2_output = tf.abs(tf.keras.layers.Subtract()([out_T, out_S]))\n",
    "\n",
    "model = Model( inputs=dataInp, outputs=[out_T, out_S ], name='distillationFramework' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] building and compilation complete!\n",
      "\n",
      " Model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"distillationFramework\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"distillationFramework\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ in (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_T (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │ in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ l1_T[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bottle_T (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,608</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_S (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span> │ in[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bottle_T[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ l1_S[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bottle_S (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,824</span> │ max_pooling1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bottle_S[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bottle_S[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">153,900</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out_T (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">83,076</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out_S (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,108</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ in (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_T (\u001b[38;5;33mConv1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m704\u001b[0m │ in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ l1_T[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m10,304\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │     \u001b[38;5;34m49,408\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bottle_T (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │     \u001b[38;5;34m24,608\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l1_S (\u001b[38;5;33mConv1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │        \u001b[38;5;34m176\u001b[0m │ in[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ bottle_T[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │         \u001b[38;5;34m32\u001b[0m │ l1_S[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │     \u001b[38;5;34m49,664\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m8\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bottle_S (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m1,824\u001b[0m │ max_pooling1d_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bottle_S[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bottle_S[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │    \u001b[38;5;34m153,900\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out_T (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │     \u001b[38;5;34m83,076\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ out_S (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │      \u001b[38;5;34m9,108\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">383,188</span> (1.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m383,188\u001b[0m (1.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">382,980</span> (1.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m382,980\u001b[0m (1.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> (832.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m208\u001b[0m (832.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = Adam(learning_rate = 0.0001) # 0.00001\n",
    "print(\"[INFO] compiling model...\")\n",
    "model.compile(loss = [\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"], loss_weights = [1., 10], optimizer= opt)\n",
    "print('[INFO] building and compilation complete!')\n",
    "print('\\n Model summary:') \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 42, 32)\n",
      "(1000, 42, 32)\n",
      "(3000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "layer_zero_train = np.zeros(shape = (y_train.shape[0], 42, 32))\n",
    "layer_zero_val = np.zeros(shape = (y_val.shape[0], 42, 32))\n",
    "\n",
    "print(layer_zero_train.shape)\n",
    "print(layer_zero_val.shape)\n",
    "\n",
    "out_zero_train = np.zeros(shape = (y_train.shape[0], ))\n",
    "out_zero_val = np.zeros(shape = (y_val.shape[0], ))\n",
    "\n",
    "print(out_zero_train.shape)\n",
    "print(out_zero_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 65.4885 - val_loss: 72.1987\n",
      "Epoch 2/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 63.6335 - val_loss: 64.9604\n",
      "Epoch 3/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 62.2224 - val_loss: 62.7403\n",
      "Epoch 4/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 61.3327 - val_loss: 61.5092\n",
      "Epoch 5/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 60.6975 - val_loss: 60.6673\n",
      "Epoch 6/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 59.9337 - val_loss: 60.0341\n",
      "Epoch 7/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 59.3240 - val_loss: 59.3900\n",
      "Epoch 8/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 58.8783 - val_loss: 58.9050\n",
      "Epoch 9/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 58.2542 - val_loss: 58.2955\n",
      "Epoch 10/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 57.7213 - val_loss: 57.7826\n",
      "Epoch 11/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 57.3987 - val_loss: 57.1744\n",
      "Epoch 12/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 56.6792 - val_loss: 56.6645\n",
      "Epoch 13/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 56.2286 - val_loss: 56.0634\n",
      "Epoch 14/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 55.6319 - val_loss: 55.5471\n",
      "Epoch 15/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 54.9508 - val_loss: 55.0064\n",
      "Epoch 16/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 54.5945 - val_loss: 54.5866\n",
      "Epoch 17/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 54.3342 - val_loss: 53.9859\n",
      "Epoch 18/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 53.9013 - val_loss: 53.5221\n",
      "Epoch 19/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 53.6663 - val_loss: 53.1937\n",
      "Epoch 20/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 53.1712 - val_loss: 52.7004\n",
      "Epoch 21/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 52.7065 - val_loss: 52.2784\n",
      "Epoch 22/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 52.6273 - val_loss: 51.8624\n",
      "Epoch 23/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 52.0995 - val_loss: 51.5342\n",
      "Epoch 24/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 51.9631 - val_loss: 51.1153\n",
      "Epoch 25/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 51.1358 - val_loss: 50.7737\n",
      "Epoch 26/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 50.8161 - val_loss: 50.4165\n",
      "Epoch 27/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 50.6137 - val_loss: 50.1293\n",
      "Epoch 28/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 50.2371 - val_loss: 49.8096\n",
      "Epoch 29/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 50.2736 - val_loss: 49.5125\n",
      "Epoch 30/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 49.7770 - val_loss: 49.2250\n",
      "Epoch 31/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 49.3518 - val_loss: 49.0092\n",
      "Epoch 32/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 49.1348 - val_loss: 48.7478\n",
      "Epoch 33/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 48.8850 - val_loss: 48.5662\n",
      "Epoch 34/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 48.9187 - val_loss: 48.2654\n",
      "Epoch 35/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 48.8583 - val_loss: 48.0830\n",
      "Epoch 36/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 48.2074 - val_loss: 47.8917\n",
      "Epoch 37/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 48.1708 - val_loss: 47.6082\n",
      "Epoch 38/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 47.8283 - val_loss: 47.4298\n",
      "Epoch 39/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 47.6165 - val_loss: 47.2784\n",
      "Epoch 40/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 47.3890 - val_loss: 46.9250\n",
      "Epoch 41/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 47.2320 - val_loss: 46.8175\n",
      "Epoch 42/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 47.3050 - val_loss: 46.5839\n",
      "Epoch 43/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 47.0544 - val_loss: 46.3416\n",
      "Epoch 44/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 47.0333 - val_loss: 46.2850\n",
      "Epoch 45/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 46.6936 - val_loss: 45.9711\n",
      "Epoch 46/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 46.5410 - val_loss: 45.8166\n",
      "Epoch 47/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 46.4718 - val_loss: 45.6811\n",
      "Epoch 48/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 46.2094 - val_loss: 45.4395\n",
      "Epoch 49/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 45.7936 - val_loss: 45.3457\n",
      "Epoch 50/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 45.7221 - val_loss: 45.0916\n",
      "Epoch 51/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 45.3700 - val_loss: 44.9554\n",
      "Epoch 52/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 45.7301 - val_loss: 44.8457\n",
      "Epoch 53/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 45.2889 - val_loss: 44.7326\n",
      "Epoch 54/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 45.6017 - val_loss: 44.5081\n",
      "Epoch 55/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 44.9686 - val_loss: 44.3365\n",
      "Epoch 56/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 44.9814 - val_loss: 44.1856\n",
      "Epoch 57/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 45.0292 - val_loss: 44.0916\n",
      "Epoch 58/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 44.3922 - val_loss: 43.9678\n",
      "Epoch 59/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 44.5278 - val_loss: 43.8249\n",
      "Epoch 60/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 44.2581 - val_loss: 43.5990\n",
      "Epoch 61/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 43.8167 - val_loss: 43.4692\n",
      "Epoch 62/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 43.7410 - val_loss: 43.3576\n",
      "Epoch 63/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 43.7662 - val_loss: 43.2172\n",
      "Epoch 64/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 44.0630 - val_loss: 43.0596\n",
      "Epoch 65/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 43.5230 - val_loss: 42.9420\n",
      "Epoch 66/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 42.9545 - val_loss: 42.8492\n",
      "Epoch 67/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 42.9407 - val_loss: 42.5513\n",
      "Epoch 68/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 43.1130 - val_loss: 42.4666\n",
      "Epoch 69/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 42.7824 - val_loss: 42.2947\n",
      "Epoch 70/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 42.2727 - val_loss: 42.2875\n",
      "Epoch 71/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 42.5963 - val_loss: 42.0328\n",
      "Epoch 72/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 42.4806 - val_loss: 41.9630\n",
      "Epoch 73/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 42.5297 - val_loss: 41.7794\n",
      "Epoch 74/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 41.9683 - val_loss: 41.6877\n",
      "Epoch 75/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 41.9563 - val_loss: 41.5356\n",
      "Epoch 76/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 41.8035 - val_loss: 41.4568\n",
      "Epoch 77/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 41.8540 - val_loss: 41.2554\n",
      "Epoch 78/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 41.3743 - val_loss: 41.0954\n",
      "Epoch 79/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 41.2938 - val_loss: 40.8941\n",
      "Epoch 80/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 41.4865 - val_loss: 40.7882\n",
      "Epoch 81/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 40.3865 - val_loss: 40.5895\n",
      "Epoch 82/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 40.8684 - val_loss: 40.3768\n",
      "Epoch 83/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 40.9410 - val_loss: 40.1524\n",
      "Epoch 84/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 40.7820 - val_loss: 40.0632\n",
      "Epoch 85/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 40.3566 - val_loss: 39.8208\n",
      "Epoch 86/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 40.4521 - val_loss: 39.6699\n",
      "Epoch 87/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 40.5312 - val_loss: 39.5788\n",
      "Epoch 88/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 40.0636 - val_loss: 39.4925\n",
      "Epoch 89/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 39.2639 - val_loss: 39.3218\n",
      "Epoch 90/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 39.7098 - val_loss: 39.1775\n",
      "Epoch 91/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 39.2762 - val_loss: 38.9896\n",
      "Epoch 92/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 39.5181 - val_loss: 38.8149\n",
      "Epoch 93/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 39.1524 - val_loss: 38.6609\n",
      "Epoch 94/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 39.6452 - val_loss: 38.5533\n",
      "Epoch 95/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 38.8159 - val_loss: 38.3245\n",
      "Epoch 96/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 38.8545 - val_loss: 38.1558\n",
      "Epoch 97/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 38.3433 - val_loss: 38.0102\n",
      "Epoch 98/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 38.7461 - val_loss: 37.8330\n",
      "Epoch 99/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 38.4817 - val_loss: 37.7681\n",
      "Epoch 100/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 38.3539 - val_loss: 37.6037\n",
      "Epoch 101/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 38.5042 - val_loss: 37.3421\n",
      "Epoch 102/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 38.1013 - val_loss: 37.2460\n",
      "Epoch 103/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 37.2930 - val_loss: 37.0248\n",
      "Epoch 104/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 37.7085 - val_loss: 36.8689\n",
      "Epoch 105/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 37.7856 - val_loss: 36.8874\n",
      "Epoch 106/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 37.8569 - val_loss: 36.6534\n",
      "Epoch 107/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 37.3410 - val_loss: 36.5472\n",
      "Epoch 108/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 36.9371 - val_loss: 36.3925\n",
      "Epoch 109/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 36.8587 - val_loss: 36.1848\n",
      "Epoch 110/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 36.9169 - val_loss: 36.0496\n",
      "Epoch 111/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 36.4112 - val_loss: 35.9611\n",
      "Epoch 112/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 36.7252 - val_loss: 35.8132\n",
      "Epoch 113/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 36.5317 - val_loss: 35.6103\n",
      "Epoch 114/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 36.2872 - val_loss: 35.4703\n",
      "Epoch 115/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 36.5133 - val_loss: 35.3512\n",
      "Epoch 116/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 36.2119 - val_loss: 35.1719\n",
      "Epoch 117/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 36.3415 - val_loss: 35.0814\n",
      "Epoch 118/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 35.8588 - val_loss: 34.9241\n",
      "Epoch 119/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 35.5349 - val_loss: 34.7515\n",
      "Epoch 120/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 35.7294 - val_loss: 34.6785\n",
      "Epoch 121/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 35.5122 - val_loss: 34.4874\n",
      "Epoch 122/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 35.8178 - val_loss: 34.4573\n",
      "Epoch 123/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 35.4873 - val_loss: 34.1886\n",
      "Epoch 124/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 35.6773 - val_loss: 34.1094\n",
      "Epoch 125/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 35.1518 - val_loss: 33.8602\n",
      "Epoch 126/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 35.1532 - val_loss: 33.7827\n",
      "Epoch 127/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 35.0197 - val_loss: 33.7434\n",
      "Epoch 128/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 34.5943 - val_loss: 33.5151\n",
      "Epoch 129/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 34.5434 - val_loss: 33.4251\n",
      "Epoch 130/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 34.6734 - val_loss: 33.2305\n",
      "Epoch 131/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 34.3405 - val_loss: 33.0969\n",
      "Epoch 132/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 34.1442 - val_loss: 33.0334\n",
      "Epoch 133/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 34.5951 - val_loss: 32.9624\n",
      "Epoch 134/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 33.9469 - val_loss: 32.7285\n",
      "Epoch 135/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 34.1164 - val_loss: 32.6047\n",
      "Epoch 136/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 33.4173 - val_loss: 32.4399\n",
      "Epoch 137/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 33.5408 - val_loss: 32.3836\n",
      "Epoch 138/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 33.7471 - val_loss: 32.3051\n",
      "Epoch 139/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 33.6023 - val_loss: 32.1385\n",
      "Epoch 140/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 33.8842 - val_loss: 31.9457\n",
      "Epoch 141/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 32.9139 - val_loss: 31.8150\n",
      "Epoch 142/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 33.3607 - val_loss: 31.7138\n",
      "Epoch 143/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 33.3568 - val_loss: 31.6927\n",
      "Epoch 144/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 32.8919 - val_loss: 31.6274\n",
      "Epoch 145/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 33.0815 - val_loss: 31.3693\n",
      "Epoch 146/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 32.7350 - val_loss: 31.4193\n",
      "Epoch 147/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 32.9293 - val_loss: 31.1653\n",
      "Epoch 148/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 32.7636 - val_loss: 31.1480\n",
      "Epoch 149/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 32.3751 - val_loss: 30.9871\n",
      "Epoch 150/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 32.4534 - val_loss: 30.9005\n",
      "Epoch 151/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 32.3937 - val_loss: 30.6901\n",
      "Epoch 152/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 32.0984 - val_loss: 30.7463\n",
      "Epoch 153/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 31.8135 - val_loss: 30.4951\n",
      "Epoch 154/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 31.9620 - val_loss: 30.4008\n",
      "Epoch 155/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 31.9759 - val_loss: 30.3401\n",
      "Epoch 156/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 32.1626 - val_loss: 30.2543\n",
      "Epoch 157/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 31.7016 - val_loss: 30.2004\n",
      "Epoch 158/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 31.2230 - val_loss: 30.0486\n",
      "Epoch 159/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 31.6634 - val_loss: 29.9409\n",
      "Epoch 160/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 31.5310 - val_loss: 29.9433\n",
      "Epoch 161/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 31.2474 - val_loss: 29.8193\n",
      "Epoch 162/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 31.1866 - val_loss: 29.6283\n",
      "Epoch 163/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 31.1056 - val_loss: 29.5060\n",
      "Epoch 164/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 31.2901 - val_loss: 29.4452\n",
      "Epoch 165/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 31.1358 - val_loss: 29.4452\n",
      "Epoch 166/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 31.1561 - val_loss: 29.3139\n",
      "Epoch 167/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 30.7476 - val_loss: 29.1929\n",
      "Epoch 168/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 31.0359 - val_loss: 29.0548\n",
      "Epoch 169/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 30.6894 - val_loss: 29.0143\n",
      "Epoch 170/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 30.9706 - val_loss: 28.8453\n",
      "Epoch 171/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 30.6423 - val_loss: 28.7095\n",
      "Epoch 172/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 29.7240 - val_loss: 28.6875\n",
      "Epoch 173/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 30.2001 - val_loss: 28.6161\n",
      "Epoch 174/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 30.1339 - val_loss: 28.5646\n",
      "Epoch 175/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 30.1993 - val_loss: 28.4232\n",
      "Epoch 176/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 30.6279 - val_loss: 28.3523\n",
      "Epoch 177/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 29.5026 - val_loss: 28.3723\n",
      "Epoch 178/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 30.0637 - val_loss: 28.2246\n",
      "Epoch 179/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 29.8763 - val_loss: 28.1280\n",
      "Epoch 180/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 29.5008 - val_loss: 28.1378\n",
      "Epoch 181/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 29.2823 - val_loss: 27.8739\n",
      "Epoch 182/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 28.6481 - val_loss: 27.8778\n",
      "Epoch 183/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 28.8409 - val_loss: 27.8026\n",
      "Epoch 184/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 29.3635 - val_loss: 27.6605\n",
      "Epoch 185/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 29.4633 - val_loss: 27.6615\n",
      "Epoch 186/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 29.7244 - val_loss: 27.4953\n",
      "Epoch 187/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 29.5931 - val_loss: 27.4051\n",
      "Epoch 188/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 29.2232 - val_loss: 27.3653\n",
      "Epoch 189/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 28.8776 - val_loss: 27.2826\n",
      "Epoch 190/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 28.9761 - val_loss: 27.1936\n",
      "Epoch 191/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 28.9149 - val_loss: 27.0959\n",
      "Epoch 192/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 28.8363 - val_loss: 27.1937\n",
      "Epoch 193/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 28.8823 - val_loss: 27.0247\n",
      "Epoch 194/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 28.8615 - val_loss: 27.0116\n",
      "Epoch 195/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 28.7418 - val_loss: 26.9134\n",
      "Epoch 196/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 28.1543 - val_loss: 26.7512\n",
      "Epoch 197/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 28.6610 - val_loss: 26.7816\n",
      "Epoch 198/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 27.9461 - val_loss: 26.5739\n",
      "Epoch 199/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 27.9183 - val_loss: 26.5178\n",
      "Epoch 200/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 28.0790 - val_loss: 26.6211\n",
      "Epoch 201/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 28.7069 - val_loss: 26.4041\n",
      "Epoch 202/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 28.3155 - val_loss: 26.6615\n",
      "Epoch 203/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 28.2013 - val_loss: 26.3305\n",
      "Epoch 204/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 28.0280 - val_loss: 26.1849\n",
      "Epoch 205/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 28.0056 - val_loss: 26.1524\n",
      "Epoch 206/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 27.5999 - val_loss: 26.1564\n",
      "Epoch 207/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 28.0268 - val_loss: 26.1114\n",
      "Epoch 208/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 27.5144 - val_loss: 25.9417\n",
      "Epoch 209/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 27.6924 - val_loss: 25.9949\n",
      "Epoch 210/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 27.8876 - val_loss: 25.8507\n",
      "Epoch 211/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 27.1810 - val_loss: 25.7499\n",
      "Epoch 212/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 27.5999 - val_loss: 25.7197\n",
      "Epoch 213/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 27.2742 - val_loss: 25.7197\n",
      "Epoch 214/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 27.7229 - val_loss: 25.5608\n",
      "Epoch 215/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 27.4301 - val_loss: 25.5556\n",
      "Epoch 216/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 27.1015 - val_loss: 25.4806\n",
      "Epoch 217/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 27.3220 - val_loss: 25.4743\n",
      "Epoch 218/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 26.4502 - val_loss: 25.3874\n",
      "Epoch 219/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 26.7018 - val_loss: 25.2867\n",
      "Epoch 220/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 26.6943 - val_loss: 25.1884\n",
      "Epoch 221/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 26.9884 - val_loss: 25.1484\n",
      "Epoch 222/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 26.6514 - val_loss: 25.0818\n",
      "Epoch 223/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 26.5609 - val_loss: 25.1454\n",
      "Epoch 224/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 26.6633 - val_loss: 25.0972\n",
      "Epoch 225/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 26.7936 - val_loss: 24.8195\n",
      "Epoch 226/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 26.2746 - val_loss: 24.8255\n",
      "Epoch 227/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 26.5615 - val_loss: 24.7658\n",
      "Epoch 228/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 26.4859 - val_loss: 24.6871\n",
      "Epoch 229/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 26.4587 - val_loss: 24.7072\n",
      "Epoch 230/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 26.7938 - val_loss: 24.5836\n",
      "Epoch 231/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 25.9488 - val_loss: 24.5990\n",
      "Epoch 232/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 26.4368 - val_loss: 24.4718\n",
      "Epoch 233/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 26.0523 - val_loss: 24.4254\n",
      "Epoch 234/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 25.7549 - val_loss: 24.5025\n",
      "Epoch 235/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 26.0399 - val_loss: 24.3087\n",
      "Epoch 236/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 25.6959 - val_loss: 24.3488\n",
      "Epoch 237/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 26.0737 - val_loss: 24.3132\n",
      "Epoch 238/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 25.4339 - val_loss: 24.2357\n",
      "Epoch 239/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 25.7903 - val_loss: 24.1924\n",
      "Epoch 240/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 25.3206 - val_loss: 24.1864\n",
      "Epoch 241/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 25.6432 - val_loss: 24.0911\n",
      "Epoch 242/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 25.5833 - val_loss: 24.0396\n",
      "Epoch 243/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 25.9915 - val_loss: 24.0520\n",
      "Epoch 244/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 25.9790 - val_loss: 23.9251\n",
      "Epoch 245/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 25.7615 - val_loss: 23.8414\n",
      "Epoch 246/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 25.1652 - val_loss: 23.9297\n",
      "Epoch 247/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 25.5021 - val_loss: 23.7179\n",
      "Epoch 248/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.9314 - val_loss: 23.7039\n",
      "Epoch 249/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 25.3278 - val_loss: 23.7566\n",
      "Epoch 250/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.8054 - val_loss: 23.7273\n",
      "Epoch 251/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.8431 - val_loss: 23.7283\n",
      "Epoch 252/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 25.0329 - val_loss: 23.4573\n",
      "Epoch 253/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 25.1584 - val_loss: 23.4835\n",
      "Epoch 254/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 25.0529 - val_loss: 23.5090\n",
      "Epoch 255/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.7876 - val_loss: 23.3468\n",
      "Epoch 256/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.6503 - val_loss: 23.3853\n",
      "Epoch 257/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 25.3998 - val_loss: 23.3520\n",
      "Epoch 258/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.7859 - val_loss: 23.3614\n",
      "Epoch 259/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 25.1047 - val_loss: 23.2190\n",
      "Epoch 260/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 25.0687 - val_loss: 23.1751\n",
      "Epoch 261/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.3685 - val_loss: 23.0846\n",
      "Epoch 262/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.8419 - val_loss: 23.1168\n",
      "Epoch 263/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.4680 - val_loss: 23.0634\n",
      "Epoch 264/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.6863 - val_loss: 23.0309\n",
      "Epoch 265/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.0208 - val_loss: 23.0713\n",
      "Epoch 266/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.3005 - val_loss: 22.9176\n",
      "Epoch 267/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.2841 - val_loss: 22.8396\n",
      "Epoch 268/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 24.3978 - val_loss: 22.9608\n",
      "Epoch 269/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.3734 - val_loss: 22.8200\n",
      "Epoch 270/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.2876 - val_loss: 22.8796\n",
      "Epoch 271/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.1628 - val_loss: 22.8243\n",
      "Epoch 272/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 23.7839 - val_loss: 22.5919\n",
      "Epoch 273/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.5959 - val_loss: 22.5556\n",
      "Epoch 274/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 23.9949 - val_loss: 22.6290\n",
      "Epoch 275/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 23.7078 - val_loss: 22.4753\n",
      "Epoch 276/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 23.9326 - val_loss: 22.4598\n",
      "Epoch 277/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 23.9879 - val_loss: 22.3597\n",
      "Epoch 278/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 24.1692 - val_loss: 22.4345\n",
      "Epoch 279/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 23.3549 - val_loss: 22.4108\n",
      "Epoch 280/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 23.8804 - val_loss: 22.3780\n",
      "Epoch 281/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 24.3942 - val_loss: 22.3197\n",
      "Epoch 282/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 23.5920 - val_loss: 22.2508\n",
      "Epoch 283/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 23.7797 - val_loss: 22.2852\n",
      "Epoch 284/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 23.7970 - val_loss: 22.3395\n",
      "Epoch 285/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 23.8867 - val_loss: 22.3739\n",
      "Epoch 286/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 24.2202 - val_loss: 22.2872\n",
      "Epoch 287/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 24.2671 - val_loss: 22.2806\n",
      "Epoch 288/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 23.2837 - val_loss: 22.1220\n",
      "Epoch 289/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 23.3247 - val_loss: 22.1228\n",
      "Epoch 290/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 23.6361 - val_loss: 22.0054\n",
      "Epoch 291/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 23.9966 - val_loss: 22.1613\n",
      "Epoch 292/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 23.5897 - val_loss: 22.0531\n",
      "Epoch 293/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 23.3354 - val_loss: 22.0580\n",
      "Epoch 294/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 23.0258 - val_loss: 21.9306\n",
      "Epoch 295/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 23.5072 - val_loss: 21.7947\n",
      "Epoch 296/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 23.2401 - val_loss: 21.7877\n",
      "Epoch 297/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 22.8090 - val_loss: 21.8297\n",
      "Epoch 298/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 23.6265 - val_loss: 21.7078\n",
      "Epoch 299/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 23.0111 - val_loss: 21.6912\n",
      "Epoch 300/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 22.6252 - val_loss: 21.6372\n",
      "Epoch 301/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 23.4319 - val_loss: 21.5814\n",
      "Epoch 302/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 23.1109 - val_loss: 21.6286\n",
      "Epoch 303/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 23.4935 - val_loss: 21.5406\n",
      "Epoch 304/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 23.1838 - val_loss: 21.4229\n",
      "Epoch 305/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 23.9380 - val_loss: 21.4357\n",
      "Epoch 306/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 23.8180 - val_loss: 21.4728\n",
      "Epoch 307/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 23.0780 - val_loss: 21.4955\n",
      "Epoch 308/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.8363 - val_loss: 21.4904\n",
      "Epoch 309/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 22.7954 - val_loss: 21.4044\n",
      "Epoch 310/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 23.0829 - val_loss: 21.3564\n",
      "Epoch 311/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 22.4504 - val_loss: 21.3156\n",
      "Epoch 312/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.8161 - val_loss: 21.2987\n",
      "Epoch 313/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 22.8460 - val_loss: 21.2177\n",
      "Epoch 314/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 22.6089 - val_loss: 21.7658\n",
      "Epoch 315/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 22.6511 - val_loss: 21.2108\n",
      "Epoch 316/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 22.5329 - val_loss: 21.0871\n",
      "Epoch 317/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 22.2667 - val_loss: 21.0545\n",
      "Epoch 318/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.3801 - val_loss: 21.0292\n",
      "Epoch 319/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 21.9702 - val_loss: 20.9795\n",
      "Epoch 320/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 22.5273 - val_loss: 20.9387\n",
      "Epoch 321/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 22.3589 - val_loss: 20.9978\n",
      "Epoch 322/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 21.8732 - val_loss: 20.9620\n",
      "Epoch 323/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 22.8771 - val_loss: 20.9504\n",
      "Epoch 324/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 22.3835 - val_loss: 20.8474\n",
      "Epoch 325/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.0950 - val_loss: 20.8846\n",
      "Epoch 326/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.5212 - val_loss: 20.9288\n",
      "Epoch 327/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.1913 - val_loss: 20.7562\n",
      "Epoch 328/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 21.9257 - val_loss: 20.8211\n",
      "Epoch 329/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.1086 - val_loss: 20.7726\n",
      "Epoch 330/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.4463 - val_loss: 20.6944\n",
      "Epoch 331/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 21.3799 - val_loss: 20.6463\n",
      "Epoch 332/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.8258 - val_loss: 20.6707\n",
      "Epoch 333/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 21.7094 - val_loss: 20.7274\n",
      "Epoch 334/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.1307 - val_loss: 20.6823\n",
      "Epoch 335/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 21.6279 - val_loss: 20.6901\n",
      "Epoch 336/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 22.1858 - val_loss: 20.6738\n",
      "Epoch 337/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 22.0244 - val_loss: 20.7066\n",
      "Epoch 338/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 21.4308 - val_loss: 20.6369\n",
      "Epoch 339/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 21.7685 - val_loss: 20.5239\n",
      "Epoch 340/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 21.9402 - val_loss: 20.4610\n",
      "Epoch 341/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.7977 - val_loss: 20.5890\n",
      "Epoch 342/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.6310 - val_loss: 20.5064\n",
      "Epoch 343/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.0971 - val_loss: 20.5226\n",
      "Epoch 344/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 22.1747 - val_loss: 20.3718\n",
      "Epoch 345/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 21.8487 - val_loss: 20.4272\n",
      "Epoch 346/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.9925 - val_loss: 20.2774\n",
      "Epoch 347/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.7330 - val_loss: 20.3558\n",
      "Epoch 348/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 21.3689 - val_loss: 20.3275\n",
      "Epoch 349/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.8740 - val_loss: 20.2455\n",
      "Epoch 350/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 21.9014 - val_loss: 20.1960\n",
      "Epoch 351/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 22.0215 - val_loss: 20.2096\n",
      "Epoch 352/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.4128 - val_loss: 20.1498\n",
      "Epoch 353/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.2862 - val_loss: 20.2570\n",
      "Epoch 354/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 21.3423 - val_loss: 20.1542\n",
      "Epoch 355/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 21.9508 - val_loss: 20.1357\n",
      "Epoch 356/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.3521 - val_loss: 20.1079\n",
      "Epoch 357/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.2772 - val_loss: 20.0493\n",
      "Epoch 358/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.4243 - val_loss: 20.0001\n",
      "Epoch 359/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.4399 - val_loss: 19.9044\n",
      "Epoch 360/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 21.4627 - val_loss: 19.9293\n",
      "Epoch 361/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.3265 - val_loss: 19.9565\n",
      "Epoch 362/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 21.0606 - val_loss: 20.0709\n",
      "Epoch 363/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.6595 - val_loss: 19.8849\n",
      "Epoch 364/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.8307 - val_loss: 19.7933\n",
      "Epoch 365/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 20.8726 - val_loss: 19.9569\n",
      "Epoch 366/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.6501 - val_loss: 19.9025\n",
      "Epoch 367/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 21.0335 - val_loss: 19.9229\n",
      "Epoch 368/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.8219 - val_loss: 19.7764\n",
      "Epoch 369/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.9747 - val_loss: 19.8014\n",
      "Epoch 370/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 21.2522 - val_loss: 19.7925\n",
      "Epoch 371/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 21.0386 - val_loss: 19.6896\n",
      "Epoch 372/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.9398 - val_loss: 19.6999\n",
      "Epoch 373/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.9245 - val_loss: 19.7006\n",
      "Epoch 374/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.9433 - val_loss: 19.8569\n",
      "Epoch 375/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 21.1166 - val_loss: 19.6970\n",
      "Epoch 376/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 21.1347 - val_loss: 19.6204\n",
      "Epoch 377/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 21.1359 - val_loss: 19.7047\n",
      "Epoch 378/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.4268 - val_loss: 19.7107\n",
      "Epoch 379/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.8349 - val_loss: 19.5442\n",
      "Epoch 380/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.6772 - val_loss: 19.6769\n",
      "Epoch 381/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.7827 - val_loss: 19.5125\n",
      "Epoch 382/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 20.7301 - val_loss: 19.4403\n",
      "Epoch 383/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.4896 - val_loss: 19.5588\n",
      "Epoch 384/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.6902 - val_loss: 19.4701\n",
      "Epoch 385/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.9094 - val_loss: 19.5487\n",
      "Epoch 386/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.6613 - val_loss: 19.5739\n",
      "Epoch 387/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.2719 - val_loss: 19.3845\n",
      "Epoch 388/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.8175 - val_loss: 19.4166\n",
      "Epoch 389/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 20.5128 - val_loss: 19.3354\n",
      "Epoch 390/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.5025 - val_loss: 19.4127\n",
      "Epoch 391/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.7791 - val_loss: 19.4654\n",
      "Epoch 392/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.8293 - val_loss: 19.2889\n",
      "Epoch 393/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.3854 - val_loss: 19.4757\n",
      "Epoch 394/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 21.1468 - val_loss: 19.3618\n",
      "Epoch 395/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 21.0269 - val_loss: 19.2537\n",
      "Epoch 396/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 20.9441 - val_loss: 19.2598\n",
      "Epoch 397/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 20.0880 - val_loss: 19.1773\n",
      "Epoch 398/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.3148 - val_loss: 19.1764\n",
      "Epoch 399/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.1187 - val_loss: 19.2110\n",
      "Epoch 400/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.1105 - val_loss: 19.1290\n",
      "Epoch 401/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.4623 - val_loss: 19.1285\n",
      "Epoch 402/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 19.9685 - val_loss: 19.0974\n",
      "Epoch 403/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.6541 - val_loss: 19.5121\n",
      "Epoch 404/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 20.4444 - val_loss: 19.0833\n",
      "Epoch 405/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.8129 - val_loss: 19.0566\n",
      "Epoch 406/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.8291 - val_loss: 19.0811\n",
      "Epoch 407/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.4267 - val_loss: 19.0490\n",
      "Epoch 408/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.0076 - val_loss: 18.9622\n",
      "Epoch 409/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.3439 - val_loss: 18.9675\n",
      "Epoch 410/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.6989 - val_loss: 18.9494\n",
      "Epoch 411/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.4667 - val_loss: 19.0694\n",
      "Epoch 412/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.9926 - val_loss: 18.9317\n",
      "Epoch 413/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.8901 - val_loss: 19.0090\n",
      "Epoch 414/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.0521 - val_loss: 18.9745\n",
      "Epoch 415/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 20.0954 - val_loss: 18.9994\n",
      "Epoch 416/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.6446 - val_loss: 18.9326\n",
      "Epoch 417/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.8893 - val_loss: 18.9123\n",
      "Epoch 418/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.1405 - val_loss: 18.9065\n",
      "Epoch 419/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.7987 - val_loss: 18.9378\n",
      "Epoch 420/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 20.0619 - val_loss: 18.8753\n",
      "Epoch 421/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.6838 - val_loss: 18.8892\n",
      "Epoch 422/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.4878 - val_loss: 18.8022\n",
      "Epoch 423/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.9527 - val_loss: 18.7867\n",
      "Epoch 424/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.8495 - val_loss: 18.7558\n",
      "Epoch 425/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.3796 - val_loss: 18.6726\n",
      "Epoch 426/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 19.5779 - val_loss: 18.7001\n",
      "Epoch 427/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 20.4156 - val_loss: 18.7869\n",
      "Epoch 428/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 20.0956 - val_loss: 18.6841\n",
      "Epoch 429/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 19.7210 - val_loss: 18.6961\n",
      "Epoch 430/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.6989 - val_loss: 18.5626\n",
      "Epoch 431/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.4525 - val_loss: 18.5557\n",
      "Epoch 432/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 19.8264 - val_loss: 18.5579\n",
      "Epoch 433/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.8263 - val_loss: 18.6378\n",
      "Epoch 434/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 19.5636 - val_loss: 18.5954\n",
      "Epoch 435/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.1718 - val_loss: 18.5012\n",
      "Epoch 436/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.7951 - val_loss: 18.5595\n",
      "Epoch 437/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.8006 - val_loss: 18.6281\n",
      "Epoch 438/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 18.9193 - val_loss: 18.5678\n",
      "Epoch 439/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.4659 - val_loss: 18.5987\n",
      "Epoch 440/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 20.0962 - val_loss: 18.6198\n",
      "Epoch 441/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 19.8643 - val_loss: 18.4364\n",
      "Epoch 442/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 19.7984 - val_loss: 18.5271\n",
      "Epoch 443/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 19.5943 - val_loss: 18.7329\n",
      "Epoch 444/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 18.9483 - val_loss: 18.3577\n",
      "Epoch 445/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.7083 - val_loss: 18.4594\n",
      "Epoch 446/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.5741 - val_loss: 18.3911\n",
      "Epoch 447/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.7662 - val_loss: 18.4002\n",
      "Epoch 448/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 19.3395 - val_loss: 18.2668\n",
      "Epoch 449/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 19.2823 - val_loss: 18.3235\n",
      "Epoch 450/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 19.6496 - val_loss: 18.3743\n",
      "Epoch 451/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 19.7874 - val_loss: 18.3778\n",
      "Epoch 452/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.3978 - val_loss: 18.3102\n",
      "Epoch 453/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 19.4935 - val_loss: 18.2800\n",
      "Epoch 454/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 18.9752 - val_loss: 18.3128\n",
      "Epoch 455/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 18.9713 - val_loss: 18.2232\n",
      "Epoch 456/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 19.2383 - val_loss: 18.2789\n",
      "Epoch 457/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 19.4094 - val_loss: 18.2057\n",
      "Epoch 458/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 19.3815 - val_loss: 18.3513\n",
      "Epoch 459/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 19.0509 - val_loss: 18.2962\n",
      "Epoch 460/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 19.3414 - val_loss: 18.2235\n",
      "Epoch 461/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 19.3174 - val_loss: 18.1499\n",
      "Epoch 462/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 18.9942 - val_loss: 18.2547\n",
      "Epoch 463/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 19.5306 - val_loss: 18.0907\n",
      "Epoch 464/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 19.3665 - val_loss: 18.0393\n",
      "Epoch 465/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 19.2651 - val_loss: 18.2014\n",
      "Epoch 466/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 19.2911 - val_loss: 18.0614\n",
      "Epoch 467/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.5758 - val_loss: 18.0994\n",
      "Epoch 468/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 19.2398 - val_loss: 18.0738\n",
      "Epoch 469/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 19.2470 - val_loss: 17.9963\n",
      "Epoch 470/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 19.7163 - val_loss: 18.0074\n",
      "Epoch 471/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 19.2055 - val_loss: 17.9425\n",
      "Epoch 472/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 19.0626 - val_loss: 17.9051\n",
      "Epoch 473/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 18.9145 - val_loss: 18.0752\n",
      "Epoch 474/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 19.4030 - val_loss: 18.0287\n",
      "Epoch 475/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 19.0833 - val_loss: 18.0596\n",
      "Epoch 476/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 19.1196 - val_loss: 18.0638\n",
      "Epoch 477/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 19.3573 - val_loss: 17.9588\n",
      "Epoch 478/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.9061 - val_loss: 18.0550\n",
      "Epoch 479/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 18.7094 - val_loss: 18.0513\n",
      "Epoch 480/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 18.5269 - val_loss: 18.1698\n",
      "Epoch 481/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 19.1083 - val_loss: 18.0563\n",
      "Epoch 482/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.8752 - val_loss: 17.9908\n",
      "Epoch 483/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 18.8889 - val_loss: 17.9014\n",
      "Epoch 484/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 18.9109 - val_loss: 17.9910\n",
      "Epoch 485/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 19.3426 - val_loss: 17.9024\n",
      "Epoch 486/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 18.7751 - val_loss: 17.9213\n",
      "Epoch 487/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 19.1490 - val_loss: 17.9595\n",
      "Epoch 488/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 19.3431 - val_loss: 17.9088\n",
      "Epoch 489/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 18.3385 - val_loss: 17.8568\n",
      "Epoch 490/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 18.8596 - val_loss: 17.7944\n",
      "Epoch 491/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 19.2593 - val_loss: 17.7791\n",
      "Epoch 492/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 18.7082 - val_loss: 17.8134\n",
      "Epoch 493/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 19.0560 - val_loss: 17.9631\n",
      "Epoch 494/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 18.7276 - val_loss: 17.7792\n",
      "Epoch 495/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 19.0922 - val_loss: 17.7862\n",
      "Epoch 496/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 19.4605 - val_loss: 17.9299\n",
      "Epoch 497/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.6048 - val_loss: 17.7531\n",
      "Epoch 498/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.8618 - val_loss: 17.9577\n",
      "Epoch 499/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.1893 - val_loss: 17.8185\n",
      "Epoch 500/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 19.0071 - val_loss: 17.7141\n",
      "Epoch 501/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.8216 - val_loss: 17.7767\n",
      "Epoch 502/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.7458 - val_loss: 17.9366\n",
      "Epoch 503/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.5651 - val_loss: 17.6642\n",
      "Epoch 504/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - loss: 18.2722 - val_loss: 17.7088\n",
      "Epoch 505/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 18.5840 - val_loss: 17.8256\n",
      "Epoch 506/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 18.6255 - val_loss: 17.8835\n",
      "Epoch 507/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 18.5980 - val_loss: 17.7357\n",
      "Epoch 508/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - loss: 19.2492 - val_loss: 17.7886\n",
      "Epoch 509/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 18.2997 - val_loss: 17.8235\n",
      "Epoch 510/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 18.8582 - val_loss: 17.6443\n",
      "Epoch 511/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 17.9305 - val_loss: 18.0105\n",
      "Epoch 512/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 18.2374 - val_loss: 17.6275\n",
      "Epoch 513/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 17.8015 - val_loss: 17.6242\n",
      "Epoch 514/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 18.6053 - val_loss: 17.5884\n",
      "Epoch 515/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 18.8117 - val_loss: 17.6234\n",
      "Epoch 516/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 18.4684 - val_loss: 17.6876\n",
      "Epoch 517/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 18.4028 - val_loss: 17.5657\n",
      "Epoch 518/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - loss: 18.4850 - val_loss: 17.5959\n",
      "Epoch 519/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 18.3711 - val_loss: 17.5772\n",
      "Epoch 520/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 18.7634 - val_loss: 17.6797\n",
      "Epoch 521/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - loss: 17.9909 - val_loss: 17.5208\n",
      "Epoch 522/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 18.3641 - val_loss: 17.6466\n",
      "Epoch 523/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 18.8148 - val_loss: 17.6183\n",
      "Epoch 524/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 19.2401 - val_loss: 17.5282\n",
      "Epoch 525/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 18.2814 - val_loss: 17.6609\n",
      "Epoch 526/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.7249 - val_loss: 17.5415\n",
      "Epoch 527/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 17.4832 - val_loss: 17.4687\n",
      "Epoch 528/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 18.7847 - val_loss: 17.4235\n",
      "Epoch 529/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 18.2055 - val_loss: 17.4147\n",
      "Epoch 530/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 18.3695 - val_loss: 17.5191\n",
      "Epoch 531/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 17.6937 - val_loss: 17.6749\n",
      "Epoch 532/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.0320 - val_loss: 17.6286\n",
      "Epoch 533/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.4510 - val_loss: 17.5130\n",
      "Epoch 534/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 17.7480 - val_loss: 18.1104\n",
      "Epoch 535/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 17.9926 - val_loss: 17.5583\n",
      "Epoch 536/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 18.2107 - val_loss: 17.3276\n",
      "Epoch 537/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 18.4142 - val_loss: 17.2820\n",
      "Epoch 538/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 17.6783 - val_loss: 17.3238\n",
      "Epoch 539/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 18.9478 - val_loss: 17.2284\n",
      "Epoch 540/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 19.4401 - val_loss: 17.3604\n",
      "Epoch 541/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 18.6101 - val_loss: 17.3890\n",
      "Epoch 542/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 18.6466 - val_loss: 17.3792\n",
      "Epoch 543/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 18.5655 - val_loss: 17.4588\n",
      "Epoch 544/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 18.0069 - val_loss: 17.5058\n",
      "Epoch 545/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 18.2198 - val_loss: 17.3535\n",
      "Epoch 546/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 18.4279 - val_loss: 17.4714\n",
      "Epoch 547/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 18.0040 - val_loss: 17.3882\n",
      "Epoch 548/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 18.3236 - val_loss: 17.4580\n",
      "Epoch 549/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 17.7572 - val_loss: 17.5233\n",
      "Epoch 550/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 18.0610 - val_loss: 17.3425\n",
      "Epoch 551/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - loss: 18.6785 - val_loss: 17.3274\n",
      "Epoch 552/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 17.5938 - val_loss: 17.3122\n",
      "Epoch 553/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - loss: 18.1569 - val_loss: 17.4202\n",
      "Epoch 554/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 18.5633 - val_loss: 17.3173\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, [y_train , y_train  ],  #pairTrain[:, 1]\n",
    "  validation_data = ( x_val, [y_val , y_val  ] ),  #pairVal[:, 1]\n",
    "    epochs=2000\n",
    "    ,batch_size = 64    #8192\n",
    "  , callbacks=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 276)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.argmax(y_pred[0][344])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_T = []\n",
    "for i in range(1520):\n",
    "    pred_T.append(np.argmax(y_pred[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_S = []\n",
    "for i in range(1520):\n",
    "    pred_S.append(np.argmax(y_pred[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8546052631578948"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_T = accuracy_score(y_test, pred_T)\n",
    "accuracy_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6072368421052632"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_S = accuracy_score(y_test, pred_S)\n",
    "accuracy_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m x_b_S \u001b[39m=\u001b[39m Conv1D(kernel_size \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m, filters\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbottle_S\u001b[39m\u001b[39m'\u001b[39m)(x_S) \u001b[39m# 42, 32\u001b[39;00m\n\u001b[1;32m     38\u001b[0m x_b2 \u001b[39m=\u001b[39m BatchNormalization()(x_b_S) \u001b[39m#old\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m at1 \u001b[39m=\u001b[39m spatial_mlp_attention(shape \u001b[39m=\u001b[39;49m (\u001b[39m42\u001b[39;49m, \u001b[39m32\u001b[39;49m), kernel \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m)\n\u001b[1;32m     41\u001b[0m x_a1 \u001b[39m=\u001b[39m at1(x_b2) \n\u001b[1;32m     42\u001b[0m x_S \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMultiply()([x_a1, x_b2])\n",
      "Cell \u001b[0;32mIn[61], line 7\u001b[0m, in \u001b[0;36mspatial_mlp_attention\u001b[0;34m(shape, kernel)\u001b[0m\n\u001b[1;32m      5\u001b[0m w, C \u001b[39m=\u001b[39m shape[\u001b[39m0\u001b[39m], shape[\u001b[39m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[39m# Reshape the feature map to (w * h, C)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m reshaped_feature_map \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(inp, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, C])\n\u001b[1;32m      8\u001b[0m \u001b[39m# Define the MLP architecture\u001b[39;00m\n\u001b[1;32m      9\u001b[0m hidden_units \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ops\u001b[39m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[39m=\u001b[39m signature\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[39m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/common/keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__tf_tensor__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mused when constructing Keras Functional models \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand `keras.operations`). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are likely doing something like:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m```\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mx = Input(...)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtf_fn(x)  # Invalid.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m```\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m```\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mclass MyLayer(Layer):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m    def call(self, x):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m        return tf_fn(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mx = MyLayer()(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m```\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "def spatial_mlp_attention(shape, kernel):\n",
    "  inp = Input(shape=shape, name='input_image')\n",
    "\n",
    "  # CHANNEL SUMMARIZATION\n",
    "  w, C = shape[0], shape[1]\n",
    "  # Reshape the feature map to (w * h, C)\n",
    "  reshaped_feature_map = tf.reshape(inp, [-1, C])\n",
    "  # Define the MLP architecture\n",
    "  hidden_units = 32\n",
    "  output_units = 1\n",
    "  mlp = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "      Dropout(0.4),\n",
    "      #BatchNormalization(),\n",
    "      tf.keras.layers.Dense(output_units, activation='sigmoid')\n",
    "  ])\n",
    "  # Apply the MLP on the reshaped feature map\n",
    "  output_mlp = mlp(reshaped_feature_map)\n",
    "  reshaped_output = tf.reshape(output_mlp, ( -1, w, 1 ))\n",
    "  # CONV & ACTIVATION\n",
    "  out = Conv1D(1, kernel_size = (kernel), activation='relu', padding = 'same')( reshaped_output )\n",
    "\n",
    "  model = Model(inputs = [inp], outputs = out)  # , name = 'spatial_attention'\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "\n",
    "dataInp = Input(shape = (200, 6))\n",
    "kernel_size = 3\n",
    "x_S = Conv1D(kernel_size = 7, filters=32, activation='relu', name='l1_S')(dataInp) # 194, 32\n",
    "x_S = BatchNormalization()(x_S) #old\n",
    "x_S = MaxPooling1D(2)(x_S) # 97, 32\n",
    "\n",
    "x_S1 = Conv1D(kernel_size = 4, filters=64, activation='relu', name='l2_S')(x_S) #94, 64\n",
    "x_S = MaxPooling1D(2)(x_S1) #47, 64\n",
    "\n",
    "x_b_S = Conv1D(kernel_size = 6, filters=32, activation='relu', name='bottle_S')(x_S) # 42, 32\n",
    "x_b2 = BatchNormalization()(x_b_S) #old\n",
    "\n",
    "at1 = spatial_mlp_attention(shape = (42, 32), kernel = 5)\n",
    "x_a1 = at1(x_b2) \n",
    "x_S = tf.keras.layers.Multiply()([x_a1, x_b2])\n",
    "# x_T = tf.multiply([x_a1, x_b2])\n",
    "x_S = Add()([x_S, x_b_S])\n",
    "\n",
    "# x_S = Conv1D(kernel_size = 3, filters=64, activation='relu', name='l3_s')(x_b_S)\n",
    "x_S3 = Conv1D(kernel_size = 3, filters=128, activation='relu', name='l4_S')(x_S) #40, 128\n",
    "x_b3 = BatchNormalization()(x_S3) \n",
    "\n",
    "at2 = spatial_mlp_attention(shape = (40, 128), kernel = 5)\n",
    "x_a2 = at2(x_b3) \n",
    "x_S = tf.keras.layers.Multiply()([x_a2, x_b3])\n",
    "# x_T = tf.multiply([x_a1, x_b2])\n",
    "x_S = Add()([x_S, x_S3])\n",
    "\n",
    "x_S = Conv1D(kernel_size = 3, filters=256, activation='relu', name='l5_S')(x_S)\n",
    "avg_S = GlobalAveragePooling1D()(x_S)\n",
    "max_S = GlobalMaxPooling1D()(x_S)\n",
    "flat_S = Add()([avg_S, max_S])\n",
    "# flat_S = Flatten()(x_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_1')(flat_S)\n",
    "d_S = Dropout(0.3)(flat_S)\n",
    "# d_S = Dense(300, activation = 'relu', name='dense_2')(d_S)\n",
    "out_S = Dense(276, activation = 'sigmoid', name='out_S')(d_S)\n",
    "model_s = Model(inputs=dataInp, outputs=[x_b_S, out_S], name='student' )\n",
    "model_s.compile(loss = [\"mean_squared_error\", \"binary_crossentropy\" ], optimizer = 'adam',loss_weights = [1, 10], metrics=[\"accuracy\"])\n",
    "# print(model_s.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Models Testing (teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"teacher_improved_params\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"teacher_improved_params\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_32          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,060</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_33          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,720</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_34          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,830</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_35          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,840</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_36          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4560</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">684,150</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,876</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_77 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_32          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │           \u001b[38;5;34m120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_53 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_78 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m60\u001b[0m)         │         \u001b[38;5;34m9,060\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_33          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m, \u001b[38;5;34m60\u001b[0m)         │           \u001b[38;5;34m240\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_54 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m60\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_79 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m120\u001b[0m)        │        \u001b[38;5;34m21,720\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_34          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m, \u001b[38;5;34m120\u001b[0m)        │           \u001b[38;5;34m480\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_80 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │        \u001b[38;5;34m10,830\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_35          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │           \u001b[38;5;34m120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_55 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m30\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_81 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m240\u001b[0m)        │        \u001b[38;5;34m21,840\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_36          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m240\u001b[0m)        │           \u001b[38;5;34m960\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_16 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4560\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │       \u001b[38;5;34m684,150\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │        \u001b[38;5;34m22,650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m15,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)            │        \u001b[38;5;34m27,876\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">816,436</span> (3.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m816,436\u001b[0m (3.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">815,476</span> (3.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m815,476\u001b[0m (3.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input layer\n",
    "dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# TEACHER MODEL with fine-tuned parameters targeting ~800k\n",
    "x_T = Conv1D(kernel_size=7, filters=30, activation='relu')(dataInp)  # Filters set to 30\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T)  # 97, 30\n",
    "\n",
    "x_T = Conv1D(kernel_size=5, filters=60, activation='relu')(x_T)  # Filters set to 60\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T)  # 46, 60\n",
    "\n",
    "x_T = Conv1D(kernel_size=3, filters=120, activation='relu')(x_T)  # Filters set to 120\n",
    "x_T = BatchNormalization()(x_T)\n",
    "\n",
    "x_b_T = Conv1D(kernel_size=3, filters=30, activation='relu')(x_T)  # Filters set to 30\n",
    "x_b_T = BatchNormalization()(x_b_T)\n",
    "x_T = MaxPooling1D(2)(x_b_T)  # 21, 30\n",
    "\n",
    "x_T = Conv1D(kernel_size=3, filters=240, activation='relu')(x_T)  # Filters set to 240\n",
    "x_T = BatchNormalization()(x_T)\n",
    "\n",
    "flat_T = Flatten()(x_T)\n",
    "d_T = Dense(150, activation='relu')(flat_T)  # Dense layer size set to 150\n",
    "d_T = Dropout(0.4)(d_T)  # Slightly increased dropout rate\n",
    "d_T = Dense(150, activation='relu')(d_T)  # Dense layer size set to 150\n",
    "d_T = Dropout(0.4)(d_T)  # Slightly increased dropout rate\n",
    "d_T = Dense(100, activation='relu')(d_T)  # Added another dense layer\n",
    "\n",
    "out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "# Create and compile the model\n",
    "model_800k = Model(inputs=dataInp, outputs=out_T, name='teacher_improved_params')\n",
    "model_800k.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Summary of the model\n",
    "model_800k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, BatchNormalization, Dense, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"teacher_gru\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"teacher_gru\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_21      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> │ input_layer_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,816</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">74,496</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ gru_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ gru_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,752</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ gru_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">141,588</span> │ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_21      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │      \u001b[38;5;34m3,840\u001b[0m │ input_layer_21[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m18,816\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m74,496\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ gru_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m296,448\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ gru_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_4 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m394,752\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ gru_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ gru_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │    \u001b[38;5;34m141,588\u001b[0m │ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">931,860</span> (3.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m931,860\u001b[0m (3.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">930,900</span> (3.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m930,900\u001b[0m (3.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# TEACHER GRU MODEL\n",
    "x_T = GRU(32, return_sequences=True)(dataInp) # Output shape: (200, 32)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(64, return_sequences=True)(x_T) # Output shape: (200, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(128, return_sequences=True)(x_T) # Output shape: (200, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(256, return_sequences=True)(x_T) # Output shape: (200, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(256, return_sequences=True)(x_T) # Output shape: (200, 256)\n",
    "\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "\n",
    "d_T = Dropout(0.3)(flat_T)\n",
    "out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "model_GRU = Model(inputs=dataInp, outputs=out_T, name='teacher_gru')\n",
    "model_GRU.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"teacher_rnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"teacher_rnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_23      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,992</span> │ input_layer_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">141,588</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_23      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │      \u001b[38;5;34m4,992\u001b[0m │ input_layer_23[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m24,832\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m98,816\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m394,240\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │    \u001b[38;5;34m141,588\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">797,460</span> (3.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m797,460\u001b[0m (3.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">796,756</span> (3.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m796,756\u001b[0m (3.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# TEACHER RNN MODEL\n",
    "x_T = LSTM(32, return_sequences=True)(dataInp) # Output shape: (200, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(64, return_sequences=True)(x_T) # Output shape: (200, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(128, return_sequences=True)(x_T) # Output shape: (200, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(128, return_sequences=True)(x_T) # Output shape: (200, 256)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(256, return_sequences=True)(x_T) # Output shape: (200, 512)\n",
    "\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "\n",
    "d_T = Dropout(0.3)(flat_T)\n",
    "out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "model_lstm = Model(inputs=dataInp, outputs=out_T, name='teacher_rnn')\n",
    "model_lstm.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"teacher_bilstm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"teacher_bilstm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,984</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ bidirectional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ bidirectional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">276</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">282,900</span> │ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m6\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │      \u001b[38;5;34m9,984\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m263,168\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ bidirectional_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m394,240\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ bidirectional_3[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │  \u001b[38;5;34m1,050,624\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_4[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_4[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ concatenate_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m276\u001b[0m)       │    \u001b[38;5;34m282,900\u001b[0m │ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,069,780</span> (7.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,069,780\u001b[0m (7.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,068,372</span> (7.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,068,372\u001b[0m (7.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataInp = Input(shape=(200, 6))\n",
    "\n",
    "# TEACHER Bi-LSTM MODEL\n",
    "x_T = Bidirectional(LSTM(32, return_sequences=True))(dataInp) # Output shape: (200, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(64, return_sequences=True))(x_T) # Output shape: (200, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(128, return_sequences=True))(x_T) # Output shape: (200, 256)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(128, return_sequences=True))(x_T) # Output shape: (200, 256)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(256, return_sequences=True))(x_T) # Output shape: (200, 512)\n",
    "\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "\n",
    "d_T = Dropout(0.3)(flat_T)\n",
    "out_T = Dense(276, activation='sigmoid')(d_T)\n",
    "\n",
    "model_bilstm = Model(inputs=dataInp, outputs=out_T, name='teacher_bilstm')\n",
    "model_bilstm.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.9833 - loss: 0.0731 - val_accuracy: 0.5730 - val_loss: 2.0443\n",
      "Epoch 2/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.9798 - loss: 0.0848 - val_accuracy: 0.4930 - val_loss: 2.4677\n",
      "Epoch 3/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.9867 - loss: 0.0604 - val_accuracy: 0.3580 - val_loss: 4.7558\n",
      "Epoch 4/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 3s/step - accuracy: 0.9782 - loss: 0.0729 - val_accuracy: 0.8780 - val_loss: 0.4167\n",
      "Epoch 5/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - accuracy: 0.9906 - loss: 0.0475 - val_accuracy: 0.8860 - val_loss: 0.3956\n",
      "Epoch 6/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.9912 - loss: 0.0453 - val_accuracy: 0.7720 - val_loss: 0.7531\n",
      "Epoch 7/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.9912 - loss: 0.0447 - val_accuracy: 0.5340 - val_loss: 2.8575\n",
      "Epoch 8/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - accuracy: 0.9907 - loss: 0.0393 - val_accuracy: 0.7540 - val_loss: 0.8793\n",
      "Epoch 9/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.9888 - loss: 0.0439 - val_accuracy: 0.6770 - val_loss: 1.3350\n",
      "Epoch 10/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.9955 - loss: 0.0266 - val_accuracy: 0.8830 - val_loss: 0.4078\n",
      "Epoch 11/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.9970 - loss: 0.0234 - val_accuracy: 0.9430 - val_loss: 0.1962\n",
      "Epoch 12/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.9912 - loss: 0.0335 - val_accuracy: 0.8660 - val_loss: 0.5227\n",
      "Epoch 13/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.9873 - loss: 0.0443 - val_accuracy: 0.7370 - val_loss: 1.4591\n",
      "Epoch 14/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.9858 - loss: 0.0412 - val_accuracy: 0.5090 - val_loss: 2.4374\n",
      "Epoch 15/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 0.9915 - loss: 0.0374 - val_accuracy: 0.7540 - val_loss: 1.1710\n",
      "Epoch 16/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - accuracy: 0.9912 - loss: 0.0401 - val_accuracy: 0.8810 - val_loss: 0.4157\n",
      "Epoch 17/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - accuracy: 0.9916 - loss: 0.0376 - val_accuracy: 0.9150 - val_loss: 0.2874\n",
      "Epoch 18/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.9955 - loss: 0.0240 - val_accuracy: 0.8960 - val_loss: 0.3442\n",
      "Epoch 19/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - accuracy: 0.9989 - loss: 0.0190 - val_accuracy: 0.9130 - val_loss: 0.2741\n",
      "Epoch 20/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.9905 - loss: 0.0352 - val_accuracy: 0.8950 - val_loss: 0.3553\n",
      "Epoch 21/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.9781 - loss: 0.0881 - val_accuracy: 0.8960 - val_loss: 0.3637\n",
      "Epoch 22/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.9796 - loss: 0.0748 - val_accuracy: 0.6950 - val_loss: 1.5717\n",
      "Epoch 23/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.9762 - loss: 0.0835 - val_accuracy: 0.4990 - val_loss: 4.9288\n",
      "Epoch 24/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - accuracy: 0.9615 - loss: 0.1319 - val_accuracy: 0.7780 - val_loss: 0.9013\n",
      "Epoch 25/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.9738 - loss: 0.1160 - val_accuracy: 0.6310 - val_loss: 1.5931\n",
      "Epoch 26/2000\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 0.9536 - loss: 0.1614 - val_accuracy: 0.1840 - val_loss: 9.6968\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = model_bilstm.fit(\n",
    "    np.array(x_train), np.array(y_train),  #pairTrain[:, 1]\n",
    "  validation_data = (np.array(x_val), np.array(y_val) ),  #pairVal[:, 1]\n",
    "    epochs=2000\n",
    "    ,batch_size = 64    #8192\n",
    "  , callbacks=callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 207ms/step - accuracy: 0.9341 - loss: 0.2169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18415102362632751, 0.9434210658073425]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bilstm.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
