{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Reshape,  GlobalAveragePooling2D, Masking, Input, MaxPooling1D, GlobalMaxPooling2D, Add, Dropout, BatchNormalization, UpSampling1D, Lambda, Conv2D,Concatenate,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import models, layers, regularizers, Input\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Loading\n",
    "x_train = np.load('x_train.npy')\n",
    "x_val = np.load('x_val.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_val = np.load('y_val.npy')\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "x_train shape: (1800, 1601, 6, 1)\n",
      "x_val shape: (300, 1601, 6, 1)\n",
      "x_test shape: (900, 1601, 6, 1)\n",
      "y_train shape: (1800,)\n",
      "y_val shape: (300,)\n",
      "y_test shape: (900,)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes (optional)\n",
    "print(\"Shapes:\")\n",
    "print(\"x_train shape:\", np.shape(x_train))\n",
    "print(\"x_val shape:\", np.shape(x_val))\n",
    "print(\"x_test shape:\", np.shape(x_test))\n",
    "print(\"y_train shape:\", np.shape(y_train))\n",
    "print(\"y_val shape:\", np.shape(y_val))\n",
    "print(\"y_test shape:\", np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_train, x_val, and x_test are your input data\n",
    "x_train = x_train.squeeze(axis=-1)  # Shape will become (1800, 1601, 6)\n",
    "x_val = x_val.squeeze(axis=-1)      # Shape will become (300, 1601, 6)\n",
    "x_test = x_test.squeeze(axis=-1)    # Shape will become (900, 1601, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "x_train shape: (1800, 1601, 6)\n",
      "x_val shape: (300, 1601, 6)\n",
      "x_test shape: (900, 1601, 6)\n",
      "y_train shape: (1800,)\n",
      "y_val shape: (300,)\n",
      "y_test shape: (900,)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes (optional)\n",
    "print(\"Shapes:\")\n",
    "print(\"x_train shape:\", np.shape(x_train))\n",
    "print(\"x_val shape:\", np.shape(x_val))\n",
    "print(\"x_test shape:\", np.shape(x_test))\n",
    "print(\"y_train shape:\", np.shape(y_train))\n",
    "print(\"y_val shape:\", np.shape(y_val))\n",
    "print(\"y_test shape:\", np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes in y_train: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming y_train is a numpy array\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Number of unique classes in y_train: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(1601, 60, 1))\n",
    "#x = layers.AveragePooling2D(pool_size=(8, 1))(inputs)\n",
    "\n",
    "# Convolutional Layer 1\n",
    "x = layers.Conv2D(16, (5,5), activation='relu', padding='same')(inputs) #\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "\n",
    "# Convolutional Layer 2\n",
    "x = layers.Conv2D(32, (5, 5), activation='relu', padding='same')(x) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x_b_T = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x_b_T) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x_b_T = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x_b_T) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "\n",
    "# Dropout Layer\n",
    "# Flatten the output to feed into fully connected layer\n",
    "f = layers.Flatten()(x)\n",
    "f = layers.Dropout(0.5)(f)\n",
    "\n",
    "d1 = layers.Dense(128, activation='relu')(f)\n",
    "d1 = layers.Dropout(0.5)(d1)\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(6, activation='sigmoid')(d1)  # Output layer with softmax activation for 6 classes\n",
    "\n",
    "\n",
    "# Create the model\n",
    "comsnet_model = models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(1601, 6, 1))\n",
    "#x = layers.AveragePooling2D(pool_size=(8, 1))(inputs)\n",
    "\n",
    "# Convolutional Layer 1\n",
    "x = layers.Conv2D(3, (3, 3), activation='relu', padding='same')(inputs) #\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Convolutional Layer 2\n",
    "x = layers.Conv2D(3, (3, 3), activation='relu', padding='same')(x) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x_b_T = layers.AveragePooling2D(pool_size=(2,1))(x)\n",
    "\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2,1))(x)\n",
    "\n",
    "# Dropout Layer\n",
    "# Flatten the output to feed into fully connected layer\n",
    "f = layers.Flatten()(x)\n",
    "f = layers.Dropout(0.5)(f)\n",
    "\n",
    "d1 = layers.Dense(128, activation='relu')(f)\n",
    "d1 = layers.Dropout(0.5)(d1)\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(6, activation='sigmoid')(d1)  # Output layer with softmax activation for 276 classes\n",
    "\n",
    "\n",
    "# Create the model\n",
    "comsnet_model = models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(1601, 6, 1))\n",
    "\n",
    "# Convolutional Layer 1\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Convolutional Layer 2\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Convolutional Layer 3\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 1))(x)\n",
    "\n",
    "# Convolutional Layer 4\n",
    "# x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.MaxPooling2D(pool_size=(2, 1))(x)\n",
    "\n",
    "# Flatten the output\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Fully Connected Layer\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output Layer\n",
    "outputs = layers.Dense(6, activation='softmax')(x)  # Adjust the number of units according to your classes\n",
    "\n",
    "# Create the model\n",
    "comsnet_model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary to check the number of parameters\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(1601, 6, 1))\n",
    "#x = layers.AveragePooling2D(pool_size=(8, 1))(inputs)\n",
    "\n",
    "# Convolutional Layer 1\n",
    "x = layers.Conv2D(3, (3, 3), activation='relu', padding='same')(inputs) #\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.AveragePooling2D(pool_size=(3, 2))(x)\n",
    "\n",
    "# Convolutional Layer 2\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.AveragePooling2D(pool_size=(3, 2))(x)\n",
    "\n",
    "\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2, 1))(x)\n",
    "\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x_b_T = layers.AveragePooling2D(pool_size=(2, 1), name='bottleneck')(x)\n",
    "\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x_b_T) #, padding='same'\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2, 1))(x)\n",
    "# Dropout Layer\n",
    "# Flatten the output to feed into fully connected layer\n",
    "f = layers.Flatten()(x)\n",
    "f = layers.Dropout(0.5)(f)\n",
    "\n",
    "d1 = layers.Dense(128, activation='relu')(f)\n",
    "d1 = layers.Dropout(0.5)(d1)\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(6, activation='sigmoid')(d1)  # Output layer with softmax activation for 276 classes\n",
    "\n",
    "\n",
    "# Create the model\n",
    "comsnet_model = models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_85\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_96 (InputLayer)       [(None, 1601, 6, 1)]      0         \n",
      "                                                                 \n",
      " conv2d_393 (Conv2D)         (None, 1601, 6, 3)        30        \n",
      "                                                                 \n",
      " batch_normalization_332 (B  (None, 1601, 6, 3)        12        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_147 (Ave  (None, 533, 3, 3)         0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " conv2d_394 (Conv2D)         (None, 533, 3, 8)         224       \n",
      "                                                                 \n",
      " batch_normalization_333 (B  (None, 533, 3, 8)         32        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_148 (Ave  (None, 177, 1, 8)         0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " conv2d_395 (Conv2D)         (None, 177, 1, 16)        1168      \n",
      "                                                                 \n",
      " batch_normalization_334 (B  (None, 177, 1, 16)        64        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_149 (Ave  (None, 88, 1, 16)         0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " conv2d_396 (Conv2D)         (None, 88, 1, 32)         4640      \n",
      "                                                                 \n",
      " batch_normalization_335 (B  (None, 88, 1, 32)         128       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " bottleneck (AveragePooling  (None, 44, 1, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_397 (Conv2D)         (None, 44, 1, 64)         18496     \n",
      "                                                                 \n",
      " batch_normalization_336 (B  (None, 44, 1, 64)         256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_150 (Ave  (None, 22, 1, 64)         0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " flatten_83 (Flatten)        (None, 1408)              0         \n",
      "                                                                 \n",
      " dropout_108 (Dropout)       (None, 1408)              0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 128)               180352    \n",
      "                                                                 \n",
      " dropout_109 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206176 (805.38 KB)\n",
      "Trainable params: 205930 (804.41 KB)\n",
      "Non-trainable params: 246 (984.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "comsnet_model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "comsnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 2.3884 - accuracy: 0.3478 - val_loss: 3.2333 - val_accuracy: 0.4167\n",
      "Epoch 2/2000\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 1.0934 - accuracy: 0.5711 - val_loss: 1.8718 - val_accuracy: 0.4200\n",
      "Epoch 3/2000\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.8438 - accuracy: 0.6644 - val_loss: 1.6693 - val_accuracy: 0.4267\n",
      "Epoch 4/2000\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.7353 - accuracy: 0.7083 - val_loss: 1.4631 - val_accuracy: 0.4833\n",
      "Epoch 5/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.6619 - accuracy: 0.7328 - val_loss: 1.2498 - val_accuracy: 0.5400\n",
      "Epoch 6/2000\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.6280 - accuracy: 0.7444 - val_loss: 0.9219 - val_accuracy: 0.6267\n",
      "Epoch 7/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.5865 - accuracy: 0.7711 - val_loss: 0.6834 - val_accuracy: 0.7333\n",
      "Epoch 8/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.5625 - accuracy: 0.7878 - val_loss: 0.6045 - val_accuracy: 0.7567\n",
      "Epoch 9/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.5316 - accuracy: 0.7906 - val_loss: 0.5108 - val_accuracy: 0.7967\n",
      "Epoch 10/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.5467 - accuracy: 0.7883 - val_loss: 0.4923 - val_accuracy: 0.8100\n",
      "Epoch 11/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.5146 - accuracy: 0.7994 - val_loss: 0.5125 - val_accuracy: 0.7933\n",
      "Epoch 12/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.5166 - accuracy: 0.8083 - val_loss: 0.5146 - val_accuracy: 0.8000\n",
      "Epoch 13/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.4814 - accuracy: 0.8122 - val_loss: 0.4822 - val_accuracy: 0.8067\n",
      "Epoch 14/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.4529 - accuracy: 0.8217 - val_loss: 0.4988 - val_accuracy: 0.8067\n",
      "Epoch 15/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.4753 - accuracy: 0.8122 - val_loss: 0.4875 - val_accuracy: 0.8133\n",
      "Epoch 16/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.4554 - accuracy: 0.8239 - val_loss: 0.5134 - val_accuracy: 0.8033\n",
      "Epoch 17/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.4485 - accuracy: 0.8233 - val_loss: 0.4787 - val_accuracy: 0.8133\n",
      "Epoch 18/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.4590 - accuracy: 0.8183 - val_loss: 0.4417 - val_accuracy: 0.8233\n",
      "Epoch 19/2000\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.4552 - accuracy: 0.8156 - val_loss: 0.5394 - val_accuracy: 0.7900\n",
      "Epoch 20/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.4390 - accuracy: 0.8339 - val_loss: 0.4307 - val_accuracy: 0.8267\n",
      "Epoch 21/2000\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.4238 - accuracy: 0.8350 - val_loss: 0.4340 - val_accuracy: 0.8300\n",
      "Epoch 22/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.4072 - accuracy: 0.8378 - val_loss: 0.4326 - val_accuracy: 0.8133\n",
      "Epoch 23/2000\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.4186 - accuracy: 0.8389 - val_loss: 0.4978 - val_accuracy: 0.8067\n",
      "Epoch 24/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.3868 - accuracy: 0.8489 - val_loss: 0.4879 - val_accuracy: 0.8100\n",
      "Epoch 25/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.3997 - accuracy: 0.8528 - val_loss: 0.4975 - val_accuracy: 0.8033\n",
      "Epoch 26/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.3727 - accuracy: 0.8450 - val_loss: 0.4610 - val_accuracy: 0.8100\n",
      "Epoch 27/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.3823 - accuracy: 0.8539 - val_loss: 0.4972 - val_accuracy: 0.8000\n",
      "Epoch 28/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.3866 - accuracy: 0.8467 - val_loss: 0.5220 - val_accuracy: 0.8100\n",
      "Epoch 29/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.3554 - accuracy: 0.8617 - val_loss: 0.4044 - val_accuracy: 0.8400\n",
      "Epoch 30/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.3549 - accuracy: 0.8583 - val_loss: 0.4095 - val_accuracy: 0.8200\n",
      "Epoch 31/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.3305 - accuracy: 0.8639 - val_loss: 0.4036 - val_accuracy: 0.8367\n",
      "Epoch 32/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.3537 - accuracy: 0.8578 - val_loss: 0.3627 - val_accuracy: 0.8600\n",
      "Epoch 33/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.3165 - accuracy: 0.8811 - val_loss: 0.4773 - val_accuracy: 0.8067\n",
      "Epoch 34/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.3191 - accuracy: 0.8778 - val_loss: 0.4359 - val_accuracy: 0.8167\n",
      "Epoch 35/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.3103 - accuracy: 0.8856 - val_loss: 0.3641 - val_accuracy: 0.8533\n",
      "Epoch 36/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.3144 - accuracy: 0.8744 - val_loss: 0.3900 - val_accuracy: 0.8367\n",
      "Epoch 37/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.2989 - accuracy: 0.8933 - val_loss: 0.3598 - val_accuracy: 0.8600\n",
      "Epoch 38/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2932 - accuracy: 0.8856 - val_loss: 0.3086 - val_accuracy: 0.8867\n",
      "Epoch 39/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2644 - accuracy: 0.9011 - val_loss: 0.3663 - val_accuracy: 0.8667\n",
      "Epoch 40/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2685 - accuracy: 0.8961 - val_loss: 0.3233 - val_accuracy: 0.8800\n",
      "Epoch 41/2000\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.2594 - accuracy: 0.8983 - val_loss: 0.2991 - val_accuracy: 0.8700\n",
      "Epoch 42/2000\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.2563 - accuracy: 0.8994 - val_loss: 0.3245 - val_accuracy: 0.8667\n",
      "Epoch 43/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2522 - accuracy: 0.9000 - val_loss: 0.2884 - val_accuracy: 0.8900\n",
      "Epoch 44/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2504 - accuracy: 0.8961 - val_loss: 0.3206 - val_accuracy: 0.8733\n",
      "Epoch 45/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2338 - accuracy: 0.9089 - val_loss: 0.3927 - val_accuracy: 0.8567\n",
      "Epoch 46/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2340 - accuracy: 0.9094 - val_loss: 0.2524 - val_accuracy: 0.9000\n",
      "Epoch 47/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.2447 - accuracy: 0.9028 - val_loss: 0.3178 - val_accuracy: 0.8633\n",
      "Epoch 48/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2285 - accuracy: 0.9128 - val_loss: 0.2862 - val_accuracy: 0.8800\n",
      "Epoch 49/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2369 - accuracy: 0.9050 - val_loss: 0.2729 - val_accuracy: 0.8900\n",
      "Epoch 50/2000\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.2243 - accuracy: 0.9144 - val_loss: 0.2659 - val_accuracy: 0.8967\n",
      "Epoch 51/2000\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.2182 - accuracy: 0.9156 - val_loss: 0.2788 - val_accuracy: 0.8900\n",
      "Epoch 52/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2330 - accuracy: 0.9044 - val_loss: 0.2858 - val_accuracy: 0.8767\n",
      "Epoch 53/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.2206 - accuracy: 0.9106 - val_loss: 0.2979 - val_accuracy: 0.8900\n",
      "Epoch 54/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2142 - accuracy: 0.9144 - val_loss: 0.2246 - val_accuracy: 0.9100\n",
      "Epoch 55/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.2096 - accuracy: 0.9167 - val_loss: 0.2476 - val_accuracy: 0.8900\n",
      "Epoch 56/2000\n",
      "57/57 [==============================] - 2s 37ms/step - loss: 0.1867 - accuracy: 0.9339 - val_loss: 0.2802 - val_accuracy: 0.8767\n",
      "Epoch 57/2000\n",
      "57/57 [==============================] - 2s 33ms/step - loss: 0.1891 - accuracy: 0.9317 - val_loss: 0.2363 - val_accuracy: 0.9133\n",
      "Epoch 58/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.2017 - accuracy: 0.9194 - val_loss: 0.2563 - val_accuracy: 0.8900\n",
      "Epoch 59/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1873 - accuracy: 0.9283 - val_loss: 0.2401 - val_accuracy: 0.9000\n",
      "Epoch 60/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.2040 - accuracy: 0.9256 - val_loss: 0.2780 - val_accuracy: 0.8867\n",
      "Epoch 61/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1851 - accuracy: 0.9306 - val_loss: 0.2021 - val_accuracy: 0.9067\n",
      "Epoch 62/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.2126 - accuracy: 0.9161 - val_loss: 0.2427 - val_accuracy: 0.9133\n",
      "Epoch 63/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1996 - accuracy: 0.9222 - val_loss: 0.2941 - val_accuracy: 0.8733\n",
      "Epoch 64/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1998 - accuracy: 0.9244 - val_loss: 0.2175 - val_accuracy: 0.9033\n",
      "Epoch 65/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1861 - accuracy: 0.9261 - val_loss: 0.2110 - val_accuracy: 0.9133\n",
      "Epoch 66/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1761 - accuracy: 0.9372 - val_loss: 0.2171 - val_accuracy: 0.9067\n",
      "Epoch 67/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1834 - accuracy: 0.9289 - val_loss: 0.4323 - val_accuracy: 0.8533\n",
      "Epoch 68/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1817 - accuracy: 0.9283 - val_loss: 0.2119 - val_accuracy: 0.9033\n",
      "Epoch 69/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1863 - accuracy: 0.9328 - val_loss: 0.2506 - val_accuracy: 0.8967\n",
      "Epoch 70/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1677 - accuracy: 0.9339 - val_loss: 0.2817 - val_accuracy: 0.8900\n",
      "Epoch 71/2000\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.1646 - accuracy: 0.9367 - val_loss: 0.3083 - val_accuracy: 0.8867\n",
      "Epoch 72/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1614 - accuracy: 0.9367 - val_loss: 0.2384 - val_accuracy: 0.9033\n",
      "Epoch 73/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1625 - accuracy: 0.9439 - val_loss: 0.2531 - val_accuracy: 0.8900\n",
      "Epoch 74/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1797 - accuracy: 0.9256 - val_loss: 0.3222 - val_accuracy: 0.8733\n",
      "Epoch 75/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1524 - accuracy: 0.9456 - val_loss: 0.2766 - val_accuracy: 0.8900\n",
      "Epoch 76/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1732 - accuracy: 0.9311 - val_loss: 0.2012 - val_accuracy: 0.9167\n",
      "Epoch 77/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1874 - accuracy: 0.9267 - val_loss: 0.2107 - val_accuracy: 0.9167\n",
      "Epoch 78/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1733 - accuracy: 0.9300 - val_loss: 0.2415 - val_accuracy: 0.9000\n",
      "Epoch 79/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1875 - accuracy: 0.9239 - val_loss: 0.2451 - val_accuracy: 0.9033\n",
      "Epoch 80/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1934 - accuracy: 0.9261 - val_loss: 0.2231 - val_accuracy: 0.9167\n",
      "Epoch 81/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1485 - accuracy: 0.9422 - val_loss: 0.2151 - val_accuracy: 0.9233\n",
      "Epoch 82/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1410 - accuracy: 0.9417 - val_loss: 0.2221 - val_accuracy: 0.9100\n",
      "Epoch 83/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1648 - accuracy: 0.9367 - val_loss: 0.2230 - val_accuracy: 0.9033\n",
      "Epoch 84/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1633 - accuracy: 0.9361 - val_loss: 0.2253 - val_accuracy: 0.9033\n",
      "Epoch 85/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1524 - accuracy: 0.9383 - val_loss: 0.1978 - val_accuracy: 0.9200\n",
      "Epoch 86/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1542 - accuracy: 0.9372 - val_loss: 0.2411 - val_accuracy: 0.9000\n",
      "Epoch 87/2000\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.1680 - accuracy: 0.9283 - val_loss: 0.2564 - val_accuracy: 0.8967\n",
      "Epoch 88/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1713 - accuracy: 0.9372 - val_loss: 0.1937 - val_accuracy: 0.9300\n",
      "Epoch 89/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1626 - accuracy: 0.9372 - val_loss: 0.1975 - val_accuracy: 0.9267\n",
      "Epoch 90/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1532 - accuracy: 0.9389 - val_loss: 0.2142 - val_accuracy: 0.9100\n",
      "Epoch 91/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1818 - accuracy: 0.9322 - val_loss: 0.1876 - val_accuracy: 0.9333\n",
      "Epoch 92/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1935 - accuracy: 0.9250 - val_loss: 0.1991 - val_accuracy: 0.9300\n",
      "Epoch 93/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1529 - accuracy: 0.9394 - val_loss: 0.2230 - val_accuracy: 0.9200\n",
      "Epoch 94/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1658 - accuracy: 0.9367 - val_loss: 0.2625 - val_accuracy: 0.8900\n",
      "Epoch 95/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1743 - accuracy: 0.9294 - val_loss: 0.2173 - val_accuracy: 0.9067\n",
      "Epoch 96/2000\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.1652 - accuracy: 0.9389 - val_loss: 0.2141 - val_accuracy: 0.9200\n",
      "Epoch 97/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1506 - accuracy: 0.9411 - val_loss: 0.2059 - val_accuracy: 0.9233\n",
      "Epoch 98/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1951 - accuracy: 0.9322 - val_loss: 0.4414 - val_accuracy: 0.8500\n",
      "Epoch 99/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1721 - accuracy: 0.9372 - val_loss: 0.2040 - val_accuracy: 0.9200\n",
      "Epoch 100/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1568 - accuracy: 0.9406 - val_loss: 0.2056 - val_accuracy: 0.9267\n",
      "Epoch 101/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1584 - accuracy: 0.9333 - val_loss: 0.2023 - val_accuracy: 0.9200\n",
      "Epoch 102/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1667 - accuracy: 0.9350 - val_loss: 0.2026 - val_accuracy: 0.9400\n",
      "Epoch 103/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1664 - accuracy: 0.9344 - val_loss: 0.2317 - val_accuracy: 0.9133\n",
      "Epoch 104/2000\n",
      "57/57 [==============================] - 2s 33ms/step - loss: 0.1683 - accuracy: 0.9356 - val_loss: 0.1906 - val_accuracy: 0.9300\n",
      "Epoch 105/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1628 - accuracy: 0.9372 - val_loss: 0.1988 - val_accuracy: 0.9200\n",
      "Epoch 106/2000\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.1530 - accuracy: 0.9456 - val_loss: 0.2070 - val_accuracy: 0.9300\n",
      "Epoch 107/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1688 - accuracy: 0.9356 - val_loss: 0.2061 - val_accuracy: 0.9267\n",
      "Epoch 108/2000\n",
      "57/57 [==============================] - 2s 34ms/step - loss: 0.1714 - accuracy: 0.9378 - val_loss: 0.1991 - val_accuracy: 0.9367\n",
      "Epoch 109/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1586 - accuracy: 0.9417 - val_loss: 0.2009 - val_accuracy: 0.9267\n",
      "Epoch 110/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1593 - accuracy: 0.9394 - val_loss: 0.1854 - val_accuracy: 0.9333\n",
      "Epoch 111/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1512 - accuracy: 0.9383 - val_loss: 0.1794 - val_accuracy: 0.9300\n",
      "Epoch 112/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1514 - accuracy: 0.9439 - val_loss: 0.2454 - val_accuracy: 0.9100\n",
      "Epoch 113/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1679 - accuracy: 0.9372 - val_loss: 0.2090 - val_accuracy: 0.9133\n",
      "Epoch 114/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1559 - accuracy: 0.9428 - val_loss: 0.2618 - val_accuracy: 0.8900\n",
      "Epoch 115/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1674 - accuracy: 0.9378 - val_loss: 0.2421 - val_accuracy: 0.9067\n",
      "Epoch 116/2000\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.1921 - accuracy: 0.9228 - val_loss: 0.3239 - val_accuracy: 0.8733\n",
      "Epoch 117/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1480 - accuracy: 0.9461 - val_loss: 0.1703 - val_accuracy: 0.9400\n",
      "Epoch 118/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1399 - accuracy: 0.9456 - val_loss: 0.1943 - val_accuracy: 0.9300\n",
      "Epoch 119/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1517 - accuracy: 0.9389 - val_loss: 0.2014 - val_accuracy: 0.9100\n",
      "Epoch 120/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1518 - accuracy: 0.9433 - val_loss: 0.2203 - val_accuracy: 0.9100\n",
      "Epoch 121/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1421 - accuracy: 0.9417 - val_loss: 0.2045 - val_accuracy: 0.9133\n",
      "Epoch 122/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1477 - accuracy: 0.9450 - val_loss: 0.2150 - val_accuracy: 0.9133\n",
      "Epoch 123/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1392 - accuracy: 0.9494 - val_loss: 0.2506 - val_accuracy: 0.8967\n",
      "Epoch 124/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1540 - accuracy: 0.9361 - val_loss: 0.1864 - val_accuracy: 0.9300\n",
      "Epoch 125/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1206 - accuracy: 0.9539 - val_loss: 0.2530 - val_accuracy: 0.8867\n",
      "Epoch 126/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1428 - accuracy: 0.9472 - val_loss: 0.2061 - val_accuracy: 0.9200\n",
      "Epoch 127/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1729 - accuracy: 0.9361 - val_loss: 0.1876 - val_accuracy: 0.9333\n",
      "Epoch 128/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1575 - accuracy: 0.9378 - val_loss: 0.2563 - val_accuracy: 0.9133\n",
      "Epoch 129/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1713 - accuracy: 0.9311 - val_loss: 0.2435 - val_accuracy: 0.9000\n",
      "Epoch 130/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1606 - accuracy: 0.9422 - val_loss: 0.2268 - val_accuracy: 0.9133\n",
      "Epoch 131/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1638 - accuracy: 0.9367 - val_loss: 0.2111 - val_accuracy: 0.9267\n",
      "Epoch 132/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1335 - accuracy: 0.9489 - val_loss: 0.2167 - val_accuracy: 0.9100\n",
      "Epoch 133/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1548 - accuracy: 0.9428 - val_loss: 0.2470 - val_accuracy: 0.9100\n",
      "Epoch 134/2000\n",
      "57/57 [==============================] - 2s 34ms/step - loss: 0.1399 - accuracy: 0.9461 - val_loss: 0.2890 - val_accuracy: 0.8900\n",
      "Epoch 135/2000\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.1387 - accuracy: 0.9450 - val_loss: 0.2165 - val_accuracy: 0.9167\n",
      "Epoch 136/2000\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1313 - accuracy: 0.9411 - val_loss: 0.2110 - val_accuracy: 0.9133\n",
      "Epoch 137/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1460 - accuracy: 0.9478 - val_loss: 0.2528 - val_accuracy: 0.8867\n",
      "Epoch 138/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1501 - accuracy: 0.9400 - val_loss: 0.1800 - val_accuracy: 0.9367\n",
      "Epoch 139/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1282 - accuracy: 0.9478 - val_loss: 0.2302 - val_accuracy: 0.9233\n",
      "Epoch 140/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1524 - accuracy: 0.9417 - val_loss: 0.2231 - val_accuracy: 0.9100\n",
      "Epoch 141/2000\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1646 - accuracy: 0.9350 - val_loss: 0.2173 - val_accuracy: 0.9033\n",
      "Epoch 142/2000\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1197 - accuracy: 0.9556 - val_loss: 0.1768 - val_accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=40, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = comsnet_model.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=2000,\n",
    "    batch_size=32,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 7ms/step - loss: 0.2764 - accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2763543426990509, 0.8999999761581421]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comsnet_model.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "comsnet_model.save(\"Jordanian_T_97.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "comsnet_model = load_model(\"Jordanian_T_97.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0795 - accuracy: 0.9711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07954056560993195, 0.9711111187934875]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comsnet_model.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_64 (InputLayer)       [(None, 1601, 6, 1)]      0         \n",
      "                                                                 \n",
      " conv2d_240 (Conv2D)         (None, 1601, 6, 3)        30        \n",
      "                                                                 \n",
      " batch_normalization_240 (B  (None, 1601, 6, 3)        12        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_113 (Ave  (None, 533, 3, 3)         0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " conv2d_241 (Conv2D)         (None, 533, 3, 8)         224       \n",
      "                                                                 \n",
      " batch_normalization_241 (B  (None, 533, 3, 8)         32        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_114 (Ave  (None, 177, 1, 8)         0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " conv2d_242 (Conv2D)         (None, 177, 1, 16)        1168      \n",
      "                                                                 \n",
      " batch_normalization_242 (B  (None, 177, 1, 16)        64        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_115 (Ave  (None, 88, 1, 16)         0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " conv2d_243 (Conv2D)         (None, 88, 1, 32)         4640      \n",
      "                                                                 \n",
      " batch_normalization_243 (B  (None, 88, 1, 32)         128       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " bottleneck (AveragePooling  (None, 44, 1, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_244 (Conv2D)         (None, 44, 1, 64)         18496     \n",
      "                                                                 \n",
      " batch_normalization_244 (B  (None, 44, 1, 64)         256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling2d_116 (Ave  (None, 22, 1, 64)         0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " flatten_53 (Flatten)        (None, 1408)              0         \n",
      "                                                                 \n",
      " dropout_101 (Dropout)       (None, 1408)              0         \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 128)               180352    \n",
      "                                                                 \n",
      " dropout_102 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206176 (805.38 KB)\n",
      "Trainable params: 205930 (804.41 KB)\n",
      "Non-trainable params: 246 (984.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "comsnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bottleneck' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_bottleneck \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39mcomsnet_model\u001b[39m.\u001b[39minput, outputs\u001b[39m=\u001b[39mbottleneck)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Create a model to extract the final output\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_final_output \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39mcomsnet_model\u001b[39m.\u001b[39minput, outputs\u001b[39m=\u001b[39moutputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bottleneck' is not defined"
     ]
    }
   ],
   "source": [
    "model_bottleneck = Model(inputs=comsnet_model.input, outputs=bottleneck)\n",
    "\n",
    "# Create a model to extract the final output\n",
    "model_final_output = Model(inputs=comsnet_model.input, outputs=outputs)\n",
    "\n",
    "# Assuming train_data, val_data, and test_data are your datasets\n",
    "# For demonstration, using random data with correct shape\n",
    "# train_data = np.random.rand(1000, 200, 6)\n",
    "# val_data = np.random.rand(200, 200, 6)\n",
    "# test_data = np.random.rand(200, 200, 6)\n",
    "\n",
    "# Predict the bottleneck outputs\n",
    "bottleneck_train = model_bottleneck.predict(np.array(x_train))\n",
    "bottleneck_val = model_bottleneck.predict(np.array(x_val))\n",
    "bottleneck_test = model_bottleneck.predict(np.array(x_test))\n",
    "\n",
    "# Predict the final outputs (probability scores)\n",
    "output_train = model_final_output.predict(np.array(x_train))\n",
    "output_val = model_final_output.predict(np.array(x_val))\n",
    "output_test = model_final_output.predict(np.array(x_test))\n",
    "\n",
    "# Print shapes of the outputs\n",
    "print(f\"Bottleneck Train Shape: {bottleneck_train.shape}\")\n",
    "print(f\"Bottleneck Val Shape: {bottleneck_val.shape}\")\n",
    "print(f\"Bottleneck Test Shape: {bottleneck_test.shape}\")\n",
    "print(f\"Final Output Train Shape: {output_train.shape}\")\n",
    "print(f\"Final Output Val Shape: {output_val.shape}\")\n",
    "print(f\"Final Output Test Shape: {output_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 8ms/step\n",
      "10/10 [==============================] - 0s 9ms/step\n",
      "29/29 [==============================] - 0s 8ms/step\n",
      "Bottleneck Train Shape: (1800, 44, 1, 32)\n",
      "Bottleneck Val Shape: (300, 44, 1, 32)\n",
      "Bottleneck Test Shape: (900, 44, 1, 32)\n",
      "Final Output Train Shape: (1800, 6)\n",
      "Final Output Val Shape: (300, 6)\n",
      "Final Output Test Shape: (900, 6)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the new model to output both the bottleneck layer and the final output layer\n",
    "bottleneck_layer_output = comsnet_model.get_layer('bottleneck').output\n",
    "final_output_layer_output = comsnet_model.output\n",
    "\n",
    "# Create a new model that will output both the bottleneck and the final output\n",
    "model_with_bottleneck_output = Model(inputs=comsnet_model.input, outputs=[bottleneck_layer_output, final_output_layer_output])\n",
    "\n",
    "# Get bottleneck and final output for training, validation, and test sets\n",
    "bottleneck_train, output_train = model_with_bottleneck_output.predict(x_train)\n",
    "bottleneck_val, output_val = model_with_bottleneck_output.predict(x_val)\n",
    "bottleneck_test, output_test = model_with_bottleneck_output.predict(x_test)\n",
    "\n",
    "# Print the shapes of the bottleneck and final output layers\n",
    "print(f\"Bottleneck Train Shape: {bottleneck_train.shape}\")\n",
    "print(f\"Bottleneck Val Shape: {bottleneck_val.shape}\")\n",
    "print(f\"Bottleneck Test Shape: {bottleneck_test.shape}\")\n",
    "print(f\"Final Output Train Shape: {output_train.shape}\")\n",
    "print(f\"Final Output Val Shape: {output_val.shape}\")\n",
    "print(f\"Final Output Test Shape: {output_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck Train Shape: (1800, 44, 1, 32)\n",
      "Bottleneck Val Shape: (300, 44, 1, 32)\n",
      "Bottleneck Test Shape: (900, 44, 1, 32)\n",
      "Final Output Train Shape: (1800, 6)\n",
      "Final Output Val Shape: (300, 6)\n",
      "Final Output Test Shape: (900, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bottleneck Train Shape: {bottleneck_train.shape}\")\n",
    "print(f\"Bottleneck Val Shape: {bottleneck_val.shape}\")\n",
    "print(f\"Bottleneck Test Shape: {bottleneck_test.shape}\")\n",
    "print(f\"Final Output Train Shape: {output_train.shape}\")\n",
    "print(f\"Final Output Val Shape: {output_val.shape}\")\n",
    "print(f\"Final Output Test Shape: {output_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_95\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_150 (InputLayer)      [(None, 1601, 6, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " average_pooling2d_98 (Aver  (None, 266, 6, 1)            0         ['input_150[0][0]']           \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_561 (Conv2D)         (None, 45, 1, 32)            800       ['average_pooling2d_98[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_562 (Conv2D)         (None, 45, 1, 32)            800       ['average_pooling2d_98[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_563 (Conv2D)         (None, 45, 1, 32)            800       ['average_pooling2d_98[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_564 (Conv2D)         (None, 45, 1, 32)            800       ['average_pooling2d_98[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_565 (Conv2D)         (None, 45, 1, 32)            800       ['average_pooling2d_98[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_196 (B  (None, 45, 1, 32)            128       ['conv2d_561[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_197 (B  (None, 45, 1, 32)            128       ['conv2d_562[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_198 (B  (None, 45, 1, 32)            128       ['conv2d_563[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_199 (B  (None, 45, 1, 32)            128       ['conv2d_564[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_200 (B  (None, 45, 1, 32)            128       ['conv2d_565[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " add_103 (Add)               (None, 45, 1, 32)            0         ['batch_normalization_196[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'batch_normalization_197[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'batch_normalization_198[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'batch_normalization_199[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'batch_normalization_200[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " flatten_118 (Flatten)       (None, 1440)                 0         ['add_103[0][0]']             \n",
      "                                                                                                  \n",
      " dense_173 (Dense)           (None, 10)                   14410     ['flatten_118[0][0]']         \n",
      "                                                                                                  \n",
      " out_s (Dense)               (None, 6)                    66        ['dense_173[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19116 (74.67 KB)\n",
      "Trainable params: 18796 (73.42 KB)\n",
      "Non-trainable params: 320 (1.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(1601, 6, 1))\n",
    "\n",
    "# MaxPooling2D layer directly after input\n",
    "x = layers.AveragePooling2D(pool_size=(6, 1))(inputs)  # Pooling with size 8x1\n",
    "\n",
    "\n",
    "# First Convolutional Layer repeated five times\n",
    "branch1 = layers.Conv2D(32, (8, 3), activation='relu', strides=(6, 6), padding='same')(x)\n",
    "branch1 = layers.BatchNormalization()(branch1)\n",
    "\n",
    "branch2 = layers.Conv2D(32, (8, 3), activation='relu', strides=(6, 6), padding='same')(x)\n",
    "branch2 = layers.BatchNormalization()(branch2)\n",
    "\n",
    "branch3 = layers.Conv2D(32, (8, 3), activation='relu', strides=(6, 6), padding='same')(x)\n",
    "branch3 = layers.BatchNormalization()(branch3)\n",
    "\n",
    "branch4 = layers.Conv2D(32, (8, 3), activation='relu', strides=(6, 6), padding='same')(x)\n",
    "branch4 = layers.BatchNormalization()(branch4)\n",
    "\n",
    "branch5 = layers.Conv2D(32, (8, 3), activation='relu', strides=(6, 6), padding='same')(x)\n",
    "branch5 = layers.BatchNormalization()(branch5)\n",
    "\n",
    "# Adding the outputs of the five branches\n",
    "x_b_S = layers.Add()([branch1, branch2, branch3, branch4, branch5])\n",
    "\n",
    "# Flatten the output\n",
    "cat = layers.Flatten()(x_b_S)\n",
    "cat = layers.Dense(10, activation='relu')(cat)\n",
    "\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(6, activation='softmax', name='out_s')(cat)  # Output layer with softmax activation for 6 classes\n",
    "\n",
    "# Create the model\n",
    "st_model = models.Model(inputs=inputs, outputs=[x_b_S, outputs])\n",
    "\n",
    "# Compile the model\n",
    "st_model.compile(optimizer='adam', loss=['mean_squared_error', 'binary_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "st_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_100\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_155 (InputLayer)      [(None, 1601, 6, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " average_pooling2d_103 (Ave  (None, 266, 6, 1)            0         ['input_155[0][0]']           \n",
      " ragePooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_589 (Conv2D)         (None, 44, 1, 32)            672       ['average_pooling2d_103[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_590 (Conv2D)         (None, 44, 1, 32)            672       ['average_pooling2d_103[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_591 (Conv2D)         (None, 44, 1, 32)            672       ['average_pooling2d_103[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_592 (Conv2D)         (None, 44, 1, 32)            672       ['average_pooling2d_103[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_593 (Conv2D)         (None, 44, 1, 32)            672       ['average_pooling2d_103[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_108 (Add)               (None, 44, 1, 32)            0         ['conv2d_589[0][0]',          \n",
      "                                                                     'conv2d_590[0][0]',          \n",
      "                                                                     'conv2d_591[0][0]',          \n",
      "                                                                     'conv2d_592[0][0]',          \n",
      "                                                                     'conv2d_593[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_202 (B  (None, 44, 1, 32)            128       ['add_108[0][0]']             \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " flatten_123 (Flatten)       (None, 1408)                 0         ['batch_normalization_202[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_179 (Dense)           (None, 32)                   45088     ['flatten_123[0][0]']         \n",
      "                                                                                                  \n",
      " out_s (Dense)               (None, 6)                    198       ['dense_179[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 48774 (190.52 KB)\n",
      "Trainable params: 48710 (190.27 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(1601, 6, 1))\n",
    "\n",
    "# MaxPooling2D layer directly after input\n",
    "x = layers.AveragePooling2D(pool_size=(6, 1))(inputs)  # Pooling with size (6,1)\n",
    "\n",
    "# Convolutional Layer with Cropping in Each Branch\n",
    "branch1 = layers.Conv2D(32, (4, 5), activation='relu', strides=(6, 2))(x)\n",
    "# branch1 = layers.BatchNormalization()(branch1)\n",
    "\n",
    "branch2 = layers.Conv2D(32, (4, 5), activation='relu', strides=(6, 2))(x)\n",
    "# branch2 = layers.BatchNormalization()(branch2)\n",
    "\n",
    "branch3 = layers.Conv2D(32, (4, 5), activation='relu', strides=(6, 2))(x)\n",
    "# branch3 = layers.BatchNormalization()(branch3)\n",
    "\n",
    "branch4 = layers.Conv2D(32, (4, 5), activation='relu', strides=(6, 2))(x)\n",
    "# branch4 = layers.BatchNormalization()(branch4)\n",
    "\n",
    "branch5 = layers.Conv2D(32, (4, 5), activation='relu', strides=(6, 2))(x)\n",
    "# branch5 = layers.BatchNormalization()(branch5)\n",
    "\n",
    "# Adding the outputs of the five branches\n",
    "x_b_S = layers.Add()([branch1, branch2, branch3, branch4, branch5])\n",
    "x = layers.BatchNormalization()(x_b_S)\n",
    "# Flatten the output\n",
    "cat = layers.Flatten()(x)\n",
    "cat = layers.Dense(32, activation='relu')(cat)\n",
    "\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(6, activation='sigmoid', name='out_s')(cat)  # Output layer with softmax activation for 6 classes\n",
    "\n",
    "# Create the model\n",
    "st_model = models.Model(inputs=inputs, outputs=[x_b_S, outputs])\n",
    "\n",
    "opt = Adam(learning_rate = 0.001)\n",
    "\n",
    "# Compile the model\n",
    "st_model.compile(optimizer=opt, loss=['mean_absolute_error', 'binary_crossentropy'], loss_weights = [1, 1],metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "st_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_163\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_218 (InputLayer)      [(None, 1601, 6, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " average_pooling2d_166 (Ave  (None, 228, 6, 1)            0         ['input_218[0][0]']           \n",
      " ragePooling2D)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_964 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_166[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_965 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_166[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_966 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_166[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_967 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_166[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_968 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_166[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_171 (Add)               (None, 45, 1, 32)            0         ['conv2d_964[0][0]',          \n",
      "                                                                     'conv2d_965[0][0]',          \n",
      "                                                                     'conv2d_966[0][0]',          \n",
      "                                                                     'conv2d_967[0][0]',          \n",
      "                                                                     'conv2d_968[0][0]']          \n",
      "                                                                                                  \n",
      " bottleneck (Conv2D)         (None, 44, 1, 32)            2080      ['add_171[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_969 (Conv2D)         (None, 38, 1, 32)            7200      ['bottleneck[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_186 (Flatten)       (None, 1216)                 0         ['conv2d_969[0][0]']          \n",
      "                                                                                                  \n",
      " dense_244 (Dense)           (None, 16)                   19472     ['flatten_186[0][0]']         \n",
      "                                                                                                  \n",
      " out_s (Dense)               (None, 6)                    102       ['dense_244[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34614 (135.21 KB)\n",
      "Trainable params: 34614 (135.21 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(1601, 6, 1))\n",
    "\n",
    "# MaxPooling2D layer directly after input\n",
    "x = layers.AveragePooling2D(pool_size=(7, 1))(inputs)  # Pooling with size (6,1)\n",
    "\n",
    "# Convolutional Layer with Cropping in Each Branch\n",
    "branch1 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "branch2 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "branch3 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "branch4 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "branch5 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "# Adding the outputs of the five branches\n",
    "x_pre = layers.Add()([branch1, branch2, branch3, branch4, branch5])\n",
    "# x_pre = layers.BatchNormalization()(x_pre)\n",
    "x_b_S = layers.Conv2D(32, (2, 1), name = 'bottleneck')(x_pre)\n",
    "x = layers.Conv2D(32, (7, 1), activation ='relu' )(x_b_S)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Flatten the output\n",
    "cat = layers.Flatten()(x)\n",
    "# avg_S = GlobalAveragePooling2D()(x)\n",
    "# max_S = GlobalMaxPooling2D()(x)\n",
    "# cat = Add()([avg_S, max_S])\n",
    "cat = layers.Dense(16, activation='relu')(cat)\n",
    "\n",
    "# cat = layers.Dense(16, activation='relu')(cat)\n",
    "# b2 = layers.Dense(16, activation='relu')(cat)\n",
    "# b3 = layers.Dense(16, activation='relu')(cat)\n",
    "# b4 = layers.Dense(16, activation='relu')(cat)\n",
    "\n",
    "# x_add = layers.Add()([b1, b2, b3, b4])\n",
    "# cat = layers.Dropout(0.3)(cat)\n",
    "# cat = layers.BatchNormalization()(cat)\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(6, activation='sigmoid', name='out_s')(cat)  # Output layer with softmax activation for 6 classes\n",
    "\n",
    "# Create the model\n",
    "st_model = models.Model(inputs=inputs, outputs=[x_b_S, outputs])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# Compile the model\n",
    "st_model.compile(optimizer= opt, loss=['mean_squared_error', 'binary_crossentropy'], loss_weights= [1, 1], metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "st_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "57/57 [==============================] - 2s 9ms/step - loss: 1.1290 - bottleneck_loss: 0.6416 - out_s_loss: 0.4874 - bottleneck_accuracy: 0.1268 - out_s_accuracy: 0.3756 - val_loss: 0.8797 - val_bottleneck_loss: 0.5009 - val_out_s_loss: 0.3788 - val_bottleneck_accuracy: 0.1839 - val_out_s_accuracy: 0.4100\n",
      "Epoch 2/1000\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.8253 - bottleneck_loss: 0.4834 - out_s_loss: 0.3419 - bottleneck_accuracy: 0.2278 - out_s_accuracy: 0.4717 - val_loss: 0.7707 - val_bottleneck_loss: 0.4373 - val_out_s_loss: 0.3334 - val_bottleneck_accuracy: 0.2684 - val_out_s_accuracy: 0.4667\n",
      "Epoch 3/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.7156 - bottleneck_loss: 0.4271 - out_s_loss: 0.2885 - bottleneck_accuracy: 0.2977 - out_s_accuracy: 0.5394 - val_loss: 0.6611 - val_bottleneck_loss: 0.3874 - val_out_s_loss: 0.2737 - val_bottleneck_accuracy: 0.3278 - val_out_s_accuracy: 0.5500\n",
      "Epoch 4/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.6283 - bottleneck_loss: 0.3832 - out_s_loss: 0.2451 - bottleneck_accuracy: 0.3594 - out_s_accuracy: 0.5617 - val_loss: 0.5985 - val_bottleneck_loss: 0.3499 - val_out_s_loss: 0.2486 - val_bottleneck_accuracy: 0.3820 - val_out_s_accuracy: 0.5567\n",
      "Epoch 5/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.5767 - bottleneck_loss: 0.3509 - out_s_loss: 0.2258 - bottleneck_accuracy: 0.3994 - out_s_accuracy: 0.5794 - val_loss: 0.5655 - val_bottleneck_loss: 0.3244 - val_out_s_loss: 0.2411 - val_bottleneck_accuracy: 0.3884 - val_out_s_accuracy: 0.6333\n",
      "Epoch 6/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.5474 - bottleneck_loss: 0.3300 - out_s_loss: 0.2175 - bottleneck_accuracy: 0.4055 - out_s_accuracy: 0.6250 - val_loss: 0.5364 - val_bottleneck_loss: 0.3061 - val_out_s_loss: 0.2303 - val_bottleneck_accuracy: 0.4077 - val_out_s_accuracy: 0.6133\n",
      "Epoch 7/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.5236 - bottleneck_loss: 0.3150 - out_s_loss: 0.2086 - bottleneck_accuracy: 0.4204 - out_s_accuracy: 0.6428 - val_loss: 0.5285 - val_bottleneck_loss: 0.2934 - val_out_s_loss: 0.2350 - val_bottleneck_accuracy: 0.4370 - val_out_s_accuracy: 0.6700\n",
      "Epoch 8/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.5004 - bottleneck_loss: 0.3011 - out_s_loss: 0.1993 - bottleneck_accuracy: 0.4340 - out_s_accuracy: 0.6539 - val_loss: 0.5074 - val_bottleneck_loss: 0.2831 - val_out_s_loss: 0.2243 - val_bottleneck_accuracy: 0.4117 - val_out_s_accuracy: 0.6467\n",
      "Epoch 9/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4862 - bottleneck_loss: 0.2930 - out_s_loss: 0.1932 - bottleneck_accuracy: 0.4362 - out_s_accuracy: 0.6667 - val_loss: 0.5037 - val_bottleneck_loss: 0.2750 - val_out_s_loss: 0.2288 - val_bottleneck_accuracy: 0.4470 - val_out_s_accuracy: 0.7067\n",
      "Epoch 10/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4744 - bottleneck_loss: 0.2848 - out_s_loss: 0.1896 - bottleneck_accuracy: 0.4424 - out_s_accuracy: 0.6950 - val_loss: 0.4869 - val_bottleneck_loss: 0.2681 - val_out_s_loss: 0.2188 - val_bottleneck_accuracy: 0.4410 - val_out_s_accuracy: 0.7033\n",
      "Epoch 11/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4625 - bottleneck_loss: 0.2788 - out_s_loss: 0.1837 - bottleneck_accuracy: 0.4535 - out_s_accuracy: 0.6967 - val_loss: 0.4846 - val_bottleneck_loss: 0.2621 - val_out_s_loss: 0.2225 - val_bottleneck_accuracy: 0.4665 - val_out_s_accuracy: 0.7033\n",
      "Epoch 12/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4533 - bottleneck_loss: 0.2726 - out_s_loss: 0.1807 - bottleneck_accuracy: 0.4607 - out_s_accuracy: 0.7200 - val_loss: 0.4820 - val_bottleneck_loss: 0.2575 - val_out_s_loss: 0.2245 - val_bottleneck_accuracy: 0.4442 - val_out_s_accuracy: 0.7033\n",
      "Epoch 13/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4478 - bottleneck_loss: 0.2692 - out_s_loss: 0.1786 - bottleneck_accuracy: 0.4641 - out_s_accuracy: 0.7028 - val_loss: 0.4761 - val_bottleneck_loss: 0.2552 - val_out_s_loss: 0.2209 - val_bottleneck_accuracy: 0.4852 - val_out_s_accuracy: 0.7100\n",
      "Epoch 14/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4410 - bottleneck_loss: 0.2648 - out_s_loss: 0.1762 - bottleneck_accuracy: 0.4686 - out_s_accuracy: 0.7422 - val_loss: 0.4666 - val_bottleneck_loss: 0.2490 - val_out_s_loss: 0.2176 - val_bottleneck_accuracy: 0.4700 - val_out_s_accuracy: 0.7400\n",
      "Epoch 15/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4305 - bottleneck_loss: 0.2600 - out_s_loss: 0.1705 - bottleneck_accuracy: 0.4753 - out_s_accuracy: 0.7439 - val_loss: 0.4670 - val_bottleneck_loss: 0.2459 - val_out_s_loss: 0.2211 - val_bottleneck_accuracy: 0.4667 - val_out_s_accuracy: 0.7467\n",
      "Epoch 16/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4291 - bottleneck_loss: 0.2576 - out_s_loss: 0.1715 - bottleneck_accuracy: 0.4716 - out_s_accuracy: 0.7400 - val_loss: 0.4599 - val_bottleneck_loss: 0.2420 - val_out_s_loss: 0.2178 - val_bottleneck_accuracy: 0.4928 - val_out_s_accuracy: 0.7467\n",
      "Epoch 17/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4227 - bottleneck_loss: 0.2538 - out_s_loss: 0.1689 - bottleneck_accuracy: 0.4828 - out_s_accuracy: 0.7539 - val_loss: 0.4598 - val_bottleneck_loss: 0.2404 - val_out_s_loss: 0.2194 - val_bottleneck_accuracy: 0.4891 - val_out_s_accuracy: 0.7233\n",
      "Epoch 18/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4179 - bottleneck_loss: 0.2510 - out_s_loss: 0.1669 - bottleneck_accuracy: 0.4877 - out_s_accuracy: 0.7483 - val_loss: 0.4582 - val_bottleneck_loss: 0.2380 - val_out_s_loss: 0.2202 - val_bottleneck_accuracy: 0.4949 - val_out_s_accuracy: 0.7733\n",
      "Epoch 19/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4108 - bottleneck_loss: 0.2477 - out_s_loss: 0.1630 - bottleneck_accuracy: 0.4859 - out_s_accuracy: 0.7500 - val_loss: 0.4586 - val_bottleneck_loss: 0.2375 - val_out_s_loss: 0.2212 - val_bottleneck_accuracy: 0.4928 - val_out_s_accuracy: 0.7533\n",
      "Epoch 20/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4083 - bottleneck_loss: 0.2457 - out_s_loss: 0.1625 - bottleneck_accuracy: 0.4943 - out_s_accuracy: 0.7561 - val_loss: 0.4535 - val_bottleneck_loss: 0.2327 - val_out_s_loss: 0.2208 - val_bottleneck_accuracy: 0.5066 - val_out_s_accuracy: 0.7333\n",
      "Epoch 21/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4042 - bottleneck_loss: 0.2434 - out_s_loss: 0.1608 - bottleneck_accuracy: 0.4964 - out_s_accuracy: 0.7461 - val_loss: 0.4483 - val_bottleneck_loss: 0.2315 - val_out_s_loss: 0.2168 - val_bottleneck_accuracy: 0.4845 - val_out_s_accuracy: 0.7433\n",
      "Epoch 22/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.4002 - bottleneck_loss: 0.2407 - out_s_loss: 0.1595 - bottleneck_accuracy: 0.5041 - out_s_accuracy: 0.7600 - val_loss: 0.4439 - val_bottleneck_loss: 0.2297 - val_out_s_loss: 0.2143 - val_bottleneck_accuracy: 0.5040 - val_out_s_accuracy: 0.7467\n",
      "Epoch 23/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3966 - bottleneck_loss: 0.2392 - out_s_loss: 0.1575 - bottleneck_accuracy: 0.5033 - out_s_accuracy: 0.7678 - val_loss: 0.4450 - val_bottleneck_loss: 0.2263 - val_out_s_loss: 0.2188 - val_bottleneck_accuracy: 0.5437 - val_out_s_accuracy: 0.7500\n",
      "Epoch 24/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3933 - bottleneck_loss: 0.2368 - out_s_loss: 0.1565 - bottleneck_accuracy: 0.5090 - out_s_accuracy: 0.7644 - val_loss: 0.4451 - val_bottleneck_loss: 0.2256 - val_out_s_loss: 0.2196 - val_bottleneck_accuracy: 0.5237 - val_out_s_accuracy: 0.7667\n",
      "Epoch 25/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3926 - bottleneck_loss: 0.2354 - out_s_loss: 0.1572 - bottleneck_accuracy: 0.5110 - out_s_accuracy: 0.7778 - val_loss: 0.4368 - val_bottleneck_loss: 0.2225 - val_out_s_loss: 0.2143 - val_bottleneck_accuracy: 0.5030 - val_out_s_accuracy: 0.7300\n",
      "Epoch 26/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3872 - bottleneck_loss: 0.2329 - out_s_loss: 0.1543 - bottleneck_accuracy: 0.5143 - out_s_accuracy: 0.7761 - val_loss: 0.4353 - val_bottleneck_loss: 0.2202 - val_out_s_loss: 0.2151 - val_bottleneck_accuracy: 0.5201 - val_out_s_accuracy: 0.7833\n",
      "Epoch 27/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3846 - bottleneck_loss: 0.2312 - out_s_loss: 0.1534 - bottleneck_accuracy: 0.5170 - out_s_accuracy: 0.7722 - val_loss: 0.4332 - val_bottleneck_loss: 0.2196 - val_out_s_loss: 0.2137 - val_bottleneck_accuracy: 0.5230 - val_out_s_accuracy: 0.7633\n",
      "Epoch 28/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3829 - bottleneck_loss: 0.2297 - out_s_loss: 0.1532 - bottleneck_accuracy: 0.5181 - out_s_accuracy: 0.7817 - val_loss: 0.4348 - val_bottleneck_loss: 0.2178 - val_out_s_loss: 0.2170 - val_bottleneck_accuracy: 0.5459 - val_out_s_accuracy: 0.7467\n",
      "Epoch 29/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3810 - bottleneck_loss: 0.2282 - out_s_loss: 0.1528 - bottleneck_accuracy: 0.5225 - out_s_accuracy: 0.7828 - val_loss: 0.4374 - val_bottleneck_loss: 0.2180 - val_out_s_loss: 0.2194 - val_bottleneck_accuracy: 0.5027 - val_out_s_accuracy: 0.7367\n",
      "Epoch 30/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3789 - bottleneck_loss: 0.2270 - out_s_loss: 0.1519 - bottleneck_accuracy: 0.5235 - out_s_accuracy: 0.7800 - val_loss: 0.4323 - val_bottleneck_loss: 0.2160 - val_out_s_loss: 0.2163 - val_bottleneck_accuracy: 0.5357 - val_out_s_accuracy: 0.7500\n",
      "Epoch 31/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3753 - bottleneck_loss: 0.2251 - out_s_loss: 0.1502 - bottleneck_accuracy: 0.5241 - out_s_accuracy: 0.7944 - val_loss: 0.4287 - val_bottleneck_loss: 0.2146 - val_out_s_loss: 0.2141 - val_bottleneck_accuracy: 0.5527 - val_out_s_accuracy: 0.7500\n",
      "Epoch 32/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3752 - bottleneck_loss: 0.2240 - out_s_loss: 0.1511 - bottleneck_accuracy: 0.5283 - out_s_accuracy: 0.7861 - val_loss: 0.4292 - val_bottleneck_loss: 0.2127 - val_out_s_loss: 0.2166 - val_bottleneck_accuracy: 0.5396 - val_out_s_accuracy: 0.7433\n",
      "Epoch 33/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3754 - bottleneck_loss: 0.2234 - out_s_loss: 0.1521 - bottleneck_accuracy: 0.5311 - out_s_accuracy: 0.7861 - val_loss: 0.4285 - val_bottleneck_loss: 0.2117 - val_out_s_loss: 0.2167 - val_bottleneck_accuracy: 0.5548 - val_out_s_accuracy: 0.7800\n",
      "Epoch 34/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3692 - bottleneck_loss: 0.2206 - out_s_loss: 0.1486 - bottleneck_accuracy: 0.5332 - out_s_accuracy: 0.7972 - val_loss: 0.4295 - val_bottleneck_loss: 0.2112 - val_out_s_loss: 0.2183 - val_bottleneck_accuracy: 0.5659 - val_out_s_accuracy: 0.7400\n",
      "Epoch 35/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3693 - bottleneck_loss: 0.2201 - out_s_loss: 0.1492 - bottleneck_accuracy: 0.5379 - out_s_accuracy: 0.7911 - val_loss: 0.4242 - val_bottleneck_loss: 0.2091 - val_out_s_loss: 0.2150 - val_bottleneck_accuracy: 0.5179 - val_out_s_accuracy: 0.7567\n",
      "Epoch 36/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3661 - bottleneck_loss: 0.2184 - out_s_loss: 0.1476 - bottleneck_accuracy: 0.5375 - out_s_accuracy: 0.8033 - val_loss: 0.4241 - val_bottleneck_loss: 0.2088 - val_out_s_loss: 0.2153 - val_bottleneck_accuracy: 0.5504 - val_out_s_accuracy: 0.7600\n",
      "Epoch 37/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3661 - bottleneck_loss: 0.2174 - out_s_loss: 0.1487 - bottleneck_accuracy: 0.5379 - out_s_accuracy: 0.8061 - val_loss: 0.4245 - val_bottleneck_loss: 0.2087 - val_out_s_loss: 0.2158 - val_bottleneck_accuracy: 0.5272 - val_out_s_accuracy: 0.7733\n",
      "Epoch 38/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3669 - bottleneck_loss: 0.2171 - out_s_loss: 0.1498 - bottleneck_accuracy: 0.5407 - out_s_accuracy: 0.8033 - val_loss: 0.4296 - val_bottleneck_loss: 0.2072 - val_out_s_loss: 0.2225 - val_bottleneck_accuracy: 0.5537 - val_out_s_accuracy: 0.7400\n",
      "Epoch 39/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3629 - bottleneck_loss: 0.2157 - out_s_loss: 0.1472 - bottleneck_accuracy: 0.5411 - out_s_accuracy: 0.8044 - val_loss: 0.4209 - val_bottleneck_loss: 0.2067 - val_out_s_loss: 0.2142 - val_bottleneck_accuracy: 0.5501 - val_out_s_accuracy: 0.7967\n",
      "Epoch 40/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3605 - bottleneck_loss: 0.2141 - out_s_loss: 0.1463 - bottleneck_accuracy: 0.5459 - out_s_accuracy: 0.8128 - val_loss: 0.4192 - val_bottleneck_loss: 0.2036 - val_out_s_loss: 0.2156 - val_bottleneck_accuracy: 0.5554 - val_out_s_accuracy: 0.7867\n",
      "Epoch 41/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3588 - bottleneck_loss: 0.2129 - out_s_loss: 0.1459 - bottleneck_accuracy: 0.5491 - out_s_accuracy: 0.7994 - val_loss: 0.4190 - val_bottleneck_loss: 0.2026 - val_out_s_loss: 0.2164 - val_bottleneck_accuracy: 0.5495 - val_out_s_accuracy: 0.7800\n",
      "Epoch 42/1000\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.3614 - bottleneck_loss: 0.2129 - out_s_loss: 0.1485 - bottleneck_accuracy: 0.5450 - out_s_accuracy: 0.7994 - val_loss: 0.4201 - val_bottleneck_loss: 0.2031 - val_out_s_loss: 0.2170 - val_bottleneck_accuracy: 0.5453 - val_out_s_accuracy: 0.8133\n",
      "Epoch 43/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3579 - bottleneck_loss: 0.2114 - out_s_loss: 0.1465 - bottleneck_accuracy: 0.5499 - out_s_accuracy: 0.8133 - val_loss: 0.4195 - val_bottleneck_loss: 0.2016 - val_out_s_loss: 0.2179 - val_bottleneck_accuracy: 0.5651 - val_out_s_accuracy: 0.7767\n",
      "Epoch 44/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3557 - bottleneck_loss: 0.2102 - out_s_loss: 0.1455 - bottleneck_accuracy: 0.5541 - out_s_accuracy: 0.8161 - val_loss: 0.4163 - val_bottleneck_loss: 0.2014 - val_out_s_loss: 0.2150 - val_bottleneck_accuracy: 0.5681 - val_out_s_accuracy: 0.7700\n",
      "Epoch 45/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3560 - bottleneck_loss: 0.2099 - out_s_loss: 0.1461 - bottleneck_accuracy: 0.5511 - out_s_accuracy: 0.8200 - val_loss: 0.4210 - val_bottleneck_loss: 0.2003 - val_out_s_loss: 0.2207 - val_bottleneck_accuracy: 0.5661 - val_out_s_accuracy: 0.8167\n",
      "Epoch 46/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3541 - bottleneck_loss: 0.2088 - out_s_loss: 0.1454 - bottleneck_accuracy: 0.5514 - out_s_accuracy: 0.8167 - val_loss: 0.4183 - val_bottleneck_loss: 0.1993 - val_out_s_loss: 0.2190 - val_bottleneck_accuracy: 0.5858 - val_out_s_accuracy: 0.8100\n",
      "Epoch 47/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3521 - bottleneck_loss: 0.2077 - out_s_loss: 0.1444 - bottleneck_accuracy: 0.5567 - out_s_accuracy: 0.8094 - val_loss: 0.4147 - val_bottleneck_loss: 0.1984 - val_out_s_loss: 0.2164 - val_bottleneck_accuracy: 0.5869 - val_out_s_accuracy: 0.7933\n",
      "Epoch 48/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3536 - bottleneck_loss: 0.2074 - out_s_loss: 0.1462 - bottleneck_accuracy: 0.5582 - out_s_accuracy: 0.8117 - val_loss: 0.4185 - val_bottleneck_loss: 0.2000 - val_out_s_loss: 0.2185 - val_bottleneck_accuracy: 0.5628 - val_out_s_accuracy: 0.7667\n",
      "Epoch 49/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3519 - bottleneck_loss: 0.2065 - out_s_loss: 0.1454 - bottleneck_accuracy: 0.5563 - out_s_accuracy: 0.8039 - val_loss: 0.4118 - val_bottleneck_loss: 0.1975 - val_out_s_loss: 0.2142 - val_bottleneck_accuracy: 0.5286 - val_out_s_accuracy: 0.7867\n",
      "Epoch 50/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3486 - bottleneck_loss: 0.2050 - out_s_loss: 0.1436 - bottleneck_accuracy: 0.5607 - out_s_accuracy: 0.8144 - val_loss: 0.4136 - val_bottleneck_loss: 0.1959 - val_out_s_loss: 0.2177 - val_bottleneck_accuracy: 0.5485 - val_out_s_accuracy: 0.7933\n",
      "Epoch 51/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3495 - bottleneck_loss: 0.2046 - out_s_loss: 0.1449 - bottleneck_accuracy: 0.5586 - out_s_accuracy: 0.8272 - val_loss: 0.4118 - val_bottleneck_loss: 0.1947 - val_out_s_loss: 0.2171 - val_bottleneck_accuracy: 0.5614 - val_out_s_accuracy: 0.8000\n",
      "Epoch 52/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3471 - bottleneck_loss: 0.2036 - out_s_loss: 0.1435 - bottleneck_accuracy: 0.5591 - out_s_accuracy: 0.8167 - val_loss: 0.4107 - val_bottleneck_loss: 0.1950 - val_out_s_loss: 0.2157 - val_bottleneck_accuracy: 0.5670 - val_out_s_accuracy: 0.7633\n",
      "Epoch 53/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3452 - bottleneck_loss: 0.2027 - out_s_loss: 0.1426 - bottleneck_accuracy: 0.5636 - out_s_accuracy: 0.8183 - val_loss: 0.4092 - val_bottleneck_loss: 0.1942 - val_out_s_loss: 0.2150 - val_bottleneck_accuracy: 0.5792 - val_out_s_accuracy: 0.7900\n",
      "Epoch 54/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3441 - bottleneck_loss: 0.2019 - out_s_loss: 0.1422 - bottleneck_accuracy: 0.5642 - out_s_accuracy: 0.8278 - val_loss: 0.4137 - val_bottleneck_loss: 0.1952 - val_out_s_loss: 0.2185 - val_bottleneck_accuracy: 0.5708 - val_out_s_accuracy: 0.7767\n",
      "Epoch 55/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3440 - bottleneck_loss: 0.2014 - out_s_loss: 0.1427 - bottleneck_accuracy: 0.5645 - out_s_accuracy: 0.8350 - val_loss: 0.4120 - val_bottleneck_loss: 0.1931 - val_out_s_loss: 0.2189 - val_bottleneck_accuracy: 0.5650 - val_out_s_accuracy: 0.8067\n",
      "Epoch 56/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3444 - bottleneck_loss: 0.2010 - out_s_loss: 0.1434 - bottleneck_accuracy: 0.5613 - out_s_accuracy: 0.8317 - val_loss: 0.4107 - val_bottleneck_loss: 0.1936 - val_out_s_loss: 0.2171 - val_bottleneck_accuracy: 0.5560 - val_out_s_accuracy: 0.8000\n",
      "Epoch 57/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3459 - bottleneck_loss: 0.2010 - out_s_loss: 0.1448 - bottleneck_accuracy: 0.5669 - out_s_accuracy: 0.8183 - val_loss: 0.4095 - val_bottleneck_loss: 0.1917 - val_out_s_loss: 0.2179 - val_bottleneck_accuracy: 0.5833 - val_out_s_accuracy: 0.8033\n",
      "Epoch 58/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3432 - bottleneck_loss: 0.1999 - out_s_loss: 0.1433 - bottleneck_accuracy: 0.5675 - out_s_accuracy: 0.8522 - val_loss: 0.4053 - val_bottleneck_loss: 0.1907 - val_out_s_loss: 0.2146 - val_bottleneck_accuracy: 0.5855 - val_out_s_accuracy: 0.8033\n",
      "Epoch 59/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3412 - bottleneck_loss: 0.1987 - out_s_loss: 0.1425 - bottleneck_accuracy: 0.5668 - out_s_accuracy: 0.8350 - val_loss: 0.4052 - val_bottleneck_loss: 0.1900 - val_out_s_loss: 0.2152 - val_bottleneck_accuracy: 0.5841 - val_out_s_accuracy: 0.7933\n",
      "Epoch 60/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3405 - bottleneck_loss: 0.1981 - out_s_loss: 0.1424 - bottleneck_accuracy: 0.5702 - out_s_accuracy: 0.8300 - val_loss: 0.4081 - val_bottleneck_loss: 0.1915 - val_out_s_loss: 0.2167 - val_bottleneck_accuracy: 0.5556 - val_out_s_accuracy: 0.8167\n",
      "Epoch 61/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3417 - bottleneck_loss: 0.1982 - out_s_loss: 0.1435 - bottleneck_accuracy: 0.5702 - out_s_accuracy: 0.8417 - val_loss: 0.4063 - val_bottleneck_loss: 0.1900 - val_out_s_loss: 0.2163 - val_bottleneck_accuracy: 0.5793 - val_out_s_accuracy: 0.8067\n",
      "Epoch 62/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3404 - bottleneck_loss: 0.1973 - out_s_loss: 0.1430 - bottleneck_accuracy: 0.5771 - out_s_accuracy: 0.8283 - val_loss: 0.4132 - val_bottleneck_loss: 0.1898 - val_out_s_loss: 0.2234 - val_bottleneck_accuracy: 0.5944 - val_out_s_accuracy: 0.8100\n",
      "Epoch 63/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3418 - bottleneck_loss: 0.1979 - out_s_loss: 0.1439 - bottleneck_accuracy: 0.5702 - out_s_accuracy: 0.8383 - val_loss: 0.4072 - val_bottleneck_loss: 0.1886 - val_out_s_loss: 0.2186 - val_bottleneck_accuracy: 0.5844 - val_out_s_accuracy: 0.7933\n",
      "Epoch 64/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3377 - bottleneck_loss: 0.1959 - out_s_loss: 0.1418 - bottleneck_accuracy: 0.5733 - out_s_accuracy: 0.8406 - val_loss: 0.4069 - val_bottleneck_loss: 0.1885 - val_out_s_loss: 0.2184 - val_bottleneck_accuracy: 0.5897 - val_out_s_accuracy: 0.8400\n",
      "Epoch 65/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3374 - bottleneck_loss: 0.1950 - out_s_loss: 0.1424 - bottleneck_accuracy: 0.5747 - out_s_accuracy: 0.8372 - val_loss: 0.4023 - val_bottleneck_loss: 0.1873 - val_out_s_loss: 0.2150 - val_bottleneck_accuracy: 0.5899 - val_out_s_accuracy: 0.8133\n",
      "Epoch 66/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3365 - bottleneck_loss: 0.1946 - out_s_loss: 0.1419 - bottleneck_accuracy: 0.5760 - out_s_accuracy: 0.8389 - val_loss: 0.4008 - val_bottleneck_loss: 0.1857 - val_out_s_loss: 0.2151 - val_bottleneck_accuracy: 0.5805 - val_out_s_accuracy: 0.7967\n",
      "Epoch 67/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3348 - bottleneck_loss: 0.1934 - out_s_loss: 0.1414 - bottleneck_accuracy: 0.5794 - out_s_accuracy: 0.8428 - val_loss: 0.4041 - val_bottleneck_loss: 0.1851 - val_out_s_loss: 0.2190 - val_bottleneck_accuracy: 0.5936 - val_out_s_accuracy: 0.8200\n",
      "Epoch 68/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3346 - bottleneck_loss: 0.1931 - out_s_loss: 0.1414 - bottleneck_accuracy: 0.5766 - out_s_accuracy: 0.8450 - val_loss: 0.4024 - val_bottleneck_loss: 0.1856 - val_out_s_loss: 0.2168 - val_bottleneck_accuracy: 0.5868 - val_out_s_accuracy: 0.7967\n",
      "Epoch 69/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3342 - bottleneck_loss: 0.1926 - out_s_loss: 0.1416 - bottleneck_accuracy: 0.5776 - out_s_accuracy: 0.8411 - val_loss: 0.3966 - val_bottleneck_loss: 0.1843 - val_out_s_loss: 0.2123 - val_bottleneck_accuracy: 0.5903 - val_out_s_accuracy: 0.8000\n",
      "Epoch 70/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3339 - bottleneck_loss: 0.1921 - out_s_loss: 0.1418 - bottleneck_accuracy: 0.5806 - out_s_accuracy: 0.8461 - val_loss: 0.4042 - val_bottleneck_loss: 0.1863 - val_out_s_loss: 0.2179 - val_bottleneck_accuracy: 0.5792 - val_out_s_accuracy: 0.8400\n",
      "Epoch 71/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3335 - bottleneck_loss: 0.1918 - out_s_loss: 0.1416 - bottleneck_accuracy: 0.5801 - out_s_accuracy: 0.8483 - val_loss: 0.4043 - val_bottleneck_loss: 0.1855 - val_out_s_loss: 0.2188 - val_bottleneck_accuracy: 0.5615 - val_out_s_accuracy: 0.7800\n",
      "Epoch 72/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3318 - bottleneck_loss: 0.1908 - out_s_loss: 0.1410 - bottleneck_accuracy: 0.5813 - out_s_accuracy: 0.8344 - val_loss: 0.4024 - val_bottleneck_loss: 0.1821 - val_out_s_loss: 0.2202 - val_bottleneck_accuracy: 0.5718 - val_out_s_accuracy: 0.8033\n",
      "Epoch 73/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3305 - bottleneck_loss: 0.1899 - out_s_loss: 0.1406 - bottleneck_accuracy: 0.5820 - out_s_accuracy: 0.8506 - val_loss: 0.3964 - val_bottleneck_loss: 0.1819 - val_out_s_loss: 0.2145 - val_bottleneck_accuracy: 0.5906 - val_out_s_accuracy: 0.8100\n",
      "Epoch 74/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3303 - bottleneck_loss: 0.1894 - out_s_loss: 0.1409 - bottleneck_accuracy: 0.5823 - out_s_accuracy: 0.8650 - val_loss: 0.3992 - val_bottleneck_loss: 0.1818 - val_out_s_loss: 0.2175 - val_bottleneck_accuracy: 0.5563 - val_out_s_accuracy: 0.7900\n",
      "Epoch 75/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3307 - bottleneck_loss: 0.1895 - out_s_loss: 0.1412 - bottleneck_accuracy: 0.5842 - out_s_accuracy: 0.8522 - val_loss: 0.3976 - val_bottleneck_loss: 0.1813 - val_out_s_loss: 0.2163 - val_bottleneck_accuracy: 0.5930 - val_out_s_accuracy: 0.7933\n",
      "Epoch 76/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3295 - bottleneck_loss: 0.1887 - out_s_loss: 0.1409 - bottleneck_accuracy: 0.5838 - out_s_accuracy: 0.8411 - val_loss: 0.3989 - val_bottleneck_loss: 0.1810 - val_out_s_loss: 0.2178 - val_bottleneck_accuracy: 0.6008 - val_out_s_accuracy: 0.8300\n",
      "Epoch 77/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3285 - bottleneck_loss: 0.1879 - out_s_loss: 0.1406 - bottleneck_accuracy: 0.5850 - out_s_accuracy: 0.8356 - val_loss: 0.3970 - val_bottleneck_loss: 0.1801 - val_out_s_loss: 0.2169 - val_bottleneck_accuracy: 0.5973 - val_out_s_accuracy: 0.8067\n",
      "Epoch 78/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3266 - bottleneck_loss: 0.1868 - out_s_loss: 0.1398 - bottleneck_accuracy: 0.5876 - out_s_accuracy: 0.8544 - val_loss: 0.3990 - val_bottleneck_loss: 0.1800 - val_out_s_loss: 0.2190 - val_bottleneck_accuracy: 0.5967 - val_out_s_accuracy: 0.8467\n",
      "Epoch 79/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3269 - bottleneck_loss: 0.1869 - out_s_loss: 0.1400 - bottleneck_accuracy: 0.5899 - out_s_accuracy: 0.8528 - val_loss: 0.3985 - val_bottleneck_loss: 0.1796 - val_out_s_loss: 0.2189 - val_bottleneck_accuracy: 0.5558 - val_out_s_accuracy: 0.7967\n",
      "Epoch 80/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3282 - bottleneck_loss: 0.1868 - out_s_loss: 0.1415 - bottleneck_accuracy: 0.5867 - out_s_accuracy: 0.8511 - val_loss: 0.4045 - val_bottleneck_loss: 0.1815 - val_out_s_loss: 0.2230 - val_bottleneck_accuracy: 0.5766 - val_out_s_accuracy: 0.8100\n",
      "Epoch 81/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3265 - bottleneck_loss: 0.1861 - out_s_loss: 0.1404 - bottleneck_accuracy: 0.5861 - out_s_accuracy: 0.8461 - val_loss: 0.3963 - val_bottleneck_loss: 0.1787 - val_out_s_loss: 0.2175 - val_bottleneck_accuracy: 0.6118 - val_out_s_accuracy: 0.8133\n",
      "Epoch 82/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3259 - bottleneck_loss: 0.1853 - out_s_loss: 0.1406 - bottleneck_accuracy: 0.5903 - out_s_accuracy: 0.8494 - val_loss: 0.3968 - val_bottleneck_loss: 0.1782 - val_out_s_loss: 0.2187 - val_bottleneck_accuracy: 0.5910 - val_out_s_accuracy: 0.8267\n",
      "Epoch 83/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3306 - bottleneck_loss: 0.1866 - out_s_loss: 0.1439 - bottleneck_accuracy: 0.5844 - out_s_accuracy: 0.8467 - val_loss: 0.3986 - val_bottleneck_loss: 0.1785 - val_out_s_loss: 0.2201 - val_bottleneck_accuracy: 0.5670 - val_out_s_accuracy: 0.8200\n",
      "Epoch 84/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3260 - bottleneck_loss: 0.1848 - out_s_loss: 0.1412 - bottleneck_accuracy: 0.5896 - out_s_accuracy: 0.8556 - val_loss: 0.4024 - val_bottleneck_loss: 0.1783 - val_out_s_loss: 0.2241 - val_bottleneck_accuracy: 0.5952 - val_out_s_accuracy: 0.8133\n",
      "Epoch 85/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3260 - bottleneck_loss: 0.1848 - out_s_loss: 0.1412 - bottleneck_accuracy: 0.5877 - out_s_accuracy: 0.8578 - val_loss: 0.3969 - val_bottleneck_loss: 0.1763 - val_out_s_loss: 0.2206 - val_bottleneck_accuracy: 0.6084 - val_out_s_accuracy: 0.8467\n",
      "Epoch 86/1000\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.3253 - bottleneck_loss: 0.1844 - out_s_loss: 0.1409 - bottleneck_accuracy: 0.5899 - out_s_accuracy: 0.8550 - val_loss: 0.4001 - val_bottleneck_loss: 0.1777 - val_out_s_loss: 0.2223 - val_bottleneck_accuracy: 0.6262 - val_out_s_accuracy: 0.8300\n",
      "Epoch 87/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3243 - bottleneck_loss: 0.1837 - out_s_loss: 0.1405 - bottleneck_accuracy: 0.5930 - out_s_accuracy: 0.8544 - val_loss: 0.3961 - val_bottleneck_loss: 0.1767 - val_out_s_loss: 0.2194 - val_bottleneck_accuracy: 0.6036 - val_out_s_accuracy: 0.8333\n",
      "Epoch 88/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3235 - bottleneck_loss: 0.1834 - out_s_loss: 0.1401 - bottleneck_accuracy: 0.5904 - out_s_accuracy: 0.8500 - val_loss: 0.3952 - val_bottleneck_loss: 0.1764 - val_out_s_loss: 0.2188 - val_bottleneck_accuracy: 0.6120 - val_out_s_accuracy: 0.7967\n",
      "Epoch 89/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3249 - bottleneck_loss: 0.1832 - out_s_loss: 0.1417 - bottleneck_accuracy: 0.5905 - out_s_accuracy: 0.8439 - val_loss: 0.4002 - val_bottleneck_loss: 0.1763 - val_out_s_loss: 0.2239 - val_bottleneck_accuracy: 0.6094 - val_out_s_accuracy: 0.8333\n",
      "Epoch 90/1000\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.3224 - bottleneck_loss: 0.1819 - out_s_loss: 0.1405 - bottleneck_accuracy: 0.5921 - out_s_accuracy: 0.8406 - val_loss: 0.3938 - val_bottleneck_loss: 0.1751 - val_out_s_loss: 0.2187 - val_bottleneck_accuracy: 0.6033 - val_out_s_accuracy: 0.8300\n",
      "Epoch 91/1000\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.3221 - bottleneck_loss: 0.1819 - out_s_loss: 0.1402 - bottleneck_accuracy: 0.5913 - out_s_accuracy: 0.8617 - val_loss: 0.3976 - val_bottleneck_loss: 0.1758 - val_out_s_loss: 0.2218 - val_bottleneck_accuracy: 0.5979 - val_out_s_accuracy: 0.8067\n",
      "Epoch 92/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3230 - bottleneck_loss: 0.1821 - out_s_loss: 0.1409 - bottleneck_accuracy: 0.5927 - out_s_accuracy: 0.8578 - val_loss: 0.3926 - val_bottleneck_loss: 0.1744 - val_out_s_loss: 0.2182 - val_bottleneck_accuracy: 0.5997 - val_out_s_accuracy: 0.8267\n",
      "Epoch 93/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3217 - bottleneck_loss: 0.1813 - out_s_loss: 0.1404 - bottleneck_accuracy: 0.5945 - out_s_accuracy: 0.8578 - val_loss: 0.3984 - val_bottleneck_loss: 0.1750 - val_out_s_loss: 0.2234 - val_bottleneck_accuracy: 0.6167 - val_out_s_accuracy: 0.8400\n",
      "Epoch 94/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3216 - bottleneck_loss: 0.1811 - out_s_loss: 0.1406 - bottleneck_accuracy: 0.5942 - out_s_accuracy: 0.8578 - val_loss: 0.3923 - val_bottleneck_loss: 0.1737 - val_out_s_loss: 0.2186 - val_bottleneck_accuracy: 0.5867 - val_out_s_accuracy: 0.8533\n",
      "Epoch 95/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3208 - bottleneck_loss: 0.1808 - out_s_loss: 0.1399 - bottleneck_accuracy: 0.5939 - out_s_accuracy: 0.8600 - val_loss: 0.3944 - val_bottleneck_loss: 0.1733 - val_out_s_loss: 0.2211 - val_bottleneck_accuracy: 0.6152 - val_out_s_accuracy: 0.8533\n",
      "Epoch 96/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3204 - bottleneck_loss: 0.1805 - out_s_loss: 0.1399 - bottleneck_accuracy: 0.5966 - out_s_accuracy: 0.8550 - val_loss: 0.3988 - val_bottleneck_loss: 0.1762 - val_out_s_loss: 0.2227 - val_bottleneck_accuracy: 0.6182 - val_out_s_accuracy: 0.8100\n",
      "Epoch 97/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3253 - bottleneck_loss: 0.1820 - out_s_loss: 0.1433 - bottleneck_accuracy: 0.5947 - out_s_accuracy: 0.8439 - val_loss: 0.3988 - val_bottleneck_loss: 0.1736 - val_out_s_loss: 0.2252 - val_bottleneck_accuracy: 0.6153 - val_out_s_accuracy: 0.8133\n",
      "Epoch 98/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3208 - bottleneck_loss: 0.1802 - out_s_loss: 0.1405 - bottleneck_accuracy: 0.5964 - out_s_accuracy: 0.8478 - val_loss: 0.3896 - val_bottleneck_loss: 0.1727 - val_out_s_loss: 0.2169 - val_bottleneck_accuracy: 0.5964 - val_out_s_accuracy: 0.8600\n",
      "Epoch 99/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3184 - bottleneck_loss: 0.1792 - out_s_loss: 0.1392 - bottleneck_accuracy: 0.5927 - out_s_accuracy: 0.8656 - val_loss: 0.3907 - val_bottleneck_loss: 0.1718 - val_out_s_loss: 0.2189 - val_bottleneck_accuracy: 0.6081 - val_out_s_accuracy: 0.8333\n",
      "Epoch 100/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3188 - bottleneck_loss: 0.1792 - out_s_loss: 0.1396 - bottleneck_accuracy: 0.5970 - out_s_accuracy: 0.8550 - val_loss: 0.3932 - val_bottleneck_loss: 0.1720 - val_out_s_loss: 0.2212 - val_bottleneck_accuracy: 0.6032 - val_out_s_accuracy: 0.7900\n",
      "Epoch 101/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3184 - bottleneck_loss: 0.1789 - out_s_loss: 0.1395 - bottleneck_accuracy: 0.5929 - out_s_accuracy: 0.8544 - val_loss: 0.3907 - val_bottleneck_loss: 0.1719 - val_out_s_loss: 0.2188 - val_bottleneck_accuracy: 0.6108 - val_out_s_accuracy: 0.8333\n",
      "Epoch 102/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3182 - bottleneck_loss: 0.1787 - out_s_loss: 0.1395 - bottleneck_accuracy: 0.5978 - out_s_accuracy: 0.8528 - val_loss: 0.3900 - val_bottleneck_loss: 0.1722 - val_out_s_loss: 0.2178 - val_bottleneck_accuracy: 0.5790 - val_out_s_accuracy: 0.8200\n",
      "Epoch 103/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3174 - bottleneck_loss: 0.1784 - out_s_loss: 0.1390 - bottleneck_accuracy: 0.5942 - out_s_accuracy: 0.8489 - val_loss: 0.3885 - val_bottleneck_loss: 0.1713 - val_out_s_loss: 0.2172 - val_bottleneck_accuracy: 0.6145 - val_out_s_accuracy: 0.8333\n",
      "Epoch 104/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3174 - bottleneck_loss: 0.1781 - out_s_loss: 0.1393 - bottleneck_accuracy: 0.5986 - out_s_accuracy: 0.8639 - val_loss: 0.3881 - val_bottleneck_loss: 0.1720 - val_out_s_loss: 0.2161 - val_bottleneck_accuracy: 0.6089 - val_out_s_accuracy: 0.8367\n",
      "Epoch 105/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3179 - bottleneck_loss: 0.1784 - out_s_loss: 0.1395 - bottleneck_accuracy: 0.5945 - out_s_accuracy: 0.8683 - val_loss: 0.3954 - val_bottleneck_loss: 0.1734 - val_out_s_loss: 0.2220 - val_bottleneck_accuracy: 0.5729 - val_out_s_accuracy: 0.8567\n",
      "Epoch 106/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3178 - bottleneck_loss: 0.1782 - out_s_loss: 0.1397 - bottleneck_accuracy: 0.5944 - out_s_accuracy: 0.8594 - val_loss: 0.3915 - val_bottleneck_loss: 0.1727 - val_out_s_loss: 0.2188 - val_bottleneck_accuracy: 0.6173 - val_out_s_accuracy: 0.8267\n",
      "Epoch 107/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3168 - bottleneck_loss: 0.1775 - out_s_loss: 0.1393 - bottleneck_accuracy: 0.5973 - out_s_accuracy: 0.8594 - val_loss: 0.3891 - val_bottleneck_loss: 0.1698 - val_out_s_loss: 0.2193 - val_bottleneck_accuracy: 0.6089 - val_out_s_accuracy: 0.8400\n",
      "Epoch 108/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3163 - bottleneck_loss: 0.1770 - out_s_loss: 0.1393 - bottleneck_accuracy: 0.5989 - out_s_accuracy: 0.8539 - val_loss: 0.3925 - val_bottleneck_loss: 0.1721 - val_out_s_loss: 0.2204 - val_bottleneck_accuracy: 0.6126 - val_out_s_accuracy: 0.8167\n",
      "Epoch 109/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3194 - bottleneck_loss: 0.1784 - out_s_loss: 0.1411 - bottleneck_accuracy: 0.5960 - out_s_accuracy: 0.8700 - val_loss: 0.3934 - val_bottleneck_loss: 0.1705 - val_out_s_loss: 0.2229 - val_bottleneck_accuracy: 0.6242 - val_out_s_accuracy: 0.8433\n",
      "Epoch 110/1000\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.3162 - bottleneck_loss: 0.1769 - out_s_loss: 0.1393 - bottleneck_accuracy: 0.5967 - out_s_accuracy: 0.8622 - val_loss: 0.3898 - val_bottleneck_loss: 0.1702 - val_out_s_loss: 0.2195 - val_bottleneck_accuracy: 0.6101 - val_out_s_accuracy: 0.8367\n",
      "Epoch 111/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3146 - bottleneck_loss: 0.1762 - out_s_loss: 0.1384 - bottleneck_accuracy: 0.5979 - out_s_accuracy: 0.8539 - val_loss: 0.3881 - val_bottleneck_loss: 0.1697 - val_out_s_loss: 0.2184 - val_bottleneck_accuracy: 0.6204 - val_out_s_accuracy: 0.8200\n",
      "Epoch 112/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3145 - bottleneck_loss: 0.1760 - out_s_loss: 0.1385 - bottleneck_accuracy: 0.5990 - out_s_accuracy: 0.8556 - val_loss: 0.3877 - val_bottleneck_loss: 0.1689 - val_out_s_loss: 0.2188 - val_bottleneck_accuracy: 0.6183 - val_out_s_accuracy: 0.8367\n",
      "Epoch 113/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3146 - bottleneck_loss: 0.1758 - out_s_loss: 0.1388 - bottleneck_accuracy: 0.6009 - out_s_accuracy: 0.8594 - val_loss: 0.3850 - val_bottleneck_loss: 0.1691 - val_out_s_loss: 0.2159 - val_bottleneck_accuracy: 0.6116 - val_out_s_accuracy: 0.8233\n",
      "Epoch 114/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3142 - bottleneck_loss: 0.1756 - out_s_loss: 0.1386 - bottleneck_accuracy: 0.6012 - out_s_accuracy: 0.8583 - val_loss: 0.3872 - val_bottleneck_loss: 0.1690 - val_out_s_loss: 0.2183 - val_bottleneck_accuracy: 0.6067 - val_out_s_accuracy: 0.8267\n",
      "Epoch 115/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3147 - bottleneck_loss: 0.1757 - out_s_loss: 0.1390 - bottleneck_accuracy: 0.5962 - out_s_accuracy: 0.8650 - val_loss: 0.3872 - val_bottleneck_loss: 0.1694 - val_out_s_loss: 0.2179 - val_bottleneck_accuracy: 0.5894 - val_out_s_accuracy: 0.8267\n",
      "Epoch 116/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3145 - bottleneck_loss: 0.1754 - out_s_loss: 0.1391 - bottleneck_accuracy: 0.6005 - out_s_accuracy: 0.8672 - val_loss: 0.3861 - val_bottleneck_loss: 0.1685 - val_out_s_loss: 0.2176 - val_bottleneck_accuracy: 0.6135 - val_out_s_accuracy: 0.8533\n",
      "Epoch 117/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3145 - bottleneck_loss: 0.1754 - out_s_loss: 0.1391 - bottleneck_accuracy: 0.5976 - out_s_accuracy: 0.8706 - val_loss: 0.3857 - val_bottleneck_loss: 0.1692 - val_out_s_loss: 0.2165 - val_bottleneck_accuracy: 0.6094 - val_out_s_accuracy: 0.8267\n",
      "Epoch 118/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3162 - bottleneck_loss: 0.1761 - out_s_loss: 0.1401 - bottleneck_accuracy: 0.6022 - out_s_accuracy: 0.8667 - val_loss: 0.3897 - val_bottleneck_loss: 0.1699 - val_out_s_loss: 0.2198 - val_bottleneck_accuracy: 0.6077 - val_out_s_accuracy: 0.8300\n",
      "Epoch 119/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3159 - bottleneck_loss: 0.1755 - out_s_loss: 0.1404 - bottleneck_accuracy: 0.5948 - out_s_accuracy: 0.8650 - val_loss: 0.3896 - val_bottleneck_loss: 0.1676 - val_out_s_loss: 0.2220 - val_bottleneck_accuracy: 0.5906 - val_out_s_accuracy: 0.8367\n",
      "Epoch 120/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3137 - bottleneck_loss: 0.1746 - out_s_loss: 0.1391 - bottleneck_accuracy: 0.6019 - out_s_accuracy: 0.8728 - val_loss: 0.3863 - val_bottleneck_loss: 0.1678 - val_out_s_loss: 0.2185 - val_bottleneck_accuracy: 0.6009 - val_out_s_accuracy: 0.8300\n",
      "Epoch 121/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3138 - bottleneck_loss: 0.1747 - out_s_loss: 0.1391 - bottleneck_accuracy: 0.5975 - out_s_accuracy: 0.8683 - val_loss: 0.3931 - val_bottleneck_loss: 0.1683 - val_out_s_loss: 0.2248 - val_bottleneck_accuracy: 0.6093 - val_out_s_accuracy: 0.8633\n",
      "Epoch 122/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3140 - bottleneck_loss: 0.1744 - out_s_loss: 0.1395 - bottleneck_accuracy: 0.6018 - out_s_accuracy: 0.8650 - val_loss: 0.3864 - val_bottleneck_loss: 0.1683 - val_out_s_loss: 0.2182 - val_bottleneck_accuracy: 0.6187 - val_out_s_accuracy: 0.8267\n",
      "Epoch 123/1000\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.3121 - bottleneck_loss: 0.1739 - out_s_loss: 0.1382 - bottleneck_accuracy: 0.6030 - out_s_accuracy: 0.8733 - val_loss: 0.3898 - val_bottleneck_loss: 0.1679 - val_out_s_loss: 0.2219 - val_bottleneck_accuracy: 0.6190 - val_out_s_accuracy: 0.8600\n",
      "Epoch 124/1000\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.3123 - bottleneck_loss: 0.1738 - out_s_loss: 0.1384 - bottleneck_accuracy: 0.6015 - out_s_accuracy: 0.8711 - val_loss: 0.3833 - val_bottleneck_loss: 0.1680 - val_out_s_loss: 0.2153 - val_bottleneck_accuracy: 0.6255 - val_out_s_accuracy: 0.8567\n",
      "Epoch 125/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3119 - bottleneck_loss: 0.1735 - out_s_loss: 0.1384 - bottleneck_accuracy: 0.5998 - out_s_accuracy: 0.8828 - val_loss: 0.3903 - val_bottleneck_loss: 0.1687 - val_out_s_loss: 0.2216 - val_bottleneck_accuracy: 0.6108 - val_out_s_accuracy: 0.8433\n",
      "Epoch 126/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3144 - bottleneck_loss: 0.1743 - out_s_loss: 0.1401 - bottleneck_accuracy: 0.5968 - out_s_accuracy: 0.8778 - val_loss: 0.3904 - val_bottleneck_loss: 0.1682 - val_out_s_loss: 0.2222 - val_bottleneck_accuracy: 0.6108 - val_out_s_accuracy: 0.8567\n",
      "Epoch 127/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3121 - bottleneck_loss: 0.1733 - out_s_loss: 0.1388 - bottleneck_accuracy: 0.6037 - out_s_accuracy: 0.8772 - val_loss: 0.3870 - val_bottleneck_loss: 0.1671 - val_out_s_loss: 0.2199 - val_bottleneck_accuracy: 0.6214 - val_out_s_accuracy: 0.8333\n",
      "Epoch 128/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3117 - bottleneck_loss: 0.1730 - out_s_loss: 0.1387 - bottleneck_accuracy: 0.6031 - out_s_accuracy: 0.8683 - val_loss: 0.3868 - val_bottleneck_loss: 0.1668 - val_out_s_loss: 0.2200 - val_bottleneck_accuracy: 0.6080 - val_out_s_accuracy: 0.8433\n",
      "Epoch 129/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3112 - bottleneck_loss: 0.1729 - out_s_loss: 0.1383 - bottleneck_accuracy: 0.6012 - out_s_accuracy: 0.8694 - val_loss: 0.3903 - val_bottleneck_loss: 0.1683 - val_out_s_loss: 0.2220 - val_bottleneck_accuracy: 0.6194 - val_out_s_accuracy: 0.8433\n",
      "Epoch 130/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3130 - bottleneck_loss: 0.1738 - out_s_loss: 0.1393 - bottleneck_accuracy: 0.5974 - out_s_accuracy: 0.8778 - val_loss: 0.3840 - val_bottleneck_loss: 0.1672 - val_out_s_loss: 0.2168 - val_bottleneck_accuracy: 0.6167 - val_out_s_accuracy: 0.8667\n",
      "Epoch 131/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3122 - bottleneck_loss: 0.1731 - out_s_loss: 0.1391 - bottleneck_accuracy: 0.6039 - out_s_accuracy: 0.8694 - val_loss: 0.3869 - val_bottleneck_loss: 0.1666 - val_out_s_loss: 0.2203 - val_bottleneck_accuracy: 0.6181 - val_out_s_accuracy: 0.8633\n",
      "Epoch 132/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3116 - bottleneck_loss: 0.1726 - out_s_loss: 0.1390 - bottleneck_accuracy: 0.6043 - out_s_accuracy: 0.8750 - val_loss: 0.3888 - val_bottleneck_loss: 0.1673 - val_out_s_loss: 0.2215 - val_bottleneck_accuracy: 0.6104 - val_out_s_accuracy: 0.8733\n",
      "Epoch 133/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3120 - bottleneck_loss: 0.1727 - out_s_loss: 0.1393 - bottleneck_accuracy: 0.6016 - out_s_accuracy: 0.8833 - val_loss: 0.3844 - val_bottleneck_loss: 0.1661 - val_out_s_loss: 0.2183 - val_bottleneck_accuracy: 0.6278 - val_out_s_accuracy: 0.8733\n",
      "Epoch 134/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3106 - bottleneck_loss: 0.1720 - out_s_loss: 0.1386 - bottleneck_accuracy: 0.6036 - out_s_accuracy: 0.8739 - val_loss: 0.3825 - val_bottleneck_loss: 0.1662 - val_out_s_loss: 0.2163 - val_bottleneck_accuracy: 0.6275 - val_out_s_accuracy: 0.8533\n",
      "Epoch 135/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3095 - bottleneck_loss: 0.1715 - out_s_loss: 0.1380 - bottleneck_accuracy: 0.6046 - out_s_accuracy: 0.8833 - val_loss: 0.3845 - val_bottleneck_loss: 0.1655 - val_out_s_loss: 0.2190 - val_bottleneck_accuracy: 0.6152 - val_out_s_accuracy: 0.8300\n",
      "Epoch 136/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3088 - bottleneck_loss: 0.1712 - out_s_loss: 0.1376 - bottleneck_accuracy: 0.6064 - out_s_accuracy: 0.8817 - val_loss: 0.3848 - val_bottleneck_loss: 0.1651 - val_out_s_loss: 0.2197 - val_bottleneck_accuracy: 0.6275 - val_out_s_accuracy: 0.8800\n",
      "Epoch 137/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3094 - bottleneck_loss: 0.1714 - out_s_loss: 0.1380 - bottleneck_accuracy: 0.6054 - out_s_accuracy: 0.8689 - val_loss: 0.3860 - val_bottleneck_loss: 0.1660 - val_out_s_loss: 0.2201 - val_bottleneck_accuracy: 0.5910 - val_out_s_accuracy: 0.8600\n",
      "Epoch 138/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3103 - bottleneck_loss: 0.1716 - out_s_loss: 0.1387 - bottleneck_accuracy: 0.6021 - out_s_accuracy: 0.8733 - val_loss: 0.3873 - val_bottleneck_loss: 0.1662 - val_out_s_loss: 0.2211 - val_bottleneck_accuracy: 0.6211 - val_out_s_accuracy: 0.8633\n",
      "Epoch 139/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3096 - bottleneck_loss: 0.1712 - out_s_loss: 0.1383 - bottleneck_accuracy: 0.6039 - out_s_accuracy: 0.8878 - val_loss: 0.3851 - val_bottleneck_loss: 0.1653 - val_out_s_loss: 0.2198 - val_bottleneck_accuracy: 0.6016 - val_out_s_accuracy: 0.8500\n",
      "Epoch 140/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3088 - bottleneck_loss: 0.1710 - out_s_loss: 0.1378 - bottleneck_accuracy: 0.6057 - out_s_accuracy: 0.8800 - val_loss: 0.3822 - val_bottleneck_loss: 0.1655 - val_out_s_loss: 0.2167 - val_bottleneck_accuracy: 0.6234 - val_out_s_accuracy: 0.8267\n",
      "Epoch 141/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3102 - bottleneck_loss: 0.1713 - out_s_loss: 0.1390 - bottleneck_accuracy: 0.6037 - out_s_accuracy: 0.8639 - val_loss: 0.3877 - val_bottleneck_loss: 0.1660 - val_out_s_loss: 0.2217 - val_bottleneck_accuracy: 0.6191 - val_out_s_accuracy: 0.8233\n",
      "Epoch 142/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3102 - bottleneck_loss: 0.1713 - out_s_loss: 0.1389 - bottleneck_accuracy: 0.6018 - out_s_accuracy: 0.8722 - val_loss: 0.3853 - val_bottleneck_loss: 0.1654 - val_out_s_loss: 0.2199 - val_bottleneck_accuracy: 0.6084 - val_out_s_accuracy: 0.8533\n",
      "Epoch 143/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3099 - bottleneck_loss: 0.1711 - out_s_loss: 0.1388 - bottleneck_accuracy: 0.6043 - out_s_accuracy: 0.8706 - val_loss: 0.3897 - val_bottleneck_loss: 0.1658 - val_out_s_loss: 0.2239 - val_bottleneck_accuracy: 0.6347 - val_out_s_accuracy: 0.8633\n",
      "Epoch 144/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3104 - bottleneck_loss: 0.1711 - out_s_loss: 0.1394 - bottleneck_accuracy: 0.6039 - out_s_accuracy: 0.8744 - val_loss: 0.3859 - val_bottleneck_loss: 0.1662 - val_out_s_loss: 0.2196 - val_bottleneck_accuracy: 0.6167 - val_out_s_accuracy: 0.8300\n",
      "Epoch 145/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3088 - bottleneck_loss: 0.1704 - out_s_loss: 0.1384 - bottleneck_accuracy: 0.6030 - out_s_accuracy: 0.8650 - val_loss: 0.3869 - val_bottleneck_loss: 0.1658 - val_out_s_loss: 0.2211 - val_bottleneck_accuracy: 0.6322 - val_out_s_accuracy: 0.8233\n",
      "Epoch 146/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3080 - bottleneck_loss: 0.1699 - out_s_loss: 0.1381 - bottleneck_accuracy: 0.6071 - out_s_accuracy: 0.8789 - val_loss: 0.3850 - val_bottleneck_loss: 0.1652 - val_out_s_loss: 0.2198 - val_bottleneck_accuracy: 0.6214 - val_out_s_accuracy: 0.8733\n",
      "Epoch 147/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3101 - bottleneck_loss: 0.1707 - out_s_loss: 0.1394 - bottleneck_accuracy: 0.6052 - out_s_accuracy: 0.8883 - val_loss: 0.3862 - val_bottleneck_loss: 0.1639 - val_out_s_loss: 0.2223 - val_bottleneck_accuracy: 0.6136 - val_out_s_accuracy: 0.8667\n",
      "Epoch 148/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3087 - bottleneck_loss: 0.1701 - out_s_loss: 0.1386 - bottleneck_accuracy: 0.6048 - out_s_accuracy: 0.8822 - val_loss: 0.3861 - val_bottleneck_loss: 0.1656 - val_out_s_loss: 0.2206 - val_bottleneck_accuracy: 0.6138 - val_out_s_accuracy: 0.8533\n",
      "Epoch 149/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3079 - bottleneck_loss: 0.1697 - out_s_loss: 0.1382 - bottleneck_accuracy: 0.6063 - out_s_accuracy: 0.8756 - val_loss: 0.3860 - val_bottleneck_loss: 0.1644 - val_out_s_loss: 0.2215 - val_bottleneck_accuracy: 0.6354 - val_out_s_accuracy: 0.8833\n",
      "Epoch 150/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3076 - bottleneck_loss: 0.1698 - out_s_loss: 0.1379 - bottleneck_accuracy: 0.6052 - out_s_accuracy: 0.8906 - val_loss: 0.3846 - val_bottleneck_loss: 0.1637 - val_out_s_loss: 0.2209 - val_bottleneck_accuracy: 0.6173 - val_out_s_accuracy: 0.8533\n",
      "Epoch 151/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3067 - bottleneck_loss: 0.1690 - out_s_loss: 0.1377 - bottleneck_accuracy: 0.6059 - out_s_accuracy: 0.8767 - val_loss: 0.3848 - val_bottleneck_loss: 0.1634 - val_out_s_loss: 0.2214 - val_bottleneck_accuracy: 0.6267 - val_out_s_accuracy: 0.8700\n",
      "Epoch 152/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3069 - bottleneck_loss: 0.1691 - out_s_loss: 0.1379 - bottleneck_accuracy: 0.6061 - out_s_accuracy: 0.8844 - val_loss: 0.3873 - val_bottleneck_loss: 0.1644 - val_out_s_loss: 0.2229 - val_bottleneck_accuracy: 0.6158 - val_out_s_accuracy: 0.8800\n",
      "Epoch 153/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3106 - bottleneck_loss: 0.1710 - out_s_loss: 0.1396 - bottleneck_accuracy: 0.6010 - out_s_accuracy: 0.8717 - val_loss: 0.3861 - val_bottleneck_loss: 0.1638 - val_out_s_loss: 0.2223 - val_bottleneck_accuracy: 0.5777 - val_out_s_accuracy: 0.8733\n",
      "Epoch 154/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3073 - bottleneck_loss: 0.1692 - out_s_loss: 0.1381 - bottleneck_accuracy: 0.6075 - out_s_accuracy: 0.8883 - val_loss: 0.3844 - val_bottleneck_loss: 0.1629 - val_out_s_loss: 0.2215 - val_bottleneck_accuracy: 0.6257 - val_out_s_accuracy: 0.8400\n",
      "Epoch 155/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.3069 - bottleneck_loss: 0.1691 - out_s_loss: 0.1378 - bottleneck_accuracy: 0.6071 - out_s_accuracy: 0.8922 - val_loss: 0.3827 - val_bottleneck_loss: 0.1628 - val_out_s_loss: 0.2199 - val_bottleneck_accuracy: 0.6059 - val_out_s_accuracy: 0.8467\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = st_model.fit(\n",
    "    np.array(x_train), [np.array(bottleneck_train),np.array(output_train)], \n",
    "    validation_data=(x_val, [np.array(bottleneck_val), np.array(output_val)]), \n",
    "    epochs=1000, \n",
    "    batch_size=32, \n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3851 - bottleneck_loss: 0.1718 - out_s_loss: 0.2134 - bottleneck_accuracy: 0.6093 - out_s_accuracy: 0.8422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38512498140335083,\n",
       " 0.17176561057567596,\n",
       " 0.21335937082767487,\n",
       " 0.609343409538269,\n",
       " 0.8422222137451172]"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_model.evaluate(np.array(x_test), [bottleneck_test, output_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "st_model.save(\"Jordanian_S_94.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1 = load_model(\"Jordanian_S_94.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)       [(None, 1601, 6, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " average_pooling2d_28 (Aver  (None, 228, 6, 1)            0         ['input_29[0][0]']            \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_158 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_28[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_159 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_28[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_160 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_28[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_161 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_28[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_28[0][0]']\n",
      "                                                                                                  \n",
      " add_30 (Add)                (None, 45, 1, 32)            0         ['conv2d_158[0][0]',          \n",
      "                                                                     'conv2d_159[0][0]',          \n",
      "                                                                     'conv2d_160[0][0]',          \n",
      "                                                                     'conv2d_161[0][0]',          \n",
      "                                                                     'conv2d_162[0][0]']          \n",
      "                                                                                                  \n",
      " bottleneck (Conv2D)         (None, 44, 1, 32)            2080      ['add_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_163 (Conv2D)         (None, 38, 1, 32)            7200      ['bottleneck[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_23 (Flatten)        (None, 1216)                 0         ['conv2d_163[0][0]']          \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 45)                   54765     ['flatten_23[0][0]']          \n",
      "                                                                                                  \n",
      " out_s (Dense)               (None, 6)                    276       ['dense_26[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70081 (273.75 KB)\n",
      "Trainable params: 70081 (273.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 3ms/step - loss: 0.3581 - bottleneck_loss: 0.1557 - out_s_loss: 0.2024 - bottleneck_accuracy: 0.5974 - out_s_accuracy: 0.9411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35808807611465454,\n",
       " 0.15567706525325775,\n",
       " 0.2024109661579132,\n",
       " 0.5974242687225342,\n",
       " 0.9411110877990723]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(np.array(x_test), [bottleneck_test, output_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_77\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_98 (InputLayer)       [(None, 1601, 6, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " average_pooling2d_80 (Aver  (None, 228, 6, 1)            0         ['input_98[0][0]']            \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_456 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_80[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_457 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_80[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_458 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_80[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_459 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_80[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_460 (Conv2D)         (None, 45, 1, 32)            1152      ['average_pooling2d_80[0][0]']\n",
      "                                                                                                  \n",
      " add_80 (Add)                (None, 45, 1, 32)            0         ['conv2d_456[0][0]',          \n",
      "                                                                     'conv2d_457[0][0]',          \n",
      "                                                                     'conv2d_458[0][0]',          \n",
      "                                                                     'conv2d_459[0][0]',          \n",
      "                                                                     'conv2d_460[0][0]']          \n",
      "                                                                                                  \n",
      " bottleneck (Conv2D)         (None, 44, 1, 32)            2080      ['add_80[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_461 (Conv2D)         (None, 38, 1, 32)            7200      ['bottleneck[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_80 (Flatten)        (None, 1216)                 0         ['conv2d_461[0][0]']          \n",
      "                                                                                                  \n",
      " dense_90 (Dense)            (None, 45)                   54765     ['flatten_80[0][0]']          \n",
      "                                                                                                  \n",
      " out_s (Dense)               (None, 6)                    276       ['dense_90[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70081 (273.75 KB)\n",
      "Trainable params: 70081 (273.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(1601, 6, 1))\n",
    "\n",
    "# MaxPooling2D layer directly after input\n",
    "x = layers.AveragePooling2D(pool_size=(7, 1))(inputs)  # Pooling with size (6,1)\n",
    "\n",
    "# Convolutional Layer with Cropping in Each Branch\n",
    "branch1 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "branch2 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "branch3 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "branch4 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "branch5 = layers.Conv2D(32, (7, 5), activation='relu', strides=(5, 2))(x)\n",
    "\n",
    "# Adding the outputs of the five branches\n",
    "x_pre = layers.Add()([branch1, branch2, branch3, branch4, branch5])\n",
    "# x_pre = layers.BatchNormalization()(x_pre)\n",
    "x_b_S = layers.Conv2D(32, (2, 1), name = 'bottleneck')(x_pre)\n",
    "x = layers.Conv2D(32, (7, 1), activation ='relu' )(x_b_S)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Flatten the output\n",
    "cat = layers.Flatten()(x)\n",
    "# avg_S = GlobalAveragePooling2D()(x)\n",
    "# max_S = GlobalMaxPooling2D()(x)\n",
    "# cat = Add()([avg_S, max_S])\n",
    "\n",
    "cat = layers.Dense(45, activation='relu')(cat)\n",
    "# cat = layers.Dropout(0.3)(cat)\n",
    "# cat = layers.BatchNormalization()(cat)\n",
    "# Fully Connected Layer\n",
    "outputs = layers.Dense(6, activation='sigmoid', name='out_s')(cat)  # Output layer with softmax activation for 6 classes\n",
    "\n",
    "# Create the model\n",
    "st_model_o = models.Model(inputs=inputs, outputs= outputs)\n",
    "\n",
    "# Compile the model\n",
    "st_model_o.compile(optimizer='adam', loss= 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "st_model_o.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 0.4247 - accuracy: 0.3794 - val_loss: 0.3655 - val_accuracy: 0.3567\n",
      "Epoch 2/1000\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3396 - accuracy: 0.4228 - val_loss: 0.3288 - val_accuracy: 0.4267\n",
      "Epoch 3/1000\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2959 - accuracy: 0.4644 - val_loss: 0.3032 - val_accuracy: 0.4600\n",
      "Epoch 4/1000\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2652 - accuracy: 0.5050 - val_loss: 0.2901 - val_accuracy: 0.4767\n",
      "Epoch 5/1000\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2459 - accuracy: 0.5417 - val_loss: 0.2697 - val_accuracy: 0.5600\n",
      "Epoch 6/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2313 - accuracy: 0.6061 - val_loss: 0.2602 - val_accuracy: 0.6200\n",
      "Epoch 7/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2197 - accuracy: 0.6683 - val_loss: 0.2521 - val_accuracy: 0.6467\n",
      "Epoch 8/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2115 - accuracy: 0.6989 - val_loss: 0.2507 - val_accuracy: 0.7000\n",
      "Epoch 9/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2037 - accuracy: 0.7206 - val_loss: 0.2581 - val_accuracy: 0.7100\n",
      "Epoch 10/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1969 - accuracy: 0.7228 - val_loss: 0.2525 - val_accuracy: 0.7033\n",
      "Epoch 11/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1885 - accuracy: 0.7383 - val_loss: 0.2441 - val_accuracy: 0.7267\n",
      "Epoch 12/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.7333 - val_loss: 0.2429 - val_accuracy: 0.7200\n",
      "Epoch 13/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1785 - accuracy: 0.7228 - val_loss: 0.2433 - val_accuracy: 0.7433\n",
      "Epoch 14/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.7311 - val_loss: 0.2435 - val_accuracy: 0.7633\n",
      "Epoch 15/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1723 - accuracy: 0.7317 - val_loss: 0.2431 - val_accuracy: 0.7133\n",
      "Epoch 16/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.7706 - val_loss: 0.2447 - val_accuracy: 0.7833\n",
      "Epoch 17/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1636 - accuracy: 0.7728 - val_loss: 0.2410 - val_accuracy: 0.7367\n",
      "Epoch 18/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1638 - accuracy: 0.7717 - val_loss: 0.2466 - val_accuracy: 0.6833\n",
      "Epoch 19/1000\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.1605 - accuracy: 0.7822 - val_loss: 0.2492 - val_accuracy: 0.8133\n",
      "Epoch 20/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1590 - accuracy: 0.7889 - val_loss: 0.2459 - val_accuracy: 0.8200\n",
      "Epoch 21/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1582 - accuracy: 0.8094 - val_loss: 0.2452 - val_accuracy: 0.8467\n",
      "Epoch 22/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.8383 - val_loss: 0.2388 - val_accuracy: 0.7733\n",
      "Epoch 23/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.8122 - val_loss: 0.2468 - val_accuracy: 0.8067\n",
      "Epoch 24/1000\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 0.1522 - accuracy: 0.8150 - val_loss: 0.2462 - val_accuracy: 0.8300\n",
      "Epoch 25/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.8289 - val_loss: 0.2492 - val_accuracy: 0.8233\n",
      "Epoch 26/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.8172 - val_loss: 0.2481 - val_accuracy: 0.7567\n",
      "Epoch 27/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.8394 - val_loss: 0.2533 - val_accuracy: 0.8167\n",
      "Epoch 28/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1496 - accuracy: 0.8206 - val_loss: 0.2491 - val_accuracy: 0.8300\n",
      "Epoch 29/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1506 - accuracy: 0.8433 - val_loss: 0.2465 - val_accuracy: 0.8167\n",
      "Epoch 30/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.8328 - val_loss: 0.2481 - val_accuracy: 0.7767\n",
      "Epoch 31/1000\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.1474 - accuracy: 0.8322 - val_loss: 0.2481 - val_accuracy: 0.8600\n",
      "Epoch 32/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1468 - accuracy: 0.8250 - val_loss: 0.2482 - val_accuracy: 0.8200\n",
      "Epoch 33/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1462 - accuracy: 0.8278 - val_loss: 0.2481 - val_accuracy: 0.8567\n",
      "Epoch 34/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.8483 - val_loss: 0.2488 - val_accuracy: 0.8300\n",
      "Epoch 35/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1466 - accuracy: 0.8361 - val_loss: 0.2531 - val_accuracy: 0.8333\n",
      "Epoch 36/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1454 - accuracy: 0.8439 - val_loss: 0.2533 - val_accuracy: 0.8267\n",
      "Epoch 37/1000\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.1450 - accuracy: 0.8444 - val_loss: 0.2503 - val_accuracy: 0.8133\n"
     ]
    }
   ],
   "source": [
    "# Train the student model using the teacher model's outputs as labels\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=15, restore_best_weights=True)\n",
    "history = st_model_o.fit(np.array(x_train), \n",
    "                      np.array(output_train), \n",
    "                      epochs=1000, \n",
    "                      batch_size=32,\n",
    "                      validation_data = ( np.array(x_val), np.array(output_val) ),\n",
    "                      callbacks = callback)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2418 - accuracy: 0.7689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24179422855377197, 0.7688888907432556]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_model_o.evaluate(np.array(x_test), output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compariosion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU,LSTM, BatchNormalization, Dense, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_112 (InputLayer)      [(None, 1601, 6)]         0         \n",
      "                                                                 \n",
      " average_pooling1d_30 (Aver  (None, 266, 6)            0         \n",
      " agePooling1D)                                                   \n",
      "                                                                 \n",
      " lstm_86 (LSTM)              (None, 266, 16)           1472      \n",
      "                                                                 \n",
      " batch_normalization_110 (B  (None, 266, 16)           64        \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " lstm_87 (LSTM)              (None, 266, 32)           6272      \n",
      "                                                                 \n",
      " batch_normalization_111 (B  (None, 266, 32)           128       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " lstm_88 (LSTM)              (None, 266, 64)           24832     \n",
      "                                                                 \n",
      " batch_normalization_112 (B  (None, 266, 64)           256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " lstm_89 (LSTM)              (None, 266, 128)          98816     \n",
      "                                                                 \n",
      " batch_normalization_113 (B  (None, 266, 128)          512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling1d_31 (Aver  (None, 44, 128)           0         \n",
      " agePooling1D)                                                   \n",
      "                                                                 \n",
      " flatten_89 (Flatten)        (None, 5632)              0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 16)                90128     \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222582 (869.46 KB)\n",
      "Trainable params: 222102 (867.59 KB)\n",
      "Non-trainable params: 480 (1.88 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense, AveragePooling1D\n",
    "\n",
    "# Define the input shape\n",
    "dataInp = Input(shape=(1601, 6))\n",
    "\n",
    "# Apply AveragePooling1D to reduce sequence length\n",
    "x = AveragePooling1D(pool_size=6)(dataInp)  # New sequence length will be 1601/6 = 267\n",
    "\n",
    "# TEACHER RNN MODEL\n",
    "x_T = LSTM(16, return_sequences=True)(x) # Output shape: (267, 32)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(32, return_sequences=True)(x_T) # Output shape: (267, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(64, return_sequences=True)(x_T) # Output shape: (267, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(128, return_sequences=True)(x_T) # Output shape: (267, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "# x_T = LSTM(128, return_sequences=True)(x_T) # Output shape: (267, 128)\n",
    "# x_T = BatchNormalization()(x_T)\n",
    "\n",
    "# Global pooling layers\n",
    "# avg_T = GlobalAveragePooling1D()(x_T)\n",
    "# max_T = GlobalMaxPooling1D()(x_T)\n",
    "# flat_T = Concatenate()([avg_T, max_T])\n",
    "x = AveragePooling1D(pool_size=6)(x_T)\n",
    "flat_T = Flatten()(x)\n",
    "d_T = Dense(16, activation='sigmoid')(flat_T)\n",
    "# Dense and Dropout layers\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(6, activation='sigmoid')(d_T)\n",
    "\n",
    "# Build and compile the model\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher_rnn')\n",
    "model_T.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Display the model summary\n",
    "model_T.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInp = Input(shape = (1601, 6))\n",
    "kernel_size = 3\n",
    "\n",
    "# TEACHER MODEL\n",
    "x_T = Conv1D(kernel_size = 7, filters=64, activation='relu')(dataInp) # 194, 32\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = MaxPooling1D(2)(x_T) # 97, 32\n",
    "x_T = Conv1D(kernel_size = 5, filters=64, activation='relu')(x_T) # 93, 64\n",
    "x_T = MaxPooling1D(2)(x_T) # 46, 64\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=128, activation='relu')(x_T) # 44, 256\n",
    "x_T = Conv1D(kernel_size = kernel_size, filters=256, activation='relu')(x_T) # 42, 32\n",
    "x_T = MaxPooling1D(2)(x_T) # 21, 32\n",
    "# x_T = Conv1D(kernel_size = kernel_size, filters=512, activation='relu')(x_T) # 19, 512\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "max_T = GlobalMaxPooling1D()(x_T)\n",
    "flat_T = Concatenate()([avg_T, max_T])\n",
    "# flat_T = Flatten()(x_T)\n",
    "d_T = Dense(128, activation = 'relu')(flat_T)\n",
    "d_T = Dropout(0.3)(d_T)\n",
    "# d_T = Dense(300, activation = 'relu')(d_T)\n",
    "out_T = Dense(6, activation = 'sigmoid')(d_T)\n",
    "\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher' )\n",
    "model_T.compile(loss = \"sparse_categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_132 (InputLayer)      [(None, 1601, 6)]         0         \n",
      "                                                                 \n",
      " average_pooling1d_61 (Aver  (None, 266, 6)            0         \n",
      " agePooling1D)                                                   \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, 266, 32)           2944      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_186 (B  (None, 266, 32)           128       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 266, 64)           16640     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_187 (B  (None, 266, 64)           256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 266, 128)          66048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_188 (B  (None, 266, 128)          512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " lstm_96 (LSTM)              (None, 266, 128)          131584    \n",
      "                                                                 \n",
      " batch_normalization_189 (B  (None, 266, 128)          512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling1d_62 (Aver  (None, 44, 128)           0         \n",
      " agePooling1D)                                                   \n",
      "                                                                 \n",
      " flatten_100 (Flatten)       (None, 5632)              0         \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 5632)              0         \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 6)                 33798     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252422 (986.02 KB)\n",
      "Trainable params: 251718 (983.27 KB)\n",
      "Non-trainable params: 704 (2.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense, AveragePooling1D\n",
    "\n",
    "# Define the input shape\n",
    "dataInp = Input(shape=(1601, 6))\n",
    "\n",
    "# Apply AveragePooling1D to reduce sequence length\n",
    "x = AveragePooling1D(pool_size=6)(dataInp)  # New sequence length will be 1601/6 = 267\n",
    "\n",
    "# TEACHER RNN MODEL\n",
    "x_T = Bidirectional(LSTM(16, return_sequences=True))(x) # Output shape: (267, 32)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(32, return_sequences=True))(x_T) # Output shape: (267, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = Bidirectional(LSTM(64, return_sequences=True))(x_T) # Output shape: (267, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = LSTM(128, return_sequences=True)(x_T) # Output shape: (267, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "\n",
    "# Global pooling layers\n",
    "# avg_T = GlobalAveragePooling1D()(x_T)\n",
    "# max_T = GlobalMaxPooling1D()(x_T)\n",
    "# flat_T = Concatenate()([avg_T, max_T])\n",
    "x = AveragePooling1D(pool_size=6)(x_T)\n",
    "flat_T = Flatten()(x)\n",
    "# Dense and Dropout layers\n",
    "d_T = Dropout(0.3)(flat_T)\n",
    "out_T = Dense(6, activation='sigmoid')(d_T)\n",
    "\n",
    "# Build and compile the model\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher_rnn')\n",
    "model_T.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Display the model summary\n",
    "model_T.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_132 (InputLayer)      [(None, 1601, 6)]         0         \n",
      "                                                                 \n",
      " average_pooling1d_61 (Aver  (None, 266, 6)            0         \n",
      " agePooling1D)                                                   \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, 266, 32)           2944      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_186 (B  (None, 266, 32)           128       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 266, 64)           16640     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_187 (B  (None, 266, 64)           256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 266, 128)          66048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_188 (B  (None, 266, 128)          512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " lstm_96 (LSTM)              (None, 266, 128)          131584    \n",
      "                                                                 \n",
      " batch_normalization_189 (B  (None, 266, 128)          512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " average_pooling1d_62 (Aver  (None, 44, 128)           0         \n",
      " agePooling1D)                                                   \n",
      "                                                                 \n",
      " flatten_100 (Flatten)       (None, 5632)              0         \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 5632)              0         \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 6)                 33798     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252422 (986.02 KB)\n",
      "Trainable params: 251718 (983.27 KB)\n",
      "Non-trainable params: 704 (2.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_T.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_130 (InputLayer)      [(None, 1601, 6)]         0         \n",
      "                                                                 \n",
      " average_pooling1d_58 (Aver  (None, 266, 6)            0         \n",
      " agePooling1D)                                                   \n",
      "                                                                 \n",
      " gru_68 (GRU)                (None, 266, 32)           3840      \n",
      "                                                                 \n",
      " batch_normalization_179 (B  (None, 266, 32)           128       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " gru_69 (GRU)                (None, 266, 64)           18816     \n",
      "                                                                 \n",
      " batch_normalization_180 (B  (None, 266, 64)           256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " gru_70 (GRU)                (None, 266, 128)          74496     \n",
      "                                                                 \n",
      " batch_normalization_181 (B  (None, 266, 128)          512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " gru_71 (GRU)                (None, 266, 128)          99072     \n",
      "                                                                 \n",
      " batch_normalization_182 (B  (None, 266, 128)          512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " global_average_pooling1d_2  (None, 128)               0         \n",
      " 1 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 214918 (839.52 KB)\n",
      "Trainable params: 214214 (836.77 KB)\n",
      "Non-trainable params: 704 (2.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense, AveragePooling1D\n",
    "\n",
    "# Define the input shape\n",
    "dataInp = Input(shape=(1601, 6))\n",
    "\n",
    "# Apply AveragePooling1D to reduce sequence length\n",
    "x = AveragePooling1D(pool_size=6)(dataInp)  # New sequence length will be 1601/6 = 267\n",
    "\n",
    "# TEACHER RNN MODEL\n",
    "x_T = GRU(32, return_sequences=True)(x) # Output shape: (267, 32)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(64, return_sequences=True)(x_T) # Output shape: (267, 64)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(128, return_sequences=True)(x_T) # Output shape: (267, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "x_T = GRU(128, return_sequences=True)(x_T) # Output shape: (267, 128)\n",
    "x_T = BatchNormalization()(x_T)\n",
    "# x_T = LSTM(128, return_sequences=True)(x_T) # Output shape: (267, 128)\n",
    "# x_T = BatchNormalization()(x_T)\n",
    "\n",
    "# Global pooling layers\n",
    "# avg_T = GlobalAveragePooling1D()(x_T)\n",
    "# max_T = GlobalMaxPooling1D()(x_T)\n",
    "# flat_T = Concatenate()([avg_T, max_T])\n",
    "# x = AveragePooling1D(pool_size=6)(x_T)\n",
    "avg_T = GlobalAveragePooling1D()(x_T)\n",
    "# max_T = GlobalMaxPooling1D()(x_T)\n",
    "# flat_T = Concatenate()([avg_T, max_T])\n",
    "# flat_T = Flatten()(x)\n",
    "d_T = Dense(128, activation='sigmoid')(avg_T)\n",
    "# Dense and Dropout layers\n",
    "# d_T = Dropout(0.3)(d_T)\n",
    "out_T = Dense(6, activation='sigmoid')(d_T)\n",
    "\n",
    "# Build and compile the model\n",
    "model_T = Model(inputs=dataInp, outputs=out_T, name='teacher_rnn')\n",
    "model_T.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Display the model summary\n",
    "model_T.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "57/57 [==============================] - 24s 337ms/step - loss: 1.4005 - accuracy: 0.5483 - val_loss: 1.2493 - val_accuracy: 0.5400\n",
      "Epoch 2/2000\n",
      "57/57 [==============================] - 21s 375ms/step - loss: 0.7448 - accuracy: 0.7589 - val_loss: 0.9706 - val_accuracy: 0.6633\n",
      "Epoch 3/2000\n",
      "57/57 [==============================] - 22s 380ms/step - loss: 0.5569 - accuracy: 0.8039 - val_loss: 0.6273 - val_accuracy: 0.7567\n",
      "Epoch 4/2000\n",
      "57/57 [==============================] - 23s 397ms/step - loss: 0.4000 - accuracy: 0.8756 - val_loss: 0.4166 - val_accuracy: 0.8633\n",
      "Epoch 5/2000\n",
      "57/57 [==============================] - 24s 415ms/step - loss: 0.3113 - accuracy: 0.8922 - val_loss: 0.4411 - val_accuracy: 0.8200\n",
      "Epoch 6/2000\n",
      "57/57 [==============================] - 25s 440ms/step - loss: 0.2368 - accuracy: 0.9178 - val_loss: 0.4273 - val_accuracy: 0.8533\n",
      "Epoch 7/2000\n",
      "57/57 [==============================] - 25s 437ms/step - loss: 0.1883 - accuracy: 0.9311 - val_loss: 0.2579 - val_accuracy: 0.9100\n",
      "Epoch 8/2000\n",
      "57/57 [==============================] - 26s 450ms/step - loss: 0.2377 - accuracy: 0.9161 - val_loss: 0.4064 - val_accuracy: 0.8967\n",
      "Epoch 9/2000\n",
      "57/57 [==============================] - 27s 470ms/step - loss: 0.1234 - accuracy: 0.9561 - val_loss: 0.3492 - val_accuracy: 0.8900\n",
      "Epoch 10/2000\n",
      "57/57 [==============================] - 26s 461ms/step - loss: 0.0929 - accuracy: 0.9628 - val_loss: 0.2775 - val_accuracy: 0.9033\n",
      "Epoch 11/2000\n",
      "57/57 [==============================] - 27s 471ms/step - loss: 0.0881 - accuracy: 0.9689 - val_loss: 0.3691 - val_accuracy: 0.9067\n",
      "Epoch 12/2000\n",
      "57/57 [==============================] - 26s 463ms/step - loss: 0.0668 - accuracy: 0.9761 - val_loss: 0.4302 - val_accuracy: 0.8900\n",
      "Epoch 13/2000\n",
      "57/57 [==============================] - 28s 492ms/step - loss: 0.0602 - accuracy: 0.9783 - val_loss: 0.3395 - val_accuracy: 0.9067\n",
      "Epoch 14/2000\n",
      "57/57 [==============================] - 29s 506ms/step - loss: 0.0496 - accuracy: 0.9817 - val_loss: 0.2940 - val_accuracy: 0.9167\n",
      "Epoch 15/2000\n",
      "57/57 [==============================] - 27s 470ms/step - loss: 0.0697 - accuracy: 0.9722 - val_loss: 0.4099 - val_accuracy: 0.9033\n",
      "Epoch 16/2000\n",
      "57/57 [==============================] - 31s 544ms/step - loss: 0.0973 - accuracy: 0.9667 - val_loss: 0.4961 - val_accuracy: 0.8833\n",
      "Epoch 17/2000\n",
      "57/57 [==============================] - 40s 706ms/step - loss: 0.1208 - accuracy: 0.9594 - val_loss: 0.4640 - val_accuracy: 0.9033\n",
      "Epoch 18/2000\n",
      "57/57 [==============================] - 37s 639ms/step - loss: 0.1104 - accuracy: 0.9600 - val_loss: 0.5326 - val_accuracy: 0.9000\n",
      "Epoch 19/2000\n",
      "57/57 [==============================] - 38s 659ms/step - loss: 0.0856 - accuracy: 0.9694 - val_loss: 0.4660 - val_accuracy: 0.9100\n",
      "Epoch 20/2000\n",
      "57/57 [==============================] - 39s 678ms/step - loss: 0.1112 - accuracy: 0.9706 - val_loss: 0.4121 - val_accuracy: 0.9033\n",
      "Epoch 21/2000\n",
      "57/57 [==============================] - 35s 619ms/step - loss: 0.1025 - accuracy: 0.9656 - val_loss: 0.3820 - val_accuracy: 0.9100\n",
      "Epoch 22/2000\n",
      "57/57 [==============================] - 33s 582ms/step - loss: 0.0901 - accuracy: 0.9728 - val_loss: 0.5774 - val_accuracy: 0.8933\n",
      "Epoch 23/2000\n",
      "57/57 [==============================] - 37s 646ms/step - loss: 0.0664 - accuracy: 0.9761 - val_loss: 0.4737 - val_accuracy: 0.9033\n",
      "Epoch 24/2000\n",
      "57/57 [==============================] - 31s 543ms/step - loss: 0.0637 - accuracy: 0.9789 - val_loss: 0.5371 - val_accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model_T.fit(\n",
    "    np.array(x_train), np.array(y_train),\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=2000,\n",
    "    batch_size=32,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 2s 79ms/step - loss: 0.4973 - accuracy: 0.8756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49728235602378845, 0.8755555748939514]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_T.evaluate(np.array(x_test), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
